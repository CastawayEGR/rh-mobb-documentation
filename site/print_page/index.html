
<!doctype html>
<html lang="en" class="no-js">
  <head>
    
      <meta charset="utf-8">
      <meta name="viewport" content="width=device-width,initial-scale=1">
      
        <meta name="description" content="Documentation from the MOBB/MOST Team">
      
      
        <meta name="author" content="The MOBB Team">
      
      
      <link rel="icon" href="../assets/images/favicon.png">
      <meta name="generator" content="docums-, docurial-8.1.8">
    
    
      
        <title>Print Site - MOBB Ninja</title>
      
    
    
      <link rel="stylesheet" href="../assets/stylesheets/main.6e60f8b8.min.css">
      
        
        <link rel="stylesheet" href="../assets/stylesheets/palette.e6a45f82.min.css">
        
      
    
    
    
      
        
        <link rel="preconnect" href="https://fonts.gstatic.com" crossorigin>
        <link rel="stylesheet" href="https://fonts.googleapis.com/css?family=Roboto:300,400,400i,700%7CRoboto+Mono&display=fallback">
        <style>:root{--md-text-font:"Roboto";--md-code-font:"Roboto Mono"}</style>
      
    
    
      <link rel="stylesheet" href="../css/print-site-enum-headings1.css">
    
      <link rel="stylesheet" href="../css/print-site-enum-headings2.css">
    
      <link rel="stylesheet" href="../css/print-site-enum-headings3.css">
    
      <link rel="stylesheet" href="../css/print-site-enum-headings4.css">
    
      <link rel="stylesheet" href="../css/print-site-enum-headings5.css">
    
      <link rel="stylesheet" href="../css/print-site-enum-headings6.css">
    
      <link rel="stylesheet" href="../css/print-site.css">
    
      <link rel="stylesheet" href="../css/print-site-material.css">
    
    <script>__md_scope=new URL("/",location),__md_get=(e,_=localStorage,t=__md_scope)=>JSON.parse(_.getItem(t.pathname+"."+e)),__md_set=(e,_,t=localStorage,a=__md_scope)=>{try{t.setItem(a.pathname+"."+e,JSON.stringify(_))}catch(e){}}</script>
    
      

    
    
  
        <script type="text/javascript">
        document.addEventListener('DOMContentLoaded', function () {
            remove_material_navigation();remove_mkdocs_theme_navigation();generate_toc();
        })
        </script>
        </head>
  
  
    
    
      
    
    
    
    
    <body dir="ltr" data-md-color-scheme="default" data-md-color-primary="" data-md-color-accent="">
  
    
    
      <script>var palette=__md_get("__palette");if(palette&&"object"==typeof palette.color)for(var key of Object.keys(palette.color))document.body.setAttribute("data-md-color-"+key,palette.color[key])</script>
    
    <input class="md-toggle" data-md-toggle="drawer" type="checkbox" id="__drawer" autocomplete="off">
    <input class="md-toggle" data-md-toggle="search" type="checkbox" id="__search" autocomplete="off">
    <label class="md-overlay" for="__drawer"></label>
    <div data-md-component="skip">
      
        
        <a href="#" class="md-skip">
          Skip to content
        </a>
      
    </div>
    <div data-md-component="announce">
      
    </div>
    
    
      

  

<header class="md-header md-header--lifted" data-md-component="header">
  <nav class="md-header__inner md-grid" aria-label="Header">
    <a href=".." title="MOBB Ninja" class="md-header__button md-logo" aria-label="MOBB Ninja" data-md-component="logo">
      
  <img src="../assets/avatar.png" alt="logo">

    </a>
    <label class="md-header__button md-icon" for="__drawer">
      <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24"><path d="M3 6h18v2H3V6m0 5h18v2H3v-2m0 5h18v2H3v-2z"/></svg>
    </label>
    <div class="md-header__title" data-md-component="header-title">
      <div class="md-header__ellipsis">
        <div class="md-header__topic">
          <span class="md-ellipsis">
            MOBB Ninja
          </span>
        </div>
        <div class="md-header__topic" data-md-component="header-topic">
          <span class="md-ellipsis">
            
              Print Site
            
          </span>
        </div>
      </div>
    </div>
    
      <form class="md-header__option" data-md-component="palette">
        
          
          
          <input class="md-option" data-md-color-media="" data-md-color-scheme="default" data-md-color-primary="" data-md-color-accent=""  aria-label="Switch to Dark Mode"  type="radio" name="__palette" id="__palette_1">
          
            <label class="md-header__button md-icon" title="Switch to Dark Mode" for="__palette_2" hidden>
              <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24"><path d="M12 8a4 4 0 0 0-4 4 4 4 0 0 0 4 4 4 4 0 0 0 4-4 4 4 0 0 0-4-4m0 10a6 6 0 0 1-6-6 6 6 0 0 1 6-6 6 6 0 0 1 6 6 6 6 0 0 1-6 6m8-9.31V4h-4.69L12 .69 8.69 4H4v4.69L.69 12 4 15.31V20h4.69L12 23.31 15.31 20H20v-4.69L23.31 12 20 8.69z"/></svg>
            </label>
          
        
          
          
          <input class="md-option" data-md-color-media="" data-md-color-scheme="slate" data-md-color-primary="" data-md-color-accent=""  aria-label="Switch to Light Mode"  type="radio" name="__palette" id="__palette_2">
          
            <label class="md-header__button md-icon" title="Switch to Light Mode" for="__palette_1" hidden>
              <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24"><path d="M12 18c-.89 0-1.74-.2-2.5-.55C11.56 16.5 13 14.42 13 12c0-2.42-1.44-4.5-3.5-5.45C10.26 6.2 11.11 6 12 6a6 6 0 0 1 6 6 6 6 0 0 1-6 6m8-9.31V4h-4.69L12 .69 8.69 4H4v4.69L.69 12 4 15.31V20h4.69L12 23.31 15.31 20H20v-4.69L23.31 12 20 8.69z"/></svg>
            </label>
          
        
      </form>
    
    
    
      <label class="md-header__button md-icon" for="__search">
        <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24"><path d="M9.5 3A6.5 6.5 0 0 1 16 9.5c0 1.61-.59 3.09-1.56 4.23l.27.27h.79l5 5-1.5 1.5-5-5v-.79l-.27-.27A6.516 6.516 0 0 1 9.5 16 6.5 6.5 0 0 1 3 9.5 6.5 6.5 0 0 1 9.5 3m0 2C7 5 5 7 5 9.5S7 14 9.5 14 14 12 14 9.5 12 5 9.5 5z"/></svg>
      </label>
      <div class="md-search" data-md-component="search" role="dialog">
  <label class="md-search__overlay" for="__search"></label>
  <div class="md-search__inner" role="search">
    <form class="md-search__form" name="search">
      <input type="text" class="md-search__input" name="query" aria-label="Search" placeholder="Search" autocapitalize="off" autocorrect="off" autocomplete="off" spellcheck="false" data-md-component="search-query" required>
      <label class="md-search__icon md-icon" for="__search">
        <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24"><path d="M9.5 3A6.5 6.5 0 0 1 16 9.5c0 1.61-.59 3.09-1.56 4.23l.27.27h.79l5 5-1.5 1.5-5-5v-.79l-.27-.27A6.516 6.516 0 0 1 9.5 16 6.5 6.5 0 0 1 3 9.5 6.5 6.5 0 0 1 9.5 3m0 2C7 5 5 7 5 9.5S7 14 9.5 14 14 12 14 9.5 12 5 9.5 5z"/></svg>
        <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24"><path d="M20 11v2H8l5.5 5.5-1.42 1.42L4.16 12l7.92-7.92L13.5 5.5 8 11h12z"/></svg>
      </label>
      <nav class="md-search__options" aria-label="Search">
        
        <button type="reset" class="md-search__icon md-icon" aria-label="Clear" tabindex="-1">
          <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24"><path d="M19 6.41 17.59 5 12 10.59 6.41 5 5 6.41 10.59 12 5 17.59 6.41 19 12 13.41 17.59 19 19 17.59 13.41 12 19 6.41z"/></svg>
        </button>
      </nav>
      
    </form>
    <div class="md-search__output">
      <div class="md-search__scrollwrap" data-md-scrollfix>
        <div class="md-search-result" data-md-component="search-result">
          <div class="md-search-result__meta">
            Initializing search
          </div>
          <ol class="md-search-result__list"></ol>
        </div>
      </div>
    </div>
  </div>
</div>
    
    
      <div class="md-header__source">
        <a href="https://github.com/rh-mobb/documentation" title="Go to repository" class="md-source" data-md-component="source">
  <div class="md-source__icon md-icon">
    
    <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 448 512"><path d="M439.55 236.05 244 40.45a28.87 28.87 0 0 0-40.81 0l-40.66 40.63 51.52 51.52c27.06-9.14 52.68 16.77 43.39 43.68l49.66 49.66c34.23-11.8 61.18 31 35.47 56.69-26.49 26.49-70.21-2.87-56-37.34L240.22 199v121.85c25.3 12.54 22.26 41.85 9.08 55a34.34 34.34 0 0 1-48.55 0c-17.57-17.6-11.07-46.91 11.25-56v-123c-20.8-8.51-24.6-30.74-18.64-45L142.57 101 8.45 235.14a28.86 28.86 0 0 0 0 40.81l195.61 195.6a28.86 28.86 0 0 0 40.8 0l194.69-194.69a28.86 28.86 0 0 0 0-40.81z"/></svg>
  </div>
  <div class="md-source__repository">
    GitHub
  </div>
</a>
      </div>
    
  </nav>
  
    
      
<nav class="md-tabs" aria-label="Tabs" data-md-component="tabs">
  <div class="md-tabs__inner md-grid">
    <ul class="md-tabs__list">
      
        
  
  


  <li class="md-tabs__item">
    <a href=".." class="md-tabs__link">
      Home
    </a>
  </li>

      
        
  
  


  
  
  
    

  
  
  
    <li class="md-tabs__item">
      <a href="../aro/private-cluster/" class="md-tabs__link">
        Products
      </a>
    </li>
  

  

      
        
  
  


  
  
  
    <li class="md-tabs__item">
      <a href="../quickstart-aro/" class="md-tabs__link">
        Quickstarts
      </a>
    </li>
  

      
        
  
  


  
  
  
    <li class="md-tabs__item">
      <a href="../acm/observability/rosa/" class="md-tabs__link">
        Advanced Cluster Manager (ACM)
      </a>
    </li>
  

      
        
  
  


  
  
  
    

  
  
  
    <li class="md-tabs__item">
      <a href="../rosa/custom-alertmanager-4.9/" class="md-tabs__link">
        Observability
      </a>
    </li>
  

  

      
        
  
  


  
  
  
    

  
  
  
    <li class="md-tabs__item">
      <a href="../security/secrets-store-csi/hashicorp-vault/" class="md-tabs__link">
        Security
      </a>
    </li>
  

  

      
        
  
  


  <li class="md-tabs__item">
    <a href="../blog/" class="md-tabs__link">
      Blogs
    </a>
  </li>

      
    </ul>
  </div>
</nav>
    
  
</header>
    
    <div class="md-container" data-md-component="container">
      
      
        
      
      <main class="md-main" data-md-component="main">
        <div class="md-main__inner md-grid">
          
            
              
              <div class="md-sidebar md-sidebar--primary" data-md-component="sidebar" data-md-type="navigation" >
                <div class="md-sidebar__scrollwrap">
                  <div class="md-sidebar__inner">
                    

  


<nav class="md-nav md-nav--primary md-nav--lifted" aria-label="Navigation" data-md-level="0">
  <label class="md-nav__title" for="__drawer">
    <a href=".." title="MOBB Ninja" class="md-nav__button md-logo" aria-label="MOBB Ninja" data-md-component="logo">
      
  <img src="../assets/avatar.png" alt="logo">

    </a>
    MOBB Ninja
  </label>
  
    <div class="md-nav__source">
      <a href="https://github.com/rh-mobb/documentation" title="Go to repository" class="md-source" data-md-component="source">
  <div class="md-source__icon md-icon">
    
    <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 448 512"><path d="M439.55 236.05 244 40.45a28.87 28.87 0 0 0-40.81 0l-40.66 40.63 51.52 51.52c27.06-9.14 52.68 16.77 43.39 43.68l49.66 49.66c34.23-11.8 61.18 31 35.47 56.69-26.49 26.49-70.21-2.87-56-37.34L240.22 199v121.85c25.3 12.54 22.26 41.85 9.08 55a34.34 34.34 0 0 1-48.55 0c-17.57-17.6-11.07-46.91 11.25-56v-123c-20.8-8.51-24.6-30.74-18.64-45L142.57 101 8.45 235.14a28.86 28.86 0 0 0 0 40.81l195.61 195.6a28.86 28.86 0 0 0 40.8 0l194.69-194.69a28.86 28.86 0 0 0 0-40.81z"/></svg>
  </div>
  <div class="md-source__repository">
    GitHub
  </div>
</a>
    </div>
  
  <ul class="md-nav__list" data-md-scrollfix>
    
      
      
      

  
  
  
    <li class="md-nav__item">
      <a href=".." class="md-nav__link">
        Home
      </a>
    </li>
  

    
      
      
      

  
  
  
    
    <li class="md-nav__item md-nav__item--nested">
      
      
        <input class="md-nav__toggle md-toggle" data-md-toggle="__nav_2" type="checkbox" id="__nav_2" >
      
      
      
      
        <label class="md-nav__link" for="__nav_2">
          Products
          <span class="md-nav__icon md-icon"></span>
        </label>
      
      <nav class="md-nav" aria-label="Products" data-md-level="1">
        <label class="md-nav__title" for="__nav_2">
          <span class="md-nav__icon md-icon"></span>
          Products
        </label>
        <ul class="md-nav__list" data-md-scrollfix>
          
            
              
  
  
  
    
    <li class="md-nav__item md-nav__item--nested">
      
      
        <input class="md-nav__toggle md-toggle" data-md-toggle="__nav_2_1" type="checkbox" id="__nav_2_1" >
      
      
      
      
        <label class="md-nav__link" for="__nav_2_1">
          ARO
          <span class="md-nav__icon md-icon"></span>
        </label>
      
      <nav class="md-nav" aria-label="ARO" data-md-level="2">
        <label class="md-nav__title" for="__nav_2_1">
          <span class="md-nav__icon md-icon"></span>
          ARO
        </label>
        <ul class="md-nav__list" data-md-scrollfix>
          
            
              
  
  
  
    <li class="md-nav__item">
      <a href="../aro/private-cluster/" class="md-nav__link">
        Private Cluster
      </a>
    </li>
  

            
          
            
              
  
  
  
    <li class="md-nav__item">
      <a href="../aro/ocm/" class="md-nav__link">
        Openshift Cluster Manager (OCM)
      </a>
    </li>
  

            
          
            
              
  
  
  
    <li class="md-nav__item">
      <a href="../aro/gpu/" class="md-nav__link">
        ARO for GPU Workloads
      </a>
    </li>
  

            
          
            
              
  
  
  
    <li class="md-nav__item">
      <a href="../aro/frontdoor/" class="md-nav__link">
        Azure Frontdoor
      </a>
    </li>
  

            
          
            
              
  
  
  
    
    <li class="md-nav__item md-nav__item--nested">
      
      
        <input class="md-nav__toggle md-toggle" data-md-toggle="__nav_2_1_5" type="checkbox" id="__nav_2_1_5" >
      
      
      
      
        <label class="md-nav__link" for="__nav_2_1_5">
          Azure Service Operator
          <span class="md-nav__icon md-icon"></span>
        </label>
      
      <nav class="md-nav" aria-label="Azure Service Operator" data-md-level="3">
        <label class="md-nav__title" for="__nav_2_1_5">
          <span class="md-nav__icon md-icon"></span>
          Azure Service Operator
        </label>
        <ul class="md-nav__list" data-md-scrollfix>
          
            
              
  
  
  
    <li class="md-nav__item">
      <a href="../aro/azure-service-operator-v1/" class="md-nav__link">
        v1
      </a>
    </li>
  

            
          
            
              
  
  
  
    <li class="md-nav__item">
      <a href="../aro/azure-service-operator-v2/" class="md-nav__link">
        v2
      </a>
    </li>
  

            
          
        </ul>
      </nav>
    </li>
  

            
          
        </ul>
      </nav>
    </li>
  

            
          
            
              
  
  
  
    
    <li class="md-nav__item md-nav__item--nested">
      
      
        <input class="md-nav__toggle md-toggle" data-md-toggle="__nav_2_2" type="checkbox" id="__nav_2_2" >
      
      
      
      
        <label class="md-nav__link" for="__nav_2_2">
          ROSA
          <span class="md-nav__icon md-icon"></span>
        </label>
      
      <nav class="md-nav" aria-label="ROSA" data-md-level="2">
        <label class="md-nav__title" for="__nav_2_2">
          <span class="md-nav__icon md-icon"></span>
          ROSA
        </label>
        <ul class="md-nav__list" data-md-scrollfix>
          
            
              
  
  
  
    <li class="md-nav__item">
      <a href="../rosa/private-link/" class="md-nav__link">
        Private Link
      </a>
    </li>
  

            
          
            
              
  
  
  
    
    <li class="md-nav__item md-nav__item--nested">
      
      
        <input class="md-nav__toggle md-toggle" data-md-toggle="__nav_2_2_2" type="checkbox" id="__nav_2_2_2" >
      
      
      
      
        <label class="md-nav__link" for="__nav_2_2_2">
          Secure Token Service (STS)
          <span class="md-nav__icon md-icon"></span>
        </label>
      
      <nav class="md-nav" aria-label="Secure Token Service (STS)" data-md-level="3">
        <label class="md-nav__title" for="__nav_2_2_2">
          <span class="md-nav__icon md-icon"></span>
          Secure Token Service (STS)
        </label>
        <ul class="md-nav__list" data-md-scrollfix>
          
            
              
  
  
  
    <li class="md-nav__item">
      <a href="../rosa/sts/" class="md-nav__link">
        Create ROSA Cluster w/STS
      </a>
    </li>
  

            
          
        </ul>
      </nav>
    </li>
  

            
          
        </ul>
      </nav>
    </li>
  

            
          
            
              
  
  
  
    
    <li class="md-nav__item md-nav__item--nested">
      
      
        <input class="md-nav__toggle md-toggle" data-md-toggle="__nav_2_3" type="checkbox" id="__nav_2_3" >
      
      
      
      
        <label class="md-nav__link" for="__nav_2_3">
          GCP
          <span class="md-nav__icon md-icon"></span>
        </label>
      
      <nav class="md-nav" aria-label="GCP" data-md-level="2">
        <label class="md-nav__title" for="__nav_2_3">
          <span class="md-nav__icon md-icon"></span>
          GCP
        </label>
        <ul class="md-nav__list" data-md-scrollfix>
          
            
              
  
  
  
    <li class="md-nav__item">
      <a href="../gcp/osd_preexisting_vpc/" class="md-nav__link">
        Deploy OSD in GCP w/ BYO VPC
      </a>
    </li>
  

            
          
            
              
  
  
  
    <li class="md-nav__item">
      <a href="../gcp/filestore/" class="md-nav__link">
        Using Filestore w/ OSD
      </a>
    </li>
  

            
          
        </ul>
      </nav>
    </li>
  

            
          
        </ul>
      </nav>
    </li>
  

    
      
      
      

  
  
  
    
    <li class="md-nav__item md-nav__item--nested">
      
      
        <input class="md-nav__toggle md-toggle" data-md-toggle="__nav_3" type="checkbox" id="__nav_3" >
      
      
      
      
        <label class="md-nav__link" for="__nav_3">
          Quickstarts
          <span class="md-nav__icon md-icon"></span>
        </label>
      
      <nav class="md-nav" aria-label="Quickstarts" data-md-level="1">
        <label class="md-nav__title" for="__nav_3">
          <span class="md-nav__icon md-icon"></span>
          Quickstarts
        </label>
        <ul class="md-nav__list" data-md-scrollfix>
          
            
              
  
  
  
    <li class="md-nav__item">
      <a href="../quickstart-aro/" class="md-nav__link">
        ARO Quickstart
      </a>
    </li>
  

            
          
            
              
  
  
  
    <li class="md-nav__item">
      <a href="../quickstart-rosa/" class="md-nav__link">
        ROSA Quickstart
      </a>
    </li>
  

            
          
        </ul>
      </nav>
    </li>
  

    
      
      
      

  
  
  
    
    <li class="md-nav__item md-nav__item--nested">
      
      
        <input class="md-nav__toggle md-toggle" data-md-toggle="__nav_4" type="checkbox" id="__nav_4" >
      
      
      
      
        <label class="md-nav__link" for="__nav_4">
          Advanced Cluster Manager (ACM)
          <span class="md-nav__icon md-icon"></span>
        </label>
      
      <nav class="md-nav" aria-label="Advanced Cluster Manager (ACM)" data-md-level="1">
        <label class="md-nav__title" for="__nav_4">
          <span class="md-nav__icon md-icon"></span>
          Advanced Cluster Manager (ACM)
        </label>
        <ul class="md-nav__list" data-md-scrollfix>
          
            
              
  
  
  
    <li class="md-nav__item">
      <a href="../acm/observability/rosa/" class="md-nav__link">
        Deploy ACM Observability to a ROSA Cluster
      </a>
    </li>
  

            
          
        </ul>
      </nav>
    </li>
  

    
      
      
      

  
  
  
    
    <li class="md-nav__item md-nav__item--nested">
      
      
        <input class="md-nav__toggle md-toggle" data-md-toggle="__nav_5" type="checkbox" id="__nav_5" >
      
      
      
      
        <label class="md-nav__link" for="__nav_5">
          Observability
          <span class="md-nav__icon md-icon"></span>
        </label>
      
      <nav class="md-nav" aria-label="Observability" data-md-level="1">
        <label class="md-nav__title" for="__nav_5">
          <span class="md-nav__icon md-icon"></span>
          Observability
        </label>
        <ul class="md-nav__list" data-md-scrollfix>
          
            
              
  
  
  
    
    <li class="md-nav__item md-nav__item--nested">
      
      
        <input class="md-nav__toggle md-toggle" data-md-toggle="__nav_5_1" type="checkbox" id="__nav_5_1" >
      
      
      
      
        <label class="md-nav__link" for="__nav_5_1">
          Configuring Alerts for User Workloads
          <span class="md-nav__icon md-icon"></span>
        </label>
      
      <nav class="md-nav" aria-label="Configuring Alerts for User Workloads" data-md-level="2">
        <label class="md-nav__title" for="__nav_5_1">
          <span class="md-nav__icon md-icon"></span>
          Configuring Alerts for User Workloads
        </label>
        <ul class="md-nav__list" data-md-scrollfix>
          
            
              
  
  
  
    <li class="md-nav__item">
      <a href="../rosa/custom-alertmanager-4.9/" class="md-nav__link">
        ROSA 4.9.X
      </a>
    </li>
  

            
          
            
              
  
  
  
    <li class="md-nav__item">
      <a href="../rosa/custom-alertmanager/" class="md-nav__link">
        ROSA 4.11+
      </a>
    </li>
  

            
          
        </ul>
      </nav>
    </li>
  

            
          
            
              
  
  
  
    <li class="md-nav__item">
      <a href="../rosa/federated-metrics/" class="md-nav__link">
        Federating ROSA Metrics to S3
      </a>
    </li>
  

            
          
        </ul>
      </nav>
    </li>
  

    
      
      
      

  
  
  
    
    <li class="md-nav__item md-nav__item--nested">
      
      
        <input class="md-nav__toggle md-toggle" data-md-toggle="__nav_6" type="checkbox" id="__nav_6" >
      
      
      
      
        <label class="md-nav__link" for="__nav_6">
          Security
          <span class="md-nav__icon md-icon"></span>
        </label>
      
      <nav class="md-nav" aria-label="Security" data-md-level="1">
        <label class="md-nav__title" for="__nav_6">
          <span class="md-nav__icon md-icon"></span>
          Security
        </label>
        <ul class="md-nav__list" data-md-scrollfix>
          
            
              
  
  
  
    
    <li class="md-nav__item md-nav__item--nested">
      
      
        <input class="md-nav__toggle md-toggle" data-md-toggle="__nav_6_1" type="checkbox" id="__nav_6_1" >
      
      
      
      
        <label class="md-nav__link" for="__nav_6_1">
          K8s Secret Store CSI Driver
          <span class="md-nav__icon md-icon"></span>
        </label>
      
      <nav class="md-nav" aria-label="K8s Secret Store CSI Driver" data-md-level="2">
        <label class="md-nav__title" for="__nav_6_1">
          <span class="md-nav__icon md-icon"></span>
          K8s Secret Store CSI Driver
        </label>
        <ul class="md-nav__list" data-md-scrollfix>
          
            
              
  
  
  
    <li class="md-nav__item">
      <a href="../security/secrets-store-csi/hashicorp-vault/" class="md-nav__link">
        HashiCorp CSI
      </a>
    </li>
  

            
          
            
              
  
  
  
    <li class="md-nav__item">
      <a href="../rosa/aws-secrets-manager-csi/" class="md-nav__link">
        AWS Secrets CSI w/ ROSA STS
      </a>
    </li>
  

            
          
            
              
  
  
  
    <li class="md-nav__item">
      <a href="../security/secrets-store-csi/azure-key-vault/" class="md-nav__link">
        Azure Key Vault CSI Driver
      </a>
    </li>
  

            
          
        </ul>
      </nav>
    </li>
  

            
          
            
              
  
  
  
    
    <li class="md-nav__item md-nav__item--nested">
      
      
        <input class="md-nav__toggle md-toggle" data-md-toggle="__nav_6_2" type="checkbox" id="__nav_6_2" >
      
      
      
      
        <label class="md-nav__link" for="__nav_6_2">
          Configuring IDPs
          <span class="md-nav__icon md-icon"></span>
        </label>
      
      <nav class="md-nav" aria-label="Configuring IDPs" data-md-level="2">
        <label class="md-nav__title" for="__nav_6_2">
          <span class="md-nav__icon md-icon"></span>
          Configuring IDPs
        </label>
        <ul class="md-nav__list" data-md-scrollfix>
          
            
              
  
  
  
    
    <li class="md-nav__item md-nav__item--nested">
      
      
        <input class="md-nav__toggle md-toggle" data-md-toggle="__nav_6_2_1" type="checkbox" id="__nav_6_2_1" >
      
      
      
      
        <label class="md-nav__link" for="__nav_6_2_1">
          Azure AD
          <span class="md-nav__icon md-icon"></span>
        </label>
      
      <nav class="md-nav" aria-label="Azure AD" data-md-level="3">
        <label class="md-nav__title" for="__nav_6_2_1">
          <span class="md-nav__icon md-icon"></span>
          Azure AD
        </label>
        <ul class="md-nav__list" data-md-scrollfix>
          
            
              
  
  
  
    <li class="md-nav__item">
      <a href="../idp/azuread-aro/" class="md-nav__link">
        Azure AD for ARO
      </a>
    </li>
  

            
          
            
              
  
  
  
    <li class="md-nav__item">
      <a href="../idp/group-claims/aro/" class="md-nav__link">
        Azure AD for ARO w/group claims
      </a>
    </li>
  

            
          
            
              
  
  
  
    <li class="md-nav__item">
      <a href="../idp/group-claims/rosa/" class="md-nav__link">
        Azure AD for ROSA w/group claims
      </a>
    </li>
  

            
          
            
              
  
  
  
    <li class="md-nav__item">
      <a href="../idp/azuread/" class="md-nav__link">
        Azure AD for ROSA/OSD
      </a>
    </li>
  

            
          
            
              
  
  
  
    <li class="md-nav__item">
      <a href="../idp/azuread-aro-cli/" class="md-nav__link">
        Azure AD for ARO via CLI
      </a>
    </li>
  

            
          
        </ul>
      </nav>
    </li>
  

            
          
            
              
  
  
  
    
    <li class="md-nav__item md-nav__item--nested">
      
      
        <input class="md-nav__toggle md-toggle" data-md-toggle="__nav_6_2_2" type="checkbox" id="__nav_6_2_2" >
      
      
      
      
        <label class="md-nav__link" for="__nav_6_2_2">
          Gitlab
          <span class="md-nav__icon md-icon"></span>
        </label>
      
      <nav class="md-nav" aria-label="Gitlab" data-md-level="3">
        <label class="md-nav__title" for="__nav_6_2_2">
          <span class="md-nav__icon md-icon"></span>
          Gitlab
        </label>
        <ul class="md-nav__list" data-md-scrollfix>
          
            
              
  
  
  
    <li class="md-nav__item">
      <a href="../idp/gitlab/" class="md-nav__link">
        ROSA/OSD
      </a>
    </li>
  

            
          
            
              
  
  
  
    <li class="md-nav__item">
      <a href="../idp/gitlab-aro/" class="md-nav__link">
        ARO
      </a>
    </li>
  

            
          
        </ul>
      </nav>
    </li>
  

            
          
        </ul>
      </nav>
    </li>
  

            
          
            
              
  
  
  
    
    <li class="md-nav__item md-nav__item--nested">
      
      
        <input class="md-nav__toggle md-toggle" data-md-toggle="__nav_6_3" type="checkbox" id="__nav_6_3" >
      
      
      
      
        <label class="md-nav__link" for="__nav_6_3">
          Advanced Cluster Security
          <span class="md-nav__icon md-icon"></span>
        </label>
      
      <nav class="md-nav" aria-label="Advanced Cluster Security" data-md-level="2">
        <label class="md-nav__title" for="__nav_6_3">
          <span class="md-nav__icon md-icon"></span>
          Advanced Cluster Security
        </label>
        <ul class="md-nav__list" data-md-scrollfix>
          
            
              
  
  
  
    <li class="md-nav__item">
      <a href="../security/rhacs/" class="md-nav__link">
        Deploying ACS in ROSA/ARO
      </a>
    </li>
  

            
          
        </ul>
      </nav>
    </li>
  

            
          
        </ul>
      </nav>
    </li>
  

    
      
      
      

  
  
  
    <li class="md-nav__item">
      <a href="../blog/" class="md-nav__link">
        Blogs
      </a>
    </li>
  

    
  </ul>
</nav>
                  </div>
                </div>
              </div>
            
            
              
              <div class="md-sidebar md-sidebar--secondary" data-md-component="sidebar" data-md-type="toc" >
                <div class="md-sidebar__scrollwrap">
                  <div class="md-sidebar__inner">
                    

<nav class="md-nav md-nav--secondary" aria-label="Table of contents">
  
  
  
  
    <label class="md-nav__title" for="__toc">
      <span class="md-nav__icon md-icon"></span>
      Table of contents
    </label>
    <ul class="md-nav__list" data-md-component="toc" data-md-scrollfix>
      
        <li class="md-nav__item">
  <a href="#" class="md-nav__link">
    1. Home
  </a>
  
</li>
      
        <li class="md-nav__item">
  <a href="#section-products" class="md-nav__link">
    I. Products
  </a>
  
</li>
      
        <li class="md-nav__item">
  <a href="#section-quickstarts" class="md-nav__link">
    II. Quickstarts
  </a>
  
    <nav class="md-nav" aria-label="II. Quickstarts">
      <ul class="md-nav__list">
        
          <li class="md-nav__item">
  <a href="#quickstart-aro" class="md-nav__link">
    2. ARO Quickstart
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#quickstart-rosa" class="md-nav__link">
    3. ROSA Quickstart
  </a>
  
</li>
        
      </ul>
    </nav>
  
</li>
      
        <li class="md-nav__item">
  <a href="#section-advanced-cluster-manager-acm-" class="md-nav__link">
    III. Advanced Cluster Manager (ACM)
  </a>
  
    <nav class="md-nav" aria-label="III. Advanced Cluster Manager (ACM)">
      <ul class="md-nav__list">
        
          <li class="md-nav__item">
  <a href="#acm-observability-rosa" class="md-nav__link">
    4. Deploy ACM Observability to a ROSA Cluster
  </a>
  
</li>
        
      </ul>
    </nav>
  
</li>
      
        <li class="md-nav__item">
  <a href="#section-observability" class="md-nav__link">
    IV. Observability
  </a>
  
    <nav class="md-nav" aria-label="IV. Observability">
      <ul class="md-nav__list">
        
          <li class="md-nav__item">
  <a href="#rosa-federated-metrics" class="md-nav__link">
    5. Federating ROSA Metrics to S3
  </a>
  
</li>
        
      </ul>
    </nav>
  
</li>
      
        <li class="md-nav__item">
  <a href="#section-security" class="md-nav__link">
    V. Security
  </a>
  
</li>
      
        <li class="md-nav__item">
  <a href="#blog" class="md-nav__link">
    6. Blogs
  </a>
  
</li>
      
    </ul>
  
</nav>
                  </div>
                </div>
              </div>
            
          
          <div class="md-content" data-md-component="content">
            <article class="md-content__inner md-typeset">
              
                

<div id="print-site-page" class="print-site-enumerate-headings print-site-enumerate-figures">
        <div id="print-site-banner">
            <p>
    <em>This box will disappear when printing</em>
    <span style="float: right"><a href="https://timvink.github.io/mkdocs-print-site-plugin/">mkdocs-print-site-plugin</a></span>
</p>
<p><b>IMPORTANT NOTE</b>: This site is not official Red Hat documentation and is provided for informational purposes only. These guides may be experimental, proof of concept, or early adoption. Officially supported documentation is available at<a href="https://docs.openshift.com" target="_blank">docs.openshift.com</a> and <a href="https://access.redhat.com" target="_blank">access.redhat.com</a></p>
        </div>
        
        <section class="print-page">
            <div id="print-page-toc" data-toc-depth="3">
                <nav role='navigation' class='print-page-toc-nav'>
                <h1 class='print-page-toc-title'>Table of Contents</h1>
                </nav>
            </div>
        </section>
        <section class="print-page" id="index"><h1 id="index-welcome-to-mobb-ninja">Welcome to MOBB Ninja</h1>
<p>For full documentation visit <a href="https://www.mkdocs.org">mkdocs.org</a>.</p>
<h2 id="index-commands">Commands</h2>
<ul>
<li><code>mkdocs new [dir-name]</code> - Create a new project.</li>
<li><code>mkdocs serve</code> - Start the live-reloading docs server.</li>
<li><code>mkdocs build</code> - Build the documentation site.</li>
<li><code>mkdocs -h</code> - Print help message and exit.</li>
</ul>
<h2 id="index-project-layout">Project layout</h2>
<pre><code>mkdocs.yml    # The configuration file.
docs/
    index.md  # The documentation homepage.
    ...       # Other markdown pages, images and other files.
</code></pre></section>
                        <h1 class='nav-section-title' id='section-products'>
                            Products <a class='headerlink' href='#section-products' title='Permanent link'>↵</a>
                        </h1>
                        
                        <h2 class='nav-section-title' id='section-aro'>
                            ARO <a class='headerlink' href='#section-aro' title='Permanent link'>↵</a>
                        </h2>
                        <section class="print-page" id="aro-private-cluster"><h1 id="aro-private-cluster-aro-quickstart-private-cluster-with-jumphost">ARO Quickstart - Private Cluster with JumpHost</h1>
<p>A Quickstart guide to deploying a Private Azure Red Hat OpenShift cluster.</p>
<blockquote>
<p>Once the cluster is running you will need a way to access the private network that ARO is deployed into.</p>
</blockquote>
<p>Author: <a href="https://twitter.com/pczarkowski">Paul Czarkowski</a></p>
<h2 id="aro-private-cluster-prerequisites">Prerequisites</h2>
<h3 id="aro-private-cluster-azure-cli">Azure CLI</h3>
<p><em>Obviously you'll need to have an Azure account to configure the CLI against.</em></p>
<p><strong>MacOS</strong></p>
<blockquote>
<p>See <a href="https://docs.microsoft.com/en-us/cli/azure/install-azure-cli-macos">Azure Docs</a> for alternative install options.</p>
</blockquote>
<ol>
<li>
<p>Install Azure CLI using homebrew</p>
<p><code>bash
brew update &amp;&amp; brew install azure-cli</code></p>
</li>
</ol>
<p><strong>Linux</strong></p>
<blockquote>
<p>See <a href="https://docs.microsoft.com/en-us/cli/azure/install-azure-cli-linux?pivots=dnf">Azure Docs</a> for alternative install options.</p>
</blockquote>
<ol>
<li>
<p>Import the Microsoft Keys</p>
<p><code>bash
sudo rpm --import https://packages.microsoft.com/keys/microsoft.asc</code></p>
</li>
<li>
<p>Add the Microsoft Yum Repository</p>
<p><code>bash
cat &lt;&lt; EOF | sudo tee /etc/yum.repos.d/azure-cli.repo
[azure-cli]
name=Azure CLI
baseurl=https://packages.microsoft.com/yumrepos/azure-cli
enabled=1
gpgcheck=1
gpgkey=https://packages.microsoft.com/keys/microsoft.asc
EOF</code></p>
</li>
<li>
<p>Install Azure CLI</p>
<p><code>bash
sudo dnf install -y azure-cli</code></p>
</li>
</ol>
<h3 id="aro-private-cluster-prepare-azure-account-for-azure-openshift">Prepare Azure Account for Azure OpenShift</h3>
<ol>
<li>
<p>Log into the Azure CLI by running the following and then authorizing through your Web Browser</p>
<p><code>bash
az login</code></p>
</li>
<li>
<p>Make sure you have enough Quota (change the location if you're not using <code>East US</code>)</p>
<p><code>bash
az vm list-usage --location "East US" -o table</code></p>
<p>see <a href="#aro-private-cluster-Adding-Quota-to-ARO-account">Addendum - Adding Quota to ARO account</a> if you have less than <code>36</code> Quota left for <code>Total Regional vCPUs</code>.</p>
</li>
<li>
<p>Register resource providers</p>
<p><code>bash
az provider register -n Microsoft.RedHatOpenShift --wait
az provider register -n Microsoft.Compute --wait
az provider register -n Microsoft.Storage --wait
az provider register -n Microsoft.Authorization --wait</code></p>
</li>
</ol>
<h3 id="aro-private-cluster-get-red-hat-pull-secret">Get Red Hat pull secret</h3>
<ol>
<li>
<p>Log into cloud.redhat.com</p>
</li>
<li>
<p>Browse to <a href="https://cloud.redhat.com/openshift/install/azure/aro-provisioned">https://cloud.redhat.com/openshift/install/azure/aro-provisioned</a></p>
</li>
<li>
<p>click the <strong>Download pull secret</strong> button and remember where you saved it, you'll reference it later.</p>
</li>
</ol>
<h2 id="aro-private-cluster-deploy-azure-openshift">Deploy Azure OpenShift</h2>
<h3 id="aro-private-cluster-variables-and-resource-group">Variables and Resource Group</h3>
<p>Set some environment variables to use later, and create an Azure Resource Group.</p>
<ol>
<li>
<p>Set the following environment variables</p>
<blockquote>
<p>Change the values to suit your environment, but these defaults should work.</p>
</blockquote>
<p><code>bash
AZR_RESOURCE_LOCATION=eastus
AZR_RESOURCE_GROUP=openshift-private
AZR_CLUSTER=private-cluster
AZR_PULL_SECRET=~/Downloads/pull-secret.txt
NETWORK_SUBNET=10.0.0.0/20
CONTROL_SUBNET=10.0.0.0/24
MACHINE_SUBNET=10.0.1.0/24
FIREWALL_SUBNET=10.0.2.0/24
JUMPHOST_SUBNET=10.0.3.0/24</code></p>
</li>
<li>
<p>Create an Azure resource group</p>
<p><code>bash
az group create                \
  --name $AZR_RESOURCE_GROUP   \
  --location $AZR_RESOURCE_LOCATION</code></p>
</li>
</ol>
<h3 id="aro-private-cluster-networking">Networking</h3>
<p>Create a virtual network with two empty subnets</p>
<ol>
<li>
<p>Create virtual network</p>
<p><code>bash
az network vnet create                                    \
  --address-prefixes $NETWORK_SUBNET                      \
  --name "$AZR_CLUSTER-aro-vnet-$AZR_RESOURCE_LOCATION"   \
  --resource-group $AZR_RESOURCE_GROUP</code></p>
</li>
<li>
<p>Create control plane subnet</p>
<p><code>bash
az network vnet subnet create                                     \
  --resource-group $AZR_RESOURCE_GROUP                            \
  --vnet-name "$AZR_CLUSTER-aro-vnet-$AZR_RESOURCE_LOCATION"      \
  --name "$AZR_CLUSTER-aro-control-subnet-$AZR_RESOURCE_LOCATION" \
  --address-prefixes $CONTROL_SUBNET                              \
  --service-endpoints Microsoft.ContainerRegistry</code></p>
</li>
<li>
<p>Create machine subnet</p>
<p><code>bash
az network vnet subnet create                                       \
  --resource-group $AZR_RESOURCE_GROUP                              \
  --vnet-name "$AZR_CLUSTER-aro-vnet-$AZR_RESOURCE_LOCATION"        \
  --name "$AZR_CLUSTER-aro-machine-subnet-$AZR_RESOURCE_LOCATION"   \
  --address-prefixes $MACHINE_SUBNET                                \
  --service-endpoints Microsoft.ContainerRegistry</code></p>
</li>
<li>
<p><a href="https://docs.microsoft.com/en-us/azure/private-link/disable-private-endpoint-network-policy">Disable network policies</a> for Private Link Service on the control plane subnet</p>
<blockquote>
<p>This is required for the service to be able to connect to and manage the cluster.</p>
</blockquote>
<p><code>bash
az network vnet subnet update                                       \
  --name "$AZR_CLUSTER-aro-control-subnet-$AZR_RESOURCE_LOCATION"   \
  --resource-group $AZR_RESOURCE_GROUP                              \
  --vnet-name "$AZR_CLUSTER-aro-vnet-$AZR_RESOURCE_LOCATION"        \
  --disable-private-link-service-network-policies true</code></p>
</li>
</ol>
<h3 id="aro-private-cluster-firewall-internet-egress">Firewall + Internet Egress</h3>
<p>This replaces the routes for the cluster to go through the Firewall for egress vs the LoadBalancer which we can later remove. It does come with extra Azure costs of course.</p>
<blockquote>
<p>You can skip this step if you don't need to restrict egress.</p>
</blockquote>
<ol>
<li>
<p>Make sure you have the AZ CLI firewall extensions</p>
<p><code>bash
az extension add -n azure-firewall
az extension update -n azure-firewall</code></p>
</li>
<li>
<p>Create a firewall network, IP, and firewall</p>
<p>```bash
az network vnet subnet create                                 \
  -g $AZR_RESOURCE_GROUP                                      \
  --vnet-name "$AZR_CLUSTER-aro-vnet-$AZR_RESOURCE_LOCATION"  \
  -n "AzureFirewallSubnet"                                    \
  --address-prefixes $FIREWALL_SUBNET</p>
<p>az network public-ip create -g $AZR_RESOURCE_GROUP -n fw-ip   \
  --sku "Standard" --location $AZR_RESOURCE_LOCATION</p>
<p>az network firewall create -g $AZR_RESOURCE_GROUP             \
  -n aro-private -l $AZR_RESOURCE_LOCATION
```</p>
</li>
<li>
<p>Configure the firewall and configure IP Config (this may take 15 minutes)</p>
<p>```bash
az network firewall ip-config create -g $AZR_RESOURCE_GROUP    \
  -f aro-private -n fw-config --public-ip-address fw-ip        \
  --vnet-name "$AZR_CLUSTER-aro-vnet-$AZR_RESOURCE_LOCATION"</p>
<p>FWPUBLIC_IP=$(az network public-ip show -g $AZR_RESOURCE_GROUP -n fw-ip --query "ipAddress" -o tsv)
FWPRIVATE_IP=$(az network firewall show -g $AZR_RESOURCE_GROUP -n aro-private --query "ipConfigurations[0].privateIpAddress" -o tsv)</p>
<p>echo $FWPUBLIC_IP
echo $FWPRIVATE_IP
```</p>
</li>
<li>
<p>Create and configure a route table</p>
<p>```bash
az network route-table create -g $AZR_RESOURCE_GROUP --name aro-udr</p>
<p>sleep 10</p>
<p>az network route-table route create -g $AZR_RESOURCE_GROUP --name aro-udr \
  --route-table-name aro-udr --address-prefix 0.0.0.0/0                   \
  --next-hop-type VirtualAppliance --next-hop-ip-address $FWPRIVATE_IP</p>
<p>az network route-table route create -g $AZR_RESOURCE_GROUP --name aro-vnet   \
  --route-table-name aro-udr --address-prefix 10.0.0.0/16 --name local-route \
  --next-hop-type VirtualNetworkGateway
```</p>
</li>
<li>
<p>Create firewall rules for ARO resources</p>
<blockquote>
<p>Note: ARO clusters do not need access to the internet, however your own workloads running on them may. You can skip this step if you don't need any egress at all.</p>
</blockquote>
<ul>
<li>
<p>Create a Network Rule to allow all http/https egress traffic (not recommended)</p>
<p><code>bash
az network firewall network-rule create -g $AZR_RESOURCE_GROUP -f aro-private \
    --collection-name 'allow-https' --name allow-all                          \
    --action allow --priority 100                                             \
    --source-addresses '*' --dest-addr '*'                                    \
    --protocols 'Any' --destination-ports 1-65535</code></p>
</li>
<li>
<p>Create Application Rules to allow to a restricted set of destinations</p>
<blockquote>
<p>replace the target-fqdns with your desired destinations</p>
</blockquote>
<p>```bash
az network firewall application-rule create -g $AZR_RESOURCE_GROUP -f aro-private     \
    --collection-name 'Allow_Egress'                                                  \
    --action allow                                                                    \
    --priority 100                                                                    \
    -n 'required'                                                                     \
    --source-addresses '<em>'                                                            \
    --protocols 'http=80' 'https=443'                                                 \
    --target-fqdns '</em>.google.com' '*.bing.com'</p>
<p>az network firewall application-rule create -g $AZR_RESOURCE_GROUP -f aro-private     \
    --collection-name 'Docker'                                                        \
    --action allow                                                                    \
    --priority 200                                                                    \
    -n 'docker'                                                                       \
    --source-addresses '<em>'                                                            \
    --protocols 'http=80' 'https=443'                                                 \
    --target-fqdns '</em>cloudflare.docker.com' '*registry-1.docker.io' 'apt.dockerproject.org' 'auth.docker.io'
```</p>
</li>
</ul>
</li>
<li>
<p>Update the subnets to use the Firewall</p>
<p>Once the cluster is deployed successfully you can update the subnets to use the firewall instead of the default outbound loadbalancer rule.</p>
<p>```bash
az network vnet subnet update -g $AZR_RESOURCE_GROUP            \
--vnet-name $AZR_CLUSTER-aro-vnet-$AZR_RESOURCE_LOCATION        \
--name "$AZR_CLUSTER-aro-control-subnet-$AZR_RESOURCE_LOCATION" \
--route-table aro-udr</p>
<p>az network vnet subnet update -g $AZR_RESOURCE_GROUP            \
--vnet-name $AZR_CLUSTER-aro-vnet-$AZR_RESOURCE_LOCATION        \
--name "$AZR_CLUSTER-aro-machine-subnet-$AZR_RESOURCE_LOCATION" \
--route-table aro-udr
```</p>
</li>
<li>
<p>Create the cluster</p>
<blockquote>
<p>This will take between 30 and 45 minutes.</p>
</blockquote>
<p><code>bash
az aro create                                                            \
--resource-group $AZR_RESOURCE_GROUP                                     \
--name $AZR_CLUSTER                                                      \
--vnet "$AZR_CLUSTER-aro-vnet-$AZR_RESOURCE_LOCATION"                    \
--master-subnet "$AZR_CLUSTER-aro-control-subnet-$AZR_RESOURCE_LOCATION" \
--worker-subnet "$AZR_CLUSTER-aro-machine-subnet-$AZR_RESOURCE_LOCATION" \
--apiserver-visibility Private                                           \
--ingress-visibility Private                                             \
--pull-secret @$AZR_PULL_SECRET</code></p>
</li>
</ol>
<h3 id="aro-private-cluster-jump-host">Jump Host</h3>
<p>With the cluster in a private network, we can create a Jump host in order to connect to it. You can do this while the cluster is being created.</p>
<ol>
<li>
<p>Create jump subnet</p>
<p><code>bash
az network vnet subnet create                                \
  --resource-group $AZR_RESOURCE_GROUP                       \
  --vnet-name "$AZR_CLUSTER-aro-vnet-$AZR_RESOURCE_LOCATION" \
  --name JumpSubnet                                          \
  --address-prefixes $JUMPHOST_SUBNET                        \
  --service-endpoints Microsoft.ContainerRegistry</code></p>
</li>
<li>
<p>Create a jump host</p>
<p><code>bash
az vm create --name jumphost                 \
    --resource-group $AZR_RESOURCE_GROUP     \
    --ssh-key-values $HOME/.ssh/id_rsa.pub   \
    --admin-username aro                     \
    --image "RedHat:RHEL:8.2:8.2.2021040911" \
    --subnet JumpSubnet                      \
    --public-ip-address jumphost-ip          \
    --vnet-name "$AZR_CLUSTER-aro-vnet-$AZR_RESOURCE_LOCATION"</code></p>
</li>
<li>
<p>Save the jump host public IP address</p>
<p><code>bash
JUMP_IP=$(az vm list-ip-addresses -g $AZR_RESOURCE_GROUP -n jumphost -o tsv \
--query '[].virtualMachine.network.publicIpAddresses[0].ipAddress')
echo $JUMP_IP</code></p>
</li>
<li>
<p>ssh to jump host forwarding port 1337 as a socks proxy.</p>
<blockquote>
<p>replace the IP with the IP of the jump box from the previous step.</p>
</blockquote>
<p><code>bash
ssh -D 1337 -C -i $HOME/.ssh/id_rsa aro@$JUMP_IP</code></p>
</li>
<li>
<p>test the socks proxy</p>
<p><code>bash
curl --socks5-hostname localhost:1337 http://www.google.com/</code></p>
</li>
<li>
<p>Install tools</p>
<p>```bash
sudo yum install -y gcc libffi-devel python3-devel openssl-devel jq
sudo rpm --import https://packages.microsoft.com/keys/microsoft.asc</p>
<p>echo -e "[azure-cli]
name=Azure CLI
baseurl=<a href="https://packages.microsoft.com/yumrepos/azure-cli">https://packages.microsoft.com/yumrepos/azure-cli</a>
enabled=1
gpgcheck=1
gpgkey=<a href="https://packages.microsoft.com/keys/microsoft.asc">https://packages.microsoft.com/keys/microsoft.asc</a>" | sudo tee /etc/yum.repos.d/azure-cli.repo</p>
<p>sudo yum install -y azure-cli
wget https://mirror.openshift.com/pub/openshift-v4/clients/ocp/latest/openshift-client-linux.tar.gz
mkdir openshift
tar -zxvf openshift-client-linux.tar.gz -C openshift
sudo install openshift/oc /usr/local/bin/oc
sudo install openshift/kubectl /usr/local/bin/kubectl
```</p>
</li>
<li>
<p>Wait until the ARO cluster is fully provisioned.</p>
</li>
<li>
<p>Login to Azure</p>
<p><code>bash
az login</code></p>
</li>
<li>
<p>Get OpenShift console URL</p>
<blockquote>
<p>set these variables to match the ones you set at the start.</p>
</blockquote>
<p><code>bash
AZR_RESOURCE_GROUP=openshift-private
AZR_CLUSTER=private-cluster
APISERVER=$(az aro show              \
--name $AZR_CLUSTER                  \
--resource-group $AZR_RESOURCE_GROUP \
-o tsv --query apiserverProfile.url)
echo $APISERVER</code></p>
</li>
<li>
<p>Get OpenShift credentials</p>
<p><code>bash
ADMINPW=$(az aro list-credentials    \
--name $AZR_CLUSTER                  \
--resource-group $AZR_RESOURCE_GROUP \
--query kubeadminPassword            \
-o tsv)</code></p>
</li>
</ol>
<h3 id="aro-private-cluster-test-access">Test Access</h3>
<ol>
<li>
<p>Test Access to the cluster via the socks proxy</p>
<p>```bash
CONSOLE=$(az aro show                  \
  --name $AZR_CLUSTER                  \
  --resource-group $AZR_RESOURCE_GROUP \
  -o tsv --query consoleProfile)
echo $CONSOLE</p>
<p>curl --socks5-hostname localhost:1337 $CONSOLE
```</p>
</li>
</ol>
<blockquote>
<p>Unfortunately you can't [easily] use the socks proxy with the <code>oc</code> command, but at least you can access the console via the socks proxy.</p>
</blockquote>
<ol>
<li>Set localhost:1337 as a socks proxy in your browser and verify you can access the cluster by browsing to the <code>$CONSOLE</code> url.</li>
</ol>
<h3 id="aro-private-cluster-delete-cluster">Delete Cluster</h3>
<p>Once you're done its a good idea to delete the cluster to ensure that you don't get a surprise bill.</p>
<ol>
<li>
<p>Delete the cluster</p>
<p><code>bash
az aro delete -y                       \
  --resource-group $AZR_RESOURCE_GROUP \
  --name $AZR_CLUSTER</code></p>
</li>
<li>
<p>Delete the Azure resource group</p>
<blockquote>
<p>Only do this if there's nothing else in the resource group.</p>
</blockquote>
<p><code>bash
az group delete -y \
  --name $AZR_RESOURCE_GROUP</code></p>
</li>
</ol>
<h2 id="aro-private-cluster-addendum">Addendum</h2>
<h3 id="aro-private-cluster-adding-quota-to-aro-account">Adding Quota to ARO account</h3>
<p><img alt="aro quota support ticket request example" src="../images/aro-quota.png" /></p>
<ol>
<li>
<p><a href="https://portal.azure.com/#blade/Microsoft_Azure_Support/HelpAndSupportBlade/newsupportrequest">Create an Azure Support Request</a></p>
</li>
<li>
<p>Set <strong>Issue Type</strong> to "Service and subscription limits (quotas)"</p>
</li>
<li>
<p>Set <strong>Quota Type</strong> to "Compute-VM (cores-vCPUs) subscription limit increases"</p>
</li>
<li>
<p>Click <strong>Next Solutions &gt;&gt;</strong></p>
</li>
<li>
<p>Click <strong>Enter details</strong></p>
</li>
<li>
<p>Set <strong>Deployment Model</strong> to "Resource Manager</p>
</li>
<li>
<p>Set <strong>Locations</strong> to "(US) East US"</p>
</li>
<li>
<p>Set <strong>Types</strong> to "Standard"</p>
</li>
<li>
<p>Under <strong>Standard</strong> check "DSv3" and "DSv4"</p>
</li>
<li>
<p>Set <strong>New vCPU Limit</strong> for each (example "60")</p>
</li>
<li>
<p>Click <strong>Save and continue</strong></p>
</li>
<li>
<p>Click <strong>Review + create &gt;&gt;</strong></p>
</li>
<li>
<p>Wait until quota is increased.</p>
</li>
</ol></section><section class="print-page" id="aro-ocm"><h1 id="aro-ocm-registering-an-aro-cluster-to-openshift-cluster-manager">Registering an ARO cluster to OpenShift Cluster Manager</h1>
<p>ARO clusters do not come connected to OpenShift Cluster Manager by default,
because Azure would like customers to specifically opt-in to connections / data
sent outside of Azure. This is the case with registering to OpenShift cluster
manager, which enables a telemetry service in ARO. </p>
<h2 id="aro-ocm-prerequisites">Prerequisites</h2>
<ul>
<li>An Red Hat account. If you have any subscriptions with Red Hat, you will have
  a Red Hat account. If not, then you can create an account easily at
  https://cloud.redhat.com. </li>
</ul>
<h2 id="aro-ocm-steps">Steps</h2>
<ol>
<li>
<p>Login to https://console.redhat.com with you Red Hat account. </p>
</li>
<li>
<p>Go to https://console.redhat.com/openshift/downloads and download your
pull-secret file. This is a file that includes an authentication for
cloud.openshift.com which is used by OpenShift Cluster Manager.</p>
</li>
<li>
<p>Follow the <a href="https://docs.microsoft.com/en-us/azure/openshift/howto-add-update-pull-secret">Update pull secret instructions</a> to merge your pull-secret (in particular cloud.openshift.com) in your ARO pull secret. Be careful not to overwrite the ARO cluster pull secrets that come by default - it explains how in that article.</p>
</li>
<li>
<p>After waiting a few minutes (but it could be up to an hour), your 
   cluster should be automatically registered in this list in OpenShift Cluster 
   Manager; https://console.redhat.com/openshift</p>
</li>
</ol>
<p>You can check the cluster ID within the Cluster Overview section of the
   admin console with the ID of the cluster in OCM to make sure the right cluster is registered.</p>
<ol>
<li>The cluster will appear as a 60-day self-supported evaluation cluster. However, again,
wait about an hour (but in this case, it can take up to 24 hours), and the
cluster will be automatically updated to an ARO type cluster, with full
support. You don't need to change the support level yourself. </li>
</ol>
<p>This makes the cluster a fully supported cluster within the Red Hat cloud
console, with access to raise support tickets, also.</p></section><section class="print-page" id="aro-gpu"><h1 id="aro-gpu-aro-with-nvidia-gpu-workloads">ARO with Nvidia GPU Workloads</h1>
<p>ARO guide to running Nvidia GPU workloads.</p>
<p>Author: <a href="https://twitter.com/byron_miller">Byron Miller</a>, <a href="https://github.com/redhatstuart">Stuart Kirk</a></p>
<h2 id="aro-gpu-table-of-contents">Table of Contents</h2>
<ul>
<li>Do not remove this line (it will not be displayed)
{:toc}</li>
</ul>
<h2 id="aro-gpu-prerequisites">Prerequisites</h2>
<ul>
<li>oc cli</li>
<li>jq, moreutils, and gettext package</li>
<li>ARO 4.10</li>
</ul>
<p>If you need to install an ARO cluster, please read our <a href="https://mobb.ninja/docs/quickstart-aro.html">ARO Quick start guide</a>. Please be sure if you're installing or using an existing ARO cluster that it is 4.10.x or higher.</p>
<blockquote>
<p>As of OpenShift 4.10, it is no longer necessary to set up entitlements to use the nVidia Operator. This has greatly simplified the setup of the cluster for GPU workloads.</p>
</blockquote>
<p>Linux:</p>
<pre><code class="language-bash">sudo dnf install jq moreutils gettext
</code></pre>
<p>MacOS</p>
<pre><code class="language-bash">brew install jq moreutils gettext
</code></pre>
<h3 id="aro-gpu-helm-prerequisites">Helm Prerequisites</h3>
<p>If you plan to use Helm to deploy the GPU operator, you will need do the following</p>
<ol>
<li>
<p>Add the MOBB chart repository to your Helm</p>
<p><code>bash
helm repo add mobb https://rh-mobb.github.io/helm-charts/</code></p>
</li>
<li>
<p>Update your repositories</p>
<p><code>bash
helm repo update</code></p>
</li>
</ol>
<h2 id="aro-gpu-gpu-quota">GPU Quota</h2>
<p>All GPU quotas in Azure are 0 by default. You will need to login to the azure portal and request GPU quota. There is a lot of competition for GPU workers, so you may have to provision an ARO cluster in a region where you can actually reserve GPU.
ARO supports the following GPU workers:
* NC4as T4 v3
* NC8as T4 v3
* NC16as T4 v3
* NC464as T4 v3</p>
<blockquote>
<p>Please remember that when you request quota that Azure is per core.  To request a single NC4as T4 v3 node, you will need to request quota in groups of 4. If you wish to request an NC16as T4 v3 you will need to request quota of 16.</p>
</blockquote>
<ol>
<li>Login to azure</li>
</ol>
<p>Login to <a href="https://portal.azure.com">portal.azure.com</a>, type "quotas" in search by, click on Compute and in the search box type "NCAsv3_T4". Select the region your cluster is in (select checkbox) and then click Request quota increase and ask for quota (I chose 8 so i can build two demo clusters of NC4as T4s).</p>
<ol>
<li>Configure quota</li>
</ol>
<p><img alt="GPU Quota Request on Azure" src="../aro/gpu/gpu-quota-azure.png" /></p>
<h2 id="aro-gpu-log-in-to-your-aro-cluster">Log in to your ARO cluster</h2>
<ol>
<li>Login to OpenShift - we'll use the kubeadmin account here but you can login with your user account as long as you have cluster-admin.</li>
</ol>
<p><code>bash
   oc login &lt;apiserver&gt; -u kubeadmin -p &lt;kubeadminpass&gt;</code></p>
<h2 id="aro-gpu-pull-secret-conditional">Pull secret (Conditional)</h2>
<p>We'll update our pull secret to make sure that we can install operators as well as connect to cloud.redhat.com.</p>
<blockquote>
<p>If you have already re-created a full pull secret with cloud.redhat.com enabled you can skip this step</p>
</blockquote>
<h3 id="aro-gpu-using-helm">Using Helm</h3>
<ol>
<li>Before Deploying the chart you need it to adopt the existing pull secret</li>
</ol>
<p><code>bash
   kubectl -n openshift-config annotate secret \
    pull-secret meta.helm.sh/release-name=pull-secret
   kubectl -n openshift-config annotate secret \
     pull-secret meta.helm.sh/release-namespace=openshift-config
   kubectl -n openshift-config label secret \
     pull-secret app.kubernetes.io/managed-by=Helm</code></p>
<ol>
<li>
<p>Download your new pull secret from <strong>https://console.redhat.com/openshift/downloads -&gt; Tokens -&gt; Pull secret</strong> and use it to update create the pull secret in your cluster.</p>
</li>
<li>
<p>Update the pull secret</p>
</li>
</ol>
<blockquote>
<p>This chart will merge the in-cluster pull secret with the new pull secret.</p>
</blockquote>
<p><code>helm upgrade --install pull-secret mobb/aro-pull-secret \
     -n openshift-config --set-file pullSecret=$HOME/Downloads/pull-secret.txt</code></p>
<ol>
<li>Enable Operator Hub</li>
</ol>
<p><code>bash
   oc patch configs.samples.operator.openshift.io cluster --type=merge \
         -p='{"spec":{"managementState":"Managed"}}'
   oc patch operatorhub cluster --type=merge \
         -p='{"spec":{"sources":[
           {"name":"redhat-operators","disabled":false},
           {"name":"certified-operators","disabled":false},
           {"name":"community-operators","disabled":false},
           {"name":"redhat-marketplace","disabled":false}
         ]}}'</code></p>
<ol>
<li>Skip to <a href="#aro-gpu-gpu-machine-set">GPU Machine Set</a></li>
</ol>
<h3 id="aro-gpu-manually">Manually</h3>
<ol>
<li>
<p>Log into <a href="https://cloud.redhat.com">cloud.redhat.com</a></p>
</li>
<li>
<p>Browse to https://cloud.redhat.com/openshift/install/azure/aro-provisioned</p>
</li>
<li>
<p>click the <strong>Download pull secret</strong> button and save it as pull-secret.txt</p>
</li>
</ol>
<blockquote>
<p>The following steps will need to be ran in the same working directory as your pull-secret.txt</p>
</blockquote>
<ol>
<li>Export existing pull secret</li>
</ol>
<p><code>bash
   oc get secret pull-secret -n openshift-config -o json | jq -r '.data.".dockerconfigjson"' | base64 --decode &gt; export-pull.json</code></p>
<ol>
<li>Merge downloaded pull secret with system pull secret to add cloud.redhat.com</li>
</ol>
<p><code>bash
   jq -s '.[0] * .[1]' export-pull.json pull-secret.txt | tr -d "\n\r" &gt; new-pull-secret.json</code></p>
<ol>
<li>Upload new secret file</li>
</ol>
<p><code>bash
   oc set data secret/pull-secret -n openshift-config --from-file=.dockerconfigjson=new-pull-secret.json</code></p>
<blockquote>
<p>You may need to wait for about ~1hr for everything to sync up with cloud.redhat.com.</p>
</blockquote>
<ol>
<li>Delete secrets</li>
</ol>
<p><code>bash
   rm pull-secret.txt export-pull.json new-pull-secret.json</code></p>
<h2 id="aro-gpu-gpu-machine-set">GPU Machine Set</h2>
<p>ARO still uses Kubernetes Machinsets to create a machine set.  I'm going to export the first machine set in my cluster (az 1) and use that as a template to build a single GPU machine in southcentralus region 1.</p>
<h3 id="aro-gpu-helm">Helm</h3>
<ol>
<li>Create a new machine-set (replicas of 1), see the Chart's <a href="https://github.com/rh-mobb/helm-charts/blob/main/charts/aro-gpu/values.yaml">values</a> file for configuration options</li>
</ol>
<p><code>helm upgrade --install -n openshift-machine-api \
      gpu mobb/aro-gpu</code></p>
<ol>
<li>Wait for the new GPU nodes to be available</li>
</ol>
<p><code>bash
   watch oc get machines</code></p>
<ol>
<li>Skip to <a href="#aro-gpu-install-nvidia-gpu-operator">Install Nvidia GPU Operator</a></li>
</ol>
<h3 id="aro-gpu-manually_1">Manually</h3>
<ol>
<li>View existing machine sets</li>
</ol>
<blockquote>
<p>For ease of set up, I'm going to grab the first machine set and use that as the one I will clone to create our GPU machine set.</p>
</blockquote>
<p><code>bash
   MACHINESET=$(oc get machineset -n openshift-machine-api -o=jsonpath='{.items[0]}' | jq -r '[.metadata.name] | @tsv')</code></p>
<ol>
<li>Save a copy of example machine set</li>
</ol>
<p><code>bash
   oc get machineset -n openshift-machine-api $MACHINESET -o json &gt; gpu_machineset.json</code></p>
<ol>
<li>Change the .metadata.name field to a new unique name</li>
</ol>
<blockquote>
<p>I'm going to create a unique name for this single node machine set that shows nvidia-worker-<region><az> that follows a similar pattern as all the other machine sets.</p>
</blockquote>
<p><code>bash
   jq '.metadata.name = "nvidia-worker-southcentralus1"' gpu_machineset.json| sponge gpu_machineset.json</code></p>
<ol>
<li>
<p>Ensure spec.replicas matches the desired replica count for the MachineSet</p>
<p><code>bash
jq '.spec.replicas = 1' gpu_machineset.json| sponge gpu_machineset.json</code></p>
</li>
<li>
<p>Change the .spec.selector.matchLabels.machine.openshift.io/cluster-api-machineset field to match the .metadata.name field</p>
</li>
</ol>
<p><code>bash
   jq '.spec.selector.matchLabels."machine.openshift.io/cluster-api-machineset" = "nvidia-worker-southcentralus1"' gpu_machineset.json| sponge gpu_machineset.json</code></p>
<ol>
<li>Change the .spec.template.metadata.labels.machine.openshift.io/cluster-api-machineset to match the .metadata.name field</li>
</ol>
<p><code>bash
   jq '.spec.template.metadata.labels."machine.openshift.io/cluster-api-machineset" = "nvidia-worker-southcentralus1"' gpu_machineset.json| sponge gpu_machineset.json</code></p>
<ol>
<li>Change the spec.template.spec.providerSpec.value.vmSize to match the desired GPU instance type from Azure.</li>
</ol>
<blockquote>
<p>The machine we're using is Standard_NC4as_T4_v3.</p>
</blockquote>
<p><code>bash
   jq '.spec.template.spec.providerSpec.value.vmSize = "Standard_NC4as_T4_v3"' gpu_machineset.json | sponge gpu_machineset.json</code></p>
<ol>
<li>
<p>Change the spec.template.spec.providerSpec.value.zone to match the desired zone from Azure</p>
<p><code>bash
jq '.spec.template.spec.providerSpec.value.zone = "1"' gpu_machineset.json | sponge gpu_machineset.json</code></p>
</li>
<li>
<p>Delete the .status section of the yaml file</p>
</li>
</ol>
<p><code>bash
   jq 'del(.status)' gpu_machineset.json | sponge gpu_machineset.json</code></p>
<ol>
<li>Verify the other data in the yaml file.</li>
</ol>
<h4 id="aro-gpu-create-gpu-machine-set">Create GPU machine set</h4>
<p>These steps will create the new GPU machine. It may take 10-15 minutes to provision a new GPU machine. If this step fails, please login to the <a href="https://portal.azure.com">azure portal</a> and ensure you didn't run across availability issues. You can go "Virtual Machines" and search for the worker name you created above to see the status of VMs.</p>
<ol>
<li>Create GPU Machine set</li>
</ol>
<p><code>bash
   oc create -f gpu_machineset.json</code></p>
<blockquote>
<p>This command will take a few minutes to complete.</p>
</blockquote>
<ol>
<li>Verify GPU machine set</li>
</ol>
<p>Machines should be getting deployed. You can view the status of the machine set with the following commands</p>
<p><code>bash
   oc get machineset -n openshift-machine-api
   oc get machine -n openshift-machine-api</code></p>
<p>Once the machines are provisioned, which could take 5-15 minutes, machines will show as nodes in the node list.</p>
<p><code>bash
   oc get nodes</code></p>
<p>You should see a node with the "nvidia-worker-southcentralus1" name it we created earlier.</p>
<h2 id="aro-gpu-install-nvidia-gpu-operator">Install Nvidia GPU Operator</h2>
<p>This will create the nvidia-gpu-operator name space, set up the operator group and install the Nvidia GPU Operator.</p>
<h3 id="aro-gpu-helm_1">Helm</h3>
<ol>
<li>
<p>Create namespaces</p>
<p><code>bash
oc create namespace openshift-nfd
oc create namespace nvidia-gpu-operator</code></p>
</li>
<li>
<p>Use the <code>mobb/operatorhub</code> chart to deploy the needed operators</p>
<p><code>bash
helm upgrade -n nvidia-gpu-operator nvidia-gpu-operator \
  mobb/operatorhub --install \
  --values https://raw.githubusercontent.com/rh-mobb/helm-charts/main/charts/nvidia-gpu/files/operatorhub.yaml</code></p>
</li>
<li>
<p>Wait until the two operators are running</p>
<p><code>bash
watch kubectl get pods -n openshift-nfd</code></p>
<p><code>NAME                                      READY   STATUS    RESTARTS   AGE
nfd-controller-manager-7b66c67bd9-rk98w   2/2     Running   0          47s</code></p>
<p><code>bash
watch oc get pods -n nvidia-gpu-operator</code></p>
<p><code>NAME                            READY   STATUS    RESTARTS   AGE
gpu-operator-5d8cb7dd5f-c4ljk   1/1     Running   0          87s</code></p>
</li>
<li>
<p>Install the Nvidia GPU Operator chart</p>
<p>```bash</p>
<p><code>bash
helm upgrade --install -n nvidia-gpu-operator nvidia-gpu \
  mobb/nvidia-gpu --disable-openapi-validation</code></p>
</li>
<li>
<p>Skip to <a href="#aro-gpu-validate-gpu">Validate GPU</a></p>
</li>
</ol>
<h3 id="aro-gpu-manually_2">Manually</h3>
<ol>
<li>Create Nvidia namespace</li>
</ol>
<p><code>yaml
   cat &lt;&lt;EOF | oc apply -f -
   apiVersion: v1
   kind: Namespace
   metadata:
     name: nvidia-gpu-operator
   EOF</code></p>
<ol>
<li>Create Operator Group</li>
</ol>
<p><code>yaml
   cat &lt;&lt;EOF | oc apply -f -
   apiVersion: operators.coreos.com/v1
   kind: OperatorGroup
   metadata:
     name: nvidia-gpu-operator-group
     namespace: nvidia-gpu-operator
   spec:
    targetNamespaces:
    - nvidia-gpu-operator
   EOF</code></p>
<ol>
<li>Get latest nvidia channel</li>
</ol>
<p><code>bash
   CHANNEL=$(oc get packagemanifest gpu-operator-certified -n openshift-marketplace -o jsonpath='{.status.defaultChannel}')</code></p>
<ol>
<li>Get latest nvidia package</li>
</ol>
<p><code>bash
   PACKAGE=$(oc get packagemanifests/gpu-operator-certified -n openshift-marketplace -ojson | jq -r '.status.channels[] | select(.name == "'$CHANNEL'") | .currentCSV')</code></p>
<ol>
<li>Create Subscription</li>
</ol>
<p><code>yaml
   envsubst  &lt;&lt;EOF | oc apply -f -
   apiVersion: operators.coreos.com/v1alpha1
   kind: Subscription
   metadata:
     name: gpu-operator-certified
     namespace: nvidia-gpu-operator
   spec:
     channel: "$CHANNEL"
     installPlanApproval: Automatic
     name: gpu-operator-certified
     source: certified-operators
     sourceNamespace: openshift-marketplace
     startingCSV: "$PACKAGE"
   EOF</code></p>
<ol>
<li>Wait for Operator to finish installing</li>
</ol>
<blockquote>
<p>Don't proceed until you have verified that the operator has finished installing. It's also a good point to ensure that your GPU worker is online.</p>
</blockquote>
<p><img alt="Verify Operator" src="../aro/gpu/nvidia-installed.png" /></p>
<h4 id="aro-gpu-install-node-feature-discovery-operator">Install Node Feature Discovery Operator</h4>
<p>The node feature discovery operator will discover the GPU on your nodes and appropriately label the nodes so you can target them for workloads.  We'll install the NFD operator into the opneshift-ndf namespace and create the "subscription" which is the configuration for NFD.</p>
<p>Official Documentation for Installing <a href="https://docs.openshift.com/container-platform/4.10/hardware_enablement/psap-node-feature-discovery-operator.html">Node Feature Discovery Operator</a></p>
<ol>
<li>Set up Name Space</li>
</ol>
<p><code>yaml
   cat &lt;&lt;EOF | oc apply -f -
   apiVersion: v1
   kind: Namespace
   metadata:
     name: openshift-nfd
   EOF</code></p>
<ol>
<li>Create OperatorGroup</li>
</ol>
<p><code>yaml
   cat &lt;&lt;EOF | oc apply -f -
   apiVersion: operators.coreos.com/v1
   kind: OperatorGroup
   metadata:
     generateName: openshift-nfd-
     name: openshift-nfd
     namespace: openshift-nfd
   EOF</code></p>
<ol>
<li>Create Subscription</li>
</ol>
<p><code>yaml
   cat &lt;&lt;EOF | oc apply -f -
   apiVersion: operators.coreos.com/v1alpha1
   kind: Subscription
   metadata:
     name: nfd
     namespace: openshift-nfd
   spec:
     channel: "stable"
     installPlanApproval: Automatic
     name: nfd
     source: redhat-operators
     sourceNamespace: openshift-marketplace
   EOF</code>
1. Wait for Node Feature discovery to complete installation</p>
<p>You can login to your openshift console and view operators or simply wait a few minutes. The next step will error until the operator has finished installing.</p>
<ol>
<li>Create NFD Instance</li>
</ol>
<p><code>yaml
   cat &lt;&lt;EOF | oc apply -f -
   kind: NodeFeatureDiscovery
   apiVersion: nfd.openshift.io/v1
   metadata:
     name: nfd-instance
     namespace: openshift-nfd
   spec:
     customConfig:
       configData: |
         #    - name: "more.kernel.features"
         #      matchOn:
         #      - loadedKMod: ["example_kmod3"]
         #    - name: "more.features.by.nodename"
         #      value: customValue
         #      matchOn:
         #      - nodename: ["special-.*-node-.*"]
     operand:
       image: &gt;-
         registry.redhat.io/openshift4/ose-node-feature-discovery@sha256:07658ef3df4b264b02396e67af813a52ba416b47ab6e1d2d08025a350ccd2b7b
       servicePort: 12000
     workerConfig:
       configData: |
         core:
         #  labelWhiteList:
         #  noPublish: false
           sleepInterval: 60s
         #  sources: [all]
         #  klog:
         #    addDirHeader: false
         #    alsologtostderr: false
         #    logBacktraceAt:
         #    logtostderr: true
         #    skipHeaders: false
         #    stderrthreshold: 2
         #    v: 0
         #    vmodule:
         ##   NOTE: the following options are not dynamically run-time
         ##          configurable and require a nfd-worker restart to take effect
         ##          after being changed
         #    logDir:
         #    logFile:
         #    logFileMaxSize: 1800
         #    skipLogHeaders: false
         sources:
         #  cpu:
         #    cpuid:
         ##     NOTE: whitelist has priority over blacklist
         #      attributeBlacklist:
         #        - "BMI1"
         #        - "BMI2"
         #        - "CLMUL"
         #        - "CMOV"
         #        - "CX16"
         #        - "ERMS"
         #        - "F16C"
         #        - "HTT"
         #        - "LZCNT"
         #        - "MMX"
         #        - "MMXEXT"
         #        - "NX"
         #        - "POPCNT"
         #        - "RDRAND"
         #        - "RDSEED"
         #        - "RDTSCP"
         #        - "SGX"
         #        - "SSE"
         #        - "SSE2"
         #        - "SSE3"
         #        - "SSE4.1"
         #        - "SSE4.2"
         #        - "SSSE3"
         #      attributeWhitelist:
         #  kernel:
         #    kconfigFile: "/path/to/kconfig"
         #    configOpts:
         #      - "NO_HZ"
         #      - "X86"
         #      - "DMI"
           pci:
             deviceClassWhitelist:
               - "0200"
               - "03"
               - "12"
             deviceLabelFields:
         #      - "class"
               - "vendor"
         #      - "device"
         #      - "subsystem_vendor"
         #      - "subsystem_device"
         #  usb:
         #    deviceClassWhitelist:
         #      - "0e"
         #      - "ef"
         #      - "fe"
         #      - "ff"
         #    deviceLabelFields:
         #      - "class"
         #      - "vendor"
         #      - "device"
         #  custom:
         #    - name: "my.kernel.feature"
         #      matchOn:
         #        - loadedKMod: ["example_kmod1", "example_kmod2"]
         #    - name: "my.pci.feature"
         #      matchOn:
         #        - pciId:
         #            class: ["0200"]
         #            vendor: ["15b3"]
         #            device: ["1014", "1017"]
         #        - pciId :
         #            vendor: ["8086"]
         #            device: ["1000", "1100"]
         #    - name: "my.usb.feature"
         #      matchOn:
         #        - usbId:
         #          class: ["ff"]
         #          vendor: ["03e7"]
         #          device: ["2485"]
         #        - usbId:
         #          class: ["fe"]
         #          vendor: ["1a6e"]
         #          device: ["089a"]
         #    - name: "my.combined.feature"
         #      matchOn:
         #        - pciId:
         #            vendor: ["15b3"]
         #            device: ["1014", "1017"]
         #          loadedKMod : ["vendor_kmod1", "vendor_kmod2"]
   EOF</code></p>
<ol>
<li>Verify NFD is ready.</li>
</ol>
<p>This operator should say Available in the status</p>
<p><img alt="NFD Operator Ready" src="../aro/gpu/nfd-ready-for-use.png" /></p>
<h4 id="aro-gpu-apply-nvidia-cluster-config">Apply nVidia Cluster Config</h4>
<p>We'll now apply the nvidia cluster config. Please read the <a href="https://docs.nvidia.com/datacenter/cloud-native/gpu-operator/openshift/install-gpu-ocp.html">nvidia documentation</a> on customizing this if you have your own private repos or specific settings. This will be another process that takes a few minutes to complete.</p>
<ol>
<li>Apply cluster config</li>
</ol>
<p><code>yaml
   cat &lt;&lt;EOF | oc apply -f -
   apiVersion: nvidia.com/v1
   kind: ClusterPolicy
   metadata:
     name: gpu-cluster-policy
   spec:
     migManager:
       enabled: true
     operator:
       defaultRuntime: crio
       initContainer: {}
       runtimeClass: nvidia
       deployGFD: true
     dcgm:
       enabled: true
     gfd: {}
     dcgmExporter:
       config:
         name: ''
     driver:
       licensingConfig:
         nlsEnabled: false
         configMapName: ''
       certConfig:
         name: ''
       kernelModuleConfig:
         name: ''
       repoConfig:
         configMapName: ''
       virtualTopology:
         config: ''
       enabled: true
       use_ocp_driver_toolkit: true
     devicePlugin: {}
     mig:
       strategy: single
     validator:
       plugin:
         env:
           - name: WITH_WORKLOAD
             value: 'true'
     nodeStatusExporter:
       enabled: true
     daemonsets: {}
     toolkit:
       enabled: true
   EOF</code></p>
<ol>
<li>Verify Cluster Policy</li>
</ol>
<p>Login to OpenShift console and browse to operators and make sure you're in nvidia-gpu-operator namespace. You should see it say State: Ready once everything is complete.</p>
<p><img alt="cluster policy" src="../aro/gpu/nvidia-cluster-policy.png" /></p>
<h2 id="aro-gpu-validate-gpu">Validate GPU</h2>
<p>It may take some time for the nVidia Operator and NFD to completely install and self-identify the machines. These commands can be ran to help validate that everything is running as expected.</p>
<ol>
<li>
<p>Verify NFD can see your GPU(s)</p>
<p><code>bash
oc describe node | egrep 'Roles|pci-10de' | grep -v master</code></p>
<p>You should see output like:</p>
<p><code>bash
Roles:              worker
                feature.node.kubernetes.io/pci-10de.present=true</code></p>
</li>
<li>
<p>Verify node labels</p>
</li>
</ol>
<p>You can see the node labels by logging into the OpenShift console -&gt; Compute -&gt; Nodes -&gt; nvidia-worker-southcentralus1-<id>.  You should see a bunch of nvidia GPU labels and the pci-10de device from above.</p>
<p><img alt="NFD Node labels" src="../aro/gpu/node-labels.png" /></p>
<ol>
<li>Nvidia SMI tool verification</li>
</ol>
<p><code>bash
   oc project nvidia-gpu-operator
   for i in $(oc get pod -lopenshift.driver-toolkit=true --no-headers |awk '{print $1}'); do echo $i; oc exec -it $i -- nvidia-smi ; echo -e '\n' ;  done</code></p>
<p>You should see output that shows the GPUs available on the host such as this example screenshot. (Varies depending on GPU worker type)</p>
<p><img alt="Nvidia SMI" src="../aro/gpu/test-gpu.png" /></p>
<ol>
<li>Create Pod to run a GPU workload</li>
</ol>
<p><code>yaml
   oc project nvidia-gpu-operator
   cat &lt;&lt;EOF | oc apply -f -
   apiVersion: v1
   kind: Pod
   metadata:
     name: cuda-vector-add
   spec:
     restartPolicy: OnFailure
     containers:
       - name: cuda-vector-add
         image: "quay.io/giantswarm/nvidia-gpu-demo:latest"
         resources:
           limits:
             nvidia.com/gpu: 1
         nodeSelector:
           nvidia.com/gpu.present: true
   EOF</code></p>
<ol>
<li>View logs</li>
</ol>
<p><code>bash
   oc logs cuda-vector-add --tail=-1</code></p>
<blockquote>
<p>Please note, if you get an error "Error from server (BadRequest): container "cuda-vector-add" in pod "cuda-vector-add" is waiting to start: ContainerCreating" try running "oc delete pod cuda-vector-add" and then re-run the create statement above. I've seen issues where if this step is ran before all of the operator consolidation is done it may just sit there.</p>
</blockquote>
<p>You should see Output like the following (mary vary depending on GPU):</p>
<p><code>bash
   [Vector addition of 5000 elements]
   Copy input data from the host memory to the CUDA device
   CUDA kernel launch with 196 blocks of 256 threads
   Copy output data from the CUDA device to the host memory
   Test PASSED
   Done</code></p>
<ol>
<li>If successful, the pod can be deleted</li>
</ol>
<p><code>bash
   oc delete pod cuda-vector-add</code></p></section><section class="print-page" id="aro-frontdoor"><h1 id="aro-frontdoor-azure-front-door-with-aro-azure-red-hat-openshift">Azure Front Door with ARO ( Azure Red Hat OpenShift )</h1>
<p>Securing exposing an Internet facing application with a private ARO Cluster.  </p>
<p>When you create a cluster on ARO you have several options in making the cluster public or private.  With a public cluster you are allowing Internet traffic to the api and *.apps endpoints.  With a private cluster you can make either or both the api and .apps endpoints private.  </p>
<p>How can you allow Internet access to an application running on your private cluster where the .apps endpoint is private?  This document will guide you through using Azure Frontdoor to expose your applications to the Internet.  There are several advantages of this approach, namely your cluster and all the resources in your Azure account can remain private, providing you an extra layer of security.  Azure FrontDoor operates at the edge so we are controlling traffic before it even gets into your Azure account.  On top of that, Azure FrontDoor also offers WAF and DDoS protection, certificate management and SSL Offloading just to name a few benefits.</p>
<p><strong>Kevin Collins</strong>
*Adopted from <a href="https://github.com/UmarMohamedUsman/aro-reference-architecture">ARO Reference Architecture</a></p>
<p><em>06/16/2022</em></p>
<h2 id="aro-frontdoor-prerequisites">Prerequisites</h2>
<ul>
<li>az cli</li>
<li>oc cli</li>
<li>a custom domain</li>
<li>a DNS zone that you can easily modify
<br></li>
</ul>
<p>To build and deploy the application
* <a href="https://maven.apache.org/install.html">maven cli</a>
* <a href="https://quarkus.io/guides/cli-tooling">quarkus cli</a>
* <a href="https://www.azul.com/downloads/?package=jdk">OpenJDK Java 8</a> </p>
<p>Make sure to use the same terminal session while going through guide for all commands as we will reference envrionment variables set or created through the guide.</p>
<h2 id="aro-frontdoor-get-started">Get Started</h2>
<ul>
<li>
<p>Create a private ARO cluster.<br></p>
<p>Follow this guide to <a href="https://mobb.ninja/docs/aro/private-cluster">Create a private ARO cluster</a>
or simply run this <a href="https://github.com/rh-mobb/documentation/blob/main/docs/aro/private-cluster/create-cluster.sh">bash script</a></p>
</li>
</ul>
<h2 id="aro-frontdoor-set-evironment-variables">Set Evironment Variables</h2>
<ol>
<li>Manually set environment variables</li>
</ol>
<p>```
   AROCLUSTER=<cluster name></p>
<p>ARORG=<resource group for the cluster></p>
<p>AFD_NAME=<name you want to use for the front door instance></p>
<p>DOMAIN='e.g. aro.kmobb.com'  This is the domain that you will be adding to Azure DNS to manage.</p>
<p>ARO_APP_FQDN='e.g. minesweeper.aro.kmobb.com'
   (note - we will be deploying an application called minesweeper to test front door.  Select a domain you would like to use for the application.  For example minesweeper.aro.kmobb.com ... where aro.kmobb.com is the domain you manage and have DNS access to.)</p>
<p>AFD_MINE_CUSTOM_DOMAIN_NAME='minesweeper-aro-kmobb-com'
   (note - this should be your domain name without and .'s for example minesweeper-aro-kmobb-com)</p>
<p>PRIVATEENDPOINTSUBNET_PREFIX= subnet in the VNET you cluster is in.  If you following the example above to create a custer where you virtual network is 10.0.0.0/20 then you can use '10.0.6.0/24' </p>
<p>PRIVATEENDPOINTSUBNET_NAME='PrivateEndpoint-subnet'
   ```</p>
<ol>
<li>Set environment variables with Bash</li>
</ol>
<p>```bash
   UNIQUEID=$RANDOM</p>
<p>ARO_RGNAME=$(az aro show -n $AROCLUSTER -g $ARORG --query "clusterProfile.resourceGroupId" -o tsv | sed 's/.*\///')</p>
<p>LOCATION=$(az aro show --name $AROCLUSTER --resource-group $ARORG --query location -o tsv)</p>
<p>INTERNAL_LBNAME=$(az network lb list --resource-group $ARO_RGNAME --query "[? contains(name, 'internal')].name" -o tsv)</p>
<p>WORKER_SUBNET_NAME=$(az aro show --name $AROCLUSTER --resource-group $ARORG --query 'workerProfiles[0].subnetId' -o tsv | sed 's/.*\///')</p>
<p>WORKER_SUBNET_ID=$(az aro show --name $AROCLUSTER --resource-group $ARORG --query 'workerProfiles[0].subnetId' -o tsv)</p>
<p>VNET_NAME=$(az network vnet list -g $ARORG --query '[0].name' -o tsv)</p>
<p>LBCONFIG_ID=$(az network lb frontend-ip list -g $ARO_RGNAME --lb-name $INTERNAL_LBNAME --query "[? contains(subnet.id,'$WORKER_SUBNET_ID')].id" -o tsv)</p>
<p>LBCONFIG_IP=$(az network lb frontend-ip list -g $ARO_RGNAME --lb-name $INTERNAL_LBNAME --query "[? contains(subnet.id,'$WORKER_SUBNET_ID')].privateIpAddress" -o tsv)</p>
<p>```</p>
<h2 id="aro-frontdoor-create-a-private-link-service">Create a Private Link Service</h2>
<p>After we have the cluster up and running, we need to create a private link service.  The private link service will provide private and secure connectivity between the Front Door Service and our cluster.</p>
<ol>
<li>Disable the worker subnet private link service network policy for the worker subnet</li>
</ol>
<p><code>bash
   az network vnet subnet update \
   --disable-private-link-service-network-policies true \
   --name $WORKER_SUBNET_NAME \
   --resource-group $ARORG \
   --vnet-name $VNET_NAME</code></p>
<ol>
<li>Create a private link service targeting the worker subnets</li>
</ol>
<p>```bash
   az network private-link-service create \
   --name $AROCLUSTER-pls \
   --resource-group $ARORG \
   --private-ip-address-version IPv4 \
   --private-ip-allocation-method Dynamic \
   --vnet-name $VNET_NAME \
   --subnet $WORKER_SUBNET_NAME \
   --lb-frontend-ip-configs $LBCONFIG_ID</p>
<p>privatelink_id=$(az network private-link-service show -n $AROCLUSTER-pls -g $ARORG --query 'id' -o tsv)
   ```</p>
<h2 id="aro-frontdoor-create-and-configure-an-instance-of-azure-front-door">Create and Configure an instance of Azure Front Door</h2>
<ol>
<li>Create a Front Door Instance  </li>
</ol>
<p>```bash
   az afd profile create \
   --resource-group $ARORG \
   --profile-name $AFD_NAME \
   --sku Premium_AzureFrontDoor</p>
<p>afd_id=$(az afd profile show -g $ARORG --profile-name $AFD_NAME --query 'id' -o tsv)
   ```</p>
<ol>
<li>Create an endpoint for the ARO Internal Load Balancer</li>
</ol>
<p><code>bash
   az afd endpoint create \
   --resource-group $ARORG \
   --enabled-state Enabled \
   --endpoint-name 'aro-ilb'$UNIQUEID \
   --profile-name $AFD_NAME</code></p>
<ol>
<li>Create a Front Door Origin Group that will point to the ARO Internal Loadbalancer</li>
</ol>
<p><code>bash
   az afd origin-group create \
   --origin-group-name 'afdorigin' \
   --probe-path '/' \
   --probe-protocol Http \
   --probe-request-type GET \
   --probe-interval-in-seconds 100 \
   --profile-name $AFD_NAME \
   --resource-group $ARORG \
   --probe-interval-in-seconds 120 \
   --sample-size 4 \
   --successful-samples-required 3 \
   --additional-latency-in-milliseconds 50</code></p>
<ol>
<li>Create a Front Door Origin with the above Origin Group that will point to the ARO Internal Loadbalancer</li>
</ol>
<p><code>bash
   az afd origin create \
   --enable-private-link true \
   --private-link-resource $privatelink_id \
   --private-link-location $LOCATION \
   --private-link-request-message 'Private link service from AFD' \
   --weight 1000 \
   --priority 1 \
   --http-port 80 \
   --https-port 443 \
   --origin-group-name 'afdorigin' \
   --enabled-state Enabled \
   --host-name $LBCONFIG_IP \
   --origin-name 'afdorigin' \
   --profile-name $AFD_NAME \
   --resource-group $ARORG</code></p>
<ol>
<li>Approve the private link connection</li>
</ol>
<p>```bash
   privatelink_pe_id=$(az network private-link-service show -n $AROCLUSTER-pls -g $ARORG --query 'privateEndpointConnections[0].id' -o tsv)</p>
<p>az network private-endpoint-connection approve \
   --description 'Approved' \
   --id $privatelink_pe_id
   ```</p>
<ol>
<li>Add your custom domain to Azure Front Door</li>
</ol>
<p><code>bash
   az afd custom-domain create \
   --certificate-type ManagedCertificate \
   --custom-domain-name $AFD_MINE_CUSTOM_DOMAIN_NAME \
   --host-name $ARO_APP_FQDN \
   --minimum-tls-version TLS12 \
   --profile-name $AFD_NAME \
   --resource-group $ARORG</code></p>
<ol>
<li>Create an Azure Front Door endpoint for your custom domain</li>
</ol>
<p><code>bash
   az afd endpoint create \
   --resource-group $ARORG \
   --enabled-state Enabled \
   --endpoint-name 'aro-mine-'$UNIQUEID \
   --profile-name $AFD_NAME</code></p>
<ol>
<li>Add an Azure Front Door route for your custom domain</li>
</ol>
<p><code>bash
   az afd route create \
   --endpoint-name 'aro-mine-'$UNIQUEID \
   --forwarding-protocol HttpOnly \
   --https-redirect Disabled \
   --origin-group 'afdorigin' \
   --profile-name $AFD_NAME \
   --resource-group $ARORG \
   --route-name 'aro-mine-route' \
   --supported-protocols Http Https \
   --patterns-to-match '/*' \
   --custom-domains $AFD_MINE_CUSTOM_DOMAIN_NAME</code></p>
<ol>
<li>Update DNS</li>
</ol>
<p>Get a validation token from Front Door so Front Door can validate your domain </p>
<p><code>bash
   afdToken=$(az afd custom-domain show \
   --resource-group $ARORG \
   --profile-name $AFD_NAME \
   --custom-domain-name $AFD_MINE_CUSTOM_DOMAIN_NAME \
   --query "validationProperties.validationToken")</code></p>
<ol>
<li>Create a DNS Zone</li>
</ol>
<p><code>bash
    az network dns zone create -g $ARORG -n $DOMAIN</code></p>
<pre><code>&gt;You will need to configure your nameservers to point to azure. The output of running this zone create will show you the nameservers for this record that you will need to set up within your domain registrar.
</code></pre>
<p>Create a new text record in your DNS server</p>
<p><code>bash
    az network dns record-set txt add-record -g $ARORG -z $DOMAIN -n _dnsauth.$(echo $ARO_APP_FQDN | sed 's/\..*//') --value $afdToken --record-set-name _dnsauth.$(echo $ARO_APP_FQDN | sed 's/\..*//')</code></p>
<ol>
<li>Check if the domain has been validated:<blockquote>
<p>Note this can take several hours 
   Your FQDN will not resolve until Front Door validates your domain.</p>
</blockquote>
</li>
</ol>
<p><code>bash
   az afd custom-domain list -g $ARORG --profile-name $AFD_NAME --query "[? contains(hostName, '$ARO_APP_FQDN')].domainValidationState"</code></p>
<ol>
<li>Add a CNAME record to DNS</li>
</ol>
<p>Get the Azure Front Door endpoint:</p>
<p><code>bash
   afdEndpoint=$(az afd endpoint show -g $ARORG --profile-name $AFD_NAME --endpoint-name aro-mine-$UNIQUEID --query "hostName" -o tsv)</code></p>
<p>Create a cname record for the application</p>
<p><code>bash
   az network dns record-set cname set-record -g $ARORG -z $DOMAIN \
    -n $(echo $ARO_APP_FQDN | sed 's/\..*//') -z $DOMAIN -c $afdEndpoint</code></p>
<h2 id="aro-frontdoor-deploy-an-application">Deploy an application</h2>
<p>Now the fun part, let's deploy an application!<br />
We will be deploying a Java based application called <a href="https://github.com/redhat-mw-demos/microsweeper-quarkus/tree/ARO">microsweeper</a>.  This is an application that runs on OpenShift and uses a PostgreSQL database to store scores.  With ARO being a first class service on Azure, we will create an Azure Database for PostgreSQL service and connect it to our cluster with a private endpoint.</p>
<ol>
<li>Create a Azure Database for PostgreSQL servers service</li>
</ol>
<p>```bash
   az postgres server create --name microsweeper-database --resource-group $ARORG --location $LOCATION --admin-user quarkus --admin-password r3dh4t1! --sku-name GP_Gen5_2</p>
<p>POSTGRES_ID=$(az postgres server show -n microsweeper-database -g $ARORG --query 'id' -o tsv)
   ```</p>
<ol>
<li>Create a private endpoint connection for the database</li>
</ol>
<p>```bash
   az network vnet subnet create \
   --resource-group $ARORG \
   --vnet-name $VNET_NAME \
   --name $PRIVATEENDPOINTSUBNET_NAME \
   --address-prefixes $PRIVATEENDPOINTSUBNET_PREFIX \
   --disable-private-endpoint-network-policies true</p>
<p>az network private-endpoint create \
   --name 'postgresPvtEndpoint' \
   --resource-group $ARORG \
   --vnet-name $VNET_NAME \
   --subnet $PRIVATEENDPOINTSUBNET_NAME \
   --private-connection-resource-id $POSTGRES_ID \
   --group-id 'postgresqlServer' \
   --connection-name 'postgresdbConnection'
   ```
1. Create and configure a private DNS Zone for the Postgres database</p>
<p>```bash
   az network private-dns zone create \
   --resource-group $ARORG \
   --name 'privatelink.postgres.database.azure.com'</p>
<p>az network private-dns link vnet create \
   --resource-group $ARORG \
   --zone-name 'privatelink.postgres.database.azure.com' \
   --name 'PostgresDNSLink' \
   --virtual-network $VNET_NAME \
   --registration-enabled false</p>
<p>az network private-endpoint dns-zone-group create \
   --resource-group $ARORG \
   --name 'PostgresDb-ZoneGroup' \
   --endpoint-name 'postgresPvtEndpoint' \
   --private-dns-zone 'privatelink.postgres.database.azure.com' \
   --zone-name 'postgresqlServer'</p>
<p>NETWORK_INTERFACE_ID=$(az network private-endpoint show --name postgresPvtEndpoint --resource-group $ARORG --query 'networkInterfaces[0].id' -o tsv)</p>
<p>POSTGRES_IP=$(az resource show --ids $NETWORK_INTERFACE_ID --api-version 2019-04-01 --query 'properties.ipConfigurations[0].properties.privateIPAddress' -o tsv)</p>
<p>az network private-dns record-set a create --name $UNIQUEID-microsweeper-database --zone-name privatelink.postgres.database.azure.com --resource-group $ARORG  </p>
<p>az network private-dns record-set a add-record --record-set-name $UNIQUEID-microsweeper-database --zone-name privatelink.postgres.database.azure.com --resource-group $ARORG -a $POSTGRES_IP
   ```</p>
<ol>
<li>Create a postgres database that will contain scores for the minesweeper application</li>
</ol>
<p><code>bash
   az postgres db create \
   --resource-group $ARORG \
   --name score \
   --server-name microsweeper-database</code></p>
<h2 id="aro-frontdoor-deploy-the-minesweeper-application">Deploy the <a href="https://github.com/redhat-mw-demos/microsweeper-quarkus/tree/ARO">minesweeper application</a></h2>
<ol>
<li>Clone the git repository</li>
</ol>
<p><code>bash
   git clone -b ARO https://github.com/redhat-mw-demos/microsweeper-quarkus.git</code></p>
<ol>
<li>change to the root directory</li>
</ol>
<p><code>bash
   cd microsweeper-quarkus</code></p>
<ol>
<li>Ensure Java 1.8 is set at your Java version</li>
</ol>
<p><code>bash
   mvn --version</code> </p>
<p>Look for Java version - 1.8XXXX
   if not set to Java 1.8 you will need to set your JAVA_HOME variable to Java 1.8 you have installed.  To find your java versions run:</p>
<p><code>bash
   java -version</code></p>
<p>then export your JAVA_HOME variable</p>
<p><code>bash
   export JAVA_HOME=`/usr/libexec/java_home -v 1.8.0_332`</code></p>
<ol>
<li>Log into your openshift cluster<blockquote>
<p>Before you deploy your application, you will need to be connected to a private network that has access to the cluster.</p>
</blockquote>
</li>
</ol>
<p>A great way to establish this connectity is with a VPN connection.  Follow this <a href="#aro-vpn">guide</a> to setup a VPN connection with your Azure account.</p>
<p>```bash
   kubeadmin_password=$(az aro list-credentials --name $AROCLUSTER --resource-group $ARORG --query kubeadminPassword --output tsv)</p>
<p>apiServer=$(az aro show -g $ARORG -n $AROCLUSTER --query apiserverProfile.url -o tsv)</p>
<p>oc login $apiServer -u kubeadmin -p $kubeadmin_password
   ```</p>
<ol>
<li>Create a new OpenShift Project</li>
</ol>
<p><code>bash
   oc new-project minesweeper</code></p>
<ol>
<li>add the openshift extension to quarkus</li>
</ol>
<p><code>bash
   quarkus ext add openshift</code></p>
<ol>
<li>Edit microsweeper-quarkus/src/main/resources/application.properties</li>
</ol>
<p>Make sure your file looks like the one below, changing the IP address on line 3 to the private ip address of your postgres instance.</p>
<p>To find your Postgres private IP address run the following commands:</p>
<p>```bash
   NETWORK_INTERFACE_ID=$(az network private-endpoint show --name postgresPvtEndpoint --resource-group $ARORG --query 'networkInterfaces[0].id' -o tsv)</p>
<p>az resource show --ids $NETWORK_INTERFACE_ID --api-version 2019-04-01 --query 'properties.ipConfigurations[0].properties.privateIPAddress' -o tsv
   ```</p>
<p>Sample microsweeper-quarkus/src/main/resources/application.properties</p>
<p>```
   # Database configurations
   %prod.quarkus.datasource.db-kind=postgresql
   %prod.quarkus.datasource.jdbc.url=jdbc:postgresql://10.1.6.9:5432/score
   %prod.quarkus.datasource.jdbc.driver=org.postgresql.Driver
   %prod.quarkus.datasource.username=quarkus@microsweeper-database
   %prod.quarkus.datasource.password=r3dh4t1!
   %prod.quarkus.hibernate-orm.database.generation=drop-and-create
   %prod.quarkus.hibernate-orm.database.generation=update</p>
<p># OpenShift configurations
   %prod.quarkus.kubernetes-client.trust-certs=true
   %prod.quarkus.kubernetes.deploy=true
   %prod.quarkus.kubernetes.deployment-target=openshift
   #%prod.quarkus.kubernetes.deployment-target=knative
   %prod.quarkus.openshift.build-strategy=docker
   #%prod.quarkus.openshift.expose=true</p>
<p># Serverless configurations
   #%prod.quarkus.container-image.group=microsweeper-%prod.quarkus
   #%prod.quarkus.container-image.registry=image-registry.openshift-image-registry.svc:5000</p>
<p># macOS configurations
   #%prod.quarkus.native.container-build=true
   ```</p>
<ol>
<li>Build and deploy the quarkus application to OpenShift</li>
</ol>
<p><code>bash
   quarkus build --no-tests</code></p>
<ol>
<li>Create a route to your custom domain
   <B>Change the snippet below replacing your hostname for the host:</B></li>
</ol>
<p><code>bash
   cat &lt;&lt; EOF | oc apply -f -
   apiVersion: route.openshift.io/v1
   kind: Route
   metadata:
     labels:
       app.kubernetes.io/name: microsweeper-appservice
       app.kubernetes.io/version: 1.0.0-SNAPSHOT
       app.openshift.io/runtime: quarkus
     name: microsweeper-appservice
     namespace: minesweeper
   spec:
     host: minesweeper.aro.kmobb.com
     to:
       kind: Service
       name: microsweeper-appservice
       weight: 100
       targetPort:
         port: 8080
     wildcardPolicy: None
   EOF</code></p>
<ol>
<li>Check the dns settings of your application.<blockquote>
<p>notice that the application URL is routed through Azure Front Door at the edge.  The only way this application that is running on your cluster can be access is through Azure Front Door which is connected to your cluster through a private endpoint.</p>
</blockquote>
</li>
</ol>
<p><code>bash
   nslookup $ARO_APP_FQDN</code></p>
<p>sample output:</p>
<p>```
   Server:      2600:1700:850:d220::1
   Address: 2600:1700:850:d220::1#53</p>
<p>Non-authoritative answer:
   minesweeper.aro.kmobb.com    canonical name = aro-mine-13947-dxh0ahd7fzfyexgx.z01.azurefd.net.
   aro-mine-13947-dxh0ahd7fzfyexgx.z01.azurefd.net  canonical name = star-azurefd-prod.trafficmanager.net.
   star-azurefd-prod.trafficmanager.net canonical name = dual.part-0013.t-0009.t-msedge.net.
   dual.part-0013.t-0009.t-msedge.net   canonical name = part-0013.t-0009.t-msedge.net.
   Name:    part-0013.t-0009.t-msedge.net
   Address: 13.107.213.41
   Name:    part-0013.t-0009.t-msedge.net
   Address: 13.107.246.41
   ```</p>
<h2 id="aro-frontdoor-test-the-application">Test the application</h2>
<p>Point your broswer to your domain!!
<img src="../aro/frontdoor/images/minesweeper.png"></p>
<h2 id="aro-frontdoor-clean-up">Clean up</h2>
<p>To clean up everything you created, simply delete the resource group</p>
<pre><code class="language-bash">az group delete -g $ARORG
</code></pre></section>
                        <h3 class='nav-section-title' id='section-azure-service-operator'>
                            Azure Service Operator <a class='headerlink' href='#section-azure-service-operator' title='Permanent link'>↵</a>
                        </h3>
                        <section class="print-page" id="aro-azure-service-operator-v1"><h1 id="aro-azure-service-operator-v1-installing-and-using-the-azure-service-operator-aso-v1-in-azure-red-hat-openshift-aro">Installing and Using the Azure Service Operator (ASO) V1 in Azure Red Hat OpenShift (ARO)</h1>
<p><strong>Paul Czarkowski</strong></p>
<p><em>last edit - 02/16/2022</em></p>
<p>The Azure Service Operator (ASO) provides Custom Resource Definitions (CRDs) for Azure resources that can be used to create, update, and delete Azure services from an OpenShift cluster.</p>
<blockquote>
<p>This example uses ASO V1, which has now been replaced by ASO V2. ASO V2 does not (as of 5/19/2022) yet have an entry in the OCP OperatorHub, but is functional and should be preferred for use, especially if V1 isn't already installed on a cluster. MOBB has documented the [install of ASO V2 on ROSA]. MOBB has <strong>not</strong> tested running the two in parallel.</p>
</blockquote>
<h2 id="aro-azure-service-operator-v1-prerequisites">Prerequisites</h2>
<ul>
<li><a href="https://docs.microsoft.com/en-us/cli/azure/install-azure-cli?view=azure-cli-latest">Azure CLI</a></li>
<li><a href="#quickstart-aro">An Azure Red Hat OpenShift (ARO) cluster</a></li>
</ul>
<h2 id="aro-azure-service-operator-v1-prepare-your-azure-account-and-aro-cluster">Prepare your Azure Account and ARO Cluster</h2>
<ol>
<li>
<p>Set the following environment variables:</p>
<blockquote>
<p>Note: modify the cluster name, region and resource group to match your cluster</p>
</blockquote>
<p><code>bash
AZURE_TENANT_ID=$(az account show -o tsv --query tenantId)
AZURE_SUBSCRIPTION_ID=$(az account show -o tsv --query id)
CLUSTER_NAME="openshift"
AZURE_RESOURCE_GROUP="openshift"
AZURE_REGION="eastus"</code></p>
</li>
<li>
<p>Create a Service Principal with Contributor permissions to your subscription:</p>
<blockquote>
<p>Note: You may want to lock this down to a specific resource group.</p>
</blockquote>
<p><code>bash
read -r ASO_USER ASO_PASS &lt; &lt;(az ad sp create-for-rbac -n "$CLUSTER_NAME-ASO" \
  --role contributor --scopes /subscriptions/$AZURE_SUBSCRIPTION_ID -o tsv \
  --query "[name,password]" | xargs)</code></p>
</li>
<li>
<p>Create a secret containing your Service Principal credentials:</p>
<p><code>bash
cat &lt;&lt;EOF | oc apply -f -
apiVersion: v1
kind: Secret
metadata:
  name: azureoperatorsettings
  namespace: openshift-operators
stringData:
  AZURE_TENANT_ID: $AZURE_TENANT_ID
  AZURE_SUBSCRIPTION_ID: $AZURE_SUBSCRIPTION_ID
  AZURE_CLIENT_ID: $ASO_USER
  AZURE_CLIENT_SECRET: $ASO_PASS
  AZURE_CLOUD_ENV: AzurePublicCloud
EOF</code></p>
</li>
<li>
<p>Deploy the ASO Operator:</p>
<p><code>bash
cat &lt;&lt;EOF | oc apply -f -
apiVersion: operators.coreos.com/v1alpha1
kind: Subscription
metadata:
  labels:
operators.coreos.com/azure-service-operator.openshift-operators: ""
  name: azure-service-operator
  namespace: openshift-operators
spec:
  channel: stable
  installPlanApproval: Automatic
  name: azure-service-operator
  source: community-operators
  sourceNamespace: openshift-marketplace
  startingCSV: azure-service-operator.v1.0.28631
EOF</code></p>
</li>
</ol>
<h2 id="aro-azure-service-operator-v1-deploy-an-azure-redis-cache">Deploy an Azure Redis Cache</h2>
<ol>
<li>
<p>Create a Project:</p>
<p><code>bash
oc new-project redis-demo</code></p>
</li>
<li>
<p>Allow the redis app to run as any user:</p>
<p><code>bash
oc adm policy add-scc-to-user anyuid -z default</code></p>
</li>
<li>
<p>Create a random string to use as the unique redis hostname:</p>
<p><code>bash
REDIS_HOSTNAME=$(cat /dev/urandom | tr -dc 'a-z0-9' | fold -w 8 | head -n 1)</code></p>
</li>
<li>
<p>Deploy a Redis service using the ASO Operator and an example application</p>
<p>```
cat &lt;&lt;EOF | oc apply -f -
apiVersion: azure.microsoft.com/v1alpha1
kind: RedisCache
metadata:
  name: $REDIS_HOSTNAME
spec:
  location: $AZURE_REGION
  resourceGroup: $AZURE_RESOURCE_GROUP
  properties:
sku:
  name: Basic
  family: C
  capacity: 1
enableNonSslPort: true</p>
<hr />
<p>apiVersion: apps/v1
kind: Deployment
metadata:
  name: azure-vote-front
spec:
  replicas: 1
  selector:
matchLabels:
  app: azure-vote-front
  template:
metadata:
  labels:
    app: azure-vote-front
spec:
  containers:
  - name: azure-vote-front
    image: mcr.microsoft.com/azuredocs/azure-vote-front:v1
    resources:
      requests:
        cpu: 100m
        memory: 128Mi
      limits:
        cpu: 250m
        memory: 256Mi
    ports:
    - containerPort: 80
    env:
    - name: REDIS_NAME
      value: $REDIS_HOSTNAME
    - name: REDIS
      value: $REDIS_HOSTNAME.redis.cache.windows.net
    - name: REDIS_PWD
      valueFrom:
        secretKeyRef:
          name: rediscache-$REDIS_HOSTNAME
          key: primaryKey</p>
<hr />
<p>apiVersion: v1
kind: Service
metadata:
  name: azure-vote-front
spec:
  ports:
  - port: 80
  selector:
app: azure-vote-front</p>
<hr />
<p>apiVersion: route.openshift.io/v1
kind: Route
metadata:
  name: azure-vote
spec:
  port:
targetPort: 80
  tls:
insecureEdgeTerminationPolicy: Redirect
termination: edge
  to:
kind: Service
name: azure-vote-front
EOF
```</p>
</li>
<li>
<p>Wait for Redis to be ready</p>
<blockquote>
<p>This may take 10 to 15 minutes.</p>
</blockquote>
<p><code>bash
watch oc get rediscache $REDIS_HOSTNAME</code></p>
<p>the output should eventually show the following:</p>
<p><code>NAME       PROVISIONED   MESSAGE
l67for49   true          successfully provisioned</code></p>
</li>
<li>
<p>Get the URL of the example app</p>
<p><code>bash
oc get route azure-vote</code></p>
</li>
<li>
<p>Browse to the URL provided by the previous command and validate that the app is working</p>
</li>
</ol>
<p><img alt="screenshot of voting app" src="../aro/azure-service-operator-v1/vote.png" /></p>
<h2 id="aro-azure-service-operator-v1-cleanup">Cleanup</h2>
<ol>
<li>
<p>Delete the project containing the demo app</p>
<p><code>bash
oc delete project redis-demo</code></p>
</li>
</ol></section><section class="print-page" id="aro-azure-service-operator-v2"><h1 id="aro-azure-service-operator-v2-installing-and-using-the-azure-service-operator-aso-v2-in-azure-red-hat-openshift-aro">Installing and Using the Azure Service Operator (ASO) V2 in Azure Red Hat OpenShift (ARO)</h1>
<p><strong>Thatcher Hubbard</strong></p>
<p>The Azure Service Operator (ASO) provides Custom Resource Definitions (CRDs) for Azure resources that can be used to create, update, and delete Azure services from an OpenShift cluster.</p>
<blockquote>
<p>This example uses ASO V2, which is a replacement for ASO V1. Equivalent documentation for ASO V1 can be found <a href="#azure-service-operator-v1">here</a>. For new installs, V2 is recommended. MOBB has not tested running them in parallel.</p>
</blockquote>
<h2 id="aro-azure-service-operator-v2-prerequisites">Prerequisites</h2>
<ul>
<li><a href="https://docs.microsoft.com/en-us/cli/azure/install-azure-cli?view=azure-cli-latest">Azure CLI</a></li>
<li><a href="#quickstart-aro">An Azure Red Hat OpenShift (ARO) cluster</a></li>
<li>The <code>helm</code> CLI tool</li>
</ul>
<h2 id="aro-azure-service-operator-v2-prepare-your-azure-account-and-aro-cluster">Prepare your Azure Account and ARO Cluster</h2>
<ol>
<li>Install <code>cert-manager</code>:</li>
</ol>
<p>ASO relies on having the CRDs provided by <a href="https://cert-manager.io">cert-manager</a> so it can request self-signed certificates. By default, cert-manager creates an <code>Issuer</code> of type <code>SelfSigned</code>, so it will work for ASO out-of-the-box. On an OpenShift cluster, the easiest way to do this is by using the OCP console, navigating to 'Operators | OperatorHub' and installing it from there; both the Red Hat certified and community versions will work. It's also possible to install by applying manifests directly as <a href="https://docs.openshift.com/container-platform/4.10/operators/admin/olm-adding-operators-to-cluster.html#olm-installing-operator-from-operatorhub-using-cli_olm-adding-operators-to-a-cluster">covered here</a>.</p>
<ol>
<li>
<p>Set the following environment variables:</p>
<blockquote>
<p>Note: modify the cluster name, region and resource group to match your cluster</p>
</blockquote>
<p><code>bash
AZURE_TENANT_ID=$(az account show -o tsv --query tenantId)
AZURE_SUBSCRIPTION_ID=$(az account show -o tsv --query id)
CLUSTER_NAME="test-cluster"
AZURE_RESOURCE_GROUP="test-rg"
AZURE_REGION="westus2"</code></p>
</li>
<li>
<p>Create a Service Principal with Contributor permissions to your subscription:</p>
<blockquote>
<p>Note: You may want to lock this down to a specific resource group.</p>
</blockquote>
<p><code>bash
az ad sp create-for-rbac -n "$CLUSTER_NAME-aso" \
--role contributor --scopes /subscriptions/$AZURE_SUBSCRIPTION_ID</code></p>
<p>The result should look something like this:</p>
<p><code>json
{
  "appId": "12f48391-31ac-4565-936a-8249232aeb18",
  "displayName": "test-cluster-aso",
  "password": "xsr5Pz3IsPnnYxhsc7LhnNkY00cYxe.IPk",
  "tenant": "xxxxxxxx-xxxx-xxxx-xxxx-xxxxxxxxxxxx"
}</code></p>
<p>You'll need two of these values for the Helm deploy of ASO:</p>
<p><code>bash
AZURE_CLIENT_ID=&lt;the_appId_from_above&gt;
AZURE_CLIENT_SECRET=&lt;the_password_from_above&gt;</code></p>
</li>
<li>
<p>Deploy the ASO Operator using Helm:</p>
<p>First, add the ASO repo (this may already be present, Helm will thow a status message if so):</p>
<p><code>bash
helm repo add aso2 https://raw.githubusercontent.com/Azure/azure-service-operator/main/v2/charts</code></p>
<p>Then install the operator itself:</p>
<p><code>bash
helm upgrade --install --devel aso2 aso2/azure-service-operator \
    --create-namespace \
    --namespace=azureserviceoperator-system \
    --set azureSubscriptionID=$AZURE_SUBSCRIPTION_ID \
    --set azureTenantID=$AZURE_TENANT_ID \
    --set azureClientID=$AZURE_CLIENT_ID \
    --set azureClientSecret=$AZURE_CLIENT_SECRET</code></p>
<p>It will typically take 2-3 minutes for resources to converge and for the controller to be read to provision Azure resources. There will be one Pod created in the <code>azureserviceoperator-system</code> namespace with two containers, an <code>oc -n azureserviceoperator-system logs &lt;pod_name&gt; manager</code> will likely show a string of 'TLS handshake error' messages as the operator waits for a Certificate to be issued, but when they stop, the operator will be ready.</p>
</li>
</ol>
<h2 id="aro-azure-service-operator-v2-deploy-an-azure-redis-cache">Deploy an Azure Redis Cache</h2>
<ol>
<li>
<p>Create a Project:</p>
<p><code>bash
oc new-project redis-demo</code></p>
</li>
<li>
<p>Allow the redis app to run as any user:</p>
<p><code>bash
oc adm policy add-scc-to-user anyuid -z redis-demo</code></p>
</li>
<li>
<p>Create an Azure Resource Group to hold project resources. Make sure the <code>namespace</code> matches the project name, and that the <code>location</code> is in the same region the cluster is:</p>
</li>
</ol>
<pre><code class="language-bash">cat &lt;&lt;EOF | oc apply -f -
apiVersion: resources.azure.com/v1beta20200601
kind: ResourceGroup
metadata:
  name: redis-demo
  namespace: redis-demo
spec:
  location: westus
EOF
</code></pre>
<ol>
<li>Deploy a Redis service using the ASO Operator. This also shows creating a random string as part of the hostname because the Azure DNS namespace is global, and a name like <code>sampleredis</code> is likely to be taken. Also make sure the location spec matches.</li>
</ol>
<pre><code class="language-yaml">REDIS_HOSTNAME=redis-$(head -c24 &lt; /dev/random | base64 | LC_CTYPE=C tr -dc 'a-z0-9' | cut -c -8)
cat &lt;&lt;EOF | oc apply -f -
apiVersion: cache.azure.com/v1beta20201201
kind: Redis
metadata:
  name: $REDIS_HOSTNAME
  namespace: redis-demo
spec:
  location: westus
  owner:
    name: redis-demo
  sku:
    family: C
    name: Basic
    capacity: 0
  enableNonSslPort: true
  redisConfiguration:
    maxmemory-delta: &quot;10&quot;
    maxmemory-policy: allkeys-lru
  redisVersion: &quot;6&quot;
  operatorSpec:
    secrets:
      primaryKey:
        name: redis-secret
        key: primaryKey
      secondaryKey:
        name: redis-secret
        key: secondaryKey
      hostName:
        name: redis-secret
        key: hostName
      port:
        name: redis-secret
        key: port
EOF
</code></pre>
<p>This will take a couple of minutes to complete as well. Also note that there is typically a bit of lag between a resource being created and showing up in the Azure Portal.</p>
<ol>
<li>Deploy the sample application</li>
</ol>
<p>This uses a published sample application from Microsoft:</p>
<pre><code class="language-yaml">cat &lt;&lt;EOF | oc -n redis-demo apply -f -
apiVersion: apps/v1
kind: Deployment
metadata:
  name: azure-vote-front
spec:
  replicas: 1
  selector:
    matchLabels:
      app: azure-vote-front
  template:
    metadata:
      labels:
        app: azure-vote-front
    spec:
      containers:
      - name: azure-vote-front
        image: mcr.microsoft.com/azuredocs/azure-vote-front:v1
        resources:
          requests:
            cpu: 100m
            memory: 128Mi
          limits:
            cpu: 250m
            memory: 256Mi
        ports:
        - containerPort: 80
        env:
        - name: REDIS
          valueFrom:
            secretKeyRef:
              name: redis-secret
              key: hostName
        - name: REDIS_NAME
          value: $REDIS_HOSTNAME
        - name: REDIS_PWD
          valueFrom:
            secretKeyRef:
              name: redis-secret
              key: primaryKey
---
apiVersion: v1
kind: Service
metadata:
  name: azure-vote-front
spec:
  ports:
  - port: 80
  selector:
    app: azure-vote-front
---
apiVersion: route.openshift.io/v1
kind: Route
metadata:
  name: azure-vote
spec:
  port:
    targetPort: 80
  tls:
    insecureEdgeTerminationPolicy: Redirect
    termination: edge
  to:
    kind: Service
    name: azure-vote-front
EOF
</code></pre>
<ol>
<li>
<p>Get the URL of the example app</p>
<p><code>bash
oc get route azure-vote</code></p>
</li>
<li>
<p>Browse to the URL provided by the previous command and validate that the app is working</p>
</li>
</ol>
<p><img alt="screenshot of voting app" src="../aro/azure-service-operator-v2/vote.png" /></p>
<h2 id="aro-azure-service-operator-v2-cleanup">Cleanup</h2>
<ol>
<li>
<p>Delete the project containing the demo app</p>
<p><code>bash
oc delete project redis-demo</code></p>
</li>
</ol>
<h2 id="aro-azure-service-operator-v2-further-resources">Further Resources</h2>
<p>There is a library of examples for creating various Azure resource types here: <a href="https://github.com/Azure/azure-service-operator/tree/main/v2/config/samples">https://github.com/Azure/azure-service-operator/tree/main/v2/config/samples</a></p></section><h1 class='nav-section-title-end'>Ended: Azure Service Operator</h1><h1 class='nav-section-title-end'>Ended: ARO</h1>
                        <h2 class='nav-section-title' id='section-rosa'>
                            ROSA <a class='headerlink' href='#section-rosa' title='Permanent link'>↵</a>
                        </h2>
                        <section class="print-page" id="rosa-private-link"><h1 id="rosa-private-link-creating-a-rosa-cluster-with-private-link-enabled">Creating a ROSA cluster with Private Link enabled</h1>
<h2 id="rosa-private-link-prerequisites">Prerequisites</h2>
<ul>
<li><a href="https://docs.aws.amazon.com/cli/latest/userguide/install-cliv2.html">AWS CLI</a></li>
<li><a href="https://github.com/openshift/rosa/releases/tag/v1.0.8">Rosa CLI</a> v1.0.8</li>
<li><a href="https://stedolan.github.io/jq/download/">jq</a></li>
</ul>
<h2 id="rosa-private-link-create-vpc-and-subnets">Create VPC and Subnets</h2>
<p>The following instructions use the AWS CLI to create the necessary networking to deploy a Private Link ROSA cluster into a Single AZ and are intended to be a guide. Ideally you would use an Automation tool like Ansible or Terraform to manage your VPCs.</p>
<blockquote>
<p>When creating subnets, make sure that subnet(s) are created to availability zone that has ROSA instances types available. If AZ is not "forced", subnet is created to random AZ in the region. Force AZ using <code>--availability-zone</code> argument in <code>create-subnet</code> command. Use <code>rosa list instance-types</code> to list ROSA instance types and check available types availability in AZ with <code>aws ec2 describe-instance-type-offerings --location-type availability-zone --filters Name=location,Values=AZ_NAME_HERE --region REGION_HERE --output text | egrep "YOU_PREFERRED_INSTANCE_TYPE"</code>. As an example, you cannot install ROSA to <code>us-east-1e</code> AZ, but <code>us-east-1b</code> works fine.</p>
</blockquote>
<h3 id="rosa-private-link-option-1-vpc-with-a-private-subnet-and-aws-site-to-site-vpn-access">Option 1 - VPC with a private subnet and AWS Site-to-Site VPN access.</h3>
<p>Todo</p>
<h3 id="rosa-private-link-option-2-vpc-with-public-and-private-subnets-and-aws-site-to-site-vpn-access">Option 2 - VPC with public and private subnets and AWS Site-to-Site VPN access</h3>
<p>Todo</p>
<h3 id="rosa-private-link-option-3-vpc-with-public-and-private-subnets-nat">Option 3 - VPC with public and private subnets (NAT)</h3>
<p>This will create both a Private and Public subnet. All cluster resources will live in the private subnet, the public subnet only exists to NAT the egress traffic to the Internet.</p>
<p><img alt="architecture diagram showing privatelink with public subnet" src="../rosa/private-link/images/architecture-pl.png" /></p>
<blockquote>
<p>As an alternative use the Terraform instructions provided <a href="#rosa-byo-vpc">here</a> then skip down to the rosa create command.</p>
</blockquote>
<ol>
<li>
<p>Set a Cluster name</p>
<p><code>ROSA_CLUSTER_NAME=private-link</code></p>
</li>
<li>
<p>Create a VPC to install a ROSA cluster into</p>
<p><code>``
VPC_ID=</code>aws ec2 create-vpc --cidr-block 10.0.0.0/16 | jq -r .Vpc.VpcId`</p>
<p>aws ec2 create-tags --resources $VPC_ID \
  --tags Key=Name,Value=$ROSA_CLUSTER_NAME | jq .</p>
<p>aws ec2 modify-vpc-attribute --vpc-id $VPC_ID --enable-dns-hostnames | jq .
```</p>
</li>
<li>
<p>Create a Public Subnet for the cluster to NAT egress traffic out of</p>
<p><code>``bash
PUBLIC_SUBNET=</code>aws ec2 create-subnet --vpc-id $VPC_ID --cidr-block 10.0.128.0/17 | jq -r .Subnet.SubnetId`</p>
<p>aws ec2 create-tags --resources $PUBLIC_SUBNET \
  --tags Key=Name,Value=$ROSA_CLUSTER_NAME-public | jq .
```</p>
</li>
<li>
<p>Create a Private Subnet for the cluster machines to live in</p>
<p><code>``bash
PRIVATE_SUBNET=</code>aws ec2 create-subnet --vpc-id $VPC_ID --cidr-block 10.0.0.0/17 | jq -r .Subnet.SubnetId`</p>
<p>aws ec2 create-tags --resources $PRIVATE_SUBNET \
  --tags Key=Name,Value=$ROSA_CLUSTER_NAME-private | jq .
```</p>
</li>
<li>
<p>Create an Internet Gateway for NAT egress traffic</p>
<p><code>``bash
I_GW=</code>aws ec2 create-internet-gateway | jq -r .InternetGateway.InternetGatewayId`
aws ec2 attach-internet-gateway --vpc-id $VPC_ID --internet-gateway-id $I_GW | jq .</p>
<p>aws ec2 create-tags --resources $I_GW \
  --tags Key=Name,Value=$ROSA_CLUSTER_NAME | jq .
```</p>
</li>
<li>
<p>Create a Route Table for NAT egress traffic</p>
<p><code>``bash
R_TABLE=</code>aws ec2 create-route-table --vpc-id $VPC_ID | jq -r .RouteTable.RouteTableId`</p>
<p>aws ec2 create-route --route-table-id $R_TABLE --destination-cidr-block 0.0.0.0/0 --gateway-id $I_GW | jq .</p>
<p>aws ec2 describe-route-tables --route-table-id $R_TABLE | jq .</p>
<p>aws ec2 associate-route-table --subnet-id $PUBLIC_SUBNET --route-table-id $R_TABLE | jq .</p>
<p>aws ec2 create-tags --resources $R_TABLE \
  --tags Key=Name,Value=$ROSA_CLUSTER_NAME | jq .
```</p>
</li>
<li>
<p>Create a NAT Gateway for the Private network</p>
<p><code>``bash
EIP=</code>aws ec2 allocate-address --domain vpc | jq -r .AllocationId<code>NAT_GW=</code>aws ec2 create-nat-gateway --subnet-id $PUBLIC_SUBNET \
  --allocation-id $EIP | jq -r .NatGateway.NatGatewayId`</p>
<p>aws ec2 create-tags --resources $EIP --resources $NAT_GW \
  --tags Key=Name,Value=$ROSA_CLUSTER_NAME | jq .
```</p>
</li>
<li>
<p>Create a Route Table for the Private subnet to the NAT</p>
<p><code>``bash
R_TABLE_NAT=</code>aws ec2 create-route-table --vpc-id $VPC_ID | jq -r .RouteTable.RouteTableId`</p>
<p>while ! aws ec2 describe-route-tables --route-table-id $R_TABLE_NAT \
  | jq .; do sleep 1; done</p>
<p>aws ec2 create-route --route-table-id $R_TABLE_NAT --destination-cidr-block 0.0.0.0/0 --gateway-id $NAT_GW | jq .</p>
<p>aws ec2 associate-route-table --subnet-id $PRIVATE_SUBNET --route-table-id $R_TABLE_NAT | jq .</p>
<p>aws ec2 create-tags --resources $R_TABLE_NAT $EIP \
  --tags Key=Name,Value=$ROSA_CLUSTER_NAME-private | jq .
```</p>
</li>
</ol>
<h2 id="rosa-private-link-deploy-rosa">Deploy ROSA</h2>
<ol>
<li>
<p>Create ROSA cluster in the private subnet</p>
<p><code>bash
rosa create cluster --private-link \
  --cluster-name=$ROSA_CLUSTER_NAME \
  --machine-cidr=10.0.0.0/16 \
  --subnet-ids=$PRIVATE_SUBNET</code></p>
</li>
</ol>
<h2 id="rosa-private-link-test-connectivity">Test Connectivity</h2>
<ol>
<li>
<p>Create an Instance to use as a jump host</p>
<p><strong>TODO: CLI instructions</strong></p>
<p><strong>Through the GUI:</strong></p>
<ol>
<li>
<p>Navigate to the EC2 console and launch a new instance</p>
</li>
<li>
<p>Select the AMI for your instance, if you don't have a standard, the Amazon Linux 2 AMI works just fine</p>
</li>
<li>
<p>Choose your instance type, the t2.micro/free tier is sufficient for our needs, and click <strong>Next: Configure Instance Details</strong></p>
</li>
<li>
<p>Change the <strong>Network</strong> settings to setup this host inside your <em>private-link</em> VPC
  <img alt="select the private link vpc from dropdown" src="../rosa/private-link/images/rosa-private-link-bastion-network.png" /></p>
</li>
<li>
<p>Change the <strong>Subnet</strong> setting to use the <em>private-link-public</em> subnet
  <img alt="select the subnet named private-link-public from the dropdown" src="../rosa/private-link/images/rosa-private-link-bastion-subnet.png" /></p>
</li>
<li>
<p>Change <strong>Auto-assign Public IP</strong> to <em>Enable</em>
  <img alt="select Enable from dropdown" src="../rosa/private-link/images/rosa-private-linke-bastion-PublicIP.png" /></p>
</li>
<li>
<p>Default settings for Storage and Tags are OK, if you do not need to change them for your own reasons, select <strong>6. Configure Security Group</strong> from the top navigation or click through using the <strong>Next</strong> buttons</p>
</li>
<li>
<p>If you already have a security group created to allow access from your computer to AWS, choose <strong>Select an existing security group</strong> and choose that group from the list and skip to <strong>Review and Launch</strong>. Otherwise, select <strong>Create a new security group</strong> and continue.</p>
</li>
<li>
<p>To allow access only from your current public IP, change the <strong>Source</strong> heading to use <em>My IP</em>
  <img alt="use the MY IP option from the Source dropdown" src="../rosa/private-link/images/rosa-private-linke-bastion-securityGroup.png" /></p>
</li>
<li>
<p>Click <strong>Review and Launch</strong>, verify all settings are correct and follow the standard AWS instructions for finalizing the setup and selecting/creating the security keys.</p>
</li>
<li>
<p>Once launched, open the instance summary for the jump host instance and note the public IP address.</p>
</li>
</ol>
</li>
<li>
<p>Create a ROSA admin user and save the login command for use later</p>
<p><code>rosa create admin -c $ROSA_CLUSTER_NAME</code></p>
</li>
<li>
<p>Note the DNS name of your private cluster, use the <code>rosa describe</code> command if needed</p>
</li>
</ol>
<p><code>rosa describe cluster -c private-link</code></p>
<ol>
<li>
<p>update /etc/hosts to point the openshift domains to localhost. Use the DNS of your openshift cluster as described in the previous step in place of <code>$YOUR_OPENSHIFT_DNS</code> below</p>
<p><code>127.0.0.1 api.$YOUR_OPENSHIFT_DNS
127.0.0.1 console-openshift-console.apps.$YOUR_OPENSHIFT_DNS
127.0.0.1 oauth-openshift.apps.$YOUR_OPENSHIFT_DNS</code></p>
</li>
<li>
<p>SSH to that instance, tunneling traffic for the appropriate hostnames. Be sure to use your new/existing private key, the OpenShift DNS for <code>$YOUR_OPENSHIFT_DNS</code> and your jump host IP for <code>$YOUR_EC2_IP</code></p>
<p><code>bash
  sudo ssh -i PATH/TO/YOUR_KEY.pem \
  -L 6443:api.$YOUR_OPENSHIFT_DNS:6443 \
  -L 443:console-openshift-console.apps.$YOUR_OPENSHIFT_DNS:443 \
  -L 80:console-openshift-console.apps.$YOUR_OPENSHIFT_DNS:80 \
   ec2-user@$YOUR_EC2_IP</code></p>
</li>
<li>
<p>Log into the cluster using oc login command from the create admin command above. ex.</p>
<p><code>bash
oc login https://api.private-test.3d1n.p1.openshiftapps.com:6443 --username cluster-admin --password GQSGJ-daqfN-8QNY3-tS9gU</code></p>
</li>
<li>
<p>Check that you can access the Console by opening the console url in your browser.</p>
</li>
</ol>
<h2 id="rosa-private-link-cleanup">Cleanup</h2>
<ol>
<li>
<p>Delete ROSA</p>
<p><code>bash
rosa delete cluster -c $ROSA_CLUSTER_NAME -y</code></p>
</li>
<li>
<p>Delete AWS resources</p>
<p><code>bash
aws ec2 delete-nat-gateway --nat-gateway-id $NAT_GW | jq .
aws ec2 release-address --allocation-id=$EIP | jq .
aws ec2 detach-internet-gateway --vpc-id $VPC_ID \
  --internet-gateway-id $I_GW | jq .
aws ec2 delete-subnet --subnet-id=$PRIVATE_SUBNET | jq .
aws ec2 delete-subnet --subnet-id=$PUBLIC_SUBNET | jq .
aws ec2 delete-route-table --route-table-id=$R_TABLE | jq .
aws ec2 delete-route-table --route-table-id=$R_TABLE_NAT | jq .
aws ec2 delete-vpc --vpc-id=$VPC_ID | jq .</code></p>
</li>
</ol></section>
                        <h3 class='nav-section-title' id='section-secure-token-service-sts-'>
                            Secure Token Service (STS) <a class='headerlink' href='#section-secure-token-service-sts-' title='Permanent link'>↵</a>
                        </h3>
                        <section class="print-page" id="rosa-sts"><h1 id="rosa-sts-creating-a-rosa-cluster-in-sts-mode">Creating a ROSA cluster in STS mode</h1>
<p><strong>Paul Czarkowski</strong></p>
<p><em>Last updated 05/31/2022</em></p>
<blockquote>
<p><strong>Tip</strong> The official documentation for installing a ROSA cluster in STS mode can be found <a href="https://docs.openshift.com/rosa/rosa_getting_started_sts/rosa-sts-getting-started-workflow.html">here</a>.</p>
</blockquote>
<p>STS allows us to deploy ROSA without needing a ROSA admin account, instead it uses roles and policies with Amazon STS (secure token service) to gain access to the AWS resources needed to install and operate the cluster.</p>
<p>This is a summary of the <a href="https://docs.openshift.com/rosa/rosa_getting_started/rosa-sts-getting-started-workflow.html">official docs</a> that can be used as a line by line install guide and later used as a basis for automation in your <a href="https://github.com/ansible/ansible">favorite automation tool</a>.</p>
<blockquote>
<p>Note that some commands (OIDC for STS) will be hard coded to US-EAST-1, do not be tempted to change these to use $REGION instead or you will fail installation.</p>
</blockquote>
<h2 id="rosa-sts-prerequisites">Prerequisites</h2>
<ul>
<li><a href="https://docs.aws.amazon.com/cli/latest/userguide/install-cliv2.html">AWS CLI</a></li>
<li><a href="https://github.com/openshift/rosa/releases/tag/v1.2.2">Rosa CLI</a> v1.2.2</li>
<li>OpenShift CLI - <code>rosa download openshift-client</code></li>
<li><a href="https://stedolan.github.io/jq/download/">jq</a></li>
</ul>
<h3 id="rosa-sts-prepare-local-environment">Prepare local environment</h3>
<ol>
<li>
<p>set some environment variables</p>
<p><code>bash
export VERSION=4.10.15 \
       ROSA_CLUSTER_NAME=mycluster \
       AWS_ACCOUNT_ID=`aws sts get-caller-identity --query Account --output text` \
       REGION=us-east-2 \
       AWS_PAGER=""</code></p>
</li>
</ol>
<h3 id="rosa-sts-prepare-aws-and-red-hat-accounts">Prepare AWS and Red Hat accounts</h3>
<ol>
<li>
<p>If this is your first time deploying ROSA you need to do some preparation as described <a href="#quickstart-rosa-Prerequisites">here</a>. Stop just before running <code>rosa init</code> we don't need to do that for STS mode.</p>
</li>
<li>
<p>If this is a brand new AWS account that has never had a AWS Load Balancer installed in it, you should run the following</p>
<p><code>bash
aws iam create-service-linked-role --aws-service-name \
"elasticloadbalancing.amazonaws.com"</code></p>
</li>
<li>
<p>Associate your AWS account   </p>
</li>
</ol>
<p>To perform ROSA cluster provisioning tasks, you must create ocm-role and user-role IAM resources in your AWS account and link them to your Red Hat organization.</p>
<p><br>
   <b>OCM Role</b><br>
   The first role you will create is the ocm-role which the OpenShift Cluster Manager will use to be able to administer and Create ROSA clusters. </p>
<p>If you haven't already created the ocm-role, you can create and link the role with one command.
   <code>bash
   rosa create ocm-role</code></p>
<blockquote>
<p><strong>Tip</strong> If you have multiple AWS accounts that you want to associate with your Red Hat Organization, you can use the <code>--profile</code> option to specify the AWS profile you would like to associate.</p>
</blockquote>
<p>If you have already created the ocm-role, you can just link the ocm-role to your Red Hat organization.  </p>
<p><code>bash
   rosa link ocm-user --role-arm &lt;arn&gt;</code></p>
<blockquote>
<p><strong>Tip</strong> You can get your OCM role arn from AWS IAM: 
   <code>bash
   aws iam list-roles | grep OCM</code></p>
</blockquote>
<p><br>
   <b>User Role</b><br>
   The second is the user-role that allows OCM to verify that users creating a cluster have access to the current AWS account.</p>
<p>If you haven't already created the user-role, you can create and link the role with one command.</p>
<p><code>bash
   rosa create user-role</code></p>
<blockquote>
<p><strong>Tip</strong> If you have multiple AWS accounts that you want to associate with your Red Hat Organization, you can use the <code>--profile</code> option to specify the AWS profile you would like to associate.</p>
</blockquote>
<p><br>
   If you have already created the user-role, you can just link the user-role to your Red Hat organization.</p>
<p><code>bash
   rosa link user-role --role-arn &lt;arn&gt;</code></p>
<blockquote>
<p><strong>Tip</strong> You can get your User role arn from the ROSA cli: <code>rosa whoami</code></p>
</blockquote>
<p>look for the <code>AWS ARN:</code> field
   <br></p>
<h2 id="rosa-sts-deploy-rosa-cluster">Deploy ROSA cluster</h2>
<ol>
<li>
<p>Make you your ROSA CLI version is correct (v1.2.2 or higher)</p>
<p><code>bash
rosa version</code>
1. Run the rosa cli to create your cluster</p>
<blockquote>
<p>You can run the command as provided in the ouput of the previous step to deploy in interactive mode.</p>
<p>Add any other arguments to this command to suit your cluster. for example <code>--private-link</code> and <code>--subnet-ids=subnet-12345678,subnet-87654321</code>.</p>
</blockquote>
<p><code>bash
rosa create cluster --sts --cluster-name ${ROSA_CLUSTER_NAME} \
  --region ${REGION} --version ${VERSION} --mode auto -y</code></p>
</li>
<li>
<p>Validate The cluster is now installing</p>
<p>The State should have moved beyond <code>pending</code> and show <code>installing</code> or <code>ready</code>.</p>
<p><code>bash
watch "rosa describe cluster -c $ROSA_CLUSTER_NAME"</code></p>
</li>
<li>
<p>Watch the install logs</p>
<p><code>bash
rosa logs install -c $ROSA_CLUSTER_NAME --watch --tail 10</code></p>
</li>
</ol>
<h2 id="rosa-sts-validate-the-cluster">Validate the cluster</h2>
<p>Once the cluster has finished installing we can validate we can access it</p>
<ol>
<li>
<p>Create an Admin user</p>
<p><code>bash
rosa create admin -c $ROSA_CLUSTER_NAME</code></p>
</li>
<li>
<p>Wait a few moments and run the <code>oc login</code> command it provides.</p>
</li>
</ol>
<h2 id="rosa-sts-cleanup">Cleanup</h2>
<ol>
<li>
<p>Delete the ROSA cluster</p>
<p><code>bash
rosa delete cluster -c $ROSA_CLUSTER_NAME</code>
1. Clean up the STS roles</p>
</li>
</ol>
<p>Once the cluster is deleted we can delete the STS roles.</p>
<pre><code>&gt; Note you can get the correct commands with the ID filled in from the output of the previous step.

```bash
rosa delete operator-roles -c &lt;id&gt; --yes --mode auto
rosa delete oidc-provider -c &lt;id&gt;  --yes --mode auto
```
</code></pre></section><h1 class='nav-section-title-end'>Ended: Secure Token Service (STS)</h1><h1 class='nav-section-title-end'>Ended: ROSA</h1>
                        <h2 class='nav-section-title' id='section-gcp'>
                            GCP <a class='headerlink' href='#section-gcp' title='Permanent link'>↵</a>
                        </h2>
                        <section class="print-page" id="gcp-osd_preexisting_vpc"><h1 id="gcp-osd_preexisting_vpc-creating-a-osd-in-gcp-with-existing-vpcs">Creating a OSD in GCP with Existing VPCs</h1>
<p><strong>Roberto Carratalá, Andrea Bozzoni</strong></p>
<p><em>Last updated 07/06/2022</em></p>
<blockquote>
<p><strong>Tip</strong> The official documentation for installing a OSD cluster in GCP can be found <a href="https://docs.openshift.com/dedicated/osd_cluster_create/creating-a-gcp-cluster.html">here</a>.</p>
</blockquote>
<p>For deploy an OSD cluster in GCP using existing Virtual Private Cloud (VPC) you need to implement some prerequisites that you must create before starting the OpenShift Dedicated installation though the OCM.</p>
<h2 id="gcp-osd_preexisting_vpc-prerequisites">Prerequisites</h2>
<ul>
<li><a href="https://cloud.google.com/sdk/gcloud">gcloud CLI</a></li>
<li><a href="https://stedolan.github.io/jq/download/">jq</a></li>
</ul>
<p>NOTE: Also the GCloud Shell can be used, and have the gcloud cli among other tools preinstalled.</p>
<h2 id="gcp-osd_preexisting_vpc-generate-gcp-vpc-and-subnets">Generate GCP VPC and Subnets</h2>
<p>This is a diagram showing the GCP infra prerequisites that are needed for the OSD installation:</p>
<p><img alt="GCP Prereqs" src="../gcp/images/osd-prereqs.png" />{: width="750" }</p>
<p>To deploy the GCP VPC and subnets among other prerequisites for install the OSD in GCP using the preexisting VPCs you have two options:</p>
<ul>
<li><strong>Option 1</strong> - GCloud CLI</li>
<li><strong>Option 2</strong> - Terraform Automation</li>
</ul>
<p>Please select one of these two options and proceed with the OSD install steps.</p>
<h3 id="gcp-osd_preexisting_vpc-option-1-generate-osd-vpc-and-subnets-using-gcloud-cli">Option 1 - Generate OSD VPC and Subnets using GCloud CLI</h3>
<p>As mentioned before, for deploy OSD in GCP using existing GCP VPC, you need to provide and create beforehand a GCP VPC and two subnets (one for the masters and another for the workers nodes).</p>
<ol>
<li>Login and configure the proper GCP project where the OSD will be deployed:</li>
</ol>
<p><code>sh
   export PROJECT_NAME=&lt;google project name&gt;
   gcloud auth list
   gcloud config set project $PROJECT_NAME
   gcloud config list project</code></p>
<ol>
<li>Export the names of the vpc and subnets:</li>
</ol>
<p><code>sh
   export REGION=&lt;region name&gt;
   export OSD_VPC=&lt;vpc name&gt;
   export MASTER_SUBNET=&lt;master subnet name&gt;
   export WORKER_SUBNET=&lt;worker subnet name&gt;</code></p>
<ol>
<li>Create a custom mode VPC network:</li>
</ol>
<p><code>sh
   gcloud compute networks create $OSD_VPC --subnet-mode=custom
   gcloud compute networks describe $OSD_VPC</code></p>
<p>NOTE: we need to create the mode custom for the VPC network, because the auto mode generates automatically the subnets with IPv4 ranges with <a href="https://cloud.google.com/vpc/docs/subnets#ip-ranges">predetermined set of ranges</a>.</p>
<ol>
<li>This example is using the standard configuration for these two subnets:</li>
</ol>
<p><code>md
   master-subnet - CIDR 10.0.0.0/17   - Gateway 10.0.0.1
   worker-subnet - CIDR 10.0.128.0/17 - Gateway 10.0.128.1</code></p>
<ol>
<li>Create the GCP Subnets for the masters and workers within the previous GCP VPC network:</li>
</ol>
<p>```sh
   gcloud compute networks subnets create $MASTER_SUBNET \
   --network=$OSD_VPC --range=10.0.0.0/17 --region=$REGION</p>
<p>gcloud compute networks subnets create $WORKER_SUBNET \
   --network=$OSD_VPC --range=10.0.128.0/17 --region=$REGION
   ```</p>
<p><img alt="GCP VPC and Subnets" src="../gcp/images/osd-gcp1.png" />{: width="750" }</p>
<ol>
<li>Once the VPC and the two subnets are provided it is needed to create one <a href="https://cloud.google.com/network-connectivity/docs/router/how-to/create-router-vpc-on-premises-network">GCP Cloud Router</a>:</li>
</ol>
<p>```sh
   export OSD_ROUTER=<router name></p>
<p>gcloud compute routers create $OSD_ROUTER \
   --project=$PROJECT_NAME --network=$OSD_VPC --region=$REGION
   ```</p>
<p><img alt="GCP Routers" src="../gcp/images/osd-gcp2.png" />{:style="display:block; margin-left:auto; margin-right:auto"}{: width="350" }</p>
<ol>
<li>
<p>Then, we will deploy two <a href="https://cloud.google.com/nat/docs/set-up-manage-network-address-translation#gcloud">GCP Cloud NATs</a> and attach them within the GCP Router:</p>
<ul>
<li>Generate the GCP Cloud Nat for the Master Subnets:</li>
</ul>
<p>```sh
export NAT_MASTER=<master subnet name></p>
</li>
</ol>
<p>gcloud compute routers nats create $NAT_MASTER \
   --region=$REGION                               \
   --router=$OSD_ROUTER                           \
   --auto-allocate-nat-external-ips               \
   --nat-custom-subnet-ip-ranges=$MASTER_SUBNET
    ```</p>
<p><img alt="GCP Nat Master" src="../gcp/images/osd-gcp3.png" />{:style="display:block; margin-left:auto; margin-right:auto"}{: width="350" }</p>
<pre><code>* Generate the GCP Cloud NAT for the Worker Subnets:

```sh
export NAT_WORKER=&lt;worker subnet name&gt;
</code></pre>
<p>gcloud compute routers nats create $NAT_WORKER \
       --region=$REGION                           \
       --router=$OSD_ROUTER                       \
       --auto-allocate-nat-external-ips           \
       --nat-custom-subnet-ip-ranges=$WORKER_SUBNET
   ```</p>
<p><img alt="GCP Nat Worker" src="../gcp/images/osd-gcp4.png" />{:style="display:block; margin-left:auto; margin-right:auto"}{: width="350" }</p>
<ol>
<li>As you can check the Cloud NATs GW are attached now to the Cloud Router:</li>
</ol>
<p><img alt="GCP Nat Master" src="../gcp/images/osd-gcp5.png" />{:style="display:block; margin-left:auto; margin-right:auto"}{: width="350" }</p>
<h3 id="gcp-osd_preexisting_vpc-option-2-deploy-osd-vpc-and-subnets-using-terraform">Option 2 - Deploy OSD VPC and Subnets using Terraform</h3>
<p>You can use also <a href="https://github.com/rh-mobb/tf-osd-gcp">automation code in Terraform</a> to deploy all the GCP infrastructure required to deploy the OSD in preexistent VPCs.</p>
<ul>
<li>Clone the tf-osd-gcp repository:</li>
</ul>
<pre><code class="language-bash">git clone https://github.com/rh-mobb/tf-osd-gcp.git
cd tf-osd-gcp
</code></pre>
<ul>
<li>Copy and modify the tfvars file in order to custom to your scenario:</li>
</ul>
<pre><code class="language-bash">cp -pr terraform.tfvars.example terraform.tfvars
</code></pre>
<ul>
<li>Deploy the network infrastructure in GCP needed for deploy the OSD cluster:</li>
</ul>
<pre><code class="language-bash">make all
</code></pre>
<h2 id="gcp-osd_preexisting_vpc-install-the-osd-cluster-using-pre-existent-vpcs">Install the OSD cluster using pre-existent VPCs</h2>
<p>These steps are based in the <a href="https://docs.openshift.com/dedicated/osd_install_access_delete_cluster/creating-a-gcp-cluster.html#osd-create-gcp-cluster-ccs_osd-creating-a-cluster-on-gcp">official OSD installation documentation</a>.</p>
<ol>
<li>
<p>Log in to OpenShift Cluster Manager and click Create cluster.</p>
</li>
<li>
<p>In the Cloud tab, click Create cluster in the Red Hat OpenShift Dedicated row.</p>
</li>
<li>
<p>Under Billing model, configure the subscription type and infrastructure type
<img alt="OSD Install" src="../gcp/images/osd-gcp6.png" />{:style="display:block; margin-left:auto; margin-right:auto"}{: width="750" }</p>
</li>
<li>
<p>Select Run on Google Cloud Platform.</p>
</li>
<li>
<p>Click Prerequisites to review the prerequisites for installing OpenShift Dedicated on GCP with CCS.</p>
</li>
<li>
<p>Provide your GCP service account private key in JSON format. You can either click Browse to locate and attach a JSON file or add the details in the Service account JSON field.
<img alt="OSD Install" src="../gcp/images/osd-gcp7.png" />{:style="display:block; margin-left:auto; margin-right:auto"}{: width="750" }</p>
</li>
<li>
<p>Validate your cloud provider account and then click Next.
On the Cluster details page, provide a name for your cluster and specify the cluster details:
<img alt="OSD Install" src="../gcp/images/osd-gcp8.png" />{:style="display:block; margin-left:auto; margin-right:auto"}{: width="750" }</p>
</li>
</ol>
<blockquote>
<p>NOTE: the Region used to be installed needs to be the same as the VPC and Subnets deployed in the early step.</p>
</blockquote>
<ol>
<li>
<p>On the Default machine pool page, select a Compute node instance type and a Compute node count:
<img alt="OSD Install" src="../gcp/images/osd-gcp9.png" />{:style="display:block; margin-left:auto; margin-right:auto"}{: width="600" }</p>
</li>
<li>
<p>In the Cluster privacy section, select <strong>Public</strong> endpoints and application routes for your cluster.</p>
</li>
<li>
<p>Select Install into an existing VPC to install the cluster in an existing GCP Virtual Private Cloud (VPC):
<img alt="OSD Install" src="../gcp/images/osd-gcp10.png" />{:style="display:block; margin-left:auto; margin-right:auto"}{: width="600" }</p>
</li>
<li>
<p>Provide your Virtual Private Cloud (VPC) subnet settings, that you deployed as prerequisites in the previous section:
<img alt="OSD Install" src="../gcp/images/osd-gcp11.png" />{:style="display:block; margin-left:auto; margin-right:auto"}{: width="600" }</p>
</li>
<li>
<p>In the CIDR ranges dialog, configure custom classless inter-domain routing (CIDR) ranges or use the defaults that are provided:
<img alt="OSD Install" src="../gcp/images/osd-gcp12.png" />{:style="display:block; margin-left:auto; margin-right:auto"}{: width="600" }</p>
</li>
<li>
<p>On the Cluster update strategy page, configure your update preferences.</p>
</li>
<li>
<p>Review the summary of your selections and click Create cluster to start the cluster installation. Check that the <strong>Install into Existing VPC</strong> is enabled and the VPC and Subnets are properly selected and defined:
<img alt="OSD Install" src="../gcp/images/osd-gcp13.png" />{:style="display:block; margin-left:auto; margin-right:auto"}{: width="600" }</p>
</li>
</ol>
<h2 id="gcp-osd_preexisting_vpc-cleanup">Cleanup</h2>
<p>Deleting a ROSA cluster consists of two parts:</p>
<ol>
<li>
<p>Deleting the OSD cluster can be done using the OCM console described in the <a href="https://docs.openshift.com/dedicated/osd_install_access_delete_cluster/creating-a-gcp-cluster.html">official OSD docs</a>.</p>
</li>
<li>
<p>Deleting the GCP infrastructure resources (VPC, Subnets, Cloud NAT, Cloud Router).
Depending of which option you selected you must perform:</p>
</li>
<li>
<p><strong>Option 1</strong>: Delete GCP resources using GCloud CLI:</p>
</li>
</ol>
<p>```sh
   gcloud compute routers nats delete $NAT_WORKER \
   --region=$REGION --router=$OSD_ROUTER --quiet</p>
<p>gcloud compute routers nats delete $NAT_MASTER \
   --region=$REGION --router=$OSD_ROUTER --quiet</p>
<p>gcloud compute routers delete $OSD_ROUTER --region=$REGION --quiet</p>
<p>gcloud compute networks subnets delete $MASTER_SUBNET --region=$REGION --quiet
   gcloud compute networks subnets delete $WORKER_SUBNET --region=$REGION --quiet</p>
<p>gcloud compute networks delete $OSD_VPC --quiet
   ```</p>
<ol>
<li><strong>Option 2</strong>: Delete GCP resources using Terraform:</li>
</ol>
<p><code>sh
   make destroy</code></p></section><section class="print-page" id="gcp-filestore"><h1 id="gcp-filestore-create-filestore-storage-for-osd-in-gcp">Create Filestore Storage for OSD in GCP</h1>
<p>Author: <a href="https://github.com/rcarrata">Roberto Carratalá</a>, <a href="https://twitter.com/pczarkowski">Paul Czarkowski</a>, <a href="https://github.com/abozzoni">Andrea Bozzoni</a></p>
<p>By default, within OSD in GCP only the <a href="https://kubernetes.io/docs/concepts/storage/storage-classes/#gce-pd">GCE-PD StorageClass</a> is available in the cluster. With this StorageClass, only ReadWriteOnce mode is permitted, and the gcePersistentDisks can only be mounted by a <a href="https://kubernetes.io/docs/concepts/storage/volumes/#gcepersistentdisk">single consumer in read-write mode</a>.</p>
<p>Because of that, and for provide Storage with Shared Access (RWX) Access Mode to our OpenShift clusters a <a href="https://cloud.google.com/filestore/docs">GCP Filestore</a> could be used.</p>
<blockquote>
<p>GCP Filestore is not managed neither supported by Red Hat or Red Hat SRE team.</p>
</blockquote>
<h2 id="gcp-filestore-prerequisites">Prerequisites</h2>
<ul>
<li><a href="https://cloud.google.com/sdk/gcloud">gcloud CLI</a></li>
<li><a href="https://stedolan.github.io/jq/download/">jq</a></li>
<li><a href="https://docs.openshift.com/container-platform/latest/cli_reference/openshift_cli/getting-started-cli.html">oc CLI</a></li>
</ul>
<blockquote>
<p>The <a href="https://cloud.google.com/shell">GCP Cloud Shell</a> can be used as well and have all the prerequisites installed already.</p>
</blockquote>
<h2 id="gcp-filestore-steps">Steps</h2>
<ol>
<li>
<p>From the CLI or GCP Cloud Shell, login within your account and your GCP project:</p>
<p><code>sh
gcloud auth login &lt;google account user&gt;
gcloud config set project &lt;google project name&gt;</code></p>
</li>
<li>
<p>Create a Filestore instance in GCP:</p>
<p>```sh
export ZONE_FS="us-west1-a"
export NAME_FS="nfs-server"
export TIER_FS="BASIC_HDD"
export VOL_NAME_FS="osd4"
export CAPACITY="1TB"
export VPC_NETWORK="projects/my-project/global/networks/demo-vpc"</p>
<p>gcloud filestore instances create $NAME_FS --zone=$ZONE_FS --tier=$TIER_FS --file-share=name="$VOL_NAME_FS",capacity=$CAPACITY --network=name="$VPC_NETWORK"
```</p>
</li>
</ol>
<blockquote>
<p>Due to the Static Provisioning through the creation of the PV/PVC the Filestore for the RWX storage needs to be created upfront.</p>
</blockquote>
<ol>
<li>
<p>After the creation, check the Filestore instance generated in the GCP project:</p>
<p><code>sh
gcloud filestore instances describe $NAME_FS --zone=$ZONE_FS</code></p>
</li>
<li>
<p>Extract the ipAddresses from the NFS share for use them into the PV definition:</p>
<p>```sh
NFS_IP=$(gcloud filestore instances describe $NAME_FS --zone=$ZONE_FS --format=json | jq -r .networks[0].ipAddresses[0])</p>
<p>echo $NFS_IP
```</p>
</li>
<li>
<p>Login your OSD in GCP cluster</p>
</li>
<li>
<p>Create a Persistent Volume using the NFS_IP of the Filestore as the nfs server into the PV definition, specifying the path of the shared Filestore:</p>
<p><code>sh
cat &lt;&lt;EOF | oc apply -f -
apiVersion: v1
kind: PersistentVolume
metadata:
  name: nfs
spec:
  capacity:
    storage: 500Gi
  accessModes:
    - ReadWriteMany
  nfs:
    server: $NFS_IP
    path: "/$VOL_NAME_FS"
EOF</code></p>
</li>
</ol>
<blockquote>
<p>As you can check the PV is generated with the accessMode of ReadWriteMany (RWX)</p>
</blockquote>
<ol>
<li>
<p>Check that the PV is generated properly:</p>
<p><code>sh
$ oc get pv nfs
NAME     CAPACITY   ACCESS MODES   RECLAIM POLICY   STATUS      CLAIM   STORAGECLASS   REASON   AGE
nfs   500Gi      RWX            Retain           Available                                   12s</code></p>
</li>
<li>
<p>Create a PersistentVolumeClaim for this PersistentVolume:</p>
<p><code>sh
cat &lt;&lt;EOF | oc apply -f -
apiVersion: v1
kind: PersistentVolumeClaim
metadata:
  name: nfs
spec:
  accessModes:
    - ReadWriteMany
  storageClassName: ""
  resources:
    requests:
      storage: 500Gi
EOF</code></p>
</li>
</ol>
<blockquote>
<p>As we can check the storageClassName is empty because we're using the Static Provisioning in this case.</p>
</blockquote>
<ol>
<li>
<p>Check that the PVC is generated properly and with the Bound status:</p>
<p><code>sh
oc get pvc nfs
NAME      STATUS   VOLUME   CAPACITY   ACCESS MODES   STORAGECLASS   AGE
nfs  Bound    nfs   500Gi      RWX                           7s</code></p>
</li>
<li>
<p>Generate an example app with more than replicas sharing the same Filestore NFS volume share:</p>
<p><code>sh
cat &lt;&lt;EOF | oc apply -f -
apiVersion: apps/v1
kind: Deployment
metadata:
  creationTimestamp: null
  labels:
    app: nfs-web2
  name: nfs-web
spec:
  replicas: 2
  selector:
    matchLabels:
      app: nfs-web
  strategy: {}
  template:
    metadata:
      creationTimestamp: null
      labels:
        app: nfs-web
    spec:
      containers:
      - image: nginxinc/nginx-unprivileged
        name: nginx-unprivileged
        ports:
          - name: web
            containerPort: 8080
        volumeMounts:
          - name: nfs
            mountPath: "/usr/share/nginx/html"
      volumes:
      - name: nfs
        persistentVolumeClaim:
          claimName: nfs
EOF</code></p>
</li>
<li>
<p>Check that the pods are up &amp;&amp; running:</p>
<p><code>sh
oc get pod
NAME                        READY   STATUS    RESTARTS   AGE
nfs-web2-54f9fb5cd8-8dcgh   1/1     Running   0          118s
nfs-web2-54f9fb5cd8-bhmkw   1/1     Running   0          118s</code></p>
</li>
<li>
<p>Check that the pods mount the same volume provided by the Filestore NFS share:</p>
<p>```sh
for i in $(oc get pod --no-headers | awk '{ print $1 }'); do echo "POD -&gt; $i"; oc exec -ti $i -- df -h | grep nginx; echo ""; done</p>
<p>POD -&gt; nfs-web2-54f9fb5cd8-8dcgh
10.124.186.98:/osd4 1007G     0  956G   0% /usr/share/nginx/html</p>
<p>POD -&gt; nfs-web2-54f9fb5cd8-bhmkw
10.124.186.98:/osd4 1007G     0  956G   0% /usr/share/nginx/html
```</p>
</li>
</ol></section><h1 class='nav-section-title-end'>Ended: GCP</h1><h1 class='nav-section-title-end'>Ended: Products</h1>
                        <h1 class='nav-section-title' id='section-quickstarts'>
                            Quickstarts <a class='headerlink' href='#section-quickstarts' title='Permanent link'>↵</a>
                        </h1>
                        <section class="print-page" id="quickstart-aro"><h1 id="quickstart-aro-aro-quickstart">ARO Quickstart</h1>
<p>A Quickstart guide to deploying an Azure Red Hat OpenShift cluster.</p>
<p>Author: <a href="https://twitter.com/pczarkowski">Paul Czarkowski</a></p>
<h2 id="quickstart-aro-video-walkthrough">Video Walkthrough</h2>
<p>If you prefer a more visual medium, you can watch <a href="https://twitter.com/pczarkowski">Paul Czarkowski</a> walk through this quickstart on <a href="https://youtu.be/VYfCltxoh40">YouTube</a>.</p>
<iframe width="560" height="315" src="https://www.youtube.com/embed/VYfCltxoh40" title="YouTube video player" frameborder="0" allow="accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture" allowfullscreen></iframe>

<h2 id="quickstart-aro-prerequisites">Prerequisites</h2>
<h3 id="quickstart-aro-azure-cli">Azure CLI</h3>
<p><em>Obviously you'll need to have an Azure account to configure the CLI against.</em></p>
<p><strong>MacOS</strong></p>
<blockquote>
<p>See <a href="https://docs.microsoft.com/en-us/cli/azure/install-azure-cli-macos">Azure Docs</a> for alternative install options.</p>
</blockquote>
<ol>
<li>
<p>Install Azure CLI using homebrew</p>
<p><code>bash
brew update &amp;&amp; brew install azure-cli</code></p>
</li>
</ol>
<p><strong>Linux</strong></p>
<blockquote>
<p>See <a href="https://docs.microsoft.com/en-us/cli/azure/install-azure-cli-linux?pivots=dnf">Azure Docs</a> for alternative install options.</p>
</blockquote>
<ol>
<li>
<p>Import the Microsoft Keys</p>
<p><code>bash
sudo rpm --import https://packages.microsoft.com/keys/microsoft.asc</code></p>
</li>
<li>
<p>Add the Microsoft Yum Repository</p>
<p><code>bash
cat &lt;&lt; EOF | sudo tee /etc/yum.repos.d/azure-cli.repo
[azure-cli]
name=Azure CLI
baseurl=https://packages.microsoft.com/yumrepos/azure-cli
enabled=1
gpgcheck=1
gpgkey=https://packages.microsoft.com/keys/microsoft.asc
EOF</code></p>
</li>
<li>
<p>Install Azure CLI</p>
<p><code>bash
sudo dnf install -y azure-cli</code></p>
</li>
</ol>
<h3 id="quickstart-aro-prepare-azure-account-for-azure-openshift">Prepare Azure Account for Azure OpenShift</h3>
<ol>
<li>
<p>Log into the Azure CLI by running the following and then authorizing through your Web Browser</p>
<p><code>bash
az login</code></p>
</li>
<li>
<p>Make sure you have enough Quota (change the location if you're not using <code>East US</code>)</p>
<p><code>bash
az vm list-usage --location "East US" -o table</code></p>
<p>see <a href="#quickstart-aro-adding-quota-to-aro-account">Addendum - Adding Quota to ARO account</a> if you have less than <code>36</code> Quota left for <code>Total Regional vCPUs</code>.</p>
</li>
<li>
<p>Register resource providers</p>
<p><code>bash
az provider register -n Microsoft.RedHatOpenShift --wait
az provider register -n Microsoft.Compute --wait
az provider register -n Microsoft.Storage --wait
az provider register -n Microsoft.Authorization --wait</code></p>
</li>
</ol>
<h3 id="quickstart-aro-get-red-hat-pull-secret">Get Red Hat pull secret</h3>
<blockquote>
<p>This step is optional, but highly recommended</p>
</blockquote>
<ol>
<li>
<p>Log into <a href="https://console.redhat.com">https://console.redhat.com</a></p>
</li>
<li>
<p>Browse to <a href="https://console.redhat.com/openshift/install/azure/aro-provisioned">https://console.redhat.com/openshift/install/azure/aro-provisioned</a></p>
</li>
<li>
<p>click the <strong>Download pull secret</strong> button and remember where you saved it, you'll reference it later.</p>
</li>
</ol>
<h2 id="quickstart-aro-deploy-azure-openshift">Deploy Azure OpenShift</h2>
<h3 id="quickstart-aro-variables-and-resource-group">Variables and Resource Group</h3>
<p>Set some environment variables to use later, and create an Azure Resource Group.</p>
<ol>
<li>
<p>Set the following environment variables</p>
<blockquote>
<p>Change the values to suit your environment, but these defaults should work.</p>
</blockquote>
<p><code>bash
AZR_RESOURCE_LOCATION=eastus
AZR_RESOURCE_GROUP=openshift
AZR_CLUSTER=cluster
AZR_PULL_SECRET=~/Downloads/pull-secret.txt</code></p>
</li>
<li>
<p>Create an Azure resource group</p>
<p><code>bash
az group create \
  --name $AZR_RESOURCE_GROUP \
  --location $AZR_RESOURCE_LOCATION</code></p>
</li>
</ol>
<h3 id="quickstart-aro-networking">Networking</h3>
<p>Create a virtual network with two empty subnets</p>
<ol>
<li>
<p>Create virtual network</p>
<p><code>bash
az network vnet create \
  --address-prefixes 10.0.0.0/22 \
  --name "$AZR_CLUSTER-aro-vnet-$AZR_RESOURCE_LOCATION" \
  --resource-group $AZR_RESOURCE_GROUP</code></p>
</li>
<li>
<p>Create control plane subnet</p>
<p><code>bash
az network vnet subnet create \
  --resource-group $AZR_RESOURCE_GROUP \
  --vnet-name "$AZR_CLUSTER-aro-vnet-$AZR_RESOURCE_LOCATION" \
  --name "$AZR_CLUSTER-aro-control-subnet-$AZR_RESOURCE_LOCATION" \
  --address-prefixes 10.0.0.0/23 \
  --service-endpoints Microsoft.ContainerRegistry</code></p>
</li>
<li>
<p>Create machine subnet</p>
<p><code>bash
az network vnet subnet create \
  --resource-group $AZR_RESOURCE_GROUP \
  --vnet-name "$AZR_CLUSTER-aro-vnet-$AZR_RESOURCE_LOCATION" \
  --name "$AZR_CLUSTER-aro-machine-subnet-$AZR_RESOURCE_LOCATION" \
  --address-prefixes 10.0.2.0/23 \
  --service-endpoints Microsoft.ContainerRegistry</code></p>
</li>
<li>
<p>Disable network policies on the control plane subnet</p>
<blockquote>
<p>This is required for the service to be able to connect to and manage the cluster.</p>
</blockquote>
<p><code>bash
az network vnet subnet update \
  --name "$AZR_CLUSTER-aro-control-subnet-$AZR_RESOURCE_LOCATION" \
  --resource-group $AZR_RESOURCE_GROUP \
  --vnet-name "$AZR_CLUSTER-aro-vnet-$AZR_RESOURCE_LOCATION" \
  --disable-private-link-service-network-policies true</code></p>
</li>
<li>
<p>Create the cluster</p>
<blockquote>
<p>This will take between 30 and 45 minutes.</p>
</blockquote>
<p><code>bash
az aro create \
  --resource-group $AZR_RESOURCE_GROUP \
  --name $AZR_CLUSTER \
  --vnet "$AZR_CLUSTER-aro-vnet-$AZR_RESOURCE_LOCATION" \
  --master-subnet "$AZR_CLUSTER-aro-control-subnet-$AZR_RESOURCE_LOCATION" \
  --worker-subnet "$AZR_CLUSTER-aro-machine-subnet-$AZR_RESOURCE_LOCATION" \
  --pull-secret @$AZR_PULL_SECRET</code></p>
</li>
<li>
<p>Get OpenShift console URL</p>
<p><code>bash
az aro show \
  --name $AZR_CLUSTER \
  --resource-group $AZR_RESOURCE_GROUP \
  -o tsv --query consoleProfile</code></p>
</li>
<li>
<p>Get OpenShift credentials</p>
<p><code>bash
az aro list-credentials \
  --name $AZR_CLUSTER \
  --resource-group $AZR_RESOURCE_GROUP \
  -o tsv</code></p>
</li>
<li>
<p>Use the URL and the credentials provided by the output of the last two commands to log into OpenShift via a web browser.</p>
</li>
</ol>
<p><img alt="ARO login page" src="../images/aro-login.png" /></p>
<ol>
<li>
<p>Deploy an application to OpenShift</p>
<blockquote>
<p>See the following video for a guide on easy application deployment on OpenShift.</p>
</blockquote>
<p><iframe width="560" height="315" src="https://www.youtube.com/embed/8uFUFJS9TA4?start=0:43" title="YouTube video player" frameborder="0" allow="accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture" allowfullscreen></iframe></p>
</li>
</ol>
<h3 id="quickstart-aro-delete-cluster">Delete Cluster</h3>
<p>Once you're done its a good idea to delete the cluster to ensure that you don't get a surprise bill.</p>
<ol>
<li>
<p>Delete the cluster</p>
<p><code>bash
az aro delete -y \
  --resource-group $AZR_RESOURCE_GROUP \
  --name $AZR_CLUSTER</code></p>
</li>
<li>
<p>Delete the Azure resource group</p>
<blockquote>
<p>Only do this if there's nothing else in the resource group.</p>
</blockquote>
<p><code>bash
az group delete -y \
  --name $AZR_RESOURCE_GROUP</code></p>
</li>
</ol>
<h2 id="quickstart-aro-adendum">Adendum</h2>
<h3 id="quickstart-aro-adding-quota-to-aro-account">Adding Quota to ARO account</h3>
<p><img alt="aro quota support ticket request example" src="../images/aro-quota.png" /></p>
<ol>
<li>
<p><a href="https://portal.azure.com/#blade/Microsoft_Azure_Support/HelpAndSupportBlade/newsupportrequest">Create an Azure Support Request</a></p>
</li>
<li>
<p>Set <strong>Issue Type</strong> to "Service and subscription limits (quotas)"</p>
</li>
<li>
<p>Set <strong>Quota Type</strong> to "Compute-VM (cores-vCPUs) subscription limit increases"</p>
</li>
<li>
<p>Click <strong>Next Solutions &gt;&gt;</strong></p>
</li>
<li>
<p>Click <strong>Enter details</strong></p>
</li>
<li>
<p>Set <strong>Deployment Model</strong> to "Resource Manager</p>
</li>
<li>
<p>Set <strong>Locations</strong> to "(US) East US"</p>
</li>
<li>
<p>Set <strong>Types</strong> to "Standard"</p>
</li>
<li>
<p>Under <strong>Standard</strong> check "DSv3" and "DSv4"</p>
</li>
<li>
<p>Set <strong>New vCPU Limit</strong> for each (example "60")</p>
</li>
<li>
<p>Click <strong>Save and continue</strong></p>
</li>
<li>
<p>Click <strong>Review + create &gt;&gt;</strong></p>
</li>
<li>
<p>Wait until quota is increased.</p>
</li>
</ol></section><section class="print-page" id="quickstart-rosa"><h1 id="quickstart-rosa-rosa-quickstart">ROSA Quickstart</h1>
<p>A Quickstart guide to deploying a Red Hat OpenShift cluster on AWS.</p>
<p>Author: <a href="https://twitter.com/stevemirman">Steve Mirman</a></p>
<h2 id="quickstart-rosa-video-walkthrough">Video Walkthrough</h2>
<p>If you prefer a more visual medium, you can watch <a href="https://twitter.com/stevemirman">Steve Mirman</a> walk through this quickstart on <a href="https://www.youtube.com/watch?v=IFNig_Z_p2Y">YouTube</a>.</p>
<iframe width="560" height="315" src="https://www.youtube.com/embed/IFNig_Z_p2Y" title="YouTube video player" frameborder="0" allow="accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture" allowfullscreen></iframe>

<h2 id="quickstart-rosa-prerequisites">Prerequisites</h2>
<h3 id="quickstart-rosa-aws-cli">AWS CLI</h3>
<p><em>You'll need to have an AWS account to configure the CLI against.</em></p>
<p><strong>MacOS</strong></p>
<blockquote>
<p>See <a href="https://docs.aws.amazon.com/cli/latest/userguide/install-cliv2-mac.html">AWS Docs</a> for alternative install options.</p>
</blockquote>
<ol>
<li>
<p>Install AWS CLI using the macOS command line</p>
<p><code>bash
curl "https://awscli.amazonaws.com/AWSCLIV2.pkg" -o "AWSCLIV2.pkg"
sudo installer -pkg AWSCLIV2.pkg -target /</code></p>
</li>
</ol>
<p><strong>Linux</strong></p>
<blockquote>
<p>See <a href="https://docs.aws.amazon.com/cli/latest/userguide/install-cliv2-linux.html">AWS Docs</a> for alternative install options.</p>
</blockquote>
<ol>
<li>
<p>Install AWS CLI using the Linux command line</p>
<p><code>bash
curl "https://awscli.amazonaws.com/awscli-exe-linux-x86_64.zip" -o "awscliv2.zip"
unzip awscliv2.zip
sudo ./aws/install</code></p>
</li>
</ol>
<p><strong>Windows</strong></p>
<blockquote>
<p>See <a href="https://docs.aws.amazon.com/cli/latest/userguide/install-cliv2-windows.html">AWS Docs</a> for alternative install options.</p>
</blockquote>
<ol>
<li>
<p>Install AWS CLI using the Windows command line</p>
<p><code>bash
C:\&gt; msiexec.exe /i https://awscli.amazonaws.com/AWSCLIV2.msi</code></p>
</li>
</ol>
<p><strong>Docker</strong></p>
<blockquote>
<p>See <a href="https://docs.aws.amazon.com/cli/latest/userguide/install-cliv2-docker.html">AWS Docs</a> for alternative install options.</p>
</blockquote>
<ol>
<li>
<p>To run the AWS CLI version 2 Docker image, use the docker run command.</p>
<p><code>bash
docker run --rm -it amazon/aws-cli command</code></p>
</li>
</ol>
<h3 id="quickstart-rosa-prepare-aws-account-for-openshift">Prepare AWS Account for OpenShift</h3>
<ol>
<li>
<p>Configure the AWS CLI by running the following command</p>
<p><code>bash
aws configure</code></p>
</li>
<li>
<p>You will be required to enter an <code>AWS Access Key ID</code> and an <code>AWS Secret Access Key</code> along with a default region name and output format</p>
<p><code>bash
% aws configure
AWS Access Key ID []: 
AWS Secret Access Key []: 
Default region name [us-east-2]: 
Default output format [json]:</code>
The <code>AWS Access Key ID</code> and <code>AWS Secret Access Key</code> values can be obtained by logging in to the AWS console and creating an <strong>Access Key</strong> in the <strong>Security Credentials</strong> section of the IAM dashboard for your user</p>
</li>
<li>
<p>Validate your credentials </p>
<p><code>bash
aws sts get-caller-identity</code></p>
<p>You should receive output similar to the following
<code>{
  "UserId": &lt;your ID&gt;,
  "Account": &lt;your account&gt;,
  "Arn": &lt;your arn&gt;
}</code></p>
</li>
<li>
<p>If this is a brand new AWS account that has never had a AWS Load Balancer installed in it, you should run the following</p>
<p><code>bash
aws iam create-service-linked-role --aws-service-name \
"elasticloadbalancing.amazonaws.com"</code></p>
</li>
</ol>
<h3 id="quickstart-rosa-get-a-red-hat-offline-access-token">Get a Red Hat Offline Access Token</h3>
<ol>
<li>
<p>Log into cloud.redhat.com</p>
</li>
<li>
<p>Browse to https://cloud.redhat.com/openshift/token/rosa</p>
</li>
<li>
<p>Copy the <strong>Offline Access Token</strong> and save it for the next step</p>
</li>
</ol>
<h3 id="quickstart-rosa-set-up-the-openshift-cli-oc">Set up the OpenShift CLI (oc)</h3>
<ol>
<li>
<p>Download the OS specific OpenShift CLI from <a href="https://mirror.openshift.com/pub/openshift-v4/clients/ocp/latest/">Red Hat</a></p>
</li>
<li>
<p>Unzip the downloaded file on your local machine</p>
</li>
<li>
<p>Place the extracted <code>oc</code> executable in your OS path or local directory</p>
</li>
</ol>
<h3 id="quickstart-rosa-set-up-the-rosa-cli">Set up the ROSA CLI</h3>
<ol>
<li>
<p>Download the OS specific ROSA CLI from <a href="https://www.openshift.com/products/amazon-openshift/download">Red Hat</a></p>
</li>
<li>
<p>Unzip the downloaded file on your local machine</p>
</li>
<li>
<p>Place the extracted <code>rosa</code> and <code>kubectl</code> executables in your OS path or local directory</p>
</li>
<li>
<p>Log in to ROSA</p>
</li>
</ol>
<p><code>bash
  rosa login</code></p>
<p>You will be prompted to enter in the <strong>Red Hat Offline Access Token</strong> you retrieved earlier and should receive the following message</p>
<p><code>Logged in as &lt;email address&gt; on 'https://api.openshift.com'</code></p>
<h3 id="quickstart-rosa-verify-rosa-privileges">Verify ROSA privileges</h3>
<p>Verify that ROSA has the minimal permissions</p>
<p><code>bash
  rosa verify permissions</code></p>
<blockquote>
<p>Expected output: <code>AWS SCP policies ok</code></p>
</blockquote>
<p>Verify that ROSA has the minimal quota</p>
<p><code>bash
  rosa verify quota</code></p>
<blockquote>
<p>Expected output: <code>AWS quota ok</code></p>
</blockquote>
<h3 id="quickstart-rosa-initialize-rosa">Initialize ROSA</h3>
<p>Initialize the ROSA CLI to complete the remaining validation checks and configurations</p>
<p><code>bash
  rosa init</code></p>
<h2 id="quickstart-rosa-deploy-red-hat-openshift-on-aws-rosa">Deploy Red Hat OpenShift on AWS (ROSA)</h2>
<h3 id="quickstart-rosa-interactive-installation">Interactive Installation</h3>
<p>ROSA can be installed using command line parameters or in interactive mode.  For an interactive installation run the following command</p>
<p><code>bash
  rosa create cluster --interactive</code></p>
<p>As part of the interactive install you will be required to enter the following parameters or accept the default values (if applicable)</p>
<p><code>Cluster name:
  Multiple availability zones (y/N):
  AWS region (select):
  OpenShift version (select):
  Install into an existing VPC (y/N):
  Compute nodes instance type (optional):
  Enable autoscaling (y/N):
  Compute nodes [2]:
  Machine CIDR [10.0.0.0/16]:
  Service CIDR [172.30.0.0/16]:
  Pod CIDR [10.128.0.0/14]:
  Host prefix [23]:
  Private cluster (y/N):</code>  </p>
<blockquote>
<p>Note: the installation process should take between 30 - 45 minutes</p>
</blockquote>
<h3 id="quickstart-rosa-get-the-web-console-link-to-the-rosa-cluster">Get the web console link to the ROSA cluster</h3>
<p>To get the web console link run the following command.</p>
<blockquote>
<p>Substitute your actual cluster name for <code>&lt;cluster-name&gt;</code></p>
</blockquote>
<p><code>bash
  rosa describe cluster --cluster=&lt;cluster-name&gt;</code></p>
<h3 id="quickstart-rosa-create-cluster-admin-user">Create cluster-admin user</h3>
<p>By default, only the OpenShift SRE team will have access to the ROSA cluster.  To add a local admin user, run the following command to create the <code>cluster-admin</code> account in your cluster.</p>
<blockquote>
<p>Substitute your actual cluster name for <code>&lt;cluster-name&gt;</code></p>
</blockquote>
<p><code>bash
  rosa create admin --cluster=&lt;cluster-name&gt;</code></p>
<blockquote>
<p>Refresh your web browser and you should see the <code>cluster-admin</code> option to log in</p>
</blockquote>
<h2 id="quickstart-rosa-delete-red-hat-openshift-on-aws-rosa">Delete Red Hat OpenShift on AWS (ROSA)</h2>
<p>Deleting a ROSA cluster consists of two parts</p>
<ol>
<li>Delete the cluster instance, including the removal of AWS resources.  </li>
</ol>
<blockquote>
<p>Substitute your actual cluster name for <code>&lt;cluster-name&gt;</code></p>
</blockquote>
<p><code>bash
  rosa delete cluster --cluster=&lt;cluster-name&gt;</code></p>
<ol>
<li>Delete the CloudFormation stack, including the removal of the <code>osdCcsAdmin</code> user</li>
</ol>
<p><code>bash
  rosa init --delete-stack</code></p></section><h1 class='nav-section-title-end'>Ended: Quickstarts</h1>
                        <h1 class='nav-section-title' id='section-advanced-cluster-manager-acm-'>
                            Advanced Cluster Manager (ACM) <a class='headerlink' href='#section-advanced-cluster-manager-acm-' title='Permanent link'>↵</a>
                        </h1>
                        <section class="print-page" id="acm-observability-rosa"><h1 id="acm-observability-rosa-advanced-cluster-management-observability-on-rosa">Advanced Cluster Management Observability on ROSA</h1>
<!-- commented sections enable STS support which isn't fully working as
the operator will on occasion wipe out the service account annotations -->

<p>This document will take you through deploying ACM Observability on a ROSA cluster. see <a href="https://access.redhat.com/documentation/en-us/red_hat_advanced_cluster_management_for_kubernetes/2.4/html/observability/observing-environments-intro#enabling-observability">here</a> for the original documentation.</p>
<!--
**Note:** This guide uses an unsupported (by ACM) method to utilize ROSA's STS authentication back into AWS S3 and is not advised for production use at this time. -->

<h2 id="acm-observability-rosa-prerequisites">Prerequisites</h2>
<ul>
<li>An existing ROSA cluster</li>
<li>An Advanced Cluster Management (ACM) deployment</li>
</ul>
<h2 id="acm-observability-rosa-set-up-environment">Set up environment</h2>
<ol>
<li>Set environment variables</li>
</ol>
<pre><code class="language-bash">    export CLUSTER_NAME=my-cluster
    export S3_BUCKET=$CLUSTER_NAME-acm-observability
    export REGION=us-east-2
    export NAMESPACE=open-cluster-management-observability
    export SA=tbd
    export SCRATCH_DIR=/tmp/scratch
    export AWS_ACCOUNT_ID=$(aws sts get-caller-identity --query Account --output text)
    export AWS_PAGER=&quot;&quot;
    rm -rf $SCRATCH_DIR
    mkdir -p $SCRATCH_DIR
</code></pre>
<!--
    export OIDC_PROVIDER=$(oc get authentication.config.openshift.io cluster -o json | jq -r .spec.serviceAccountIssuer| sed -e "s/^https:\/\///")
-->

<h2 id="acm-observability-rosa-prepare-aws-account">Prepare AWS Account</h2>
<ol>
<li>
<p>Create an S3 bucket</p>
<p><code>bash
aws s3 mb s3://$S3_BUCKET</code></p>
</li>
<li>
<p>Create a Policy for access to S3</p>
<p><code>bash
cat &lt;&lt;EOF &gt; $SCRATCH_DIR/s3-policy.json
{
"Version": "2012-10-17",
"Statement": [
    {
        "Sid": "Statement",
        "Effect": "Allow",
        "Action": [
            "s3:ListBucket",
            "s3:GetObject",
            "s3:DeleteObject",
            "s3:PutObject",
            "s3:PutObjectAcl",
            "s3:CreateBucket",
            "s3:DeleteBucket"
        ],
        "Resource": [
            "arn:aws:s3:::$S3_BUCKET/*",
            "arn:aws:s3:::$S3_BUCKET"
        ]
    }
]
}
EOF</code></p>
</li>
<li>
<p>Apply the Policy</p>
<p><code>bash
S3_POLICY=$(aws iam create-policy --policy-name $CLUSTER_NAME-acm-obs \
  --policy-document file://$SCRATCH_DIR/s3-policy.json \
  --query 'Policy.Arn' --output text)
echo $S3_POLICY</code></p>
</li>
<li>
<p>Create service account</p>
<p><code>bash
aws iam create-user --user-name $CLUSTER_NAME-acm-obs  \
  --query User.Arn --output text</code></p>
</li>
<li>
<p>Attach policy to user</p>
<p><code>bash
aws iam attach-user-policy --user-name $CLUSTER_NAME-acm-obs \
  --policy-arn ${S3_POLICY}</code></p>
</li>
<li>
<p>Create Access Keys</p>
<p><code>bash
read -r ACCESS_KEY_ID ACCESS_KEY &lt; &lt;(aws iam create-access-key \
  --user-name $CLUSTER_NAME-acm-obs \
  --query 'AccessKey.[AccessKeyId,SecretAccessKey]' --output text)</code></p>
</li>
</ol>
<!--
1. Create a Trust Policy

    ```bash
cat <<EOF > $SCRATCH_DIR/TrustPolicy.json
{
  "Version": "2012-10-17",
  "Statement": [
    {
      "Effect": "Allow",
      "Principal": {
        "Federated": "arn:aws:iam::${AWS_ACCOUNT_ID}:oidc-provider/${OIDC_PROVIDER}"
      },
      "Action": "sts:AssumeRoleWithWebIdentity",
      "Condition": {
        "StringEquals": {
          "${OIDC_PROVIDER}:sub": [
            "system:serviceaccount:${NAMESPACE}:observability-thanos-query",
            "system:serviceaccount:${NAMESPACE}:observability-thanos-store-shard",
            "system:serviceaccount:${NAMESPACE}:observability-thanos-compact"
          ]
        }
      }
    }
  ]
}
EOF
    ```


1. Create Role for AWS Prometheus and CloudWatch

    ```bash
    S3_ROLE=$(aws iam create-role \
      --role-name "$CLUSTER_NAME-acm-obs-s3" \
      --assume-role-policy-document file://$SCRATCH_DIR/TrustPolicy.json \
      --query "Role.Arn" --output text)
    echo $S3_ROLE
    ```

1. Attach the Policies to the Role

    ```bash
    aws iam attach-role-policy \
      --role-name "$CLUSTER_NAME-acm-obs-s3" \
      --policy-arn $S3_POLICY
    ```
-->

<h2 id="acm-observability-rosa-acm-hub">ACM Hub</h2>
<p>Log into the OpenShift cluster that is running your ACM Hub.  We'll set up Observability here</p>
<ol>
<li>
<p>Create a namespace for the observability</p>
<p><code>bash
oc new-project $NAMESPACE</code></p>
</li>
<li>
<p>Generate a pull secret (this will check if the pull secret exists, if not, it will create it)</p>
<p><code>bash
DOCKER_CONFIG_JSON=`oc extract secret/multiclusterhub-operator-pull-secret -n open-cluster-management --to=-` || \
DOCKER_CONFIG_JSON=`oc extract secret/pull-secret -n openshift-config --to=-` &amp;&amp; \
oc create secret generic multiclusterhub-operator-pull-secret \
-n open-cluster-management-observability \
--from-literal=.dockerconfigjson="$DOCKER_CONFIG_JSON" \
--type=kubernetes.io/dockerconfigjson</code></p>
</li>
<li>
<p>Create a Secret containing your S3 details</p>
<p><code>bash
cat &lt;&lt; EOF | kubectl apply -f -
apiVersion: v1
kind: Secret
metadata:
  name: thanos-object-storage
  namespace: open-cluster-management-observability
type: Opaque
stringData:
  thanos.yaml: |
type: s3
config:
  bucket: $S3_BUCKET
  endpoint: s3.$REGION.amazonaws.com
  signature_version2: false
  access_key: $ACCESS_KEY_ID
  secret_key: $ACCESS_KEY
EOF</code></p>
</li>
<li>
<p>Create a CR for <code>MulticlusterHub</code></p>
<p><code>bash
cat &lt;&lt; EOF | kubectl apply -f -
apiVersion: observability.open-cluster-management.io/v1beta2
kind: MultiClusterObservability
metadata:
  name: observability
spec:
  observabilityAddonSpec: {}
  storageConfig:
metricObjectStorage:
  name: thanos-object-storage
  key: thanos.yaml
EOF</code></p>
</li>
</ol>
<!--
1. Annotate the service accounts to use STS

    ```bash
oc annotate -n open-cluster-management-observability serviceaccount observability-thanos-query \
      eks.amazonaws.com/role-arn=$S3_ROLE
oc annotate -n open-cluster-management-observability serviceaccount observability-thanos-store-shard \
      eks.amazonaws.com/role-arn=$S3_ROLE
oc annotate -n open-cluster-management-observability serviceaccount observability-thanos-compact \
      eks.amazonaws.com/role-arn=$S3_ROLE
    ```

1. Restart thanos pods to use the new credentials

    ```bash
kubectl rollout restart deployment/observability-thanos-query
kubectl rollout restart statefulset/observability-thanos-compact
kubectl rollout restart statefulset/observability-thanos-store-shard-0
kubectl rollout restart statefulset/observability-thanos-store-shard-1
kubectl rollout restart statefulset/observability-thanos-store-shard-2
    ```

1. Check for any pods that aren't running

    ```bash
    kubectl get pods --field-selector status.phase!=Running
    ```

    Delete any pods that are not running in order to have them recreated with the correct credentials

    ```bash
    kubectl delete pods $(kubectl get pods --field-selector status.phase!=Running -o name | xargs)
    ```
-->

<h2 id="acm-observability-rosa-access-acm-observability">Access ACM Observability</h2>
<ol>
<li>Log into Advanced Cluster management and access the new Grafana dashboard</li>
</ol>
<p><img alt="ACM Grafana Dashboard" src="../acm/observability/rosa/acm-grafana.png" /></p></section><h1 class='nav-section-title-end'>Ended: Advanced Cluster Manager (ACM)</h1>
                        <h1 class='nav-section-title' id='section-observability'>
                            Observability <a class='headerlink' href='#section-observability' title='Permanent link'>↵</a>
                        </h1>
                        
                        <h2 class='nav-section-title' id='section-configuring-alerts-for-user-workloads'>
                            Configuring Alerts for User Workloads <a class='headerlink' href='#section-configuring-alerts-for-user-workloads' title='Permanent link'>↵</a>
                        </h2>
                        <section class="print-page" id="rosa-custom-alertmanager-4.9"><h1 id="rosa-custom-alertmanager-4.9-custom-alertmanager-in-rosa-49x">Custom AlertManager in ROSA 4.9.x</h1>
<p>ROSA 4.9.x introduces a new way to provide custom AlertManager configuration to receive alerts from User Workload Management.</p>
<p>The OpenShift Administrator can use the Prometheus Operator to create a custom AlertManager resource and then use the AlertManagerConfig resource to configure User Workload Monitoring to use the custom AlertManager.</p>
<h2 id="rosa-custom-alertmanager-4.9-prerequisites">Prerequisites</h2>
<ul>
<li><a href="https://docs.aws.amazon.com/cli/latest/userguide/cli-chap-install.html">AWS CLI</a></li>
<li>A Red Hat OpenShift for AWS (ROSA) cluster 4.9.0 or higher</li>
</ul>
<h2 id="rosa-custom-alertmanager-4.9-create-environment-variables">Create Environment Variables</h2>
<ol>
<li>Before we get started we need to set some environment variables to be used throughout the guide.</li>
</ol>
<p><code>bash
   export PROM_NAMESPACE=custom-alert-manager</code></p>
<h2 id="rosa-custom-alertmanager-4.9-install-prometheus-operator">Install Prometheus Operator</h2>
<blockquote>
<p>If you prefer you can do this from the Operator Hub in the cluster console itself.</p>
</blockquote>
<ol>
<li>Create a OperatorGroup and Subscription for the Prometheus Operator</li>
</ol>
<p>```bash
   cat &lt;&lt; EOF | kubectl apply -f -</p>
<hr />
<p>apiVersion: v1
   kind: Namespace
   metadata:
     name: ${PROM_NAMESPACE}</p>
<hr />
<p>apiVersion: operators.coreos.com/v1
   kind: OperatorGroup
   metadata:
     name: federated-metrics
     namespace: ${PROM_NAMESPACE}
   spec:
     targetNamespaces:
     - ${PROM_NAMESPACE}</p>
<hr />
<p>apiVersion: operators.coreos.com/v1alpha1
   kind: Subscription
   metadata:
     name: prometheus
     namespace: ${PROM_NAMESPACE}
   spec:
     channel: beta
     installPlanApproval: Automatic
     name: prometheus
     source: community-operators
     sourceNamespace: openshift-marketplace
   EOF
   ```</p>
<h2 id="rosa-custom-alertmanager-4.9-deploy-alertmanager">Deploy AlertManager</h2>
<ol>
<li>Create an Alert Manager Configuration file</li>
</ol>
<blockquote>
<p>This will create a basic AlertManager configuration to send alerts to a slack channel. Configuring slack is outside the scope of this document. Update the variables to suit your slack integration.</p>
</blockquote>
<p>```bash
   SLACK_API_URL=https://hooks.slack.com/services/XXX/XXX/XXX
   SLACK_CHANNEL='#paultest'
   cat &lt;&lt; EOF | kubectl apply -n ${PROM_NAMESPACE} -f -
   apiVersion: v1
   kind: Secret
   metadata:
     name: custom-alertmanager
     namespace: ${PROM_NAMESPACE}
   stringData:
     alertmanager.yaml: |
       global:
         slack_api_url: "${SLACK_API_URL}"
       route:
         receiver: slack-notifications
         group_by: [alertname]
       receivers:
       - name: slack-notifications
         slack_configs:
         - channel: ${SLACK_CHANNEL}
           send_resolved: true</p>
<hr />
<p>apiVersion: monitoring.coreos.com/v1
   kind: Alertmanager
   metadata:
     name: custom-alertmanager
     namespace: ${PROM_NAMESPACE}
   spec:
     securityContext: {}
     replicas: 3
     configSecret: custom-alertmanager</p>
<hr />
<p>apiVersion: v1
   kind: Service
   metadata:
     name: custom-alertmanager
     namespace: ${PROM_NAMESPACE}
   spec:
     type: ClusterIP
     ports:
     - name: web
       port: 9093
       protocol: TCP
       targetPort: web
     selector:
       alertmanager: custom-alertmanager
   EOF
   ```</p>
<h2 id="rosa-custom-alertmanager-4.9-configure-user-workload-monitoring-to-use-the-custom-alertmanager">Configure User Workload Monitoring to use the custom AlertManager</h2>
<ol>
<li>Create an AlertManagerConfig for User Workload Monitoring</li>
</ol>
<blockquote>
<p>Note: This next command assumes the existing <code>config.yaml</code> in the <code>user-workload-monitoring-config</code> config map is empty. You should verify it with <code>kubectl get -n openshift-user-workload-monitoring cm user-workload-monitoring-config -o yaml</code> and simply edit in the differences if its not.</p>
</blockquote>
<p><code>bash
   cat &lt;&lt; EOF | kubectl apply -f -
   apiVersion: v1
   kind: ConfigMap
   metadata:
     name: user-workload-monitoring-config
     namespace: openshift-user-workload-monitoring
   data:
     config.yaml: |
       thanosRuler:
         additionalAlertmanagerConfigs:
         - scheme: http
           pathPrefix: /
           timeout: "30s"
           apiVersion: v1
           staticConfigs: ["custom-alertmanager.$PROM_NAMESPACE.svc.cluster.local:9093"]
   EOF</code></p>
<h2 id="rosa-custom-alertmanager-4.9-create-an-example-alert">Create an Example Alert</h2>
<ol>
<li>Verify it works by creating a Prometheus Rule that will fire off an alert</li>
</ol>
<p><code>bash
   cat &lt;&lt; EOF | kubectl apply -n $PROM_NAMESPACE -f -
   apiVersion: monitoring.coreos.com/v1
   kind: PrometheusRule
   metadata:
     name: prometheus-example-rules
     namespace: ${PROM_NAMESPACE}
   spec:
     groups:
     - name: example.rules
       rules:
       - alert: ExampleAlert
         expr: vector(1)
   EOF</code></p>
<ol>
<li>Forward a port to the alert manager service</li>
</ol>
<p><code>bash
    kubectl port-forward -n ${PROM_NAMESPACE} svc/custom-alertmanager 9093:9093</code></p>
<ol>
<li>
<p>Browse to http://localhost:9093/#/alerts to see the alert "ExampleAlert"</p>
<p><img alt="Screenshot of Alert Manager" src="../rosa/custom-alertmanager-4.9/alert-manager.png" /></p>
</li>
<li>
<p>Check the Alert was sent to Slack</p>
<p><img alt="Screenshot of Alert in Slack" src="../rosa/custom-alertmanager-4.9/slack.png" /></p>
</li>
</ol></section><section class="print-page" id="rosa-custom-alertmanager"><h1 id="rosa-custom-alertmanager-custom-alerts-in-rosa-411x">Custom Alerts in ROSA 4.11.x</h1>
<p>Starting with ROSA 4.11 clusters the OpenShift Administrator can enable a second AlertManager instance in user workload metrics which can be used to create custom alerts.</p>
<h2 id="rosa-custom-alertmanager-prerequisites">Prerequisites</h2>
<ul>
<li><a href="https://docs.aws.amazon.com/cli/latest/userguide/cli-chap-install.html">AWS CLI</a></li>
<li>A Red Hat OpenShift for AWS (ROSA) cluster 4.11.0 or higher</li>
</ul>
<h2 id="rosa-custom-alertmanager-create-environment-variables">Create Environment Variables</h2>
<h2 id="rosa-custom-alertmanager-configure-user-workload-monitoring-to-include-alertmanager">Configure User Workload Monitoring to include AlertManager</h2>
<ol>
<li>Edit the user workload config to include AlertManager</li>
</ol>
<blockquote>
<p>Note: If you have other modifications to this config, you will need to hand edit the resource rather than brute forcing it like below.</p>
</blockquote>
<p><code>bash
   cat &lt;&lt; EOF | oc apply -f -
   apiVersion: v1
   kind: ConfigMap
   metadata:
     name: user-workload-monitoring-config
     namespace: openshift-user-workload-monitoring
   data:
     config.yaml: |
       alertmanager:
         enabled: true
         enableAlertmanagerConfig: true
   EOF</code></p>
<ol>
<li>Verify that a new Alert Manager instance is defined</li>
</ol>
<p><code>bash
   oc -n openshift-user-workload-monitoring get alertmanager</code></p>
<p><code>NAME            VERSION   REPLICAS   AGE
   user-workload   0.24.0    2          2m</code></p>
<ol>
<li>If you want non-admin users to be able to define alerts in their own namespaces you can run the following.</li>
</ol>
<p><code>bash
   oc -n &lt;namespace&gt; adm policy add-role-to-user alert-routing-edit &lt;user&gt;</code></p>
<ol>
<li>Update the Alert Manager Configuration file</li>
</ol>
<blockquote>
<p>This will create a basic AlertManager configuration to send alerts to a slack channel. Configuring slack is outside the scope of this document. Update the variables to suit your slack integration.</p>
</blockquote>
<p><code>bash
   SLACK_API_URL=https://hooks.slack.com/services/XXX/XXX/XXX
   SLACK_CHANNEL='#paultest'
   cat &lt;&lt; EOF | kubectl apply -f -
   apiVersion: v1
   kind: Secret
   metadata:
     name: alertmanager-user-workload
     namespace: openshift-user-workload-monitoring
   stringData:
     alertmanager.yaml: |
       global:
         slack_api_url: "${SLACK_API_URL}"
       route:
         receiver: Default
         group_by: [alertname]
       receivers:
         - name: Default
           slack_configs:
             - channel: ${SLACK_CHANNEL}
               send_resolved: true
     EOF</code></p>
<h2 id="rosa-custom-alertmanager-create-an-example-alert">Create an Example Alert</h2>
<ol>
<li>Create a Namespace for your custom alert</li>
</ol>
<p><code>bash
   oc create namespace custom-alert</code></p>
<ol>
<li>Verify it works by creating a Prometheus Rule that will fire off an alert</li>
</ol>
<p><code>bash
   cat &lt;&lt; EOF | kubectl apply -n custom-alert -f -
   apiVersion: monitoring.coreos.com/v1
   kind: PrometheusRule
   metadata:
     name: prometheus-example-rules
   spec:
     groups:
     - name: example.rules
       rules:
       - alert: ExampleAlert
         expr: vector(1)
   EOF</code></p>
<ol>
<li>Forward a port to the alert manager service</li>
</ol>
<p><code>bash
    kubectl port-forward -n openshift-user-workload-monitoring \
      svc/alertmanager-operated 9093:9093</code></p>
<ol>
<li>
<p>Browse to http://localhost:9093/#/alerts to see the alert "ExampleAlert"</p>
<p><img alt="Screenshot of Alert Manager" src="../rosa/custom-alertmanager/alert-manager.png" /></p>
</li>
<li>
<p>Check the Alert was sent to Slack</p>
<p><img alt="Screenshot of Alert in Slack" src="../rosa/custom-alertmanager/slack.png" /></p>
</li>
</ol></section><h1 class='nav-section-title-end'>Ended: Configuring Alerts for User Workloads</h1><section class="print-page" id="rosa-federated-metrics"><h1 id="rosa-federated-metrics-federating-system-and-user-metrics-to-s3-in-red-hat-openshift-for-aws">Federating System and User metrics to S3 in Red Hat OpenShift for AWS</h1>
<p><strong>Paul Czarkowski</strong></p>
<p><em>06/07/2021</em></p>
<p>This guide walks through setting up federating Prometheus metrics to S3 storage.</p>
<blockquote>
<p>ToDo - Add Authorization in front of Thanos APIs</p>
</blockquote>
<h2 id="rosa-federated-metrics-prerequisites">Prerequisites</h2>
<ul>
<li><a href="#docs-rosa-sts">A ROSA cluster deployed with STS</a></li>
<li>aws CLI</li>
</ul>
<h2 id="rosa-federated-metrics-set-up-environment">Set up environment</h2>
<ol>
<li>
<p>Create environment variables</p>
<p><code>bash
export CLUSTER_NAME=my-cluster
export S3_BUCKET=my-thanos-bucket
export REGION=us-east-2
export NAMESPACE=federated-metrics
export SA=aws-prometheus-proxy
export SCRATCH_DIR=/tmp/scratch
export OIDC_PROVIDER=$(oc get authentication.config.openshift.io cluster -o json | jq -r .spec.serviceAccountIssuer| sed -e "s/^https:\/\///")
export AWS_ACCOUNT_ID=$(aws sts get-caller-identity --query Account --output text)
export AWS_PAGER=""
rm -rf $SCRATCH_DIR
mkdir -p $SCRATCH_DIR</code></p>
</li>
<li>
<p>Create namespace</p>
<p><code>bash
oc new-project $NAMESPACE</code></p>
</li>
</ol>
<h2 id="rosa-federated-metrics-aws-preperation">AWS Preperation</h2>
<ol>
<li>
<p>Create an S3 bucket</p>
<p><code>bash
aws s3 mb s3://$S3_BUCKET</code></p>
</li>
<li>
<p>Create a Policy for access to S3</p>
<p><code>bash
cat &lt;&lt;EOF &gt; $SCRATCH_DIR/s3-policy.json
{
"Version": "2012-10-17",
"Statement": [
    {
        "Sid": "Statement",
        "Effect": "Allow",
        "Action": [
            "s3:ListBucket",
            "s3:GetObject",
            "s3:DeleteObject",
            "s3:PutObject",
            "s3:PutObjectAcl"
        ],
        "Resource": [
            "arn:aws:s3:::$S3_BUCKET/*",
            "arn:aws:s3:::$S3_BUCKET"
        ]
    }
]
}
EOF</code></p>
</li>
<li>
<p>Apply the Policy</p>
<p><code>bash
S3_POLICY=$(aws iam create-policy --policy-name $CLUSTER_NAME-thanos \
  --policy-document file://$SCRATCH_DIR/s3-policy.json \
  --query 'Policy.Arn' --output text)
echo $S3_POLICY</code></p>
</li>
<li>
<p>Create a Trust Policy</p>
<p><code>bash
cat &lt;&lt;EOF &gt; $SCRATCH_DIR/TrustPolicy.json
{
  "Version": "2012-10-17",
  "Statement": [
{
  "Effect": "Allow",
  "Principal": {
    "Federated": "arn:aws:iam::${AWS_ACCOUNT_ID}:oidc-provider/${OIDC_PROVIDER}"
  },
  "Action": "sts:AssumeRoleWithWebIdentity",
  "Condition": {
    "StringEquals": {
      "${OIDC_PROVIDER}:sub": [
        "system:serviceaccount:${NAMESPACE}:${SA}"
      ]
    }
  }
}
  ]
}
EOF</code></p>
</li>
<li>
<p>Create Role for AWS Prometheus and CloudWatch</p>
<p><code>bash
S3_ROLE=$(aws iam create-role \
  --role-name "$CLUSTER-thanos-s3" \
  --assume-role-policy-document file://$SCRATCH_DIR/TrustPolicy.json \
  --query "Role.Arn" --output text)
echo $S3_ROLE</code></p>
</li>
<li>
<p>Attach the Policies to the Role</p>
<p><code>bash
aws iam attach-role-policy \
  --role-name "$CLUSTER-thanos-s3" \
  --policy-arn $S3_POLICY</code></p>
</li>
</ol>
<!--
1. Grant access for the thanos user to the s3 bucket

aws s3api put-bucket-policy --bucket my-thanos-metrics \
  --policy file://s3-policy.json

1. Get the account key and secret and update in `thanos-store-credentials.yaml`
-->

<h2 id="rosa-federated-metrics-deploy-operators">Deploy Operators</h2>
<ol>
<li>
<p>Add the MOBB chart repository to your Helm</p>
<p><code>bash
helm repo add mobb https://rh-mobb.github.io/helm-charts/</code></p>
</li>
<li>
<p>Update your repositories</p>
<p><code>bash
helm repo update</code></p>
</li>
<li>
<p>Use the <code>mobb/operatorhub</code> chart to deploy the needed operators</p>
<p><code>bash
helm upgrade -n $echNAMESPACE custom-metrics-operators \
  mobb/operatorhub --version 0.1.1 --install \
  --values https://raw.githubusercontent.com/rh-mobb/helm-charts/main/charts/rosa-thanos-s3/files/operatorhub.yaml</code></p>
</li>
</ol>
<h2 id="rosa-federated-metrics-deploy-thanos-store-gateway">Deploy Thanos Store Gateway</h2>
<ol>
<li>
<p>Deploy ROSA Thanos S3 Helm Chart</p>
<p><code>helm upgrade -n $NAMESPACE rosa-thanos-s3 --install mobb/rosa-thanos-s3 \
  --set "aws.roleArn=$ROLE_ARN" \
  --set "rosa.clusterName=$CLUSTER_NAME"</code></p>
</li>
<li>
<p>Append remoteWrite settings to the user-workload-monitoring config to forward user workload metrics to Thanos.</p>
<p><strong>Check if the User Workload Config Map exists:</strong></p>
<p><code>bash
oc -n openshift-user-workload-monitoring get \
  configmaps user-workload-monitoring-config</code></p>
<p><strong>If the config doesn't exist run:</strong></p>
<p><code>bash
cat &lt;&lt; EOF | kubectl apply -f -
apiVersion: v1
kind: ConfigMap
metadata:
  name: user-workload-monitoring-config
  namespace: openshift-user-workload-monitoring
data:
  config.yaml: |
prometheus:
  remoteWrite:
    - url: "http://thanos-receive.${NAMESPACE}.svc.cluster.local:9091/api/v1/receive"
EOF</code></p>
<p><strong>Otherwise update it with the following:</strong></p>
<p><code>bash
oc -n openshift-user-workload-monitoring edit \
  configmaps user-workload-monitoring-config</code></p>
<p><code>yaml
  data:
    config.yaml: |
      ...
      prometheus:
      ...
        remoteWrite:
          - url: "http://thanos-receive.thanos-receiver.svc.cluster.local:9091/api/v1/receive"</code></p>
</li>
</ol>
<h2 id="rosa-federated-metrics-check-metrics-are-flowing-by-logging-into-grafana">Check metrics are flowing by logging into Grafana</h2>
<ol>
<li>
<p>get the Route URL for Grafana (remember its https) and login using username <code>root</code> and the password you updated to (or the default of <code>secret</code>).</p>
<p><code>bash
oc -n thanos-receiver get route grafana-route</code></p>
</li>
<li>
<p>Once logged in go to <strong>Dashboards-&gt;Manage</strong> and expand the <strong>federated-metrics</strong> group and you should see the cluster metrics dashboards.  Click on the <strong>Use Method / Cluster</strong> Dashboard and you should see metrics.  \o/.</p>
</li>
</ol>
<p><img alt="screenshot of grafana with federated cluster metrics" src="../rosa/federated-metrics/grafana-metrics.png" /></p></section><h1 class='nav-section-title-end'>Ended: Observability</h1>
                        <h1 class='nav-section-title' id='section-security'>
                            Security <a class='headerlink' href='#section-security' title='Permanent link'>↵</a>
                        </h1>
                        
                        <h2 class='nav-section-title' id='section-k8s-secret-store-csi-driver'>
                            K8s Secret Store CSI Driver <a class='headerlink' href='#section-k8s-secret-store-csi-driver' title='Permanent link'>↵</a>
                        </h2>
                        <section class="print-page" id="security-secrets-store-csi-hashicorp-vault"><h1 id="security-secrets-store-csi-hashicorp-vault-installing-the-hashicorp-vault-secret-csi-driver">Installing the HashiCorp Vault Secret CSI Driver</h1>
<p>The HashiCorp Vault Secret CSI Driver allows you to access secrets stored in HashiCorp Vault as Kubernetes Volumes.</p>
<h2 id="security-secrets-store-csi-hashicorp-vault-prerequisites">Prerequisites</h2>
<ol>
<li>An OpenShift Cluster (ROSA, ARO, OSD, and OCP 4.x all work)</li>
<li>kubectl</li>
<li>helm v3</li>
</ol>
<p>{% include_relative install-kubernetes-secret-store-driver.md %}</p>
<h2 id="security-secrets-store-csi-hashicorp-vault-install-hashicorp-vault-with-csi-driver-enabled">Install HashiCorp Vault with CSI driver enabled</h2>
<ol>
<li>
<p>Add the HashiCorp Helm Repository</p>
<p><code>bash
helm repo add hashicorp https://helm.releases.hashicorp.com</code></p>
</li>
<li>
<p>Update your Helm Repositories</p>
<p><code>bash
helm repo update</code></p>
</li>
<li>
<p>Create a namespace for Vault</p>
<p><code>bash
oc new-project hashicorp-vault</code></p>
</li>
<li>
<p>Create a SCC for the CSI driver</p>
<p><code>bash
oc adm policy add-scc-to-user privileged \
  system:serviceaccount:hashicorp-vault:vault-csi-provider</code></p>
</li>
<li>
<p>Create a values file for Helm to use</p>
<p><code>bash
cat &lt;&lt; EOF &gt; values.yaml
global:
  openshift: true
csi:
  enabled: true
  daemonSet:
    providersDir: /var/run/secrets-store-csi-providers
injector:
  enabled: false
server:
  image:
    repository: "registry.connect.redhat.com/hashicorp/vault"
    tag: "1.8.0-ubi"
  dev:
    enabled: true
EOF</code></p>
</li>
<li>
<p>Install Hashicorp Vault with CSI enabled</p>
<p><code>bash
helm install -n hashicorp-vault vault \
  hashicorp/vault --values values.yaml</code></p>
</li>
<li>
<p>Patch the CSI daemonset</p>
<blockquote>
<p>Currently the CSI has a bug in its manifest which we need to patch</p>
</blockquote>
<p><code>bash
kubectl patch daemonset vault-csi-provider --type='json' \
    -p='[{"op": "add", "path": "/spec/template/spec/containers/0/securityContext", "value": {"privileged": true} }]'</code></p>
</li>
</ol>
<h2 id="security-secrets-store-csi-hashicorp-vault-configure-hashicorp-vault">Configure Hashicorp Vault</h2>
<ol>
<li>
<p>Get a bash prompt inside the Vault pod</p>
<p><code>bash
oc exec -it vault-0 -- bash</code></p>
</li>
<li>
<p>Create a Secret in Vault</p>
<p><code>bash
vault kv put secret/db-pass password="hunter2"</code></p>
</li>
<li>
<p>Configure Vault to use Kubernetes Auth</p>
<p><code>bash
vault auth enable kubernetes</code></p>
</li>
<li>
<p>Check your Cluster's token issuer</p>
<p><code>bash
oc get authentication.config cluster \
  -o json | jq -r .spec.serviceAccountIssuer</code></p>
</li>
<li>
<p>Configure Kubernetes auth method</p>
<blockquote>
<p>If the issuer here does not match the above, update it.</p>
</blockquote>
<p><code>bash
vault write auth/kubernetes/config \
issuer="https://kubernetes.default.svc.cluster.local" \
token_reviewer_jwt="$(cat /var/run/secrets/kubernetes.io/serviceaccount/token)" \
kubernetes_host="https://$KUBERNETES_PORT_443_TCP_ADDR:443" \
kubernetes_ca_cert=@/var/run/secrets/kubernetes.io/serviceaccount/ca.crt</code></p>
</li>
<li>
<p>Create a policy for our app</p>
<p><code>bash
vault policy write internal-app - &lt;&lt;EOF
path "secret/data/db-pass" {
  capabilities = ["read"]
}
EOF</code></p>
</li>
<li>
<p>Create an auth role to access it</p>
<p><code>bash
vault write auth/kubernetes/role/database \
  bound_service_account_names=webapp-sa \
  bound_service_account_namespaces=default \
  policies=internal-app \
  ttl=20m</code></p>
</li>
<li>
<p>exit from the vault-0 pod</p>
<p><code>bash
exit</code></p>
</li>
</ol>
<h2 id="security-secrets-store-csi-hashicorp-vault-deploy-a-sample-application">Deploy a sample application</h2>
<ol>
<li>
<p>Create a SecretProviderClass in the default namespace</p>
<p><code>bash
cat &lt;&lt;EOF | kubectl apply -f -
apiVersion: secrets-store.csi.x-k8s.io/v1alpha1
kind: SecretProviderClass
metadata:
  name: vault-database
  namespace: default
spec:
  provider: vault
  parameters:
    vaultAddress: "http://vault.hashicorp-vault:8200"
    roleName: "database"
    objects: |
      - objectName: "db-password"
        secretPath: "secret/data/db-pass"
        secretKey: "password"
EOF</code></p>
</li>
<li>
<p>Create a service account <code>webapp-sa</code></p>
<p><code>bash
kubectl create serviceaccount -n default webapp-sa</code></p>
</li>
<li>
<p>Create a Pod to use the secret</p>
<p><code>bash
cat &lt;&lt; EOF | kubectl apply -f -
kind: Pod
apiVersion: v1
metadata:
  name: webapp
  namespace: default
spec:
  serviceAccountName: webapp-sa
  containers:
  - image: jweissig/app:0.0.1
    name: webapp
    volumeMounts:
    - name: secrets-store-inline
      mountPath: "/mnt/secrets-store"
      readOnly: true
  volumes:
    - name: secrets-store-inline
      csi:
        driver: secrets-store.csi.k8s.io
        readOnly: true
        volumeAttributes:
          secretProviderClass: "vault-database"
EOF</code></p>
</li>
<li>
<p>Check the Pod has the secret</p>
<p><code>bash
kubectl -n default exec webapp \
  -- cat /mnt/secrets-store/db-password</code></p>
<p>The output should match</p>
<p><code>bash
hunter2</code></p>
</li>
</ol>
<h2 id="security-secrets-store-csi-hashicorp-vault-uninstall-hashicorp-vault-with-csi-driver-enabled">Uninstall HashiCorp Vault with CSI driver enabled</h2>
<ol>
<li>
<p>Delete the pod and</p>
<p><code>bash
kubectl delete -n default pod webapp
kubectl delete -n default secretproviderclass vault-database
kubectl delete -n default serviceaccount webapp-sa</code></p>
</li>
<li>
<p>Delete the Hashicorp Vault Helm</p>
<p><code>bash
helm delete -n hashicorp-vault vault</code></p>
</li>
<li>
<p>Delete the SCC for Hashicorp Vault</p>
<p><code>bash
oc adm policy remove-scc-from-user privileged \
  system:serviceaccount:hashicorp-vault:vault-csi-provider</code></p>
</li>
<li>
<p>Delete the Hashicorp vault project</p>
<p><code>bash
oc delete project hashicorp-vault</code></p>
</li>
</ol>
<p>{% include_relative uninstall-kubernetes-secret-store-driver.md %}</p></section><section class="print-page" id="rosa-aws-secrets-manager-csi"><h1 id="rosa-aws-secrets-manager-csi-using-aws-secrets-manager-csi-on-red-hat-openshift-on-aws-with-sts">Using AWS Secrets Manager CSI on Red Hat OpenShift on AWS with STS</h1>
<p>Author <strong>Paul Czarkowski</strong></p>
<p><em>last modified 2021-08-17</em></p>
<p>The AWS Secrets and Configuration Provider (ASCP) provides a way to expose AWS Secrets as Kubernetes storage volumes. With the ASCP, you can store and manage your secrets in Secrets Manager and then retrieve them through your workloads running on ROSA or OSD.</p>
<p>This is made even easier / more secure through the use of AWS STS and Kubernetes PodIdentity.</p>
<h2 id="rosa-aws-secrets-manager-csi-prerequisites">Prerequisites</h2>
<ul>
<li><a href="#docs-rosa-sts">A ROSA cluster deployed with STS</a></li>
<li>Helm 3</li>
<li>aws CLI</li>
<li>jq</li>
</ul>
<h3 id="rosa-aws-secrets-manager-csi-preparing-environment">Preparing Environment</h3>
<ol>
<li>
<p>Validate that your cluster has STS</p>
<p><code>bash
oc get authentication.config.openshift.io cluster -o json \
| jq .spec.serviceAccountIssuer</code></p>
<p>You should see something like the following, if not you should not proceed, instead look to the <a href="https://docs.openshift.com/rosa/rosa_getting_started_sts/rosa_creating_a_cluster_with_sts/rosa-sts-creating-a-cluster-quickly.html">Red Hat documentation on creating an STS cluster</a>.</p>
<p><code>"https://rh-oidc.s3.us-east-1.amazonaws.com/xxxxxx"</code></p>
</li>
<li>
<p>Set SecurityContextConstraints to allow the CSI driver to run</p>
<p><code>bash
oc new-project csi-secrets-store
oc adm policy add-scc-to-user privileged \
  system:serviceaccount:csi-secrets-store:secrets-store-csi-driver
oc adm policy add-scc-to-user privileged \
  system:serviceaccount:csi-secrets-store:csi-secrets-store-provider-aws</code></p>
</li>
<li>
<p>Create some environment variables to refer to later</p>
<p><code>bash
export ROSA_CLUSTER_NAME=my-cluster
export ROSA_CLUSTER_ID=$(rosa describe cluster -c $ROSA_CLUSTER_NAME --output json | jq -r .id)
export REGION=us-east-2 
export OIDC_ENDPOINT=$(oc get authentication.config.openshift.io cluster -o json | jq .spec.serviceAccountIssuer)
export AWS_ACCOUNT_ID=`aws sts get-caller-identity --query Account --output text`
export AWS_PAGER=""</code></p>
</li>
</ol>
<h2 id="rosa-aws-secrets-manager-csi-deploy-the-aws-secrets-and-configuration-provider">Deploy the AWS Secrets and Configuration Provider</h2>
<ol>
<li>
<p>Use Helm to register the secrets store csi driver</p>
<p><code>bash
helm repo add secrets-store-csi-driver https://kubernetes-sigs.github.io/secrets-store-csi-driver/charts</code></p>
</li>
<li>
<p>Update your Helm Repositories</p>
<p><code>bash
helm repo update</code></p>
</li>
<li>
<p>Install the secrets store csi driver</p>
<p><code>bash
helm upgrade --install -n csi-secrets-store csi-secrets-store-driver secrets-store-csi-driver/secrets-store-csi-driver</code></p>
</li>
<li>
<p>Deploy the AWS provider</p>
<p><code>bash
kubectl -n csi-secrets-store apply -f \
  https://raw.githubusercontent.com/rh-mobb/documentation/main/docs/security/secrets-store-csi/aws-provider-installer.yaml</code></p>
</li>
<li>
<p>Check that both Daemonsets are running</p>
<p><code>bash
kubectl -n csi-secrets-store get ds \
  csi-secrets-store-provider-aws \
  csi-secrets-store-driver-secrets-store-csi-driver</code></p>
</li>
</ol>
<h2 id="rosa-aws-secrets-manager-csi-creating-a-secret-and-iam-access-policies">Creating a Secret and IAM Access Policies</h2>
<ol>
<li>
<p>Create a secret in Secrets Manager</p>
<p>```bash
SECRET_ARN=$(aws --region "$REGION" secretsmanager  create-secret \
  --name MySecret --secret-string \
  '{"username":"shadowman", "password":"hunter2"}' \
  --query ARN --output text)</p>
<p>echo $SECRET_ARN
```</p>
</li>
<li>
<p>Create IAM Access Policy document</p>
<p><code>bash
cat &lt;&lt; EOF &gt; policy.json
{
  "Version": "2012-10-17",
  "Statement": [{
      "Effect": "Allow",
      "Action": [
        "secretsmanager:GetSecretValue",
        "secretsmanager:DescribeSecret"
      ],
      "Resource": ["$SECRET_ARN"]
      }]
}
EOF</code></p>
</li>
<li>
<p>Create an IAM Access Policy</p>
<p><code>bash
POLICY_ARN=$(aws --region "$REGION" --query Policy.Arn \
  --output text iam create-policy \
  --policy-name openshift-access-to-mysecret-policy \
  --policy-document file://policy.json)
echo $POLICY_ARN</code></p>
</li>
<li>
<p>Create IAM Role trust policy document</p>
<blockquote>
<p>Note you can use Conditions to lock down to a specific namespace or service account here. But for simplicity we're keeping it open.</p>
</blockquote>
<p><code>bash
cat &lt;&lt;EOF &gt; trust-policy.json
{
"Version": "2012-10-17",
"Statement": [
  {
  "Effect": "Allow",
  "Principal": {
    "Federated": "arn:aws:iam::$AWS_ACCOUNT_ID:oidc-provider/rh-oidc.s3.us-east-1.amazonaws.com/$ROSA_CLUSTER_ID"
  },
  "Action": "sts:AssumeRoleWithWebIdentity"
  }
]
}
EOF</code></p>
</li>
<li>
<p>Create IAM Role</p>
<p><code>bash
ROLE_ARN=$(aws iam create-role --role-name openshift-access-to-mysecret \
  --assume-role-policy-document file://trust-policy.json \
  --query Role.Arn --output text)
echo $ROLE_ARN</code></p>
</li>
<li>
<p>Attach Role to the Policy</p>
<p><code>bash
aws iam attach-role-policy --role-name openshift-access-to-mysecret --policy-arn $POLICY_ARN</code></p>
</li>
</ol>
<h2 id="rosa-aws-secrets-manager-csi-create-an-application-to-use-this-secret">Create an Application to use this secret</h2>
<ol>
<li>
<p>Create an OpenShift project</p>
<p><code>bash
oc new-project my-application</code></p>
</li>
<li>
<p>Annotate the default service account to use the STS Role</p>
<p><code>bash
oc annotate -n my-application serviceaccount default \
  eks.amazonaws.com/role-arn=$ROLE_ARN</code></p>
</li>
<li>
<p>Create a secret provider class to access our secret</p>
<p><code>bash
cat &lt;&lt; EOF | kubectl apply -f -
apiVersion: secrets-store.csi.x-k8s.io/v1
kind: SecretProviderClass
metadata:
  name: my-application-aws-secrets
spec:
  provider: aws
  parameters:
    objects: |
        - objectName: "MySecret"
          objectType: "secretsmanager"
EOF</code></p>
</li>
<li>
<p>Create a Deployment using our secret</p>
<p><code>bash
cat &lt;&lt; EOF | kubectl apply -f -
apiVersion: v1
kind: Pod
metadata:
  name: my-application
  labels:
    app: my-application
spec:
  volumes:
  - name: secrets-store-inline
    csi:
      driver: secrets-store.csi.k8s.io
      readOnly: true
      volumeAttributes:
        secretProviderClass: "my-application-aws-secrets"
  containers:
  - name: my-application-deployment
    image: k8s.gcr.io/e2e-test-images/busybox:1.29
    command:
      - "/bin/sleep"
      - "10000"
    volumeMounts:
    - name: secrets-store-inline
      mountPath: "/mnt/secrets-store"
      readOnly: true
EOF</code></p>
</li>
<li>
<p>Verify the Pod has the secret mounted</p>
<p><code>bash
kubectl exec -it my-application -- cat /mnt/secrets-store/MySecret</code></p>
</li>
</ol>
<h2 id="rosa-aws-secrets-manager-csi-cleanup">Cleanup</h2>
<ol>
<li>
<p>Delete application</p>
<p><code>bash
oc delete project my-application</code></p>
</li>
<li>
<p>Delete the secrets store csi driver</p>
<p><code>bash
helm delete -n kube-system csi-secrets-store</code></p>
</li>
<li>
<p>Delete the AWS provider</p>
<p><code>bash
kubectl -n kube-system delete -f \
  https://raw.githubusercontent.com/aws/secrets-store-csi-driver-provider-aws/main/deployment/aws-provider-installer.yaml</code></p>
</li>
<li>
<p>Delete Security Context Constraints</p>
<p><code>bash
oc adm policy remove-scc-from-user privileged \
  system:serviceaccount:kube-system:secrets-store-csi-driver
oc adm policy remove-scc-from-user privileged \
  system:serviceaccount:kube-system:csi-secrets-store-provider-aws</code></p>
</li>
<li>
<p>Delete AWS Roles and Policies</p>
<p><code>bash
aws iam detach-role-policy --role-name openshift-access-to-mysecret --policy-arn $POLICY_ARN
aws iam delete-role --role-name openshift-access-to-mysecret
aws iam delete-policy --policy-arn $POLICY_ARN</code></p>
</li>
</ol></section><section class="print-page" id="security-secrets-store-csi-azure-key-vault"><h1 id="security-secrets-store-csi-azure-key-vault-azure-key-vault-csi-on-azure-red-hat-openshift">Azure Key Vault CSI on Azure Red Hat OpenShift</h1>
<p><strong>Author: Paul Czarkowski</strong>
<em>Modified: 08/16/2021</em></p>
<p>This document is adapted from the <a href="https://azure.github.io/secrets-store-csi-driver-provider-azure/demos/standard-walkthrough/">Azure Key Vault CSI Walkthrough</a> specifically to run with Azure Red Hat OpenShift (ARO).</p>
<h2 id="security-secrets-store-csi-azure-key-vault-prerequisites">Prerequisites</h2>
<ol>
<li><a href="#docs-quickstart-aro">An ARO cluster</a></li>
<li>The AZ CLI (logged in)</li>
<li>Helm 3.x CLI</li>
</ol>
<h3 id="security-secrets-store-csi-azure-key-vault-environment-variables">Environment Variables</h3>
<ol>
<li>
<p>Run this command to set some environment variables to use throughout</p>
<blockquote>
<p>Note if you created the cluster from the instructions linked <a href="#docs-quickstart-aro">above</a> these will re-use the same environment variables, or default them to <code>openshift</code> and <code>eastus</code>.</p>
</blockquote>
<p><code>bash
export KEYVAULT_RESOURCE_GROUP=${AZR_RESOURCE_GROUP:-"openshift"}
export KEYVAULT_LOCATION=${AZR_RESOURCE_LOCATION:-"eastus"}
export KEYVAULT_NAME=secret-store-$(cat /dev/urandom | tr -dc 'a-zA-Z0-9' | fold -w 10 | head -n 1)
export AZ_TENANT_ID=$(az account show -o tsv --query tenantId)</code></p>
</li>
</ol>
<p>{% include_relative install-kubernetes-secret-store-driver.md %}</p>
<h2 id="security-secrets-store-csi-azure-key-vault-deploy-azure-key-store-csi">Deploy Azure Key Store CSI</h2>
<ol>
<li>
<p>Add the Azure Helm Repository</p>
<p><code>bash
helm repo add csi-secrets-store-provider-azure \
  https://raw.githubusercontent.com/Azure/secrets-store-csi-driver-provider-azure/master/charts</code></p>
</li>
<li>
<p>Update your local Helm Repositories</p>
<p><code>bash
helm repo update</code></p>
</li>
<li>
<p>Install the Azure Key Vault CSI provider</p>
<p><code>bash
helm install -n k8s-secrets-store-csi azure-csi-provider \
  csi-secrets-store-provider-azure/csi-secrets-store-provider-azure \
  --set linux.privileged=true --set secrets-store-csi-driver.install=false \
  --set "linux.providersDir=/var/run/secrets-store-csi-providers" \
  --version=v1.0.1</code></p>
</li>
<li>
<p>Set SecurityContextConstraints to allow the CSI driver to run</p>
<p><code>bash
oc adm policy add-scc-to-user privileged \
  system:serviceaccount:k8s-secrets-store-csi:csi-secrets-store-provider-azure</code></p>
</li>
</ol>
<h2 id="security-secrets-store-csi-azure-key-vault-create-keyvault-and-a-secret">Create Keyvault and a Secret</h2>
<ol>
<li>
<p>Create a namespace for your application</p>
<p><code>bash
oc new-project my-application</code></p>
</li>
<li>
<p>Create an Azure Keyvault in your Resource Group that contains ARO</p>
<p><code>bash
az keyvault create -n ${KEYVAULT_NAME} \
  -g ${KEYVAULT_RESOURCE_GROUP} \
  --location ${KEYVAULT_LOCATION}</code></p>
</li>
<li>
<p>Create a secret in the Keyvault</p>
<p><code>bash
az keyvault secret set \
  --vault-name ${KEYVAULT_NAME} \
  --name secret1 --value "Hello"</code></p>
</li>
<li>
<p>Create a Service Principal for the keyvault</p>
</li>
</ol>
<blockquote>
<p>Note: If this gives you an error, you may need upgrade your Azure CLI to the latest version.</p>
</blockquote>
<pre><code>```bash
export SERVICE_PRINCIPAL_CLIENT_SECRET="$(az ad sp create-for-rbac --skip-assignment --name http://$KEYVAULT_NAME --query 'password' -otsv)"
export SERVICE_PRINCIPAL_CLIENT_ID="$(az ad sp list --display-name http://$KEYVAULT_NAME --query '[0].appId' -otsv)"
```
</code></pre>
<ol>
<li>
<p>Set an Access Policy for the Service Principal</p>
<p><code>bash
az keyvault set-policy -n ${KEYVAULT_NAME} \
  --secret-permissions get \
  --spn ${SERVICE_PRINCIPAL_CLIENT_ID}</code></p>
</li>
<li>
<p>Create and label a secret for Kubernetes to use to access the Key Vault</p>
<p><code>bash
kubectl create secret generic secrets-store-creds \
  -n my-application \
  --from-literal clientid=${SERVICE_PRINCIPAL_CLIENT_ID} \
  --from-literal clientsecret=${SERVICE_PRINCIPAL_CLIENT_SECRET}
kubectl -n my-application label secret \
  secrets-store-creds secrets-store.csi.k8s.io/used=true</code></p>
</li>
</ol>
<h2 id="security-secrets-store-csi-azure-key-vault-deploy-an-application-that-uses-the-csi">Deploy an Application that uses the CSI</h2>
<ol>
<li>
<p>Create a Secret Provider Class to give access to this secret</p>
<p><code>bash
cat &lt;&lt;EOF | kubectl apply -f -
apiVersion: secrets-store.csi.x-k8s.io/v1alpha1
kind: SecretProviderClass
metadata:
  name: azure-kvname
  namespace: my-application
spec:
  provider: azure
  parameters:
usePodIdentity: "false"
useVMManagedIdentity: "false"
userAssignedIdentityID: ""
keyvaultName: "${KEYVAULT_NAME}"
objects: |
  array:
    - |
      objectName: secret1
      objectType: secret
      objectVersion: ""
tenantId: "${AZ_TENANT_ID}"
EOF</code></p>
</li>
<li>
<p>Create a Pod that uses the above Secret Provider Class</p>
<p><code>bash
cat &lt;&lt;EOF | kubectl apply -f -
kind: Pod
apiVersion: v1
metadata:
  name: busybox-secrets-store-inline
  namespace: my-application
spec:
  containers:
  - name: busybox
image: k8s.gcr.io/e2e-test-images/busybox:1.29
command:
  - "/bin/sleep"
  - "10000"
volumeMounts:
- name: secrets-store-inline
  mountPath: "/mnt/secrets-store"
  readOnly: true
  volumes:
- name: secrets-store-inline
  csi:
    driver: secrets-store.csi.k8s.io
    readOnly: true
    volumeAttributes:
      secretProviderClass: "azure-kvname"
    nodePublishSecretRef:
      name: secrets-store-creds
EOF</code></p>
</li>
<li>
<p>Check the Secret is mounted</p>
<p><code>bash
kubectl exec busybox-secrets-store-inline -- ls /mnt/secrets-store/</code></p>
<p>Output should match:</p>
<p><code>secret1</code></p>
</li>
<li>
<p>Print the Secret</p>
<p><code>bash
kubectl exec busybox-secrets-store-inline \
  -- cat /mnt/secrets-store/secret1</code></p>
<p>Output should match:</p>
<p><code>Hello</code></p>
</li>
</ol>
<h2 id="security-secrets-store-csi-azure-key-vault-cleanup">Cleanup</h2>
<ol>
<li>
<p>Uninstall Helm</p>
<p><code>bash
helm uninstall -n k8s-secrets-store-csi azure-csi-provider</code></p>
</li>
<li>
<p>Delete the app</p>
<p><code>bash
oc delete project my-application</code></p>
</li>
<li>
<p>Delete the Azure Key Vault</p>
<p><code>bash
az keyvault delete -n ${KEYVAULT_NAME}</code></p>
</li>
<li>
<p>Delete the Service Principal</p>
<p><code>bash
az ad sp delete --id ${SERVICE_PRINCIPAL_CLIENT_ID}</code></p>
</li>
</ol>
<p>{% include_relative uninstall-kubernetes-secret-store-driver.md %}</p></section><h1 class='nav-section-title-end'>Ended: K8s Secret Store CSI Driver</h1>
                        <h2 class='nav-section-title' id='section-configuring-idps'>
                            Configuring IDPs <a class='headerlink' href='#section-configuring-idps' title='Permanent link'>↵</a>
                        </h2>
                        
                        <h3 class='nav-section-title' id='section-azure-ad'>
                            Azure AD <a class='headerlink' href='#section-azure-ad' title='Permanent link'>↵</a>
                        </h3>
                        <section class="print-page" id="idp-azuread-aro"><h1 id="idp-azuread-aro-configure-aro-to-use-azure-ad">Configure ARO to use Azure AD</h1>
<p><strong>Michael McNeill, Sohaib Azed</strong></p>
<p><em>28 July 2022</em></p>
<p>This guide demonstrates how to configure Azure AD as the cluster identity provider in Azure Red Hat OpenShift. This guide will walk through the creation of an Azure Active Directory (Azure AD) application and configure Azure Red Hat OpenShift (ARO) to authenticate using Azure AD. </p>
<p>This guide will walk through the following steps:</p>
<ol>
<li>Register a new application in Azure AD for authentication. </li>
<li>Configure the application registration in Azure AD to include optional claims in tokens.</li>
<li>Configure the Azure Red Hat OpenShift (ARO) cluster to use Azure AD as the identity provider.</li>
<li>Grant additional permissions to individual users.</li>
</ol>
<h2 id="idp-azuread-aro-before-you-begin">Before you Begin</h2>
<p>If you are using <code>zsh</code> as your shell (which is the default shell on macOS) you may need to run <code>set -k</code> to get the below commands to run without errors. <a href="https://zsh.sourceforge.io/Doc/Release/Options.html">This is because <code>zsh</code> disables comments in interactive shells from being used</a>. </p>
<h2 id="idp-azuread-aro-1-register-a-new-application-in-azure-ad-for-authenitcation">1. Register a new application in Azure AD for authenitcation</h2>
<h3 id="idp-azuread-aro-capture-the-oauth-callback-url">Capture the OAuth callback URL</h3>
<p>First, construct the cluster's OAuth callback URL and make note of it. To do so, run the following command, making sure to replace the variables specified:</p>
<p>The "AAD" directory at the end of the the OAuth callback URL should match the OAuth identity provider name you'll setup later.</p>
<pre><code class="language-bash">RESOURCE_GROUP=example-rg # Replace this with the name of your ARO cluster's resource group
CLUSTER_NAME=example-cluster # Replace this with the name of your ARO cluster
echo 'OAuth callback URL: '$(az aro show -g $RESOURCE_GROUP -n $CLUSTER_NAME --query consoleProfile.url -o tsv | sed 's/console-openshift-console/oauth-openshift/')'oauth2callback/AAD'
</code></pre>
<h3 id="idp-azuread-aro-register-a-new-application-in-azure-ad">Register a new application in Azure AD</h3>
<p>Second, you need to create the Azure AD application itself. To do so, login to the Azure portal, and navigate to <a href="https://portal.azure.com/#blade/Microsoft_AAD_RegisteredApps/ApplicationsListBlade">App registrations blade</a>, then click on "New registration" to create a new application.</p>
<p><img alt="Azure Portal - App registrations blade" src="../idp/azuread-aro/images/azure-portal_app-registrations-blade.png" /></p>
<p>Provide a name for the application, for example <code>openshift-auth</code>. Select "Web" from the Redirect URI dropdown and fill in the Redirect URI using the value of the OAuth callback URL you retrieved in the previous step. Once you fill in the necessary information, click "Register" to create the application.</p>
<p><img alt="Azure Portal - Register an application page" src="../idp/azuread-aro/images/azure-portal_register-an-application-page.png" /></p>
<p>Then, click on the "Certificates &amp; secrets" sub-blade and select "New client secret". Fill in the details request and make note of the generated client secret value, as you'll use it in a later step. You won't be able to retrieve it again.</p>
<p><img alt="Azure Portal - Certificates &amp; secrets page" src="../idp/azuread-aro/images/azure-portal_certificates-secrets-page.png" />
<img alt="Azure Portal - Add a Client Secret page" src="../idp/azuread-aro/images/azure-portal_add-a-client-secret-page.png" />
<img alt="Azure Portal - Copy Client Secret page" src="../idp/azuread-aro/images/azure-portal_copy-client-secret-page.png" /></p>
<p>Then, click on the "Overview" sub-blade and make note of the "Application (client) ID" and "Directory (tenant) ID". You'll need those values in a later step as well.</p>
<h2 id="idp-azuread-aro-2-configure-optional-claims">2. Configure optional claims</h2>
<p>In order to provide OpenShift with enough information about the user to create their account, we will configure Azure AD to provide two optional claims, specifically "email" and "upn" when a user logs in. For more information on optional claims in Azure AD, see <a href="https://docs.microsoft.com/en-us/azure/active-directory/develop/active-directory-optional-claims">the Microsoft documentation</a>.</p>
<p>Click on the "Token configuration" sub-blade and select the "Add optional claim" button. </p>
<p><img alt="Azure Portal - Add Optional Claims Page" src="../idp/azuread-aro/images/azure-portal_optional-claims-page.png" /></p>
<p>Select ID then check the "email" and "upn" claims and click the "Add" button to configure them for your Azure AD application. </p>
<p><img alt="Azure Portal - Add Optional Claims - Token Type" src="../idp/azuread-aro/images/azure-portal_add-optional-claims-page.png" />
<img alt="Azure Portal - Add Optional Claims - email" src="../idp/azuread-aro/images/azure-portal_add-optional-email-claims-page.png" />
<img alt="Azure Portal - Add Optional Claims - upn" src="../idp/azuread-aro/images/azure-portal_add-optional-upn-claims-page.png" /></p>
<p>When prompted, follow the prompt to enable the necessary Microsoft Graph permissions.</p>
<p><img alt="Azure Portal - Add Optional Claims - Graph Permissions Prompt" src="../idp/azuread-aro/images/azure-portal_add-optional-claims-graph-permissions-prompt.png" /></p>
<h2 id="idp-azuread-aro-3-configure-the-openshift-cluster-to-use-azure-ad-as-the-identity-provider">3. Configure the OpenShift cluster to use Azure AD as the identity provider</h2>
<p>Finally, we need to configure OpenShift to use Azure AD as its identity provider. </p>
<p>To do so, ensure you are logged in to the OpenShift command line interface (<code>oc</code>) by running the following command, making sure to replace the variables specified:</p>
<pre><code class="language-bash">RESOURCE_GROUP=example-rg # Replace this with the name of your ARO cluster's resource group
CLUSTER_NAME=example-cluster # Replace this with the name of your ARO cluster
oc login \
    $(az aro show -g $RESOURCE_GROUP -n $CLUSTER_NAME --query apiserverProfile.url -o tsv) \
    -u $(az aro list-credentials -g $RESOURCE_GROUP -n $CLUSTER_NAME --query kubeadminUsername -o tsv) \
    -p $(az aro list-credentials -g $RESOURCE_GROUP -n $CLUSTER_NAME --query kubeadminPassword -o tsv)
</code></pre>
<p>Next, create a secret that contains the client secret that you captured in step 2 above. To do so, run the following command, making sure to replace the variable specified:</p>
<pre><code class="language-bash">CLIENT_SECRET=xxxxxxxx-xxxx-xxxx-xxxx-xxxxxxxxxxxx # Replace this with the Client Secret
oc create secret generic openid-client-secret --from-literal=clientSecret=${CLIENT_SECRET} -n openshift-config
</code></pre>
<p>Next, generate the necessary YAML for the cluster's OAuth provider to use Azure AD as its identity provider. To do so, run the following command, making sure to replace the variables specified:</p>
<pre><code class="language-bash">IDP_NAME=AAD # Replace this with the name you used in the OAuth callback URL
APP_ID=yyyyyyyy-yyyy-yyyy-yyyy-yyyyyyyyyyyy # Replace this with the Application (client) ID
TENANT_ID=zzzzzzzz-zzzz-zzzz-zzzz-zzzzzzzzzzzz # Replace this with the Directory (tenant) ID
cat &lt;&lt; EOF &gt; cluster-oauth-config.yaml
apiVersion: config.openshift.io/v1
kind: OAuth
metadata:
  name: cluster
spec:
  identityProviders:
  - mappingMethod: claim
    name: ${IDP_NAME}
    openID:
      claims:
        email:
        - email
        name:
        - name
        preferredUsername:
        - upn
      clientID: ${APP_ID}
      clientSecret:
        name: openid-client-secret
      extraScopes: []
      issuer: https://login.microsoftonline.com/${TENANT_ID}/v2.0
    type: OpenID
EOF
</code></pre>
<p>Feel free to further modify this output (which is saved in your current directory as <code>cluster-oauth-config.yaml</code>).</p>
<p>Finally, apply the new configuration to the cluster's OAuth provider by running the following command:</p>
<pre><code class="language-bash">oc apply -f ./cluster-oauth-config.yaml
</code></pre>
<blockquote>
<p><strong>Note:</strong> It is normal to receive an error that says an annotation is missing when you run <code>oc apply</code> for the first time. This can be safely ignored.</p>
</blockquote>
<h2 id="idp-azuread-aro-4-grant-additional-permissions-to-individual-users">4. Grant additional permissions to individual users</h2>
<p>Once the cluster authentication operator reconciles your changes (generally within a few minutes), you will be able to login to the cluster using Azure AD. </p>
<p>Once you login, you will notice that you have very limited permissions. This is because, by default, OpenShift only grants you the ability to create new projects (namespaces) in the cluster. Other projects (namespaces) are restricted from view. </p>
<p>OpenShift includes a signifcant number of pre-configured roles, including the <code>cluster-admin</code> role that grants full access and control over the clster. To grant your user access to the <code>cluster-admin</code> role, you must create a ClusterRoleBinding to your user account.</p>
<pre><code class="language-bash">USERNAME=example@redhat.com # Replace with your Azure AD username
oc create clusterrolebinding cluster-admin-user \
    --clusterrole=cluster-admin \
    --user=$USERNAME
</code></pre>
<p>For more information on how to use RBAC to define and apply permissions in OpenShift, see <a href="https://docs.openshift.com/container-platform/latest/authentication/using-rbac.html">the OpenShift documentation</a>.</p></section><section class="print-page" id="idp-group-claims-aro"><h1 id="idp-group-claims-aro-configure-aro-to-use-azure-ad-group-claims">Configure ARO to use Azure AD Group Claims</h1>
<p><strong>Michael McNeill</strong></p>
<p><em>28 July 2022</em></p>
<p>This guide demonstrates how to utilize the OpenID Connect group claim functionality implemented in OpenShift 4.10. This functionality allows an identity provider to provide a user's group membership for use within OpenShift. This guide will walk through the creation of an Azure Active Directory (Azure AD) application, configure the necessary Azure AD groups, and configure Azure Red Hat OpenShift (ARO) to authenticate and manage authorization using Azure AD. </p>
<p>This guide will walk through the following steps:</p>
<ol>
<li>Register a new application in Azure AD for authentication. </li>
<li>Configure the application registration in Azure AD to include optional and group claims in tokens.</li>
<li>Configure the Azure Red Hat OpenShift (ARO) cluster to use Azure AD as the identity provider.</li>
<li>Grant additional permissions to individual groups.</li>
</ol>
<h2 id="idp-group-claims-aro-before-you-begin">Before you Begin</h2>
<p>Create a set of security groups and assign users by following <a href="https://docs.microsoft.com/en-us/azure/active-directory/fundamentals/active-directory-groups-create-azure-portal">the Microsoft documentation</a>.</p>
<p>In addition, if you are using <code>zsh</code> as your shell (which is the default shell on macOS) you may need to run <code>set -k</code> to get the below commands to run without errors. <a href="https://zsh.sourceforge.io/Doc/Release/Options.html">This is because <code>zsh</code> disables comments in interactive shells from being used</a>. </p>
<h2 id="idp-group-claims-aro-1-register-a-new-application-in-azure-ad-for-authenitcation">1. Register a new application in Azure AD for authenitcation</h2>
<h3 id="idp-group-claims-aro-capture-the-oauth-callback-url">Capture the OAuth callback URL</h3>
<p>First, construct the cluster's OAuth callback URL and make note of it. To do so, run the following command, making sure to replace the variables specified:</p>
<p>The "AAD" directory at the end of the the OAuth callback URL should match the OAuth identity provider name you'll setup later.</p>
<pre><code class="language-bash">RESOURCE_GROUP=example-rg # Replace this with the name of your ARO cluster's resource group
CLUSTER_NAME=example-cluster # Replace this with the name of your ARO cluster
echo 'OAuth callback URL: '$(az aro show -g $RESOURCE_GROUP -n $CLUSTER_NAME --query consoleProfile.url -o tsv | sed 's/console-openshift-console/oauth-openshift/')'oauth2callback/AAD'
</code></pre>
<h3 id="idp-group-claims-aro-register-a-new-application-in-azure-ad">Register a new application in Azure AD</h3>
<p>Second, you need to create the Azure AD application itself. To do so, login to the Azure portal, and navigate to <a href="https://portal.azure.com/#blade/Microsoft_AAD_RegisteredApps/ApplicationsListBlade">App registrations blade</a>, then click on "New registration" to create a new application.</p>
<p><img alt="Azure Portal - App registrations blade" src="../idp/group-claims/images/azure-portal_app-registrations-blade.png" /></p>
<p>Provide a name for the application, for example <code>openshift-auth</code>. Select "Web" from the Redirect URI dropdown and fill in the Redirect URI using the value of the OAuth callback URL you retrieved in the previous step. Once you fill in the necessary information, click "Register" to create the application.</p>
<p><img alt="Azure Portal - Register an application page" src="../idp/group-claims/images/azure-portal_register-an-application-page.png" /></p>
<p>Then, click on the "Certificates &amp; secrets" sub-blade and select "New client secret". Fill in the details request and make note of the generated client secret value, as you'll use it in a later step. You won't be able to retrieve it again.</p>
<p><img alt="Azure Portal - Certificates &amp; secrets page" src="../idp/group-claims/images/azure-portal_certificates-secrets-page.png" />
<img alt="Azure Portal - Add a Client Secret page" src="../idp/group-claims/images/azure-portal_add-a-client-secret-page.png" />
<img alt="Azure Portal - Copy Client Secret page" src="../idp/group-claims/images/azure-portal_copy-client-secret-page.png" /></p>
<p>Then, click on the "Overview" sub-blade and make note of the "Application (client) ID" and "Directory (tenant) ID". You'll need those values in a later step as well.</p>
<h2 id="idp-group-claims-aro-2-configure-optional-claims-for-optional-and-group-claims">2. Configure optional claims (for optional and group claims)</h2>
<p>In order to provide OpenShift with enough information about the user to create their account, we will configure Azure AD to provide two optional claims, specifically "email" and "upn", as well as a group claim when a user logs in. For more information on optional claims in Azure AD, see <a href="https://docs.microsoft.com/en-us/azure/active-directory/develop/active-directory-optional-claims">the Microsoft documentation</a>.</p>
<p>Click on the "Token configuration" sub-blade and select the "Add optional claim" button. </p>
<p><img alt="Azure Portal - Add Optional Claims Page" src="../idp/group-claims/images/azure-portal_optional-claims-page.png" /></p>
<p>Select ID then check the "email" and "upn" claims and click the "Add" button to configure them for your Azure AD application. </p>
<p><img alt="Azure Portal - Add Optional Claims - Token Type" src="../idp/group-claims/images/azure-portal_add-optional-claims-page.png" />
<img alt="Azure Portal - Add Optional Claims - email" src="../idp/group-claims/images/azure-portal_add-optional-email-claims-page.png" />
<img alt="Azure Portal - Add Optional Claims - upn" src="../idp/group-claims/images/azure-portal_add-optional-upn-claims-page.png" /></p>
<p>When prompted, follow the prompt to enable the necessary Microsoft Graph permissions.</p>
<p><img alt="Azure Portal - Add Optional Claims - Graph Permissions Prompt" src="../idp/group-claims/images/azure-portal_add-optional-claims-graph-permissions-prompt.png" /></p>
<p>Next, select the "Add groups claim" button. </p>
<p><img alt="Azure Portal - Add Groups Claim Page" src="../idp/group-claims/images/azure-portal_optional-group-claims-page.png" /></p>
<p>Select the "Security groups" option and click the "Add" button to configure group claims for your Azure AD application. </p>
<blockquote>
<p><strong>Note:</strong> In this example, we are providing all security groups a user is a member of via the group claim. In a real production environment, we highly recommend <em>scoping the groups provided by the group claim to _only those groups which are applicable to OpenShift</em>.</p>
</blockquote>
<p><img alt="Azure Portal - Edit Groups Claim Page" src="../idp/group-claims/images/azure-portal_edit-group-claims-page.png" /></p>
<h2 id="idp-group-claims-aro-3-configure-the-openshift-cluster-to-use-azure-ad-as-the-identity-provider">3. Configure the OpenShift cluster to use Azure AD as the identity provider</h2>
<p>Finally, we need to configure OpenShift to use Azure AD as its identity provider. </p>
<p>To do so, ensure you are logged in to the OpenShift command line interface (<code>oc</code>) by running the following command, making sure to replace the variables specified:</p>
<pre><code class="language-bash">RESOURCE_GROUP=example-rg # Replace this with the name of your ARO cluster's resource group
CLUSTER_NAME=example-cluster # Replace this with the name of your ARO cluster
oc login \
    $(az aro show -g $RESOURCE_GROUP -n $CLUSTER_NAME --query apiserverProfile.url -o tsv) \
    -u $(az aro list-credentials -g $RESOURCE_GROUP -n $CLUSTER_NAME --query kubeadminUsername -o tsv) \
    -p $(az aro list-credentials -g $RESOURCE_GROUP -n $CLUSTER_NAME --query kubeadminPassword -o tsv)
</code></pre>
<p>Next, create a secret that contains the client secret that you captured in step 2 above. To do so, run the following command, making sure to replace the variable specified:</p>
<pre><code class="language-bash">CLIENT_SECRET=xxxxxxxx-xxxx-xxxx-xxxx-xxxxxxxxxxxx # Replace this with the Client Secret
oc create secret generic openid-client-secret --from-literal=clientSecret=${CLIENT_SECRET} -n openshift-config
</code></pre>
<p>Next, generate the necessary YAML for the cluster's OAuth provider to use Azure AD as its identity provider. To do so, run the following command, making sure to replace the variables specified:</p>
<pre><code class="language-bash">IDP_NAME=AAD # Replace this with the name you used in the OAuth callback URL
APP_ID=yyyyyyyy-yyyy-yyyy-yyyy-yyyyyyyyyyyy # Replace this with the Application (client) ID
TENANT_ID=zzzzzzzz-zzzz-zzzz-zzzz-zzzzzzzzzzzz # Replace this with the Directory (tenant) ID
cat &lt;&lt; EOF &gt; cluster-oauth-config.yaml
apiVersion: config.openshift.io/v1
kind: OAuth
metadata:
  name: cluster
spec:
  identityProviders:
  - mappingMethod: claim
    name: ${IDP_NAME}
    openID:
      claims:
        email:
        - email
        groups:
        - groups
        name:
        - name
        preferredUsername:
        - upn
      clientID: ${APP_ID}
      clientSecret:
        name: openid-client-secret
      extraScopes: []
      issuer: https://login.microsoftonline.com/${TENANT_ID}/v2.0
    type: OpenID
EOF
</code></pre>
<p>Feel free to further modify this output (which is saved in your current directory as <code>cluster-oauth-config.yaml</code>).</p>
<p>Finally, apply the new configuration to the cluster's OAuth provider by running the following command:</p>
<pre><code class="language-bash">oc apply -f ./cluster-oauth-config.yaml
</code></pre>
<blockquote>
<p><strong>Note:</strong> It is normal to receive an error that says an annotation is missing when you run <code>oc apply</code> for the first time. This can be safely ignored.</p>
</blockquote>
<p>Once the cluster authentication operator reconciles your changes (generally within a few minutes), you will be able to login to the cluster using Azure AD. In addition, the cluster OAuth provider will automatically create or update the membership of groups the user is a member of (using the group ID). The provider <strong>does not</strong> automatically create RoleBindings and ClusterRoleBindings for the groups that are created, you are responsible for creating those via your own processes. </p>
<h2 id="idp-group-claims-aro-4-grant-additional-permissions-to-individual-groups">4. Grant additional permissions to individual groups</h2>
<p>Once the cluster authentication operator reconciles your changes (generally within a few minutes), you will be able to login to the cluster using Azure AD. In addition, the cluster OAuth provider will automatically create or update the membership of groups the user is a member of (using the group ID). </p>
<p>Once you login, you will notice that you have very limited permissions. This is because, by default, OpenShift only grants you the ability to create new projects (namespaces) in the cluster. Other projects (namespaces) are restricted from view. The cluster OAth provider <strong>does not</strong> automatically create RoleBindings and ClusterRoleBindings for the groups that are created, you are responsible for creating those via your own processes. </p>
<p>OpenShift includes a signifcant number of pre-configured roles, including the <code>cluster-admin</code> role that grants full access and control over the clster. To grant an automatically generated group access to the <code>cluster-admin</code> role, you must create a ClusterRoleBinding to the group ID.</p>
<pre><code class="language-bash">GROUP_ID=wwwwwwww-wwww-wwww-wwww-wwwwwwwwwwww # Replace with your Azure AD Group ID that you would like to have cluster admin permissions
oc create clusterrolebinding cluster-admin-group \
    --clusterrole=cluster-admin \
    --group=$GROUP_ID
</code></pre>
<p>Now, any user in the specified group will automatically be granted <code>cluster-admin</code> access.</p>
<p>For more information on how to use RBAC to define and apply permissions in OpenShift, see <a href="https://docs.openshift.com/container-platform/latest/authentication/using-rbac.html">the OpenShift documentation</a>.</p></section><section class="print-page" id="idp-group-claims-rosa"><h1 id="idp-group-claims-rosa-configure-rosa-to-use-azure-ad-group-claims">Configure ROSA to use Azure AD Group Claims</h1>
<p><strong>Michael McNeill</strong></p>
<p><em>28 July 2022</em></p>
<p>This guide demonstrates how to utilize the OpenID Connect group claim functionality implemented in OpenShift 4.10. This functionality allows an identity provider to provide a user's group membership for use within OpenShift. This guide will walk through the creation of an Azure Active Directory (Azure AD) application, configure the necessary Azure AD groups, and configure Red Hat OpenShift Service on AWS (ROSA) to authenticate and manage authorization using Azure AD. </p>
<p>This guide will walk through the following steps:</p>
<ol>
<li>Register a new application in Azure AD for authentication. </li>
<li>Configure the application registration in Azure AD to include optional and group claims in tokens.</li>
<li>Configure the OpenShift cluster to use Azure AD as the identity provider.</li>
<li>Grant additional permissions to individual groups.</li>
</ol>
<h2 id="idp-group-claims-rosa-before-you-begin">Before you Begin</h2>
<p>Create a set of security groups and assign users by following <a href="https://docs.microsoft.com/en-us/azure/active-directory/fundamentals/active-directory-groups-create-azure-portal">the Microsoft documentation</a>.</p>
<p>In addition, if you are using <code>zsh</code> as your shell (which is the default shell on macOS) you may need to run <code>set -k</code> to get the below commands to run without errors. <a href="https://zsh.sourceforge.io/Doc/Release/Options.html">This is because <code>zsh</code> disables comments in interactive shells from being used</a>. </p>
<h2 id="idp-group-claims-rosa-1-register-a-new-application-in-azure-ad-for-authenitcation">1. Register a new application in Azure AD for authenitcation</h2>
<h3 id="idp-group-claims-rosa-capture-the-oauth-callback-url">Capture the OAuth callback URL</h3>
<p>First, construct the cluster's OAuth callback URL and make note of it. To do so, run the following command, making sure to replace the variable specified:</p>
<p>The "AAD" directory at the end of the the OAuth callback URL should match the OAuth identity provider name you'll setup later.</p>
<pre><code class="language-bash">CLUSTER_NAME=example-cluster # Replace this with the name of your ROSA cluster
domain=$(rosa describe cluster -c $CLUSTER_NAME | grep &quot;DNS&quot; | grep -oE '\S+.openshiftapps.com')
echo &quot;OAuth callback URL: https://oauth-openshift.apps.$domain/oauth2callback/AAD&quot;
</code></pre>
<h3 id="idp-group-claims-rosa-register-a-new-application-in-azure-ad">Register a new application in Azure AD</h3>
<p>Second, you need to create the Azure AD application itself. To do so, login to the Azure portal, and navigate to <a href="https://portal.azure.com/#blade/Microsoft_AAD_RegisteredApps/ApplicationsListBlade">App registrations blade</a>, then click on "New registration" to create a new application.</p>
<p><img alt="Azure Portal - App registrations blade" src="../idp/group-claims/images/azure-portal_app-registrations-blade.png" /></p>
<p>Provide a name for the application, for example <code>openshift-auth</code>. Select "Web" from the Redirect URI dropdown and fill in the Redirect URI using the value of the OAuth callback URL you retrieved in the previous step. Once you fill in the necessary information, click "Register" to create the application.</p>
<p><img alt="Azure Portal - Register an application page" src="../idp/group-claims/images/azure-portal_register-an-application-page.png" /></p>
<p>Then, click on the "Certificates &amp; secrets" sub-blade and select "New client secret". Fill in the details request and make note of the generated client secret value, as you'll use it in a later step. You won't be able to retrieve it again.</p>
<p><img alt="Azure Portal - Certificates &amp; secrets page" src="../idp/group-claims/images/azure-portal_certificates-secrets-page.png" />
<img alt="Azure Portal - Add a Client Secret page" src="../idp/group-claims/images/azure-portal_add-a-client-secret-page.png" />
<img alt="Azure Portal - Copy Client Secret page" src="../idp/group-claims/images/azure-portal_copy-client-secret-page.png" /></p>
<p>Then, click on the "Overview" sub-blade and make note of the "Application (client) ID" and "Directory (tenant) ID". You'll need those values in a later step as well.</p>
<h2 id="idp-group-claims-rosa-2-configure-optional-claims-for-optional-and-group-claims">2. Configure optional claims (for optional and group claims)</h2>
<p>In order to provide OpenShift with enough information about the user to create their account, we will configure Azure AD to provide two optional claims, specifically "email" and "upn", as well as a group claim when a user logs in. For more information on optional claims in Azure AD, see <a href="https://docs.microsoft.com/en-us/azure/active-directory/develop/active-directory-optional-claims">the Microsoft documentation</a>.</p>
<p>Click on the "Token configuration" sub-blade and select the "Add optional claim" button. </p>
<p><img alt="Azure Portal - Add Optional Claims Page" src="../idp/group-claims/images/azure-portal_optional-claims-page.png" /></p>
<p>Select ID then check the "email" and "upn" claims and click the "Add" button to configure them for your Azure AD application. </p>
<p><img alt="Azure Portal - Add Optional Claims - Token Type" src="../idp/group-claims/images/azure-portal_add-optional-claims-page.png" />
<img alt="Azure Portal - Add Optional Claims - email" src="../idp/group-claims/images/azure-portal_add-optional-email-claims-page.png" />
<img alt="Azure Portal - Add Optional Claims - upn" src="../idp/group-claims/images/azure-portal_add-optional-upn-claims-page.png" /></p>
<p>When prompted, follow the prompt to enable the necessary Microsoft Graph permissions.</p>
<p><img alt="Azure Portal - Add Optional Claims - Graph Permissions Prompt" src="../idp/group-claims/images/azure-portal_add-optional-claims-graph-permissions-prompt.png" /></p>
<p>Next, select the "Add groups claim" button. </p>
<p><img alt="Azure Portal - Add Groups Claim Page" src="../idp/group-claims/images/azure-portal_optional-group-claims-page.png" /></p>
<p>Select the "Security groups" option and click the "Add" button to configure group claims for your Azure AD application. </p>
<blockquote>
<p><strong>Note:</strong> In this example, we are providing all security groups a user is a member of via the group claim. In a real production environment, we highly recommend <em>scoping the groups provided by the group claim to _only those groups which are applicable to OpenShift</em>.</p>
</blockquote>
<p><img alt="Azure Portal - Edit Groups Claim Page" src="../idp/group-claims/images/azure-portal_edit-group-claims-page.png" /></p>
<h2 id="idp-group-claims-rosa-3-configure-the-openshift-cluster-to-use-azure-ad-as-the-identity-provider">3. Configure the OpenShift cluster to use Azure AD as the identity provider</h2>
<p>Finally, we need to configure OpenShift to use Azure AD as its identity provider. While Red Hat OpenShift Service on AWS (ROSA) offers the ability to configure identity providers via the OpenShift Cluster Manager (OCM), that functionality does not currently support group claims. Instead, we'll configure the cluster's OAuth provider to use Azure AD as its identity provider via the <code>rosa</code> CLI. To do so, run the following command, making sure to replace the variable specified:</p>
<pre><code class="language-bash">CLUSTER_NAME=example-cluster # Replace this with the name of your ROSA cluster
IDP_NAME=AAD # Replace this with the name you used in the OAuth callback URL
APP_ID=yyyyyyyy-yyyy-yyyy-yyyy-yyyyyyyyyyyy # Replace this with the Application (client) ID
CLIENT_SECRET=xxxxxxxx-xxxx-xxxx-xxxx-xxxxxxxxxxxx # Replace this with the Client Secret
TENANT_ID=zzzzzzzz-zzzz-zzzz-zzzz-zzzzzzzzzzzz # Replace this with the Directory (tenant) ID
rosa create idp \
    --cluster ${CLUSTER_NAME} \
    --type openid \
    --name ${IDP_NAME} \
    --client-id ${APP_ID} \
    --client-secret ${CLIENT_SECRET} \
    --issuer-url https://login.microsoftonline.com/${TENANT_ID}/v2.0 \
    --email-claims email \
    --name-claims name \
    --username-claims upn \
    --groups-claims groups
</code></pre>
<h2 id="idp-group-claims-rosa-4-grant-additional-permissions-to-individual-groups">4. Grant additional permissions to individual groups</h2>
<p>Once the cluster authentication operator reconciles your changes (generally within a few minutes), you will be able to login to the cluster using Azure AD. In addition, the cluster OAuth provider will automatically create or update the membership of groups the user is a member of (using the group ID). </p>
<p>Once you login, you will notice that you have very limited permissions. This is because, by default, OpenShift only grants you the ability to create new projects (namespaces) in the cluster. Other projects (namespaces) are restricted from view. The cluster OAth provider <strong>does not</strong> automatically create RoleBindings and ClusterRoleBindings for the groups that are created, you are responsible for creating those via your own processes. </p>
<p>OpenShift includes a signifcant number of pre-configured roles, including the <code>cluster-admin</code> role that grants full access and control over the clster. To grant an automatically generated group access to the <code>cluster-admin</code> role, you must create a ClusterRoleBinding to the group ID.</p>
<pre><code class="language-bash">GROUP_ID=wwwwwwww-wwww-wwww-wwww-wwwwwwwwwwww # Replace with your Azure AD Group ID that you would like to have cluster admin permissions
oc create clusterrolebinding cluster-admin-group \
    --clusterrole=cluster-admin \
    --group=$GROUP_ID
</code></pre>
<p>Now, any user in the specified group will automatically be granted <code>cluster-admin</code> access.</p>
<p>For more information on how to use RBAC to define and apply permissions in OpenShift, see <a href="https://docs.openshift.com/container-platform/latest/authentication/using-rbac.html">the OpenShift documentation</a>.</p></section><section class="print-page" id="idp-azuread"><h1 id="idp-azuread-configure-azure-ad-as-an-oidc-identity-provider-for-rosaosd">Configure Azure AD as an OIDC identity provider for ROSA/OSD</h1>
<p><strong>Andrea Bozzoni, Steve Mirman</strong></p>
<p><em>27 October 2021</em></p>
<p>The steps to add Azure AD as an identity provider for Red Hat OpenShift on AWS (ROSA) and OpenShift Dedicated (OSD) are:</p>
<ol>
<li>Define the OAuth callback URL</li>
<li>Register a new Webapp on Azure AD</li>
<li>Create the client secret</li>
<li>Configure the Token</li>
<li>Configure the OAuth identity provider in OCM</li>
</ol>
<h2 id="idp-azuread-define-the-oauth-callback-url">Define the OAuth callback URL</h2>
<p>You can find the callback URL in <a href="https://console.redhat.com/openshift/">OpenShift Cluster Manager</a> (OCM)</p>
<ol>
<li>
<p>Select your cluster in OCM and then go to the <strong>'Access control'</strong> tab.</p>
<p><img alt="ocm select access control tab" src="../idp/images/ocm_access_control.png" /></p>
</li>
<li>
<p>Pick OpenID as identity provider from the identity providers list.</p>
<p><img alt="ocm select OpenID as indenity provider" src="../idp/images/ocm_identity_providers_list.png" /></p>
</li>
<li>
<p>Give a name to the identity provider that we are adding to the OCP cluster</p>
<p><img alt="ocm set a name to the OpenID identity provider" src="../idp/images/ocm_indentity_providers_callback_url.png" /></p>
</li>
<li>
<p>Keep the OAuth callback URL to use later.</p>
<blockquote>
<p><strong>Note:</strong> the OAuth Callback has the following format:</p>
</blockquote>
<p><code>https://oauth-openshift.apps.&lt;cluster_name&gt;.&lt;cluster_domain&gt;/oauth2callback/&lt;idp_name&gt;</code></p>
</li>
</ol>
<h2 id="idp-azuread-register-a-new-webapp-on-azure-ad">Register a new Webapp on Azure AD</h2>
<p>Access your Azure account and select the Azure Active Directory service and execute the following steps:</p>
<ol>
<li>
<p>From the main menu add a new Webapp</p>
<p><img alt="azuread create a new webapp" src="../idp/images/azuread_add_webapp.png" /></p>
</li>
<li>
<p>Set the <strong>Name</strong> to <cluster_name> or something else unique to the cluster, set the  <strong>Redirect URI</strong> to the callback URL from above and click 'Register'</p>
<p><img alt="azuread add the callback URI" src="../idp/images/azuread_configure_webapp.png" /></p>
</li>
<li>
<p>Remember <strong>Application (client) ID</strong> and <strong>Directory (tenant) ID</strong> to be used later</p>
<p><img alt="azuread display the Webapp info registration" src="../idp/images/azuread_webapp_info.png" /></p>
</li>
</ol>
<h2 id="idp-azuread-create-the-client-secret">Create the client secret</h2>
<ol>
<li>
<p>Create a new Secret for the Webapp</p>
<p><img alt="azuread create a new Webapp secret" src="../idp/images/azuread_new_client_secret.png" /></p>
</li>
<li>
<p>Remember the <strong>Secret Value</strong> to be used later in the OCM OAuth configuration</p>
<p><img alt="azuread secret id" src="../idp/images/azuread_secret_id.png" /></p>
</li>
</ol>
<h2 id="idp-azuread-configure-the-token">Configure the Token</h2>
<ol>
<li>
<p>Create a new token configuration</p>
<p><img alt="azuread create a new token configuration" src="../idp/images/azuread_token_configuration.png" /></p>
</li>
<li>
<p>Select <strong>upn</strong> and <strong>email</strong> as optional claims</p>
<p><img alt="azuread add token claims" src="../idp/images/azuread_add_token_claims.png" /></p>
</li>
<li>
<p>Specify that the claim must be returned in the token.</p>
<p><img alt="azuread add token claim check" src="../idp/images/azuread_add_token_claims_2.png" /></p>
</li>
</ol>
<h2 id="idp-azuread-configure-the-oauth-identity-provider-in-ocm">Configure the OAuth identity provider in OCM</h2>
<ol>
<li>
<p>In the OCM fill all the fields with the values collected during the registration of the new Webapp in the Azure AD and click the 'Add' button.</p>
<p><img alt="ocm fill the oauth fields" src="../idp/images/ocm_oauth_id_filled.png" /></p>
</li>
<li>
<p>After a few minutes the Azure AD authentication methos will be available in the OpenShift console login screen</p>
<p><img alt="ocp login screen" src="../idp/images/ocp_login.png" /></p>
</li>
</ol></section><section class="print-page" id="idp-azuread-aro-cli"><h1 id="idp-azuread-aro-cli-configure-azure-ad-as-an-oidc-identity-provider-for-aro-with-cli">Configure Azure AD as an OIDC identity provider for ARO with cli</h1>
<p><strong>Daniel Moessner</strong></p>
<p><em>26 June 2022</em></p>
<p>The steps to add Azure AD as an identity provider for Azure Red Hat OpenShift (ARO) via cli are:</p>
<ul>
<li><a href="#idp-azuread-aro-cli-prerewuisites">Prerequisites</a><ul>
<li><a href="#idp-azuread-aro-cli-have-azure-cli-installed">Have Azure cli installed</a></li>
<li><a href="#idp-azuread-aro-cli-login-to-azure">Login to Azure</a></li>
</ul>
</li>
<li><a href="#idp-azuread-aro-cli-azure">Azure</a><ul>
<li><a href="#idp-azuread-aro-cli-define-needed-variables">Define needed variables</a></li>
<li><a href="#idp-azuread-aro-cli-get-oauthcallbackurl">Get oauthCallbackURL</a></li>
<li><a href="#idp-azuread-aro-cli-create-manifestjson-file-to-configure-the-azure-active-directory-application">Create manifest.json file to configure the Azure Active Directory application</a></li>
<li><a href="#idp-azuread-aro-cli-registercreate-app">Register/create app</a></li>
<li><a href="#idp-azuread-aro-cli-add-servive-principal-for-the-new-app">Add Servive Principal for the new app</a></li>
<li><a href="#idp-azuread-aro-cli-make-service-principal-and-enterprise-application">Make Service Principal an Enterprise Application</a></li>
<li><a href="#idp-azuread-aro-cli-create-the-client-secret">Create the client secret</a></li>
<li><a href="#idp-azuread-aro-cli-update-the-azure-ad-application-scope-permissions">Update the Azure AD application scope permissions</a></li>
<li><a href="#idp-azuread-aro-cli-get-tenant-id">Get Tenant ID</a></li>
</ul>
</li>
<li><a href="#idp-azuread-aro-cli-openshift">OpenShift</a><ul>
<li><a href="#idp-azuread-aro-cli-login-to-openshift-as-kubeadmin">Login to OpenShift as kubeadmin</a></li>
<li><a href="#idp-azuread-aro-cli-create-an-openshift-secret">Create an OpenShift secret</a></li>
<li><a href="#idp-azuread-aro-cli-apply-openshift-openid-authentication">Apply OpenShift OpenID authentication</a></li>
<li><a href="#idp-azuread-aro-cli-wait-for-authentication-operator-to-roll-out">Wait for authentication operator to roll out</a></li>
<li><a href="#idp-azuread-aro-cli-verify-login-through-azure-active-directory">Verify login through Azure Active Directory</a></li>
<li><a href="#idp-azuread-aro-cli-last-steps">Last steps</a></li>
</ul>
</li>
</ul>
<h2 id="idp-azuread-aro-cli-prerequisites">Prerequisites</h2>
<h3 id="idp-azuread-aro-cli-have-azure-cli-installed">Have Azure cli installed</h3>
<p>Follow the Microsoft instuctions: https://docs.microsoft.com/en-us/cli/azure/install-azure-cli</p>
<blockquote>
<p><strong>Note</strong>
This has been written for az cli verion <code>2.37.0</code> some commands will not work with previous versions, however, there is a known issue https://github.com/Azure/azure-cli/issues/23027 where we will use an older version via <code>podman run -it mcr.microsoft.com/azure-cli:2.36.0</code> . 
In case you're using <code>docker</code>, just replace <code>podman</code> command by <code>docker</code> . 
For podman installation on  Mac, Windows &amp; Linux, please refer to https://podman.io/getting-started/installation</p>
</blockquote>
<h3 id="idp-azuread-aro-cli-login-to-azure">Login to Azure</h3>
<p>Login to Azure as follows:</p>
<p><code>az login</code></p>
<p>If you're logging in from a system you have no access to your browser you can authenticate, you can also use</p>
<p><code>az login --use-device-code</code> </p>
<h2 id="idp-azuread-aro-cli-azure">Azure</h2>
<h3 id="idp-azuread-aro-cli-define-needed-variables">Define needed variables</h3>
<p>To simplly follow along, first define the following variables according to your set-up:</p>
<p><code>RESOURCEGROUP=&lt;cluster-dmoessne-aro01&gt; # replave with your name
   CLUSTERNAME=&lt;rg-dmoessne-aro01&gt;  # replave with your name</code></p>
<h3 id="idp-azuread-aro-cli-get-oauthcallbackurl">Get oauthCallbackURL</h3>
<p>To get the <code>oauthCallbackURL</code> for the Azure AD integration, run the following commands:
   ```
   DOMAIN=$(az aro show -g $RESOURCEGROUP -n $CLUSTERNAME --query clusterProfile.domain -o tsv)
   APISERVER=$(az aro show -g $RESOURCEGROUP -n $CLUSTERNAME --query apiserverProfile.url -o tsv)</p>
<p>oauthCallbackURL=https://oauth-openshift.apps.$DOMAIN/oauth2callback/AAD
   echo $oauthCallbackURL
   ```</p>
<blockquote>
<p><strong>Note</strong> 
<code>oauthCallbackURL</code>, in particular <code>AAD</code> can be changed but <strong>must</strong> match the name in the oauth providerwhen creating the OpenShift OpenID authentication   </p>
</blockquote>
<h3 id="idp-azuread-aro-cli-create-manifestjson-file-to-configure-the-azure-active-directory-application">Create <code>manifest.json</code> file to configure the Azure Active Directory application</h3>
<p>Configure OpenShift to use the <code>email</code> claim and fall back to <code>upn</code> to set the Preferred Username by adding the <code>upn</code> as part of the ID token returned by Azure Active Directory.</p>
<p>Create a <code>manifest.json</code> file to configure the Azure Active Directory application.</p>
<p><code>cat &lt;&lt; EOF &gt; manifest.json
   {
    "idToken": [
      {
       "name": "upn",
       "source": null,
       "essential": false,
       "additionalProperties": []
      },
      {
       "name": "email",
       "source": null,
       "essential": false,
       "additionalProperties": []
      }
     ]
   }  
   EOF</code></p>
<h3 id="idp-azuread-aro-cli-registercreate-app">Register/create app</h3>
<p>Create an Azure AD application and retrieve app id:</p>
<p>```
   DISPLAYNAME=<auth-dmoessne-aro01> # set you name accordingly </p>
<p>az ad app create \
   --display-name $DISPLAYNAME \
   --web-redirect-uris $oauthCallbackURL \
   --sign-in-audience AzureADMyOrg \
   --optional-claims @manifest.json
   ```</p>
<p><code>APPID=$(az ad app list --display-name $DISPLAYNAME --query [].appId -o tsv)</code></p>
<h3 id="idp-azuread-aro-cli-add-servive-principal-for-the-new-app">Add Servive Principal for the new app</h3>
<p>Create Pervice Principal for the app created:</p>
<p><code>az ad sp create --id $APPID</code></p>
<h3 id="idp-azuread-aro-cli-make-service-principal-an-enterprise-application">Make Service Principal an Enterprise Application</h3>
<p>We need this Service Principal to be an Enterprise Application to be able to add users and groups, so we add the needed tag (az cli &gt;= <code>2.38.0</code>)</p>
<p><code>az ad sp update --id $APPID --set 'tags=["WindowsAzureActiveDirectoryIntegratedApp"]'</code></p>
<blockquote>
<p><strong>Note</strong> 
 In case you get a trace back (az cli = <code>2.37.0</code>) check out https://github.com/Azure/azure-cli/issues/23027
 To overcome that issue, we'll do the following
```</p>
<h1 id="idp-azuread-aro-cli-app_idaz-ad-app-list-display-name-displayname-query-id-o-tsv">APP_ID=$(az ad app list --display-name $DISPLAYNAME --query [].id -o tsv)</h1>
<h1 id="idp-azuread-aro-cli-az-rest-method-patch-url-httpsgraphmicrosoftcomv10applicationsapp_id-body-tagswindowsazureactivedirectoryintegratedapp">az rest --method PATCH --url https://graph.microsoft.com/v1.0/applications/$APP_ID --body '{"tags":["WindowsAzureActiveDirectoryIntegratedApp"]}'</h1>
<p>```</p>
</blockquote>
<h3 id="idp-azuread-aro-cli-create-the-client-secret">Create the client secret</h3>
<p>The password for the app created is retrieved by resetting the same:</p>
<p><code>PASSWD=$(az ad app credential reset --id $APPID --query password -o tsv)</code> </p>
<blockquote>
<p><strong>Note</strong> 
The password generated with above command is by default valid for one year and you may want to change that by adding either and end date via 
<code>--end-date</code> or set validity in years with <code>--years</code>. 
For details consult the <a href="https://docs.microsoft.com/en-us/cli/azure/ad/app/credential?view=azure-cli-latest#az-ad-app-credential-reset">documentation</a></p>
</blockquote>
<h3 id="idp-azuread-aro-cli-update-the-azure-ad-application-scope-permissions">Update the Azure AD application scope permissions</h3>
<p>To be able to read the user information from Azure Active Directory, we need to add the following Azure Active Directory Graph permissions</p>
<p>Add permission for the Azure Active Directory as follows:</p>
<ul>
<li>
<p>read email
   <code>az ad app permission add \
   --api 00000003-0000-0000-c000-000000000000 \
   --api-permissions 64a6cdd6-aab1-4aaf-94b8-3cc8405e90d0=Scope \
   --id $APPID</code></p>
</li>
<li>
<p>read profile
   <code>az ad app permission add \
   --api 00000003-0000-0000-c000-000000000000 \
   --api-permissions 14dad69e-099b-42c9-810b-d002981feec1=Scope \
   --id $APPID</code></p>
</li>
<li>
<p>User.Read
   <code>az ad app permission add \
   --api 00000003-0000-0000-c000-000000000000 \
   --api-permissions e1fe6dd8-ba31-4d61-89e7-88639da4683d=Scope \
   --id $APPID</code></p>
<blockquote>
<p><strong>Note</strong>
If you see a message that you need to grant consent you can safely ignore it, unless you are authenticated as a alobal administrator for this Azure Active Directory. Standard domain users will be asked to grant consent when they first login to the cluster using their AAD credentials.</p>
</blockquote>
</li>
</ul>
<h3 id="idp-azuread-aro-cli-get-tenant-id">Get Tenant ID</h3>
<p>We do need the Tenant ID for setting up the Oauth provider later on:</p>
<p><code>TENANTID=$(az account show --query tenantId -o tsv)</code></p>
<blockquote>
<p><strong>Note</strong>
Now we can switch over to our OpenShift installation and apply the needed configuraion.
Please refer to https://docs.openshift.com/container-platform/latest/cli_reference/openshift_cli/getting-started-cli.html to get the latest <code>oc</code> cli</p>
</blockquote>
<h2 id="idp-azuread-aro-cli-openshift">OpenShift</h2>
<h3 id="idp-azuread-aro-cli-login-to-openshift-as-kubeadmin">Login to OpenShift as kubeadmin</h3>
<p>Fetch kubeadmin password and login to your cluster via <code>oc</code> cli (you can use any other cluster-admin user in case you have already created/added other oauth providers)</p>
<p>```
   KUBEPW=$(az aro list-credentials \
   --name $CLUSTERNAME \
   --resource-group $RESOURCEGROUP \
   --query kubeadminPassword --output tsv)</p>
<p>oc login $APISERVER -u kubeadmin -p $KUBEPW
   ``` </p>
<h3 id="idp-azuread-aro-cli-create-an-openshift-secret">Create an OpenShift secret</h3>
<p>Create an OpenShift secret to store the Azure Active Directory application secret from the application password we created/reset earlier:</p>
<p><code>oc create secret generic openid-client-secret-azuread \
   -n openshift-config \
   --from-literal=clientSecret=$PASSWD</code></p>
<h3 id="idp-azuread-aro-cli-apply-openshift-openid-authentication">Apply OpenShift OpenID authentication</h3>
<p>As a last step we need to apply the OpenShift OpenID authentication for Azure Active Directory:</p>
<p><code>cat &lt;&lt; EOF | oc apply -f -
   apiVersion: config.openshift.io/v1
   kind: OAuth
   metadata:
     name: cluster
   spec:
     identityProviders:
     - name: AAD
       mappingMethod: claim
       type: OpenID
       openID:
         clientID: $APPID
         clientSecret:
           name: openid-client-secret-azuread
         extraScopes:
         - email
         - profile
         extraAuthorizeParameters:
           include_granted_scopes: "true"
         claims:
           preferredUsername:
           - email
           - upn
           name:
           - name
           email:
           - email
         issuer: https://login.microsoftonline.com/$TENANTID
   EOF</code></p>
<h3 id="idp-azuread-aro-cli-wait-for-authentication-operator-to-roll-out">Wait for authentication operator to roll out</h3>
<p>Before we move over to the OpenShift login, let's wait for the new version of the authentication cluster operator to be rolled out</p>
<p><code>watch -n 5 oc get co authentication</code></p>
<blockquote>
<p><strong>Note</strong>
it may take some time until the rollout starts </p>
</blockquote>
<h3 id="idp-azuread-aro-cli-verify-login-through-azure-active-directory">Verify login through Azure Active Directory</h3>
<p>Get console url to login:</p>
<p><code>az aro show --name $CLUSTERNAME --resource-group $RESOURCEGROUP --query "consoleProfile.url" -o tsv</code></p>
<p>Opening the url in a browser, we can see the login to Azure AD is available</p>
<p><img alt="AADLoginPage" src="../idp/azuread-aro-cli/AAD_enabled.png" /></p>
<p>At first login you may have to accept application permissions</p>
<p><img alt="AcceptPerms" src="../idp/azuread-aro-cli/accept_permissions.png" /></p>
<h3 id="idp-azuread-aro-cli-last-steps">Last steps</h3>
<p>As a last step you may want to grant a user or group cluster-admin permissions and remove kubeadmin user, see
- https://docs.openshift.com/container-platform/4.10/authentication/using-rbac.html#cluster-role-binding-commands_using-rbac
- https://docs.openshift.com/container-platform/4.10/authentication/remove-kubeadmin.html</p></section><h1 class='nav-section-title-end'>Ended: Azure AD</h1>
                        <h3 class='nav-section-title' id='section-gitlab'>
                            Gitlab <a class='headerlink' href='#section-gitlab' title='Permanent link'>↵</a>
                        </h3>
                        <section class="print-page" id="idp-gitlab"><h1 id="idp-gitlab-configure-gitlab-as-an-identity-provider-for-rosaosd">Configure GitLab as an identity provider for ROSA/OSD</h1>
<p><strong>Steve Mirman</strong></p>
<p><em>16 February 2022</em></p>
<p>The following instructions will detail how to configure GitLab as the identity provider for Managed OpenShift through the OpenShift Cluster Manager (OCM):</p>
<ol>
<li>Create OAuth callback URL in OCM</li>
<li>Register a new application in GitLab</li>
<li>Configure the identity provider credentials and URL</li>
<li>Add cluster-admin or dedicated-admin users</li>
<li>Log in and confirm</li>
</ol>
<h2 id="idp-gitlab-create-oauth-callback-url-in-ocm">Create OAuth callback URL in OCM</h2>
<p>Log in to the <a href="https://console.redhat.com/openshift/">OpenShift Cluster Manager</a> (OCM) to add a GitLab identity provider</p>
<ol>
<li>
<p>Select your cluster in OCM and then go to the <strong>'Access control'</strong> tab and select <strong>'Identity Providers'</strong></p>
<p><img alt="ocm select access control tab" src="../idp/gitlab/images/select_idp.png" /></p>
</li>
<li>
<p>Choose GitLab as identity provider from the identity providers list</p>
<p><img alt="ocm select OpenID as indenity provider" src="../idp/gitlab/images/select_gitlab.png" /></p>
</li>
<li>
<p>Provide a name for the new identity provider</p>
<p><img alt="ocm set a name to the OpenID identity provider" src="../idp/gitlab/images/idp_name.png" /></p>
</li>
<li>
<p>Copy the <strong>OAuth callback URL</strong>. It will be needed later</p>
<p><img alt="ocm set a name to the OpenID identity provider" src="../idp/gitlab/images/callback_url.png" /></p>
<blockquote>
<p><strong>Note:</strong> the OAuth Callback has the following format:</p>
</blockquote>
<p><code>https://oauth-openshift.apps.&lt;cluster_name&gt;.&lt;cluster_domain&gt;/oauth2callback/&lt;idp_name&gt;</code></p>
</li>
<li>
<p>At this point, leave the <strong>Client ID</strong>, <strong>Client secret</strong>, and <strong>URL</strong> blank while configuring GitLab</p>
<p><img alt="blank values" src="../idp/gitlab/images/blank_values.png" /></p>
</li>
</ol>
<h2 id="idp-gitlab-register-a-new-application-in-gitlab">Register a new application in GitLab</h2>
<p>Log into <strong>GitLab</strong> and execute the following steps:</p>
<ol>
<li>
<p>Go to <strong>Preferences</strong></p>
<p><img alt="GitLab Preferences" src="../idp/gitlab/images/gitlab_preferences.png" /></p>
</li>
<li>
<p>Select <strong>Applications</strong> from the left navigation bar</p>
<p><img alt="GitLab applications" src="../idp/gitlab/images/gitlab_apps.png" /></p>
</li>
<li>
<p>Provide a <strong>Name</strong> and enter the <strong>OAuth Callback URL</strong> copied from OCM above and enter it as the <strong>Redirect URL</strong> in GitLab</p>
<p><img alt="GitLab Redirect URL" src="../idp/gitlab/images/gitlab_redirect.png" /></p>
</li>
<li>
<p>Check the <strong>openid</strong> box and save the application</p>
<p><img alt="GitLab OpenID" src="../idp/gitlab/images/gitlab_openid.png" /></p>
</li>
<li>
<p>After saving the GitLab application you will be provided with an <strong>Application ID</strong> and a <strong>Secret</strong></p>
<p><img alt="GitLab Confirmation" src="../idp/gitlab/images/gitlab_saveapp.png" /></p>
</li>
<li>
<p>Copy both the <strong>Application ID</strong> and <strong>Secret</strong> and return to the OCM console</p>
<p><img alt="GitLab AppID" src="../idp/gitlab/images/gitlab_appid.png" />
<img alt="GitLab Secret" src="../idp/gitlab/images/gitlab_secret.png" /></p>
</li>
</ol>
<h2 id="idp-gitlab-configure-the-identity-provider-credentials-and-url">Configure the identity provider credentials and URL</h2>
<ol>
<li>
<p>Returning to the OCM console, enter the <strong>Application ID</strong> and <strong>Secret</strong> obtained from GitLab in the previous step and enter them as <strong>Client ID</strong> and <strong>Client Secret</strong> respectively in the OCM console. Additionally, provide the GitLab <strong>URL</strong> where credentials were obtained and click <strong>Add</strong></p>
<p><img alt="OCM Credentials" src="../idp/gitlab/images/ocm_credentials.png" /></p>
</li>
<li>
<p>The new GitLab identity provider should display in the IDP list</p>
<p><img alt="new IDP" src="../idp/gitlab/images/ocm_idplist.png" /></p>
</li>
</ol>
<h2 id="idp-gitlab-add-cluster-admin-or-dedicated-admin-users">Add cluster-admin or dedicated-admin users</h2>
<ol>
<li>
<p>Now that the GitLab identity provider is configured, it is possible to add authenticated users to elevated OCM and OpenShift roles. Under <strong>Cluster Roles and Access</strong> select <strong>Add user</strong> and enter an existing GitLab user. Then choose to assign <code>dedicated-admin</code> or <code>cluster-admin</code> permissions to the user and click <strong>Add user</strong></p>
<p><img alt="add cluster-admin user" src="../idp/gitlab/images/ocm_clusteradmin.png" /></p>
</li>
<li>
<p>The new user should now display, with proper permissions, in the cluster-admin or dedicated-admin user lists</p>
<p><img alt="confirm user" src="../idp/gitlab/images/ocm_confirm_ca.png" /></p>
</li>
</ol>
<h2 id="idp-gitlab-log-in-and-confirm">Log in and confirm</h2>
<ol>
<li>
<p>Select the <strong>Open console</strong> button in OCM to bring up the OpenShift login page. An option for <strong>GitLab</strong> should now be available.</p>
<blockquote>
<p>Note: I can take 1-2 minutes for this update to occur</p>
</blockquote>
<p><img alt="OpenShift GitLab login" src="../idp/gitlab/images/gitlab_login.png" /></p>
</li>
<li>
<p>After selecting GitLab for the first time an authorization message will appear. Click <strong>Authorize</strong> to confirm.</p>
<p><img alt="GitLab Authorize" src="../idp/gitlab/images/gitlab_authorize.png" /></p>
</li>
<li>
<p>Congratulations!</p>
<p><img alt="GitLab Authorize" src="../idp/gitlab/images/gitlab_complete.png" /></p>
</li>
</ol></section><section class="print-page" id="idp-gitlab-aro"><h1 id="idp-gitlab-aro-configure-gitlab-as-an-identity-provider-for-aro">Configure GitLab as an identity provider for ARO</h1>
<p><strong>Steve Mirman</strong></p>
<p><em>28 March 2022</em></p>
<p>The following instructions will detail how to configure GitLab as the identity provider for Azure Red Hat OpenShift:</p>
<ol>
<li>Register a new application in GitLab</li>
<li>Create OAuth callback URL in ARO</li>
<li>Log in and confirm</li>
<li>Add administrative users or groups</li>
</ol>
<h2 id="idp-gitlab-aro-register-a-new-application-in-gitlab">Register a new application in GitLab</h2>
<p>Log into <strong>GitLab</strong> and execute the following steps:</p>
<ol>
<li>
<p>Go to <strong>Preferences</strong></p>
<p><img alt="GitLab Preferences" src="../idp/gitlab/images/gitlab_preferences.png" /></p>
</li>
<li>
<p>Select <strong>Applications</strong> from the left navigation bar</p>
<p><img alt="GitLab applications" src="../idp/gitlab/images/gitlab_apps.png" /></p>
</li>
<li>
<p>Provide a <strong>Name</strong> and enter an <strong>OAuth Callback URL</strong> as the <strong>Redirect URI</strong> in GitLab</p>
<blockquote>
<p><strong>Note:</strong> the OAuth Callback has the following format:
    <code>https://oauth-openshift.apps.&lt;cluster-id&gt;.&lt;region&gt;.aroapp.io/oauth2callback/GitLab</code></p>
</blockquote>
<p><img alt="GitLab Redirect URI" src="../idp/gitlab-aro/images/oauth-url.png" /></p>
</li>
<li>
<p>Check the <strong>openid</strong> box and save the application</p>
<p><img alt="GitLab OpenID" src="../idp/gitlab/images/gitlab_openid.png" /></p>
</li>
<li>
<p>After saving the GitLab application you will be provided with an <strong>Application ID</strong> and a <strong>Secret</strong></p>
<p><img alt="GitLab Confirmation" src="../idp/gitlab-aro/images/gitlab-saveapp.png" /></p>
</li>
<li>
<p>Copy both the <strong>Application ID</strong> and <strong>Secret</strong> for use in the ARO console</p>
</li>
</ol>
<h2 id="idp-gitlab-aro-create-oauth-provider-in-aro">Create OAuth provider in ARO</h2>
<p>Log in to the ARO console as an administrator to add a GitLab identity provider</p>
<ol>
<li>
<p>Select the <strong>'Administration'</strong> drop down and click <strong>'Cluster Settings'</strong></p>
<p><img alt="aro administration" src="../idp/gitlab-aro/images/cluster-settings.png" /></p>
</li>
<li>
<p>On the <strong>'Configuration'</strong> scroll down and click on <strong>'OAuth'</strong></p>
<p><img alt="aro select OAuth" src="../idp/gitlab-aro/images/oauth-select.png" /></p>
</li>
<li>
<p>Select <strong>'GitLab'</strong> from the Identity Providers drop down</p>
<p><img alt="aro select GitLab" src="../idp/gitlab-aro/images/idp-gitlab.png" /></p>
</li>
<li>
<p>Enter a <strong>Name</strong>, the base <strong>URL</strong> of your GitLab OAuth server, and the <strong>Client ID</strong> and <strong>CLient Secret</strong> from the previous step</p>
<p><img alt="Add the IDP" src="../idp/gitlab-aro/images/add-idp.png" /></p>
</li>
<li>
<p>Click <strong>Add</strong> to confirm the configuration</p>
<p><img alt="blank values" src="../idp/gitlab-aro/images/add-idp-complete.png" /></p>
</li>
</ol>
<h2 id="idp-gitlab-aro-log-in-and-confirm">Log in and confirm</h2>
<ol>
<li>
<p>Go to the ARO console in a new browser to bring up the OpenShift login page. An option for <strong>GitLab</strong> should now be available.</p>
<blockquote>
<p>Note: I can take 2-3 minutes for this update to occur</p>
</blockquote>
<p><img alt="OpenShift GitLab login" src="../idp/gitlab-aro/images/gitlab-login.png" /></p>
</li>
<li>
<p>After selecting GitLab for the first time an authorization message will appear. Click <strong>Authorize</strong> to confirm.</p>
<p><img alt="GitLab Authorize" src="../idp/gitlab-aro/images/gitlab-authorize.png" /></p>
</li>
<li>
<p>Once you have successfully logged in using GitLab, your userid should display under <strong>Users</strong> in the <strong>User Management</strong> section of the ARO console</p>
<p><img alt="GitLab user" src="../idp/gitlab-aro/images/gitlab-user.png" /></p>
<blockquote>
<p>Note: On initial login users do NOT have elevated access</p>
</blockquote>
</li>
</ol>
<h2 id="idp-gitlab-aro-add-administrative-users-or-groups">Add administrative users or groups</h2>
<ol>
<li>
<p>Now that the GitLab identity provider is configured, it is possible to add authenticated users to elevated OpenShift roles. This can be accomplished at the user or group level.</p>
</li>
<li>
<p>To elevate a users permissions, select the user in the OpenShift console and click <strong>Create Binding</strong> from the <strong>RoleBindings</strong> tab</p>
<p><img alt="GitLab user details" src="../idp/gitlab-aro/images/user-details.png" /></p>
</li>
<li>
<p>Choose the scope (namespace/cluster), assign a name to the RoleBinding, and choose a role.</p>
<p><img alt="GitLab user role" src="../idp/gitlab-aro/images/user-role.png" /></p>
</li>
<li>
<p>After clicking <strong>Create</strong> the assigned user will have elevated access once they log in.</p>
<p><img alt="GitLab user role confirm" src="../idp/gitlab-aro/images/user-role-confirm.png" /></p>
</li>
<li>
<p>To elevate a groups permissions, create a group in the OpenShift console.</p>
<p><img alt="GitLab group create" src="../idp/gitlab-aro/images/group-create.png" /></p>
</li>
<li>
<p>Edit the group YAML to specify a custom name and initial user set</p>
<p><img alt="GitLab user role" src="../idp/gitlab-aro/images/group-set.png" /></p>
</li>
<li>
<p>Create a RoleBinding for the group, similar to what was configured previously for an individual user</p>
<p><img alt="GitLab user role confirm" src="../idp/gitlab-aro/images/group-rolebinding.png" /></p>
</li>
<li>
<p>Add additional users to the YAML file as needed and they will assume the elevated access</p>
<p><img alt="GitLab Add Users" src="../idp/gitlab-aro/images/group-add-users.png" /></p>
</li>
</ol></section><h1 class='nav-section-title-end'>Ended: Gitlab</h1><h1 class='nav-section-title-end'>Ended: Configuring IDPs</h1>
                        <h2 class='nav-section-title' id='section-advanced-cluster-security'>
                            Advanced Cluster Security <a class='headerlink' href='#section-advanced-cluster-security' title='Permanent link'>↵</a>
                        </h2>
                        <section class="print-page" id="security-rhacs"><h1 id="security-rhacs-deploying-red-hat-advanced-cluster-security-in-arorosa">Deploying Red Hat Advanced Cluster Security in ARO/ROSA</h1>
<p><strong>Author: Roberto Carratalá</strong></p>
<p><em>Updated: 10/06/2022</em></p>
<p>This document is based in the <a href="https://redhat-scholars.github.io/acs-workshop/acs-workshop/index.html">RHACS workshop</a> and in the <a href="https://docs.openshift.com/acs/3.70/installing/install-ocp-operator.html">RHACS official documentation</a>.</p>
<h2 id="security-rhacs-prerequisites">Prerequisites</h2>
<ol>
<li><a href="#docs-quickstart-aro">An ARO cluster</a> or <a href="#docs-quickstart-rosa">a ROSA cluster</a>.</li>
</ol>
<h3 id="security-rhacs-set-up-the-openshift-cli-oc">Set up the OpenShift CLI (oc)</h3>
<ol>
<li>
<p>Download the OS specific OpenShift CLI from <a href="https://mirror.openshift.com/pub/openshift-v4/clients/ocp/latest/">Red Hat</a></p>
</li>
<li>
<p>Unzip the downloaded file on your local machine</p>
</li>
<li>
<p>Place the extracted <code>oc</code> executable in your OS path or local directory</p>
</li>
</ol>
<h2 id="security-rhacs-login-to-aro-rosa">Login to ARO / ROSA</h2>
<ul>
<li>Login to your ARO / ROSA clusters with user with cluster-admin privileges.</li>
</ul>
<h2 id="security-rhacs-installing-red-hat-advanced-cluster-security-in-arorosa">Installing Red Hat Advanced Cluster Security in ARO/ROSA</h2>
<p>For install RHACS in ARO/ROSA you have two options:</p>
<ul>
<li><strong>Option 1</strong> - Manual Installation</li>
<li><strong>Option 2</strong> - Automated Installation using Ansible</li>
</ul>
<h3 id="security-rhacs-option-1-manual-installation">Option 1 - Manual Installation</h3>
<p>For install RHACS using the Option 1 - Manual installation:</p>
<ol>
<li>
<p>Follow the steps within the <a href="https://redhat-scholars.github.io/acs-workshop/acs-workshop/02-getting_started.html#install_acs_operator">RHACS Operator Installation Workshop</a> to install the RHACS Operator.</p>
</li>
<li>
<p>Follow the steps within the <a href="https://redhat-scholars.github.io/acs-workshop/acs-workshop/02-getting_started.html#install_acs_central">RHACS Central Cluster Installation Workshop</a> to install the RHACS Central Cluster.</p>
</li>
<li>
<p>Follow the steps within the <a href="https://redhat-scholars.github.io/acs-workshop/acs-workshop/02-getting_started.html#config_acs_securedcluster">RHACS Secured Cluster Configuration</a>, to import the ARO/ROSA cluster into RHACS.</p>
</li>
</ol>
<h3 id="security-rhacs-option-2-automated-installation-using-ansible">Option 2 - Automated Installation using Ansible</h3>
<p>For install the RHACS in ROSA/ARO you can use the <a href="https://github.com/rh-mobb/rhacs-demo">rhacs-demo repository</a> that will install RH-ACS using Ansible playbooks:</p>
<ol>
<li>Clone the rhacm-demo repo and install the galaxy collection:</li>
</ol>
<pre><code class="language-bash">ansible-galaxy collection install kubernetes.core
pip3 install kubernetes jmespath
git clone https://github.com/rh-mobb/rhacs-demo
cd rhacs-demo
</code></pre>
<ol>
<li>Deploy the RHACS with the ansible-playbook command:</li>
</ol>
<pre><code class="language-bash">ansible-playbook rhacs-install.yaml
</code></pre>
<blockquote>
<p>This will install RHACS and also a couple of example Apps to demo. If you want just the plain RHACS installation, use the rhacs-only-install.yaml playbook.</p>
</blockquote>
<h2 id="security-rhacs-deploying-example-apps-for-demo-rhacs">Deploying Example Apps for demo RHACS</h2>
<ol>
<li>Deploy some example apps for demo RHACS policies and violations:</li>
</ol>
<pre><code class="language-bash">oc new-project test

oc run shell --labels=app=shellshock,team=test-team \
--image=vulnerables/cve-2014-6271 -n test

oc run samba --labels=app=rce \
--image=vulnerables/cve-2017-7494 -n test
</code></pre></section><h1 class='nav-section-title-end'>Ended: Advanced Cluster Security</h1><h1 class='nav-section-title-end'>Ended: Security</h1><section class="print-page" id="blog"><h1 id="blog-mobb-blogs">MOBB Blogs</h1>
<p>{{ blog_content }}</p></section></div>

              
            </article>
          </div>
        </div>
        
      </main>
      
        <footer class="md-footer">
  
  <div class="md-footer-meta md-typeset">
    <div class="md-footer-meta__inner md-grid">
      <div class="md-copyright">
  
  
    Made with
    <a href="https://khanhduy1407.github.io/docurial/" target="_blank" rel="noopener">
      Docurial
    </a>
  
</div>
      
    </div>
  </div>
</footer>
      
    </div>
    <div class="md-dialog" data-md-component="dialog">
      <div class="md-dialog__inner md-typeset"></div>
    </div>
    <script id="__config" type="application/json">{"base": "/", "features": ["navigation.tabs", "navigation.tabs.sticky"], "search": "../assets/javascripts/workers/search.22074ed6.min.js", "translations": {"clipboard.copied": "Copied to clipboard", "clipboard.copy": "Copy to clipboard", "search.config.lang": "en", "search.config.pipeline": "trimmer, stopWordFilter", "search.config.separator": "[\\s\\-]+", "search.placeholder": "Search", "search.result.more.one": "1 more on this page", "search.result.more.other": "# more on this page", "search.result.none": "No matching documents", "search.result.one": "1 matching document", "search.result.other": "# matching documents", "search.result.placeholder": "Type to start searching", "search.result.term.missing": "Missing", "select.version.title": "Select version"}}</script>
    
    
      <script src="../assets/javascripts/bundle.960e086b.min.js"></script>
      
        <script src="../js/print-site.js"></script>
      
    
  </body>
</html>
{"config":{"indexing":"full","lang":["en"],"min_search_length":3,"prebuild_index":false,"separator":"[\\s\\-]+"},"docs":[{"location":"","text":"Welcome to MOBB Ninja For full documentation visit mkdocs.org . Commands mkdocs new [dir-name] - Create a new project. mkdocs serve - Start the live-reloading docs server. mkdocs build - Build the documentation site. mkdocs -h - Print help message and exit. Project layout mkdocs.yml # The configuration file. docs/ index.md # The documentation homepage. ... # Other markdown pages, images and other files.","title":"Home"},{"location":"#welcome-to-mobb-ninja","text":"For full documentation visit mkdocs.org .","title":"Welcome to MOBB Ninja"},{"location":"#commands","text":"mkdocs new [dir-name] - Create a new project. mkdocs serve - Start the live-reloading docs server. mkdocs build - Build the documentation site. mkdocs -h - Print help message and exit.","title":"Commands"},{"location":"#project-layout","text":"mkdocs.yml # The configuration file. docs/ index.md # The documentation homepage. ... # Other markdown pages, images and other files.","title":"Project layout"},{"location":"quickstart-aro/","text":"ARO Quickstart A Quickstart guide to deploying an Azure Red Hat OpenShift cluster. Author: Paul Czarkowski Video Walkthrough If you prefer a more visual medium, you can watch Paul Czarkowski walk through this quickstart on YouTube . Prerequisites Azure CLI Obviously you'll need to have an Azure account to configure the CLI against. MacOS See Azure Docs for alternative install options. Install Azure CLI using homebrew bash brew update && brew install azure-cli Linux See Azure Docs for alternative install options. Import the Microsoft Keys bash sudo rpm --import https://packages.microsoft.com/keys/microsoft.asc Add the Microsoft Yum Repository bash cat << EOF | sudo tee /etc/yum.repos.d/azure-cli.repo [azure-cli] name=Azure CLI baseurl=https://packages.microsoft.com/yumrepos/azure-cli enabled=1 gpgcheck=1 gpgkey=https://packages.microsoft.com/keys/microsoft.asc EOF Install Azure CLI bash sudo dnf install -y azure-cli Prepare Azure Account for Azure OpenShift Log into the Azure CLI by running the following and then authorizing through your Web Browser bash az login Make sure you have enough Quota (change the location if you're not using East US ) bash az vm list-usage --location \"East US\" -o table see Addendum - Adding Quota to ARO account if you have less than 36 Quota left for Total Regional vCPUs . Register resource providers bash az provider register -n Microsoft.RedHatOpenShift --wait az provider register -n Microsoft.Compute --wait az provider register -n Microsoft.Storage --wait az provider register -n Microsoft.Authorization --wait Get Red Hat pull secret This step is optional, but highly recommended Log into https://console.redhat.com Browse to https://console.redhat.com/openshift/install/azure/aro-provisioned click the Download pull secret button and remember where you saved it, you'll reference it later. Deploy Azure OpenShift Variables and Resource Group Set some environment variables to use later, and create an Azure Resource Group. Set the following environment variables Change the values to suit your environment, but these defaults should work. bash AZR_RESOURCE_LOCATION=eastus AZR_RESOURCE_GROUP=openshift AZR_CLUSTER=cluster AZR_PULL_SECRET=~/Downloads/pull-secret.txt Create an Azure resource group bash az group create \\ --name $AZR_RESOURCE_GROUP \\ --location $AZR_RESOURCE_LOCATION Networking Create a virtual network with two empty subnets Create virtual network bash az network vnet create \\ --address-prefixes 10.0.0.0/22 \\ --name \"$AZR_CLUSTER-aro-vnet-$AZR_RESOURCE_LOCATION\" \\ --resource-group $AZR_RESOURCE_GROUP Create control plane subnet bash az network vnet subnet create \\ --resource-group $AZR_RESOURCE_GROUP \\ --vnet-name \"$AZR_CLUSTER-aro-vnet-$AZR_RESOURCE_LOCATION\" \\ --name \"$AZR_CLUSTER-aro-control-subnet-$AZR_RESOURCE_LOCATION\" \\ --address-prefixes 10.0.0.0/23 \\ --service-endpoints Microsoft.ContainerRegistry Create machine subnet bash az network vnet subnet create \\ --resource-group $AZR_RESOURCE_GROUP \\ --vnet-name \"$AZR_CLUSTER-aro-vnet-$AZR_RESOURCE_LOCATION\" \\ --name \"$AZR_CLUSTER-aro-machine-subnet-$AZR_RESOURCE_LOCATION\" \\ --address-prefixes 10.0.2.0/23 \\ --service-endpoints Microsoft.ContainerRegistry Disable network policies on the control plane subnet This is required for the service to be able to connect to and manage the cluster. bash az network vnet subnet update \\ --name \"$AZR_CLUSTER-aro-control-subnet-$AZR_RESOURCE_LOCATION\" \\ --resource-group $AZR_RESOURCE_GROUP \\ --vnet-name \"$AZR_CLUSTER-aro-vnet-$AZR_RESOURCE_LOCATION\" \\ --disable-private-link-service-network-policies true Create the cluster This will take between 30 and 45 minutes. bash az aro create \\ --resource-group $AZR_RESOURCE_GROUP \\ --name $AZR_CLUSTER \\ --vnet \"$AZR_CLUSTER-aro-vnet-$AZR_RESOURCE_LOCATION\" \\ --master-subnet \"$AZR_CLUSTER-aro-control-subnet-$AZR_RESOURCE_LOCATION\" \\ --worker-subnet \"$AZR_CLUSTER-aro-machine-subnet-$AZR_RESOURCE_LOCATION\" \\ --pull-secret @$AZR_PULL_SECRET Get OpenShift console URL bash az aro show \\ --name $AZR_CLUSTER \\ --resource-group $AZR_RESOURCE_GROUP \\ -o tsv --query consoleProfile Get OpenShift credentials bash az aro list-credentials \\ --name $AZR_CLUSTER \\ --resource-group $AZR_RESOURCE_GROUP \\ -o tsv Use the URL and the credentials provided by the output of the last two commands to log into OpenShift via a web browser. Deploy an application to OpenShift See the following video for a guide on easy application deployment on OpenShift. Delete Cluster Once you're done its a good idea to delete the cluster to ensure that you don't get a surprise bill. Delete the cluster bash az aro delete -y \\ --resource-group $AZR_RESOURCE_GROUP \\ --name $AZR_CLUSTER Delete the Azure resource group Only do this if there's nothing else in the resource group. bash az group delete -y \\ --name $AZR_RESOURCE_GROUP Adendum Adding Quota to ARO account Create an Azure Support Request Set Issue Type to \"Service and subscription limits (quotas)\" Set Quota Type to \"Compute-VM (cores-vCPUs) subscription limit increases\" Click Next Solutions >> Click Enter details Set Deployment Model to \"Resource Manager Set Locations to \"(US) East US\" Set Types to \"Standard\" Under Standard check \"DSv3\" and \"DSv4\" Set New vCPU Limit for each (example \"60\") Click Save and continue Click Review + create >> Wait until quota is increased.","title":"ARO Quickstart"},{"location":"quickstart-aro/#aro-quickstart","text":"A Quickstart guide to deploying an Azure Red Hat OpenShift cluster. Author: Paul Czarkowski","title":"ARO Quickstart"},{"location":"quickstart-aro/#video-walkthrough","text":"If you prefer a more visual medium, you can watch Paul Czarkowski walk through this quickstart on YouTube .","title":"Video Walkthrough"},{"location":"quickstart-aro/#prerequisites","text":"","title":"Prerequisites"},{"location":"quickstart-aro/#azure-cli","text":"Obviously you'll need to have an Azure account to configure the CLI against. MacOS See Azure Docs for alternative install options. Install Azure CLI using homebrew bash brew update && brew install azure-cli Linux See Azure Docs for alternative install options. Import the Microsoft Keys bash sudo rpm --import https://packages.microsoft.com/keys/microsoft.asc Add the Microsoft Yum Repository bash cat << EOF | sudo tee /etc/yum.repos.d/azure-cli.repo [azure-cli] name=Azure CLI baseurl=https://packages.microsoft.com/yumrepos/azure-cli enabled=1 gpgcheck=1 gpgkey=https://packages.microsoft.com/keys/microsoft.asc EOF Install Azure CLI bash sudo dnf install -y azure-cli","title":"Azure CLI"},{"location":"quickstart-aro/#prepare-azure-account-for-azure-openshift","text":"Log into the Azure CLI by running the following and then authorizing through your Web Browser bash az login Make sure you have enough Quota (change the location if you're not using East US ) bash az vm list-usage --location \"East US\" -o table see Addendum - Adding Quota to ARO account if you have less than 36 Quota left for Total Regional vCPUs . Register resource providers bash az provider register -n Microsoft.RedHatOpenShift --wait az provider register -n Microsoft.Compute --wait az provider register -n Microsoft.Storage --wait az provider register -n Microsoft.Authorization --wait","title":"Prepare Azure Account for Azure OpenShift"},{"location":"quickstart-aro/#get-red-hat-pull-secret","text":"This step is optional, but highly recommended Log into https://console.redhat.com Browse to https://console.redhat.com/openshift/install/azure/aro-provisioned click the Download pull secret button and remember where you saved it, you'll reference it later.","title":"Get Red Hat pull secret"},{"location":"quickstart-aro/#deploy-azure-openshift","text":"","title":"Deploy Azure OpenShift"},{"location":"quickstart-aro/#variables-and-resource-group","text":"Set some environment variables to use later, and create an Azure Resource Group. Set the following environment variables Change the values to suit your environment, but these defaults should work. bash AZR_RESOURCE_LOCATION=eastus AZR_RESOURCE_GROUP=openshift AZR_CLUSTER=cluster AZR_PULL_SECRET=~/Downloads/pull-secret.txt Create an Azure resource group bash az group create \\ --name $AZR_RESOURCE_GROUP \\ --location $AZR_RESOURCE_LOCATION","title":"Variables and Resource Group"},{"location":"quickstart-aro/#networking","text":"Create a virtual network with two empty subnets Create virtual network bash az network vnet create \\ --address-prefixes 10.0.0.0/22 \\ --name \"$AZR_CLUSTER-aro-vnet-$AZR_RESOURCE_LOCATION\" \\ --resource-group $AZR_RESOURCE_GROUP Create control plane subnet bash az network vnet subnet create \\ --resource-group $AZR_RESOURCE_GROUP \\ --vnet-name \"$AZR_CLUSTER-aro-vnet-$AZR_RESOURCE_LOCATION\" \\ --name \"$AZR_CLUSTER-aro-control-subnet-$AZR_RESOURCE_LOCATION\" \\ --address-prefixes 10.0.0.0/23 \\ --service-endpoints Microsoft.ContainerRegistry Create machine subnet bash az network vnet subnet create \\ --resource-group $AZR_RESOURCE_GROUP \\ --vnet-name \"$AZR_CLUSTER-aro-vnet-$AZR_RESOURCE_LOCATION\" \\ --name \"$AZR_CLUSTER-aro-machine-subnet-$AZR_RESOURCE_LOCATION\" \\ --address-prefixes 10.0.2.0/23 \\ --service-endpoints Microsoft.ContainerRegistry Disable network policies on the control plane subnet This is required for the service to be able to connect to and manage the cluster. bash az network vnet subnet update \\ --name \"$AZR_CLUSTER-aro-control-subnet-$AZR_RESOURCE_LOCATION\" \\ --resource-group $AZR_RESOURCE_GROUP \\ --vnet-name \"$AZR_CLUSTER-aro-vnet-$AZR_RESOURCE_LOCATION\" \\ --disable-private-link-service-network-policies true Create the cluster This will take between 30 and 45 minutes. bash az aro create \\ --resource-group $AZR_RESOURCE_GROUP \\ --name $AZR_CLUSTER \\ --vnet \"$AZR_CLUSTER-aro-vnet-$AZR_RESOURCE_LOCATION\" \\ --master-subnet \"$AZR_CLUSTER-aro-control-subnet-$AZR_RESOURCE_LOCATION\" \\ --worker-subnet \"$AZR_CLUSTER-aro-machine-subnet-$AZR_RESOURCE_LOCATION\" \\ --pull-secret @$AZR_PULL_SECRET Get OpenShift console URL bash az aro show \\ --name $AZR_CLUSTER \\ --resource-group $AZR_RESOURCE_GROUP \\ -o tsv --query consoleProfile Get OpenShift credentials bash az aro list-credentials \\ --name $AZR_CLUSTER \\ --resource-group $AZR_RESOURCE_GROUP \\ -o tsv Use the URL and the credentials provided by the output of the last two commands to log into OpenShift via a web browser. Deploy an application to OpenShift See the following video for a guide on easy application deployment on OpenShift.","title":"Networking"},{"location":"quickstart-aro/#delete-cluster","text":"Once you're done its a good idea to delete the cluster to ensure that you don't get a surprise bill. Delete the cluster bash az aro delete -y \\ --resource-group $AZR_RESOURCE_GROUP \\ --name $AZR_CLUSTER Delete the Azure resource group Only do this if there's nothing else in the resource group. bash az group delete -y \\ --name $AZR_RESOURCE_GROUP","title":"Delete Cluster"},{"location":"quickstart-aro/#adendum","text":"","title":"Adendum"},{"location":"quickstart-aro/#adding-quota-to-aro-account","text":"Create an Azure Support Request Set Issue Type to \"Service and subscription limits (quotas)\" Set Quota Type to \"Compute-VM (cores-vCPUs) subscription limit increases\" Click Next Solutions >> Click Enter details Set Deployment Model to \"Resource Manager Set Locations to \"(US) East US\" Set Types to \"Standard\" Under Standard check \"DSv3\" and \"DSv4\" Set New vCPU Limit for each (example \"60\") Click Save and continue Click Review + create >> Wait until quota is increased.","title":"Adding Quota to ARO account"},{"location":"quickstart-rosa/","text":"ROSA Quickstart A Quickstart guide to deploying a Red Hat OpenShift cluster on AWS. Author: Steve Mirman Video Walkthrough If you prefer a more visual medium, you can watch Steve Mirman walk through this quickstart on YouTube . Prerequisites AWS CLI You'll need to have an AWS account to configure the CLI against. MacOS See AWS Docs for alternative install options. Install AWS CLI using the macOS command line bash curl \"https://awscli.amazonaws.com/AWSCLIV2.pkg\" -o \"AWSCLIV2.pkg\" sudo installer -pkg AWSCLIV2.pkg -target / Linux See AWS Docs for alternative install options. Install AWS CLI using the Linux command line bash curl \"https://awscli.amazonaws.com/awscli-exe-linux-x86_64.zip\" -o \"awscliv2.zip\" unzip awscliv2.zip sudo ./aws/install Windows See AWS Docs for alternative install options. Install AWS CLI using the Windows command line bash C:\\> msiexec.exe /i https://awscli.amazonaws.com/AWSCLIV2.msi Docker See AWS Docs for alternative install options. To run the AWS CLI version 2 Docker image, use the docker run command. bash docker run --rm -it amazon/aws-cli command Prepare AWS Account for OpenShift Configure the AWS CLI by running the following command bash aws configure You will be required to enter an AWS Access Key ID and an AWS Secret Access Key along with a default region name and output format bash % aws configure AWS Access Key ID []: AWS Secret Access Key []: Default region name [us-east-2]: Default output format [json]: The AWS Access Key ID and AWS Secret Access Key values can be obtained by logging in to the AWS console and creating an Access Key in the Security Credentials section of the IAM dashboard for your user Validate your credentials bash aws sts get-caller-identity You should receive output similar to the following { \"UserId\": <your ID>, \"Account\": <your account>, \"Arn\": <your arn> } If this is a brand new AWS account that has never had a AWS Load Balancer installed in it, you should run the following bash aws iam create-service-linked-role --aws-service-name \\ \"elasticloadbalancing.amazonaws.com\" Get a Red Hat Offline Access Token Log into cloud.redhat.com Browse to https://cloud.redhat.com/openshift/token/rosa Copy the Offline Access Token and save it for the next step Set up the OpenShift CLI (oc) Download the OS specific OpenShift CLI from Red Hat Unzip the downloaded file on your local machine Place the extracted oc executable in your OS path or local directory Set up the ROSA CLI Download the OS specific ROSA CLI from Red Hat Unzip the downloaded file on your local machine Place the extracted rosa and kubectl executables in your OS path or local directory Log in to ROSA bash rosa login You will be prompted to enter in the Red Hat Offline Access Token you retrieved earlier and should receive the following message Logged in as <email address> on 'https://api.openshift.com' Verify ROSA privileges Verify that ROSA has the minimal permissions bash rosa verify permissions Expected output: AWS SCP policies ok Verify that ROSA has the minimal quota bash rosa verify quota Expected output: AWS quota ok Initialize ROSA Initialize the ROSA CLI to complete the remaining validation checks and configurations bash rosa init Deploy Red Hat OpenShift on AWS (ROSA) Interactive Installation ROSA can be installed using command line parameters or in interactive mode. For an interactive installation run the following command bash rosa create cluster --interactive As part of the interactive install you will be required to enter the following parameters or accept the default values (if applicable) Cluster name: Multiple availability zones (y/N): AWS region (select): OpenShift version (select): Install into an existing VPC (y/N): Compute nodes instance type (optional): Enable autoscaling (y/N): Compute nodes [2]: Machine CIDR [10.0.0.0/16]: Service CIDR [172.30.0.0/16]: Pod CIDR [10.128.0.0/14]: Host prefix [23]: Private cluster (y/N): Note: the installation process should take between 30 - 45 minutes Get the web console link to the ROSA cluster To get the web console link run the following command. Substitute your actual cluster name for <cluster-name> bash rosa describe cluster --cluster=<cluster-name> Create cluster-admin user By default, only the OpenShift SRE team will have access to the ROSA cluster. To add a local admin user, run the following command to create the cluster-admin account in your cluster. Substitute your actual cluster name for <cluster-name> bash rosa create admin --cluster=<cluster-name> Refresh your web browser and you should see the cluster-admin option to log in Delete Red Hat OpenShift on AWS (ROSA) Deleting a ROSA cluster consists of two parts Delete the cluster instance, including the removal of AWS resources. Substitute your actual cluster name for <cluster-name> bash rosa delete cluster --cluster=<cluster-name> Delete the CloudFormation stack, including the removal of the osdCcsAdmin user bash rosa init --delete-stack","title":"ROSA Quickstart"},{"location":"quickstart-rosa/#rosa-quickstart","text":"A Quickstart guide to deploying a Red Hat OpenShift cluster on AWS. Author: Steve Mirman","title":"ROSA Quickstart"},{"location":"quickstart-rosa/#video-walkthrough","text":"If you prefer a more visual medium, you can watch Steve Mirman walk through this quickstart on YouTube .","title":"Video Walkthrough"},{"location":"quickstart-rosa/#prerequisites","text":"","title":"Prerequisites"},{"location":"quickstart-rosa/#aws-cli","text":"You'll need to have an AWS account to configure the CLI against. MacOS See AWS Docs for alternative install options. Install AWS CLI using the macOS command line bash curl \"https://awscli.amazonaws.com/AWSCLIV2.pkg\" -o \"AWSCLIV2.pkg\" sudo installer -pkg AWSCLIV2.pkg -target / Linux See AWS Docs for alternative install options. Install AWS CLI using the Linux command line bash curl \"https://awscli.amazonaws.com/awscli-exe-linux-x86_64.zip\" -o \"awscliv2.zip\" unzip awscliv2.zip sudo ./aws/install Windows See AWS Docs for alternative install options. Install AWS CLI using the Windows command line bash C:\\> msiexec.exe /i https://awscli.amazonaws.com/AWSCLIV2.msi Docker See AWS Docs for alternative install options. To run the AWS CLI version 2 Docker image, use the docker run command. bash docker run --rm -it amazon/aws-cli command","title":"AWS CLI"},{"location":"quickstart-rosa/#prepare-aws-account-for-openshift","text":"Configure the AWS CLI by running the following command bash aws configure You will be required to enter an AWS Access Key ID and an AWS Secret Access Key along with a default region name and output format bash % aws configure AWS Access Key ID []: AWS Secret Access Key []: Default region name [us-east-2]: Default output format [json]: The AWS Access Key ID and AWS Secret Access Key values can be obtained by logging in to the AWS console and creating an Access Key in the Security Credentials section of the IAM dashboard for your user Validate your credentials bash aws sts get-caller-identity You should receive output similar to the following { \"UserId\": <your ID>, \"Account\": <your account>, \"Arn\": <your arn> } If this is a brand new AWS account that has never had a AWS Load Balancer installed in it, you should run the following bash aws iam create-service-linked-role --aws-service-name \\ \"elasticloadbalancing.amazonaws.com\"","title":"Prepare AWS Account for OpenShift"},{"location":"quickstart-rosa/#get-a-red-hat-offline-access-token","text":"Log into cloud.redhat.com Browse to https://cloud.redhat.com/openshift/token/rosa Copy the Offline Access Token and save it for the next step","title":"Get a Red Hat Offline Access Token"},{"location":"quickstart-rosa/#set-up-the-openshift-cli-oc","text":"Download the OS specific OpenShift CLI from Red Hat Unzip the downloaded file on your local machine Place the extracted oc executable in your OS path or local directory","title":"Set up the OpenShift CLI (oc)"},{"location":"quickstart-rosa/#set-up-the-rosa-cli","text":"Download the OS specific ROSA CLI from Red Hat Unzip the downloaded file on your local machine Place the extracted rosa and kubectl executables in your OS path or local directory Log in to ROSA bash rosa login You will be prompted to enter in the Red Hat Offline Access Token you retrieved earlier and should receive the following message Logged in as <email address> on 'https://api.openshift.com'","title":"Set up the ROSA CLI"},{"location":"quickstart-rosa/#verify-rosa-privileges","text":"Verify that ROSA has the minimal permissions bash rosa verify permissions Expected output: AWS SCP policies ok Verify that ROSA has the minimal quota bash rosa verify quota Expected output: AWS quota ok","title":"Verify ROSA privileges"},{"location":"quickstart-rosa/#initialize-rosa","text":"Initialize the ROSA CLI to complete the remaining validation checks and configurations bash rosa init","title":"Initialize ROSA"},{"location":"quickstart-rosa/#deploy-red-hat-openshift-on-aws-rosa","text":"","title":"Deploy Red Hat OpenShift on AWS (ROSA)"},{"location":"quickstart-rosa/#interactive-installation","text":"ROSA can be installed using command line parameters or in interactive mode. For an interactive installation run the following command bash rosa create cluster --interactive As part of the interactive install you will be required to enter the following parameters or accept the default values (if applicable) Cluster name: Multiple availability zones (y/N): AWS region (select): OpenShift version (select): Install into an existing VPC (y/N): Compute nodes instance type (optional): Enable autoscaling (y/N): Compute nodes [2]: Machine CIDR [10.0.0.0/16]: Service CIDR [172.30.0.0/16]: Pod CIDR [10.128.0.0/14]: Host prefix [23]: Private cluster (y/N): Note: the installation process should take between 30 - 45 minutes","title":"Interactive Installation"},{"location":"quickstart-rosa/#get-the-web-console-link-to-the-rosa-cluster","text":"To get the web console link run the following command. Substitute your actual cluster name for <cluster-name> bash rosa describe cluster --cluster=<cluster-name>","title":"Get the web console link to the ROSA cluster"},{"location":"quickstart-rosa/#create-cluster-admin-user","text":"By default, only the OpenShift SRE team will have access to the ROSA cluster. To add a local admin user, run the following command to create the cluster-admin account in your cluster. Substitute your actual cluster name for <cluster-name> bash rosa create admin --cluster=<cluster-name> Refresh your web browser and you should see the cluster-admin option to log in","title":"Create cluster-admin user"},{"location":"quickstart-rosa/#delete-red-hat-openshift-on-aws-rosa","text":"Deleting a ROSA cluster consists of two parts Delete the cluster instance, including the removal of AWS resources. Substitute your actual cluster name for <cluster-name> bash rosa delete cluster --cluster=<cluster-name> Delete the CloudFormation stack, including the removal of the osdCcsAdmin user bash rosa init --delete-stack","title":"Delete Red Hat OpenShift on AWS (ROSA)"},{"location":"acm/observability/","text":"Advanced Cluster Management - Observability","title":"Advanced Cluster Management - Observability"},{"location":"acm/observability/#advanced-cluster-management-observability","text":"","title":"Advanced Cluster Management - Observability"},{"location":"acm/observability/rosa/","text":"Advanced Cluster Management Observability on ROSA This document will take you through deploying ACM Observability on a ROSA cluster. see here for the original documentation. Prerequisites An existing ROSA cluster An Advanced Cluster Management (ACM) deployment Set up environment Set environment variables export CLUSTER_NAME=my-cluster export S3_BUCKET=$CLUSTER_NAME-acm-observability export REGION=us-east-2 export NAMESPACE=open-cluster-management-observability export SA=tbd export SCRATCH_DIR=/tmp/scratch export AWS_ACCOUNT_ID=$(aws sts get-caller-identity --query Account --output text) export AWS_PAGER=\"\" rm -rf $SCRATCH_DIR mkdir -p $SCRATCH_DIR Prepare AWS Account Create an S3 bucket bash aws s3 mb s3://$S3_BUCKET Create a Policy for access to S3 bash cat <<EOF > $SCRATCH_DIR/s3-policy.json { \"Version\": \"2012-10-17\", \"Statement\": [ { \"Sid\": \"Statement\", \"Effect\": \"Allow\", \"Action\": [ \"s3:ListBucket\", \"s3:GetObject\", \"s3:DeleteObject\", \"s3:PutObject\", \"s3:PutObjectAcl\", \"s3:CreateBucket\", \"s3:DeleteBucket\" ], \"Resource\": [ \"arn:aws:s3:::$S3_BUCKET/*\", \"arn:aws:s3:::$S3_BUCKET\" ] } ] } EOF Apply the Policy bash S3_POLICY=$(aws iam create-policy --policy-name $CLUSTER_NAME-acm-obs \\ --policy-document file://$SCRATCH_DIR/s3-policy.json \\ --query 'Policy.Arn' --output text) echo $S3_POLICY Create service account bash aws iam create-user --user-name $CLUSTER_NAME-acm-obs \\ --query User.Arn --output text Attach policy to user bash aws iam attach-user-policy --user-name $CLUSTER_NAME-acm-obs \\ --policy-arn ${S3_POLICY} Create Access Keys bash read -r ACCESS_KEY_ID ACCESS_KEY < <(aws iam create-access-key \\ --user-name $CLUSTER_NAME-acm-obs \\ --query 'AccessKey.[AccessKeyId,SecretAccessKey]' --output text) ACM Hub Log into the OpenShift cluster that is running your ACM Hub. We'll set up Observability here Create a namespace for the observability bash oc new-project $NAMESPACE Generate a pull secret (this will check if the pull secret exists, if not, it will create it) bash DOCKER_CONFIG_JSON=`oc extract secret/multiclusterhub-operator-pull-secret -n open-cluster-management --to=-` || \\ DOCKER_CONFIG_JSON=`oc extract secret/pull-secret -n openshift-config --to=-` && \\ oc create secret generic multiclusterhub-operator-pull-secret \\ -n open-cluster-management-observability \\ --from-literal=.dockerconfigjson=\"$DOCKER_CONFIG_JSON\" \\ --type=kubernetes.io/dockerconfigjson Create a Secret containing your S3 details bash cat << EOF | kubectl apply -f - apiVersion: v1 kind: Secret metadata: name: thanos-object-storage namespace: open-cluster-management-observability type: Opaque stringData: thanos.yaml: | type: s3 config: bucket: $S3_BUCKET endpoint: s3.$REGION.amazonaws.com signature_version2: false access_key: $ACCESS_KEY_ID secret_key: $ACCESS_KEY EOF Create a CR for MulticlusterHub bash cat << EOF | kubectl apply -f - apiVersion: observability.open-cluster-management.io/v1beta2 kind: MultiClusterObservability metadata: name: observability spec: observabilityAddonSpec: {} storageConfig: metricObjectStorage: name: thanos-object-storage key: thanos.yaml EOF Access ACM Observability Log into Advanced Cluster management and access the new Grafana dashboard","title":"Deploy ACM Observability to a ROSA Cluster"},{"location":"acm/observability/rosa/#advanced-cluster-management-observability-on-rosa","text":"This document will take you through deploying ACM Observability on a ROSA cluster. see here for the original documentation.","title":"Advanced Cluster Management Observability on ROSA"},{"location":"acm/observability/rosa/#prerequisites","text":"An existing ROSA cluster An Advanced Cluster Management (ACM) deployment","title":"Prerequisites"},{"location":"acm/observability/rosa/#set-up-environment","text":"Set environment variables export CLUSTER_NAME=my-cluster export S3_BUCKET=$CLUSTER_NAME-acm-observability export REGION=us-east-2 export NAMESPACE=open-cluster-management-observability export SA=tbd export SCRATCH_DIR=/tmp/scratch export AWS_ACCOUNT_ID=$(aws sts get-caller-identity --query Account --output text) export AWS_PAGER=\"\" rm -rf $SCRATCH_DIR mkdir -p $SCRATCH_DIR","title":"Set up environment"},{"location":"acm/observability/rosa/#prepare-aws-account","text":"Create an S3 bucket bash aws s3 mb s3://$S3_BUCKET Create a Policy for access to S3 bash cat <<EOF > $SCRATCH_DIR/s3-policy.json { \"Version\": \"2012-10-17\", \"Statement\": [ { \"Sid\": \"Statement\", \"Effect\": \"Allow\", \"Action\": [ \"s3:ListBucket\", \"s3:GetObject\", \"s3:DeleteObject\", \"s3:PutObject\", \"s3:PutObjectAcl\", \"s3:CreateBucket\", \"s3:DeleteBucket\" ], \"Resource\": [ \"arn:aws:s3:::$S3_BUCKET/*\", \"arn:aws:s3:::$S3_BUCKET\" ] } ] } EOF Apply the Policy bash S3_POLICY=$(aws iam create-policy --policy-name $CLUSTER_NAME-acm-obs \\ --policy-document file://$SCRATCH_DIR/s3-policy.json \\ --query 'Policy.Arn' --output text) echo $S3_POLICY Create service account bash aws iam create-user --user-name $CLUSTER_NAME-acm-obs \\ --query User.Arn --output text Attach policy to user bash aws iam attach-user-policy --user-name $CLUSTER_NAME-acm-obs \\ --policy-arn ${S3_POLICY} Create Access Keys bash read -r ACCESS_KEY_ID ACCESS_KEY < <(aws iam create-access-key \\ --user-name $CLUSTER_NAME-acm-obs \\ --query 'AccessKey.[AccessKeyId,SecretAccessKey]' --output text)","title":"Prepare AWS Account"},{"location":"acm/observability/rosa/#acm-hub","text":"Log into the OpenShift cluster that is running your ACM Hub. We'll set up Observability here Create a namespace for the observability bash oc new-project $NAMESPACE Generate a pull secret (this will check if the pull secret exists, if not, it will create it) bash DOCKER_CONFIG_JSON=`oc extract secret/multiclusterhub-operator-pull-secret -n open-cluster-management --to=-` || \\ DOCKER_CONFIG_JSON=`oc extract secret/pull-secret -n openshift-config --to=-` && \\ oc create secret generic multiclusterhub-operator-pull-secret \\ -n open-cluster-management-observability \\ --from-literal=.dockerconfigjson=\"$DOCKER_CONFIG_JSON\" \\ --type=kubernetes.io/dockerconfigjson Create a Secret containing your S3 details bash cat << EOF | kubectl apply -f - apiVersion: v1 kind: Secret metadata: name: thanos-object-storage namespace: open-cluster-management-observability type: Opaque stringData: thanos.yaml: | type: s3 config: bucket: $S3_BUCKET endpoint: s3.$REGION.amazonaws.com signature_version2: false access_key: $ACCESS_KEY_ID secret_key: $ACCESS_KEY EOF Create a CR for MulticlusterHub bash cat << EOF | kubectl apply -f - apiVersion: observability.open-cluster-management.io/v1beta2 kind: MultiClusterObservability metadata: name: observability spec: observabilityAddonSpec: {} storageConfig: metricObjectStorage: name: thanos-object-storage key: thanos.yaml EOF","title":"ACM Hub"},{"location":"acm/observability/rosa/#access-acm-observability","text":"Log into Advanced Cluster management and access the new Grafana dashboard","title":"Access ACM Observability"},{"location":"app-services/3scale/","text":"Deploying 3scale API Management to ROSA and OSD Michael McNeill 26 January 2022 This document will take you through deploying 3scale in any OSD or ROSA cluster. Review the official documentation here for more information or how to further customize or use 3scale. Prerequisites An existing ROSA or OSD cluster Access to an AWS account with permissions to create S3 buckets, IAM users, and IAM policies A subscription for 3scale API Management A wildcard domain configured with a CNAME to your cluster's ingress controller Prepare AWS Account Set environment variables (ensuring you update the variables appropriately!) bash export S3_BUCKET=<your-bucket-name-here> export REGION=us-east-1 export S3_IAM_USER_NAME=<your-s3-user-name-here> export S3_IAM_POLICY_NAME=<your-s3-policy-name-here> export AWS_PAGER=\"\" export PROJECT_NAME=<your-project-name-here> export WILDCARD_DOMAIN=<your-wildcard-domain-here> For my example, I'll be using the following variables: bash export S3_BUCKET=mobb-3scale-bucket export REGION=us-east-1 export S3_IAM_USER_NAME=mobb-3scale-user export S3_IAM_POLICY_NAME=3scale-s3-access export AWS_PAGER=\"\" export PROJECT_NAME=3scale-example export WILDCARD_DOMAIN=3scale.example.com Create an S3 bucket bash aws s3 mb s3://$S3_BUCKET Apply the proper S3 bucket CORS configuration bash aws s3api put-bucket-cors --bucket $S3_BUCKET --cors-configuration \\ '{ \"CORSRules\": [{ \"AllowedMethods\": [ \"GET\" ], \"AllowedOrigins\": [ \"https://*\" ] }] }' Create an IAM policy for access to the S3 bucket bash POLICY_ARN=$(aws iam create-policy --policy-name \"$S3_IAM_POLICY_NAME\" \\ --output text --query \"Policy.Arn\" \\ --policy-document \\ '{ \"Version\": \"2012-10-17\", \"Statement\": [ { \"Effect\": \"Allow\", \"Action\": \"s3:ListAllMyBuckets\", \"Resource\": \"arn:aws:s3:::*\" }, { \"Effect\": \"Allow\", \"Action\": \"s3:*\", \"Resource\": [ \"arn:aws:s3:::'$S3_BUCKET'\", \"arn:aws:s3:::'$S3_BUCKET'/*\" ] } ] }') Create an IAM user to access the S3 bucket bash aws iam create-user --user-name $S3_IAM_USER_NAME Generate an access key for the newly created S3 user bash ACCESS_CREDS=$(aws iam create-access-key --user-name $S3_IAM_USER_NAME \\ --output text --query \"AccessKey.[AccessKeyId, SecretAccessKey]\") Apply the IAM policy to the newly created S3 user bash aws iam attach-user-policy --user-name $S3_IAM_USER_NAME \\ --policy-arn $POLICY_ARN Install the 3Scale API Management Operator Create a new project to install 3Scale API Management into. bash oc new-project $PROJECT_NAME Inside of the OpenShift Web Console, navigate to Operators -> OperatorHub. Search for \"3scale\" and select the \"Red Hat Integration - 3scale\" Operator. Click \"Install\" and select the project you wish to install the operator into. For this example, I'm deploying into the \"3scale-example\" project that I have just created. Once the 3Scale operator successfully installs, return to your terminal. Deploy 3Scale API Management Create a secret that contains the Amazon S3 configuration. bash echo << EOF | oc apply -f - apiVersion: v1 kind: Secret metadata: creationTimestamp: null name: aws-auth stringData: AWS_ACCESS_KEY_ID: \"$(echo $ACCESS_CREDS | cut -f 1)\" AWS_SECRET_ACCESS_KEY: \"$(echo $ACCESS_CREDS | cut -f 2)\" AWS_BUCKET: \"$S3_BUCKET\" AWS_REGION: \"$REGION\" type: Opaque EOF Create an APIManager custom resource bash cat << EOF | oc apply -f - echo 'apiVersion: apps.3scale.net/v1alpha1 kind: APIManager metadata: name: example-apimanager spec: wildcardDomain: '$WILDCARD_DOMAIN' system: fileStorage: simpleStorageService: configurationSecretRef: name: aws-auth EOF Once the APIManager instance becomes available, you can login to the 3Scale Admin (located at https://3scale-admin.$WILDCARD_DOMAIN) using the credentials from the below commands: bash oc get secret system-seed -o jsonpath={.data.ADMIN_USER} | base64 -d oc get secret system-seed -o jsonpath={.data.ADMIN_PASSWORD} | base64 -d Congratulations! You've successfully deployed 3Scale API Management to ROSA/OSD.","title":"Deploying 3scale API Management to ROSA and OSD"},{"location":"app-services/3scale/#deploying-3scale-api-management-to-rosa-and-osd","text":"Michael McNeill 26 January 2022 This document will take you through deploying 3scale in any OSD or ROSA cluster. Review the official documentation here for more information or how to further customize or use 3scale.","title":"Deploying 3scale API Management to ROSA and OSD"},{"location":"app-services/3scale/#prerequisites","text":"An existing ROSA or OSD cluster Access to an AWS account with permissions to create S3 buckets, IAM users, and IAM policies A subscription for 3scale API Management A wildcard domain configured with a CNAME to your cluster's ingress controller","title":"Prerequisites"},{"location":"app-services/3scale/#prepare-aws-account","text":"Set environment variables (ensuring you update the variables appropriately!) bash export S3_BUCKET=<your-bucket-name-here> export REGION=us-east-1 export S3_IAM_USER_NAME=<your-s3-user-name-here> export S3_IAM_POLICY_NAME=<your-s3-policy-name-here> export AWS_PAGER=\"\" export PROJECT_NAME=<your-project-name-here> export WILDCARD_DOMAIN=<your-wildcard-domain-here> For my example, I'll be using the following variables: bash export S3_BUCKET=mobb-3scale-bucket export REGION=us-east-1 export S3_IAM_USER_NAME=mobb-3scale-user export S3_IAM_POLICY_NAME=3scale-s3-access export AWS_PAGER=\"\" export PROJECT_NAME=3scale-example export WILDCARD_DOMAIN=3scale.example.com Create an S3 bucket bash aws s3 mb s3://$S3_BUCKET Apply the proper S3 bucket CORS configuration bash aws s3api put-bucket-cors --bucket $S3_BUCKET --cors-configuration \\ '{ \"CORSRules\": [{ \"AllowedMethods\": [ \"GET\" ], \"AllowedOrigins\": [ \"https://*\" ] }] }' Create an IAM policy for access to the S3 bucket bash POLICY_ARN=$(aws iam create-policy --policy-name \"$S3_IAM_POLICY_NAME\" \\ --output text --query \"Policy.Arn\" \\ --policy-document \\ '{ \"Version\": \"2012-10-17\", \"Statement\": [ { \"Effect\": \"Allow\", \"Action\": \"s3:ListAllMyBuckets\", \"Resource\": \"arn:aws:s3:::*\" }, { \"Effect\": \"Allow\", \"Action\": \"s3:*\", \"Resource\": [ \"arn:aws:s3:::'$S3_BUCKET'\", \"arn:aws:s3:::'$S3_BUCKET'/*\" ] } ] }') Create an IAM user to access the S3 bucket bash aws iam create-user --user-name $S3_IAM_USER_NAME Generate an access key for the newly created S3 user bash ACCESS_CREDS=$(aws iam create-access-key --user-name $S3_IAM_USER_NAME \\ --output text --query \"AccessKey.[AccessKeyId, SecretAccessKey]\") Apply the IAM policy to the newly created S3 user bash aws iam attach-user-policy --user-name $S3_IAM_USER_NAME \\ --policy-arn $POLICY_ARN","title":"Prepare AWS Account"},{"location":"app-services/3scale/#install-the-3scale-api-management-operator","text":"Create a new project to install 3Scale API Management into. bash oc new-project $PROJECT_NAME Inside of the OpenShift Web Console, navigate to Operators -> OperatorHub. Search for \"3scale\" and select the \"Red Hat Integration - 3scale\" Operator. Click \"Install\" and select the project you wish to install the operator into. For this example, I'm deploying into the \"3scale-example\" project that I have just created. Once the 3Scale operator successfully installs, return to your terminal.","title":"Install the 3Scale API Management Operator"},{"location":"app-services/3scale/#deploy-3scale-api-management","text":"Create a secret that contains the Amazon S3 configuration. bash echo << EOF | oc apply -f - apiVersion: v1 kind: Secret metadata: creationTimestamp: null name: aws-auth stringData: AWS_ACCESS_KEY_ID: \"$(echo $ACCESS_CREDS | cut -f 1)\" AWS_SECRET_ACCESS_KEY: \"$(echo $ACCESS_CREDS | cut -f 2)\" AWS_BUCKET: \"$S3_BUCKET\" AWS_REGION: \"$REGION\" type: Opaque EOF Create an APIManager custom resource bash cat << EOF | oc apply -f - echo 'apiVersion: apps.3scale.net/v1alpha1 kind: APIManager metadata: name: example-apimanager spec: wildcardDomain: '$WILDCARD_DOMAIN' system: fileStorage: simpleStorageService: configurationSecretRef: name: aws-auth EOF Once the APIManager instance becomes available, you can login to the 3Scale Admin (located at https://3scale-admin.$WILDCARD_DOMAIN) using the credentials from the below commands: bash oc get secret system-seed -o jsonpath={.data.ADMIN_USER} | base64 -d oc get secret system-seed -o jsonpath={.data.ADMIN_PASSWORD} | base64 -d Congratulations! You've successfully deployed 3Scale API Management to ROSA/OSD.","title":"Deploy 3Scale API Management"},{"location":"aro/egress-ipam-operator/","text":"Using the Egressip Ipam Operator with a Private ARO Cluster Prerequisites A private ARO cluster with a VPN Connection and the egress LB removed Deploy the Egressip Ipam Operator Via GUI Log into the ARO cluster's Console Switch to the Administrator view Click on Operators -> Operator Hub Search for \"Egressip Ipam Operator\" Install it with the default settings or Via CLI Deploy the egress-ipam-operator ```bash cat << EOF | kubectl apply -f - apiVersion: v1 kind: Namespace metadata: name: egressip-ipam-operator apiVersion: operators.coreos.com/v1alpha1 kind: Subscription metadata: name: egressip-ipam-operator namespace: openshift-operators labels: operators.coreos.com/egressip-ipam-operator.egressip-ipam-operator: '' spec: channel: alpha installPlanApproval: Automatic name: egressip-ipam-operator source: community-operators sourceNamespace: openshift-marketplace startingCSV: egressip-ipam-operator.v1.2.2 EOF ``` Configure EgressIP Create an EgressIPAM resource for your cluster. Update the CIDR to reflect the worker node subnet. bash cat << EOF | kubectl apply -f - apiVersion: redhatcop.redhat.io/v1alpha1 kind: EgressIPAM metadata: name: egressipam-azure annotations: egressip-ipam-operator.redhat-cop.io/azure-egress-load-balancer: none spec: cidrAssignments: - labelValue: \"\" CIDR: 10.0.1.0/24 reservedIPs: [] topologyLabel: \"node-role.kubernetes.io/worker\" nodeSelector: matchLabels: node-role.kubernetes.io/worker: \"\" EOF Create test namespaces ```bash cat << EOF | kubectl apply -f - apiVersion: v1 kind: Namespace metadata: name: egressipam-azure-test annotations: egressip-ipam-operator.redhat-cop.io/egressipam: egressipam-azure apiVersion: v1 kind: Namespace metadata: name: egressipam-azure-test-1 annotations: egressip-ipam-operator.redhat-cop.io/egressipam: egressipam-azure EOF ``` Check the namespaces have IPs assigned bash kubectl get namespace egressipam-azure-test \\ egressipam-azure-test-1 -o yaml | grep egressips The output should look like: egressip-ipam-operator.redhat-cop.io/egressips: 10.0.1.8 egressip-ipam-operator.redhat-cop.io/egressips: 10.0.1.7 Check they're actually set as Egress IPs bash oc get netnamespaces | egrep 'NAME|egress' The output should look like: NAME NETID EGRESS IPS egressip-ipam-operator 6374875 egressipam-azure-test 6917470 [\"10.0.1.8\"] egressipam-azure-test-1 16320378 [\"10.0.1.7\"] Finally check the Host Subnets for Egress IPS bash oc get hostsubnets The output should look like: NAME HOST HOST IP SUBNET EGRESS CIDRS EGRESS IPS private-cluster-bj275-master-0 private-cluster-bj275-master-0 10.0.0.8 10.129.0.0/23 private-cluster-bj275-master-1 private-cluster-bj275-master-1 10.0.0.7 10.128.0.0/23 private-cluster-bj275-master-2 private-cluster-bj275-master-2 10.0.0.9 10.130.0.0/23 private-cluster-bj275-worker-eastus1-zt59t private-cluster-bj275-worker-eastus1-zt59t 10.0.1.4 10.128.2.0/23 [\"10.0.1.8\"] private-cluster-bj275-worker-eastus2-bfrwt private-cluster-bj275-worker-eastus2-bfrwt 10.0.1.5 10.129.2.0/23 [\"10.0.1.7\"] private-cluster-bj275-worker-eastus3-fgjzk private-cluster-bj275-worker-eastus3-fgjzk 10.0.1.6 10.131.0.0/23 Test Egress Log into your jumpbox and allow http into firewall bash sudo firewall-cmd --zone=public --add-service=http Install and start apache httpd bash sudo yum -y install httpd sudo systemctl start httpd Create a index.html bash echo HELLO | sudo tee /var/www/html/index.html tail apache logs bash sudo tail -f /var/log/httpd/access_log Start an interactive pod in one of your new namespaces bash kubectl run -n egressipam-azure-test -i \\ --tty --rm debug --image=alpine \\ --restart=Never -- wget -O - 10.0.3.4 The output should look the following (the IP should match the egress IP of your namespace): bash 10.0.1.7 - - [03/Feb/2022:19:33:54 +0000] \"GET / HTTP/1.1\" 200 6 \"-\" \"Wget\"","title":"Using the Egressip Ipam Operator with a Private ARO Cluster"},{"location":"aro/egress-ipam-operator/#using-the-egressip-ipam-operator-with-a-private-aro-cluster","text":"","title":"Using the Egressip Ipam Operator with a Private ARO Cluster"},{"location":"aro/egress-ipam-operator/#prerequisites","text":"A private ARO cluster with a VPN Connection and the egress LB removed","title":"Prerequisites"},{"location":"aro/egress-ipam-operator/#deploy-the-egressip-ipam-operator","text":"","title":"Deploy the Egressip Ipam Operator"},{"location":"aro/egress-ipam-operator/#via-gui","text":"Log into the ARO cluster's Console Switch to the Administrator view Click on Operators -> Operator Hub Search for \"Egressip Ipam Operator\" Install it with the default settings or","title":"Via GUI"},{"location":"aro/egress-ipam-operator/#via-cli","text":"Deploy the egress-ipam-operator ```bash cat << EOF | kubectl apply -f - apiVersion: v1 kind: Namespace metadata: name: egressip-ipam-operator apiVersion: operators.coreos.com/v1alpha1 kind: Subscription metadata: name: egressip-ipam-operator namespace: openshift-operators labels: operators.coreos.com/egressip-ipam-operator.egressip-ipam-operator: '' spec: channel: alpha installPlanApproval: Automatic name: egressip-ipam-operator source: community-operators sourceNamespace: openshift-marketplace startingCSV: egressip-ipam-operator.v1.2.2 EOF ```","title":"Via CLI"},{"location":"aro/egress-ipam-operator/#configure-egressip","text":"Create an EgressIPAM resource for your cluster. Update the CIDR to reflect the worker node subnet. bash cat << EOF | kubectl apply -f - apiVersion: redhatcop.redhat.io/v1alpha1 kind: EgressIPAM metadata: name: egressipam-azure annotations: egressip-ipam-operator.redhat-cop.io/azure-egress-load-balancer: none spec: cidrAssignments: - labelValue: \"\" CIDR: 10.0.1.0/24 reservedIPs: [] topologyLabel: \"node-role.kubernetes.io/worker\" nodeSelector: matchLabels: node-role.kubernetes.io/worker: \"\" EOF Create test namespaces ```bash cat << EOF | kubectl apply -f - apiVersion: v1 kind: Namespace metadata: name: egressipam-azure-test annotations: egressip-ipam-operator.redhat-cop.io/egressipam: egressipam-azure apiVersion: v1 kind: Namespace metadata: name: egressipam-azure-test-1 annotations: egressip-ipam-operator.redhat-cop.io/egressipam: egressipam-azure EOF ``` Check the namespaces have IPs assigned bash kubectl get namespace egressipam-azure-test \\ egressipam-azure-test-1 -o yaml | grep egressips The output should look like: egressip-ipam-operator.redhat-cop.io/egressips: 10.0.1.8 egressip-ipam-operator.redhat-cop.io/egressips: 10.0.1.7 Check they're actually set as Egress IPs bash oc get netnamespaces | egrep 'NAME|egress' The output should look like: NAME NETID EGRESS IPS egressip-ipam-operator 6374875 egressipam-azure-test 6917470 [\"10.0.1.8\"] egressipam-azure-test-1 16320378 [\"10.0.1.7\"] Finally check the Host Subnets for Egress IPS bash oc get hostsubnets The output should look like: NAME HOST HOST IP SUBNET EGRESS CIDRS EGRESS IPS private-cluster-bj275-master-0 private-cluster-bj275-master-0 10.0.0.8 10.129.0.0/23 private-cluster-bj275-master-1 private-cluster-bj275-master-1 10.0.0.7 10.128.0.0/23 private-cluster-bj275-master-2 private-cluster-bj275-master-2 10.0.0.9 10.130.0.0/23 private-cluster-bj275-worker-eastus1-zt59t private-cluster-bj275-worker-eastus1-zt59t 10.0.1.4 10.128.2.0/23 [\"10.0.1.8\"] private-cluster-bj275-worker-eastus2-bfrwt private-cluster-bj275-worker-eastus2-bfrwt 10.0.1.5 10.129.2.0/23 [\"10.0.1.7\"] private-cluster-bj275-worker-eastus3-fgjzk private-cluster-bj275-worker-eastus3-fgjzk 10.0.1.6 10.131.0.0/23","title":"Configure EgressIP"},{"location":"aro/egress-ipam-operator/#test-egress","text":"Log into your jumpbox and allow http into firewall bash sudo firewall-cmd --zone=public --add-service=http Install and start apache httpd bash sudo yum -y install httpd sudo systemctl start httpd Create a index.html bash echo HELLO | sudo tee /var/www/html/index.html tail apache logs bash sudo tail -f /var/log/httpd/access_log Start an interactive pod in one of your new namespaces bash kubectl run -n egressipam-azure-test -i \\ --tty --rm debug --image=alpine \\ --restart=Never -- wget -O - 10.0.3.4 The output should look the following (the IP should match the egress IP of your namespace): bash 10.0.1.7 - - [03/Feb/2022:19:33:54 +0000] \"GET / HTTP/1.1\" 200 6 \"-\" \"Wget\"","title":"Test Egress"},{"location":"aro/ocm/","text":"Registering an ARO cluster to OpenShift Cluster Manager ARO clusters do not come connected to OpenShift Cluster Manager by default, because Azure would like customers to specifically opt-in to connections / data sent outside of Azure. This is the case with registering to OpenShift cluster manager, which enables a telemetry service in ARO. Prerequisites An Red Hat account. If you have any subscriptions with Red Hat, you will have a Red Hat account. If not, then you can create an account easily at https://cloud.redhat.com. Steps Login to https://console.redhat.com with you Red Hat account. Go to https://console.redhat.com/openshift/downloads and download your pull-secret file. This is a file that includes an authentication for cloud.openshift.com which is used by OpenShift Cluster Manager. Follow the Update pull secret instructions to merge your pull-secret (in particular cloud.openshift.com) in your ARO pull secret. Be careful not to overwrite the ARO cluster pull secrets that come by default - it explains how in that article. After waiting a few minutes (but it could be up to an hour), your cluster should be automatically registered in this list in OpenShift Cluster Manager; https://console.redhat.com/openshift You can check the cluster ID within the Cluster Overview section of the admin console with the ID of the cluster in OCM to make sure the right cluster is registered. The cluster will appear as a 60-day self-supported evaluation cluster. However, again, wait about an hour (but in this case, it can take up to 24 hours), and the cluster will be automatically updated to an ARO type cluster, with full support. You don't need to change the support level yourself. This makes the cluster a fully supported cluster within the Red Hat cloud console, with access to raise support tickets, also.","title":"Openshift Cluster Manager (OCM)"},{"location":"aro/ocm/#registering-an-aro-cluster-to-openshift-cluster-manager","text":"ARO clusters do not come connected to OpenShift Cluster Manager by default, because Azure would like customers to specifically opt-in to connections / data sent outside of Azure. This is the case with registering to OpenShift cluster manager, which enables a telemetry service in ARO.","title":"Registering an ARO cluster to OpenShift Cluster Manager"},{"location":"aro/ocm/#prerequisites","text":"An Red Hat account. If you have any subscriptions with Red Hat, you will have a Red Hat account. If not, then you can create an account easily at https://cloud.redhat.com.","title":"Prerequisites"},{"location":"aro/ocm/#steps","text":"Login to https://console.redhat.com with you Red Hat account. Go to https://console.redhat.com/openshift/downloads and download your pull-secret file. This is a file that includes an authentication for cloud.openshift.com which is used by OpenShift Cluster Manager. Follow the Update pull secret instructions to merge your pull-secret (in particular cloud.openshift.com) in your ARO pull secret. Be careful not to overwrite the ARO cluster pull secrets that come by default - it explains how in that article. After waiting a few minutes (but it could be up to an hour), your cluster should be automatically registered in this list in OpenShift Cluster Manager; https://console.redhat.com/openshift You can check the cluster ID within the Cluster Overview section of the admin console with the ID of the cluster in OCM to make sure the right cluster is registered. The cluster will appear as a 60-day self-supported evaluation cluster. However, again, wait about an hour (but in this case, it can take up to 24 hours), and the cluster will be automatically updated to an ARO type cluster, with full support. You don't need to change the support level yourself. This makes the cluster a fully supported cluster within the Red Hat cloud console, with access to raise support tickets, also.","title":"Steps"},{"location":"aro/private-cluster/","text":"ARO Quickstart - Private Cluster with JumpHost A Quickstart guide to deploying a Private Azure Red Hat OpenShift cluster. Once the cluster is running you will need a way to access the private network that ARO is deployed into. Author: Paul Czarkowski Prerequisites Azure CLI Obviously you'll need to have an Azure account to configure the CLI against. MacOS See Azure Docs for alternative install options. Install Azure CLI using homebrew bash brew update && brew install azure-cli Linux See Azure Docs for alternative install options. Import the Microsoft Keys bash sudo rpm --import https://packages.microsoft.com/keys/microsoft.asc Add the Microsoft Yum Repository bash cat << EOF | sudo tee /etc/yum.repos.d/azure-cli.repo [azure-cli] name=Azure CLI baseurl=https://packages.microsoft.com/yumrepos/azure-cli enabled=1 gpgcheck=1 gpgkey=https://packages.microsoft.com/keys/microsoft.asc EOF Install Azure CLI bash sudo dnf install -y azure-cli Prepare Azure Account for Azure OpenShift Log into the Azure CLI by running the following and then authorizing through your Web Browser bash az login Make sure you have enough Quota (change the location if you're not using East US ) bash az vm list-usage --location \"East US\" -o table see Addendum - Adding Quota to ARO account if you have less than 36 Quota left for Total Regional vCPUs . Register resource providers bash az provider register -n Microsoft.RedHatOpenShift --wait az provider register -n Microsoft.Compute --wait az provider register -n Microsoft.Storage --wait az provider register -n Microsoft.Authorization --wait Get Red Hat pull secret Log into cloud.redhat.com Browse to https://cloud.redhat.com/openshift/install/azure/aro-provisioned click the Download pull secret button and remember where you saved it, you'll reference it later. Deploy Azure OpenShift Variables and Resource Group Set some environment variables to use later, and create an Azure Resource Group. Set the following environment variables Change the values to suit your environment, but these defaults should work. bash AZR_RESOURCE_LOCATION=eastus AZR_RESOURCE_GROUP=openshift-private AZR_CLUSTER=private-cluster AZR_PULL_SECRET=~/Downloads/pull-secret.txt NETWORK_SUBNET=10.0.0.0/20 CONTROL_SUBNET=10.0.0.0/24 MACHINE_SUBNET=10.0.1.0/24 FIREWALL_SUBNET=10.0.2.0/24 JUMPHOST_SUBNET=10.0.3.0/24 Create an Azure resource group bash az group create \\ --name $AZR_RESOURCE_GROUP \\ --location $AZR_RESOURCE_LOCATION Networking Create a virtual network with two empty subnets Create virtual network bash az network vnet create \\ --address-prefixes $NETWORK_SUBNET \\ --name \"$AZR_CLUSTER-aro-vnet-$AZR_RESOURCE_LOCATION\" \\ --resource-group $AZR_RESOURCE_GROUP Create control plane subnet bash az network vnet subnet create \\ --resource-group $AZR_RESOURCE_GROUP \\ --vnet-name \"$AZR_CLUSTER-aro-vnet-$AZR_RESOURCE_LOCATION\" \\ --name \"$AZR_CLUSTER-aro-control-subnet-$AZR_RESOURCE_LOCATION\" \\ --address-prefixes $CONTROL_SUBNET \\ --service-endpoints Microsoft.ContainerRegistry Create machine subnet bash az network vnet subnet create \\ --resource-group $AZR_RESOURCE_GROUP \\ --vnet-name \"$AZR_CLUSTER-aro-vnet-$AZR_RESOURCE_LOCATION\" \\ --name \"$AZR_CLUSTER-aro-machine-subnet-$AZR_RESOURCE_LOCATION\" \\ --address-prefixes $MACHINE_SUBNET \\ --service-endpoints Microsoft.ContainerRegistry Disable network policies for Private Link Service on the control plane subnet This is required for the service to be able to connect to and manage the cluster. bash az network vnet subnet update \\ --name \"$AZR_CLUSTER-aro-control-subnet-$AZR_RESOURCE_LOCATION\" \\ --resource-group $AZR_RESOURCE_GROUP \\ --vnet-name \"$AZR_CLUSTER-aro-vnet-$AZR_RESOURCE_LOCATION\" \\ --disable-private-link-service-network-policies true Firewall + Internet Egress This replaces the routes for the cluster to go through the Firewall for egress vs the LoadBalancer which we can later remove. It does come with extra Azure costs of course. You can skip this step if you don't need to restrict egress. Make sure you have the AZ CLI firewall extensions bash az extension add -n azure-firewall az extension update -n azure-firewall Create a firewall network, IP, and firewall ```bash az network vnet subnet create \\ -g $AZR_RESOURCE_GROUP \\ --vnet-name \"$AZR_CLUSTER-aro-vnet-$AZR_RESOURCE_LOCATION\" \\ -n \"AzureFirewallSubnet\" \\ --address-prefixes $FIREWALL_SUBNET az network public-ip create -g $AZR_RESOURCE_GROUP -n fw-ip \\ --sku \"Standard\" --location $AZR_RESOURCE_LOCATION az network firewall create -g $AZR_RESOURCE_GROUP \\ -n aro-private -l $AZR_RESOURCE_LOCATION ``` Configure the firewall and configure IP Config (this may take 15 minutes) ```bash az network firewall ip-config create -g $AZR_RESOURCE_GROUP \\ -f aro-private -n fw-config --public-ip-address fw-ip \\ --vnet-name \"$AZR_CLUSTER-aro-vnet-$AZR_RESOURCE_LOCATION\" FWPUBLIC_IP=$(az network public-ip show -g $AZR_RESOURCE_GROUP -n fw-ip --query \"ipAddress\" -o tsv) FWPRIVATE_IP=$(az network firewall show -g $AZR_RESOURCE_GROUP -n aro-private --query \"ipConfigurations[0].privateIpAddress\" -o tsv) echo $FWPUBLIC_IP echo $FWPRIVATE_IP ``` Create and configure a route table ```bash az network route-table create -g $AZR_RESOURCE_GROUP --name aro-udr sleep 10 az network route-table route create -g $AZR_RESOURCE_GROUP --name aro-udr \\ --route-table-name aro-udr --address-prefix 0.0.0.0/0 \\ --next-hop-type VirtualAppliance --next-hop-ip-address $FWPRIVATE_IP az network route-table route create -g $AZR_RESOURCE_GROUP --name aro-vnet \\ --route-table-name aro-udr --address-prefix 10.0.0.0/16 --name local-route \\ --next-hop-type VirtualNetworkGateway ``` Create firewall rules for ARO resources Note: ARO clusters do not need access to the internet, however your own workloads running on them may. You can skip this step if you don't need any egress at all. Create a Network Rule to allow all http/https egress traffic (not recommended) bash az network firewall network-rule create -g $AZR_RESOURCE_GROUP -f aro-private \\ --collection-name 'allow-https' --name allow-all \\ --action allow --priority 100 \\ --source-addresses '*' --dest-addr '*' \\ --protocols 'Any' --destination-ports 1-65535 Create Application Rules to allow to a restricted set of destinations replace the target-fqdns with your desired destinations ```bash az network firewall application-rule create -g $AZR_RESOURCE_GROUP -f aro-private \\ --collection-name 'Allow_Egress' \\ --action allow \\ --priority 100 \\ -n 'required' \\ --source-addresses ' ' \\ --protocols 'http=80' 'https=443' \\ --target-fqdns ' .google.com' '*.bing.com' az network firewall application-rule create -g $AZR_RESOURCE_GROUP -f aro-private \\ --collection-name 'Docker' \\ --action allow \\ --priority 200 \\ -n 'docker' \\ --source-addresses ' ' \\ --protocols 'http=80' 'https=443' \\ --target-fqdns ' cloudflare.docker.com' '*registry-1.docker.io' 'apt.dockerproject.org' 'auth.docker.io' ``` Update the subnets to use the Firewall Once the cluster is deployed successfully you can update the subnets to use the firewall instead of the default outbound loadbalancer rule. ```bash az network vnet subnet update -g $AZR_RESOURCE_GROUP \\ --vnet-name $AZR_CLUSTER-aro-vnet-$AZR_RESOURCE_LOCATION \\ --name \"$AZR_CLUSTER-aro-control-subnet-$AZR_RESOURCE_LOCATION\" \\ --route-table aro-udr az network vnet subnet update -g $AZR_RESOURCE_GROUP \\ --vnet-name $AZR_CLUSTER-aro-vnet-$AZR_RESOURCE_LOCATION \\ --name \"$AZR_CLUSTER-aro-machine-subnet-$AZR_RESOURCE_LOCATION\" \\ --route-table aro-udr ``` Create the cluster This will take between 30 and 45 minutes. bash az aro create \\ --resource-group $AZR_RESOURCE_GROUP \\ --name $AZR_CLUSTER \\ --vnet \"$AZR_CLUSTER-aro-vnet-$AZR_RESOURCE_LOCATION\" \\ --master-subnet \"$AZR_CLUSTER-aro-control-subnet-$AZR_RESOURCE_LOCATION\" \\ --worker-subnet \"$AZR_CLUSTER-aro-machine-subnet-$AZR_RESOURCE_LOCATION\" \\ --apiserver-visibility Private \\ --ingress-visibility Private \\ --pull-secret @$AZR_PULL_SECRET Jump Host With the cluster in a private network, we can create a Jump host in order to connect to it. You can do this while the cluster is being created. Create jump subnet bash az network vnet subnet create \\ --resource-group $AZR_RESOURCE_GROUP \\ --vnet-name \"$AZR_CLUSTER-aro-vnet-$AZR_RESOURCE_LOCATION\" \\ --name JumpSubnet \\ --address-prefixes $JUMPHOST_SUBNET \\ --service-endpoints Microsoft.ContainerRegistry Create a jump host bash az vm create --name jumphost \\ --resource-group $AZR_RESOURCE_GROUP \\ --ssh-key-values $HOME/.ssh/id_rsa.pub \\ --admin-username aro \\ --image \"RedHat:RHEL:8.2:8.2.2021040911\" \\ --subnet JumpSubnet \\ --public-ip-address jumphost-ip \\ --vnet-name \"$AZR_CLUSTER-aro-vnet-$AZR_RESOURCE_LOCATION\" Save the jump host public IP address bash JUMP_IP=$(az vm list-ip-addresses -g $AZR_RESOURCE_GROUP -n jumphost -o tsv \\ --query '[].virtualMachine.network.publicIpAddresses[0].ipAddress') echo $JUMP_IP ssh to jump host forwarding port 1337 as a socks proxy. replace the IP with the IP of the jump box from the previous step. bash ssh -D 1337 -C -i $HOME/.ssh/id_rsa aro@$JUMP_IP test the socks proxy bash curl --socks5-hostname localhost:1337 http://www.google.com/ Install tools ```bash sudo yum install -y gcc libffi-devel python3-devel openssl-devel jq sudo rpm --import https://packages.microsoft.com/keys/microsoft.asc echo -e \"[azure-cli] name=Azure CLI baseurl= https://packages.microsoft.com/yumrepos/azure-cli enabled=1 gpgcheck=1 gpgkey= https://packages.microsoft.com/keys/microsoft.asc \" | sudo tee /etc/yum.repos.d/azure-cli.repo sudo yum install -y azure-cli wget https://mirror.openshift.com/pub/openshift-v4/clients/ocp/latest/openshift-client-linux.tar.gz mkdir openshift tar -zxvf openshift-client-linux.tar.gz -C openshift sudo install openshift/oc /usr/local/bin/oc sudo install openshift/kubectl /usr/local/bin/kubectl ``` Wait until the ARO cluster is fully provisioned. Login to Azure bash az login Get OpenShift console URL set these variables to match the ones you set at the start. bash AZR_RESOURCE_GROUP=openshift-private AZR_CLUSTER=private-cluster APISERVER=$(az aro show \\ --name $AZR_CLUSTER \\ --resource-group $AZR_RESOURCE_GROUP \\ -o tsv --query apiserverProfile.url) echo $APISERVER Get OpenShift credentials bash ADMINPW=$(az aro list-credentials \\ --name $AZR_CLUSTER \\ --resource-group $AZR_RESOURCE_GROUP \\ --query kubeadminPassword \\ -o tsv) Test Access Test Access to the cluster via the socks proxy ```bash CONSOLE=$(az aro show \\ --name $AZR_CLUSTER \\ --resource-group $AZR_RESOURCE_GROUP \\ -o tsv --query consoleProfile) echo $CONSOLE curl --socks5-hostname localhost:1337 $CONSOLE ``` Unfortunately you can't [easily] use the socks proxy with the oc command, but at least you can access the console via the socks proxy. Set localhost:1337 as a socks proxy in your browser and verify you can access the cluster by browsing to the $CONSOLE url. Delete Cluster Once you're done its a good idea to delete the cluster to ensure that you don't get a surprise bill. Delete the cluster bash az aro delete -y \\ --resource-group $AZR_RESOURCE_GROUP \\ --name $AZR_CLUSTER Delete the Azure resource group Only do this if there's nothing else in the resource group. bash az group delete -y \\ --name $AZR_RESOURCE_GROUP Addendum Adding Quota to ARO account Create an Azure Support Request Set Issue Type to \"Service and subscription limits (quotas)\" Set Quota Type to \"Compute-VM (cores-vCPUs) subscription limit increases\" Click Next Solutions >> Click Enter details Set Deployment Model to \"Resource Manager Set Locations to \"(US) East US\" Set Types to \"Standard\" Under Standard check \"DSv3\" and \"DSv4\" Set New vCPU Limit for each (example \"60\") Click Save and continue Click Review + create >> Wait until quota is increased.","title":"Private Cluster"},{"location":"aro/private-cluster/#aro-quickstart-private-cluster-with-jumphost","text":"A Quickstart guide to deploying a Private Azure Red Hat OpenShift cluster. Once the cluster is running you will need a way to access the private network that ARO is deployed into. Author: Paul Czarkowski","title":"ARO Quickstart - Private Cluster with JumpHost"},{"location":"aro/private-cluster/#prerequisites","text":"","title":"Prerequisites"},{"location":"aro/private-cluster/#azure-cli","text":"Obviously you'll need to have an Azure account to configure the CLI against. MacOS See Azure Docs for alternative install options. Install Azure CLI using homebrew bash brew update && brew install azure-cli Linux See Azure Docs for alternative install options. Import the Microsoft Keys bash sudo rpm --import https://packages.microsoft.com/keys/microsoft.asc Add the Microsoft Yum Repository bash cat << EOF | sudo tee /etc/yum.repos.d/azure-cli.repo [azure-cli] name=Azure CLI baseurl=https://packages.microsoft.com/yumrepos/azure-cli enabled=1 gpgcheck=1 gpgkey=https://packages.microsoft.com/keys/microsoft.asc EOF Install Azure CLI bash sudo dnf install -y azure-cli","title":"Azure CLI"},{"location":"aro/private-cluster/#prepare-azure-account-for-azure-openshift","text":"Log into the Azure CLI by running the following and then authorizing through your Web Browser bash az login Make sure you have enough Quota (change the location if you're not using East US ) bash az vm list-usage --location \"East US\" -o table see Addendum - Adding Quota to ARO account if you have less than 36 Quota left for Total Regional vCPUs . Register resource providers bash az provider register -n Microsoft.RedHatOpenShift --wait az provider register -n Microsoft.Compute --wait az provider register -n Microsoft.Storage --wait az provider register -n Microsoft.Authorization --wait","title":"Prepare Azure Account for Azure OpenShift"},{"location":"aro/private-cluster/#get-red-hat-pull-secret","text":"Log into cloud.redhat.com Browse to https://cloud.redhat.com/openshift/install/azure/aro-provisioned click the Download pull secret button and remember where you saved it, you'll reference it later.","title":"Get Red Hat pull secret"},{"location":"aro/private-cluster/#deploy-azure-openshift","text":"","title":"Deploy Azure OpenShift"},{"location":"aro/private-cluster/#variables-and-resource-group","text":"Set some environment variables to use later, and create an Azure Resource Group. Set the following environment variables Change the values to suit your environment, but these defaults should work. bash AZR_RESOURCE_LOCATION=eastus AZR_RESOURCE_GROUP=openshift-private AZR_CLUSTER=private-cluster AZR_PULL_SECRET=~/Downloads/pull-secret.txt NETWORK_SUBNET=10.0.0.0/20 CONTROL_SUBNET=10.0.0.0/24 MACHINE_SUBNET=10.0.1.0/24 FIREWALL_SUBNET=10.0.2.0/24 JUMPHOST_SUBNET=10.0.3.0/24 Create an Azure resource group bash az group create \\ --name $AZR_RESOURCE_GROUP \\ --location $AZR_RESOURCE_LOCATION","title":"Variables and Resource Group"},{"location":"aro/private-cluster/#networking","text":"Create a virtual network with two empty subnets Create virtual network bash az network vnet create \\ --address-prefixes $NETWORK_SUBNET \\ --name \"$AZR_CLUSTER-aro-vnet-$AZR_RESOURCE_LOCATION\" \\ --resource-group $AZR_RESOURCE_GROUP Create control plane subnet bash az network vnet subnet create \\ --resource-group $AZR_RESOURCE_GROUP \\ --vnet-name \"$AZR_CLUSTER-aro-vnet-$AZR_RESOURCE_LOCATION\" \\ --name \"$AZR_CLUSTER-aro-control-subnet-$AZR_RESOURCE_LOCATION\" \\ --address-prefixes $CONTROL_SUBNET \\ --service-endpoints Microsoft.ContainerRegistry Create machine subnet bash az network vnet subnet create \\ --resource-group $AZR_RESOURCE_GROUP \\ --vnet-name \"$AZR_CLUSTER-aro-vnet-$AZR_RESOURCE_LOCATION\" \\ --name \"$AZR_CLUSTER-aro-machine-subnet-$AZR_RESOURCE_LOCATION\" \\ --address-prefixes $MACHINE_SUBNET \\ --service-endpoints Microsoft.ContainerRegistry Disable network policies for Private Link Service on the control plane subnet This is required for the service to be able to connect to and manage the cluster. bash az network vnet subnet update \\ --name \"$AZR_CLUSTER-aro-control-subnet-$AZR_RESOURCE_LOCATION\" \\ --resource-group $AZR_RESOURCE_GROUP \\ --vnet-name \"$AZR_CLUSTER-aro-vnet-$AZR_RESOURCE_LOCATION\" \\ --disable-private-link-service-network-policies true","title":"Networking"},{"location":"aro/private-cluster/#firewall-internet-egress","text":"This replaces the routes for the cluster to go through the Firewall for egress vs the LoadBalancer which we can later remove. It does come with extra Azure costs of course. You can skip this step if you don't need to restrict egress. Make sure you have the AZ CLI firewall extensions bash az extension add -n azure-firewall az extension update -n azure-firewall Create a firewall network, IP, and firewall ```bash az network vnet subnet create \\ -g $AZR_RESOURCE_GROUP \\ --vnet-name \"$AZR_CLUSTER-aro-vnet-$AZR_RESOURCE_LOCATION\" \\ -n \"AzureFirewallSubnet\" \\ --address-prefixes $FIREWALL_SUBNET az network public-ip create -g $AZR_RESOURCE_GROUP -n fw-ip \\ --sku \"Standard\" --location $AZR_RESOURCE_LOCATION az network firewall create -g $AZR_RESOURCE_GROUP \\ -n aro-private -l $AZR_RESOURCE_LOCATION ``` Configure the firewall and configure IP Config (this may take 15 minutes) ```bash az network firewall ip-config create -g $AZR_RESOURCE_GROUP \\ -f aro-private -n fw-config --public-ip-address fw-ip \\ --vnet-name \"$AZR_CLUSTER-aro-vnet-$AZR_RESOURCE_LOCATION\" FWPUBLIC_IP=$(az network public-ip show -g $AZR_RESOURCE_GROUP -n fw-ip --query \"ipAddress\" -o tsv) FWPRIVATE_IP=$(az network firewall show -g $AZR_RESOURCE_GROUP -n aro-private --query \"ipConfigurations[0].privateIpAddress\" -o tsv) echo $FWPUBLIC_IP echo $FWPRIVATE_IP ``` Create and configure a route table ```bash az network route-table create -g $AZR_RESOURCE_GROUP --name aro-udr sleep 10 az network route-table route create -g $AZR_RESOURCE_GROUP --name aro-udr \\ --route-table-name aro-udr --address-prefix 0.0.0.0/0 \\ --next-hop-type VirtualAppliance --next-hop-ip-address $FWPRIVATE_IP az network route-table route create -g $AZR_RESOURCE_GROUP --name aro-vnet \\ --route-table-name aro-udr --address-prefix 10.0.0.0/16 --name local-route \\ --next-hop-type VirtualNetworkGateway ``` Create firewall rules for ARO resources Note: ARO clusters do not need access to the internet, however your own workloads running on them may. You can skip this step if you don't need any egress at all. Create a Network Rule to allow all http/https egress traffic (not recommended) bash az network firewall network-rule create -g $AZR_RESOURCE_GROUP -f aro-private \\ --collection-name 'allow-https' --name allow-all \\ --action allow --priority 100 \\ --source-addresses '*' --dest-addr '*' \\ --protocols 'Any' --destination-ports 1-65535 Create Application Rules to allow to a restricted set of destinations replace the target-fqdns with your desired destinations ```bash az network firewall application-rule create -g $AZR_RESOURCE_GROUP -f aro-private \\ --collection-name 'Allow_Egress' \\ --action allow \\ --priority 100 \\ -n 'required' \\ --source-addresses ' ' \\ --protocols 'http=80' 'https=443' \\ --target-fqdns ' .google.com' '*.bing.com' az network firewall application-rule create -g $AZR_RESOURCE_GROUP -f aro-private \\ --collection-name 'Docker' \\ --action allow \\ --priority 200 \\ -n 'docker' \\ --source-addresses ' ' \\ --protocols 'http=80' 'https=443' \\ --target-fqdns ' cloudflare.docker.com' '*registry-1.docker.io' 'apt.dockerproject.org' 'auth.docker.io' ``` Update the subnets to use the Firewall Once the cluster is deployed successfully you can update the subnets to use the firewall instead of the default outbound loadbalancer rule. ```bash az network vnet subnet update -g $AZR_RESOURCE_GROUP \\ --vnet-name $AZR_CLUSTER-aro-vnet-$AZR_RESOURCE_LOCATION \\ --name \"$AZR_CLUSTER-aro-control-subnet-$AZR_RESOURCE_LOCATION\" \\ --route-table aro-udr az network vnet subnet update -g $AZR_RESOURCE_GROUP \\ --vnet-name $AZR_CLUSTER-aro-vnet-$AZR_RESOURCE_LOCATION \\ --name \"$AZR_CLUSTER-aro-machine-subnet-$AZR_RESOURCE_LOCATION\" \\ --route-table aro-udr ``` Create the cluster This will take between 30 and 45 minutes. bash az aro create \\ --resource-group $AZR_RESOURCE_GROUP \\ --name $AZR_CLUSTER \\ --vnet \"$AZR_CLUSTER-aro-vnet-$AZR_RESOURCE_LOCATION\" \\ --master-subnet \"$AZR_CLUSTER-aro-control-subnet-$AZR_RESOURCE_LOCATION\" \\ --worker-subnet \"$AZR_CLUSTER-aro-machine-subnet-$AZR_RESOURCE_LOCATION\" \\ --apiserver-visibility Private \\ --ingress-visibility Private \\ --pull-secret @$AZR_PULL_SECRET","title":"Firewall + Internet Egress"},{"location":"aro/private-cluster/#jump-host","text":"With the cluster in a private network, we can create a Jump host in order to connect to it. You can do this while the cluster is being created. Create jump subnet bash az network vnet subnet create \\ --resource-group $AZR_RESOURCE_GROUP \\ --vnet-name \"$AZR_CLUSTER-aro-vnet-$AZR_RESOURCE_LOCATION\" \\ --name JumpSubnet \\ --address-prefixes $JUMPHOST_SUBNET \\ --service-endpoints Microsoft.ContainerRegistry Create a jump host bash az vm create --name jumphost \\ --resource-group $AZR_RESOURCE_GROUP \\ --ssh-key-values $HOME/.ssh/id_rsa.pub \\ --admin-username aro \\ --image \"RedHat:RHEL:8.2:8.2.2021040911\" \\ --subnet JumpSubnet \\ --public-ip-address jumphost-ip \\ --vnet-name \"$AZR_CLUSTER-aro-vnet-$AZR_RESOURCE_LOCATION\" Save the jump host public IP address bash JUMP_IP=$(az vm list-ip-addresses -g $AZR_RESOURCE_GROUP -n jumphost -o tsv \\ --query '[].virtualMachine.network.publicIpAddresses[0].ipAddress') echo $JUMP_IP ssh to jump host forwarding port 1337 as a socks proxy. replace the IP with the IP of the jump box from the previous step. bash ssh -D 1337 -C -i $HOME/.ssh/id_rsa aro@$JUMP_IP test the socks proxy bash curl --socks5-hostname localhost:1337 http://www.google.com/ Install tools ```bash sudo yum install -y gcc libffi-devel python3-devel openssl-devel jq sudo rpm --import https://packages.microsoft.com/keys/microsoft.asc echo -e \"[azure-cli] name=Azure CLI baseurl= https://packages.microsoft.com/yumrepos/azure-cli enabled=1 gpgcheck=1 gpgkey= https://packages.microsoft.com/keys/microsoft.asc \" | sudo tee /etc/yum.repos.d/azure-cli.repo sudo yum install -y azure-cli wget https://mirror.openshift.com/pub/openshift-v4/clients/ocp/latest/openshift-client-linux.tar.gz mkdir openshift tar -zxvf openshift-client-linux.tar.gz -C openshift sudo install openshift/oc /usr/local/bin/oc sudo install openshift/kubectl /usr/local/bin/kubectl ``` Wait until the ARO cluster is fully provisioned. Login to Azure bash az login Get OpenShift console URL set these variables to match the ones you set at the start. bash AZR_RESOURCE_GROUP=openshift-private AZR_CLUSTER=private-cluster APISERVER=$(az aro show \\ --name $AZR_CLUSTER \\ --resource-group $AZR_RESOURCE_GROUP \\ -o tsv --query apiserverProfile.url) echo $APISERVER Get OpenShift credentials bash ADMINPW=$(az aro list-credentials \\ --name $AZR_CLUSTER \\ --resource-group $AZR_RESOURCE_GROUP \\ --query kubeadminPassword \\ -o tsv)","title":"Jump Host"},{"location":"aro/private-cluster/#test-access","text":"Test Access to the cluster via the socks proxy ```bash CONSOLE=$(az aro show \\ --name $AZR_CLUSTER \\ --resource-group $AZR_RESOURCE_GROUP \\ -o tsv --query consoleProfile) echo $CONSOLE curl --socks5-hostname localhost:1337 $CONSOLE ``` Unfortunately you can't [easily] use the socks proxy with the oc command, but at least you can access the console via the socks proxy. Set localhost:1337 as a socks proxy in your browser and verify you can access the cluster by browsing to the $CONSOLE url.","title":"Test Access"},{"location":"aro/private-cluster/#delete-cluster","text":"Once you're done its a good idea to delete the cluster to ensure that you don't get a surprise bill. Delete the cluster bash az aro delete -y \\ --resource-group $AZR_RESOURCE_GROUP \\ --name $AZR_CLUSTER Delete the Azure resource group Only do this if there's nothing else in the resource group. bash az group delete -y \\ --name $AZR_RESOURCE_GROUP","title":"Delete Cluster"},{"location":"aro/private-cluster/#addendum","text":"","title":"Addendum"},{"location":"aro/private-cluster/#adding-quota-to-aro-account","text":"Create an Azure Support Request Set Issue Type to \"Service and subscription limits (quotas)\" Set Quota Type to \"Compute-VM (cores-vCPUs) subscription limit increases\" Click Next Solutions >> Click Enter details Set Deployment Model to \"Resource Manager Set Locations to \"(US) East US\" Set Types to \"Standard\" Under Standard check \"DSv3\" and \"DSv4\" Set New vCPU Limit for each (example \"60\") Click Save and continue Click Review + create >> Wait until quota is increased.","title":"Adding Quota to ARO account"},{"location":"aro/add-infra-nodes/","text":"Adding infrastructure nodes to an ARO cluster Paul Czarkowski 08/17/2022 This document shows how to set up infrastructure nodes in an ARO cluster and move infrastructure related workloads to them. This can help with larger clusters that have resource contention between user workloads and infrastructure workloads such as Prometheus. Important note: Infrastructure nodes are billed at the same rates as your existing ARO worker nodes. You can find the original (and more detailed) document describing the process for a self-managed OpenShift Container Platform cluster here Prerequisites Azure Red Hat OpenShift cluster Helm CLI Create Infra Nodes We'll use the MOBB Helm Chart for adding ARO machinesets which defaults to creating infra nodes, it looks up an existing machineset to collect cluster specific settings and then creates a new machineset specific for infra nodes with the same settings. Add the MOBB chart repository to your Helm bash helm repo add mobb https://rh-mobb.github.io/helm-charts/ Update your repositories bash helm repo update Install the mobb/aro-machinesets Chart to create infra nodes bash helm upgrade --install -n openshift-machine-api \\ infra mobb/aro-machinesets Wait for the new nodes to be available bash watch oc get machines Moving Infra workloads Ingress You may choose this for any additional Ingress controllers you may have in the cluster, however if you application has very high Ingress resource requirements it may make sense to allow them to spread across the worker nodes, or even a dedicated MachineSet . Set the nodePlacement on the ingresscontroller to node-role.kubernetes.io/infra and increase the replicas to match the number of infra nodes bash oc patch -n openshift-ingress-operator ingresscontroller default --type=merge \\ -p='{\"spec\":{\"replicas\":3,\"nodePlacement\":{\"nodeSelector\":{\"matchLabels\":{\"node-role.kubernetes.io/infra\":\"\"}},\"tolerations\":[{\"effect\":\"NoSchedule\",\"key\":\"node-role.kubernetes.io/infra\",\"operator\":\"Exists\"}]}}}' Check the Ingress Controller Operator is starting pods on the new infra nodes bash oc -n openshift-ingress get pods -o wide NAME READY STATUS RESTARTS AGE IP NODE NOMINATED NODE READINESS GATES router-default-69f58645b7-6xkvh 1/1 Running 0 66s 10.129.6.6 cz-cluster-hsmtw-infra-aro-machinesets-eastus-3-l6dqw <none> <none> router-default-69f58645b7-vttqz 1/1 Running 0 66s 10.131.4.6 cz-cluster-hsmtw-infra-aro-machinesets-eastus-1-vr56r <none> <none> router-default-6cb5ccf9f5-xjgcp 1/1 Terminating 0 23h 10.131.0.11 cz-cluster-hsmtw-worker-eastus2-xj9qx <none> <none> Registry Set the nodePlacement on the registry to node-role.kubernetes.io/infra bash oc patch configs.imageregistry.operator.openshift.io/cluster --type=merge \\ -p='{\"spec\":{\"affinity\":{\"podAntiAffinity\":{\"preferredDuringSchedulingIgnoredDuringExecution\":[{\"podAffinityTerm\":{\"namespaces\":[\"openshift-image-registry\"],\"topologyKey\":\"kubernetes.io/hostname\"},\"weight\":100}]}},\"logLevel\":\"Normal\",\"managementState\":\"Managed\",\"nodeSelector\":{\"node-role.kubernetes.io/infra\":\"\"},\"tolerations\":[{\"effect\":\"NoSchedule\",\"key\":\"node-role.kubernetes.io/infra\",\"operator\":\"Exists\"}]}}' Check the Registry Operator is starting pods on the new infra nodes bash oc -n openshift-image-registry get pods -l \"docker-registry\" -o wide ``` NAME READY STATUS RESTARTS AGE IP NODE NOMINATED NODE READINESS GATES image-registry-84cbd76d5d-cfsw7 1/1 Running 0 3h46m 10.128.6.7 cz-cluster-hsmtw-infra-aro-machinesets-eastus-2-kljml <none> <none> image-registry-84cbd76d5d-p2jf9 1/1 Running 0 3h46m 10.129.6.7 cz-cluster-hsmtw-infra-aro-machinesets-eastus-3-l6dqw <none> <none> ``` Cluster Monitoring Configure the cluster monitoring stack to use the infra nodes Note: This will override any other customizations to the cluster monitoring stack, so you may want to merge your existing customizations into this before running the command. bash cat << EOF | oc apply -f - apiVersion: v1 kind: ConfigMap metadata: name: cluster-monitoring-config namespace: openshift-monitoring data: config.yaml: |+ alertmanagerMain: nodeSelector: node-role.kubernetes.io/infra: \"\" tolerations: - effect: \"NoSchedule\" key: \"node-role.kubernetes.io/infra\" operator: \"Exists\" prometheusK8s: nodeSelector: node-role.kubernetes.io/infra: \"\" tolerations: - effect: \"NoSchedule\" key: \"node-role.kubernetes.io/infra\" operator: \"Exists\" prometheusOperator: {} grafana: nodeSelector: node-role.kubernetes.io/infra: \"\" tolerations: - effect: \"NoSchedule\" key: \"node-role.kubernetes.io/infra\" operator: \"Exists\" k8sPrometheusAdapter: nodeSelector: node-role.kubernetes.io/infra: \"\" tolerations: - effect: \"NoSchedule\" key: \"node-role.kubernetes.io/infra\" operator: \"Exists\" kubeStateMetrics: nodeSelector: node-role.kubernetes.io/infra: \"\" tolerations: - effect: \"NoSchedule\" key: \"node-role.kubernetes.io/infra\" operator: \"Exists\" telemeterClient: nodeSelector: node-role.kubernetes.io/infra: \"\" tolerations: - effect: \"NoSchedule\" key: \"node-role.kubernetes.io/infra\" operator: \"Exists\" openshiftStateMetrics: nodeSelector: node-role.kubernetes.io/infra: \"\" tolerations: - effect: \"NoSchedule\" key: \"node-role.kubernetes.io/infra\" operator: \"Exists\" thanosQuerier: nodeSelector: node-role.kubernetes.io/infra: \"\" tolerations: - effect: \"NoSchedule\" key: \"node-role.kubernetes.io/infra\" operator: \"Exists\" EOF Check the OpenShift Monitoring Operator is starting pods on the new infra nodes some Pods like prometheus-operator will remain on master nodes. bash oc -n openshift-monitoring get pods -o wide ``` NAME READY STATUS RESTARTS AGE IP NODE NOMINATED NODE READINESS GATES alertmanager-main-0 6/6 Running 0 2m14s 10.128.6.11 cz-cluster-hsmtw-infra-aro-machinesets-eastus-2-kljml <none> <none> alertmanager-main-1 6/6 Running 0 2m46s 10.131.4.11 cz-cluster-hsmtw-infra-aro-machinesets-eastus-1-vr56r <none> <none> cluster-monitoring-operator-5bbfd998c6-m9w62 2/2 Running 0 28h 10.128.0.23 cz-cluster-hsmtw-master-1 <none> <none> grafana-599d4b948c-btlp2 3/3 Running 0 2m48s 10.131.4.10 cz-cluster-hsmtw-infra-aro-machinesets-eastus-1-vr56r <none> <none> kube-state-metrics-574c5bfdd7-f7fjk 3/3 Running 0 2m49s 10.131.4.8 cz-cluster-hsmtw-infra-aro-machinesets-eastus-1-vr56r <none> <none> ... ... ```","title":"Adding infrastructure nodes to an ARO cluster"},{"location":"aro/add-infra-nodes/#adding-infrastructure-nodes-to-an-aro-cluster","text":"Paul Czarkowski 08/17/2022 This document shows how to set up infrastructure nodes in an ARO cluster and move infrastructure related workloads to them. This can help with larger clusters that have resource contention between user workloads and infrastructure workloads such as Prometheus. Important note: Infrastructure nodes are billed at the same rates as your existing ARO worker nodes. You can find the original (and more detailed) document describing the process for a self-managed OpenShift Container Platform cluster here","title":"Adding infrastructure nodes to an ARO cluster"},{"location":"aro/add-infra-nodes/#prerequisites","text":"Azure Red Hat OpenShift cluster Helm CLI","title":"Prerequisites"},{"location":"aro/add-infra-nodes/#create-infra-nodes","text":"We'll use the MOBB Helm Chart for adding ARO machinesets which defaults to creating infra nodes, it looks up an existing machineset to collect cluster specific settings and then creates a new machineset specific for infra nodes with the same settings. Add the MOBB chart repository to your Helm bash helm repo add mobb https://rh-mobb.github.io/helm-charts/ Update your repositories bash helm repo update Install the mobb/aro-machinesets Chart to create infra nodes bash helm upgrade --install -n openshift-machine-api \\ infra mobb/aro-machinesets Wait for the new nodes to be available bash watch oc get machines","title":"Create Infra Nodes"},{"location":"aro/add-infra-nodes/#moving-infra-workloads","text":"","title":"Moving Infra workloads"},{"location":"aro/add-infra-nodes/#ingress","text":"You may choose this for any additional Ingress controllers you may have in the cluster, however if you application has very high Ingress resource requirements it may make sense to allow them to spread across the worker nodes, or even a dedicated MachineSet . Set the nodePlacement on the ingresscontroller to node-role.kubernetes.io/infra and increase the replicas to match the number of infra nodes bash oc patch -n openshift-ingress-operator ingresscontroller default --type=merge \\ -p='{\"spec\":{\"replicas\":3,\"nodePlacement\":{\"nodeSelector\":{\"matchLabels\":{\"node-role.kubernetes.io/infra\":\"\"}},\"tolerations\":[{\"effect\":\"NoSchedule\",\"key\":\"node-role.kubernetes.io/infra\",\"operator\":\"Exists\"}]}}}' Check the Ingress Controller Operator is starting pods on the new infra nodes bash oc -n openshift-ingress get pods -o wide NAME READY STATUS RESTARTS AGE IP NODE NOMINATED NODE READINESS GATES router-default-69f58645b7-6xkvh 1/1 Running 0 66s 10.129.6.6 cz-cluster-hsmtw-infra-aro-machinesets-eastus-3-l6dqw <none> <none> router-default-69f58645b7-vttqz 1/1 Running 0 66s 10.131.4.6 cz-cluster-hsmtw-infra-aro-machinesets-eastus-1-vr56r <none> <none> router-default-6cb5ccf9f5-xjgcp 1/1 Terminating 0 23h 10.131.0.11 cz-cluster-hsmtw-worker-eastus2-xj9qx <none> <none>","title":"Ingress"},{"location":"aro/add-infra-nodes/#registry","text":"Set the nodePlacement on the registry to node-role.kubernetes.io/infra bash oc patch configs.imageregistry.operator.openshift.io/cluster --type=merge \\ -p='{\"spec\":{\"affinity\":{\"podAntiAffinity\":{\"preferredDuringSchedulingIgnoredDuringExecution\":[{\"podAffinityTerm\":{\"namespaces\":[\"openshift-image-registry\"],\"topologyKey\":\"kubernetes.io/hostname\"},\"weight\":100}]}},\"logLevel\":\"Normal\",\"managementState\":\"Managed\",\"nodeSelector\":{\"node-role.kubernetes.io/infra\":\"\"},\"tolerations\":[{\"effect\":\"NoSchedule\",\"key\":\"node-role.kubernetes.io/infra\",\"operator\":\"Exists\"}]}}' Check the Registry Operator is starting pods on the new infra nodes bash oc -n openshift-image-registry get pods -l \"docker-registry\" -o wide ``` NAME READY STATUS RESTARTS AGE IP NODE NOMINATED NODE READINESS GATES image-registry-84cbd76d5d-cfsw7 1/1 Running 0 3h46m 10.128.6.7 cz-cluster-hsmtw-infra-aro-machinesets-eastus-2-kljml <none> <none> image-registry-84cbd76d5d-p2jf9 1/1 Running 0 3h46m 10.129.6.7 cz-cluster-hsmtw-infra-aro-machinesets-eastus-3-l6dqw <none> <none> ```","title":"Registry"},{"location":"aro/add-infra-nodes/#cluster-monitoring","text":"Configure the cluster monitoring stack to use the infra nodes Note: This will override any other customizations to the cluster monitoring stack, so you may want to merge your existing customizations into this before running the command. bash cat << EOF | oc apply -f - apiVersion: v1 kind: ConfigMap metadata: name: cluster-monitoring-config namespace: openshift-monitoring data: config.yaml: |+ alertmanagerMain: nodeSelector: node-role.kubernetes.io/infra: \"\" tolerations: - effect: \"NoSchedule\" key: \"node-role.kubernetes.io/infra\" operator: \"Exists\" prometheusK8s: nodeSelector: node-role.kubernetes.io/infra: \"\" tolerations: - effect: \"NoSchedule\" key: \"node-role.kubernetes.io/infra\" operator: \"Exists\" prometheusOperator: {} grafana: nodeSelector: node-role.kubernetes.io/infra: \"\" tolerations: - effect: \"NoSchedule\" key: \"node-role.kubernetes.io/infra\" operator: \"Exists\" k8sPrometheusAdapter: nodeSelector: node-role.kubernetes.io/infra: \"\" tolerations: - effect: \"NoSchedule\" key: \"node-role.kubernetes.io/infra\" operator: \"Exists\" kubeStateMetrics: nodeSelector: node-role.kubernetes.io/infra: \"\" tolerations: - effect: \"NoSchedule\" key: \"node-role.kubernetes.io/infra\" operator: \"Exists\" telemeterClient: nodeSelector: node-role.kubernetes.io/infra: \"\" tolerations: - effect: \"NoSchedule\" key: \"node-role.kubernetes.io/infra\" operator: \"Exists\" openshiftStateMetrics: nodeSelector: node-role.kubernetes.io/infra: \"\" tolerations: - effect: \"NoSchedule\" key: \"node-role.kubernetes.io/infra\" operator: \"Exists\" thanosQuerier: nodeSelector: node-role.kubernetes.io/infra: \"\" tolerations: - effect: \"NoSchedule\" key: \"node-role.kubernetes.io/infra\" operator: \"Exists\" EOF Check the OpenShift Monitoring Operator is starting pods on the new infra nodes some Pods like prometheus-operator will remain on master nodes. bash oc -n openshift-monitoring get pods -o wide ``` NAME READY STATUS RESTARTS AGE IP NODE NOMINATED NODE READINESS GATES alertmanager-main-0 6/6 Running 0 2m14s 10.128.6.11 cz-cluster-hsmtw-infra-aro-machinesets-eastus-2-kljml <none> <none> alertmanager-main-1 6/6 Running 0 2m46s 10.131.4.11 cz-cluster-hsmtw-infra-aro-machinesets-eastus-1-vr56r <none> <none> cluster-monitoring-operator-5bbfd998c6-m9w62 2/2 Running 0 28h 10.128.0.23 cz-cluster-hsmtw-master-1 <none> <none> grafana-599d4b948c-btlp2 3/3 Running 0 2m48s 10.131.4.10 cz-cluster-hsmtw-infra-aro-machinesets-eastus-1-vr56r <none> <none> kube-state-metrics-574c5bfdd7-f7fjk 3/3 Running 0 2m49s 10.131.4.8 cz-cluster-hsmtw-infra-aro-machinesets-eastus-1-vr56r <none> <none> ... ... ```","title":"Cluster Monitoring"},{"location":"aro/additional-ingress-controller/","text":"Adding an additional ingress controller to an ARO cluster Paul Czarkowski, Stuart Kirk 03/30/2022 Prerequisites an Azure Red Hat OpenShift cluster a DNS zone that you can easily modify Get Started Create some environment variables bash DOMAIN=custom.azure.mobb.ninja EMAIL=example@email.com SCRATCH_DIR=/tmp/aro Create a certificate for the ingress controller bash certbot certonly --manual \\ --preferred-challenges=dns \\ --email $EMAIL \\ --server https://acme-v02.api.letsencrypt.org/directory \\ --agree-tos \\ --manual-public-ip-logging-ok \\ -d \"*.$DOMAIN\" \\ --config-dir \"$SCRATCH_DIR/config\" \\ --work-dir \"$SCRATCH_DIR/work\" \\ --logs-dir \"$SCRATCH_DIR/logs\" Create a secret for the certificate bash oc create secret tls custom-tls \\ -n openshift-ingress \\ --cert=$SCRATCH_DIR/config/live/$DOMAIN/fullchain.pem \\ --key=$SCRATCH_DIR/config/live/$DOMAIN/privkey.pem Create an ingress controller bash cat <<EOF | oc apply -f - apiVersion: operator.openshift.io/v1 kind: IngressController metadata: name: custom namespace: openshift-ingress-operator spec: domain: $DOMAIN nodePlacement: nodeSelector: matchLabels: node-role.kubernetes.io/worker: \"\" routeSelector: matchLabels: type: custom defaultCertificate: name: custom-tls httpEmptyRequestsPolicy: Respond httpErrorCodePages: name: \"\" replicas: 3 EOF Wait a few moments then get the EXTERNAL-IP of the new ingress controller bash oc get -n openshift-ingress svc router-custom The output should look like: NAME TYPE CLUSTER-IP EXTERNAL-IP PORT(S) AGE router-custom LoadBalancer 172.30.90.84 20.120.48.78 80:32160/TCP,443:32511/TCP 49s Create a wildcard DNS record pointing at the EXTERNAL-IP Test that the Ingress is working bash curl -s https://test.$DOMAIN | head <html> <head> <meta name=\"viewport\" content=\"width=device-width, initial-scale=1\"> Create a new project to deploy an application to bash oc new-project demo Create a new application bash oc new-app --docker-image=docker.io/openshift/hello-openshift Expose bash cat << EOF | oc apply -f - apiVersion: route.openshift.io/v1 kind: Route metadata: labels: app: hello-openshift app.kubernetes.io/component: hello-openshift app.kubernetes.io/instance: hello-openshift type: custom name: hello-openshift-tls spec: host: hello.$DOMAIN port: targetPort: 8080-tcp tls: termination: edge insecureEdgeTerminationPolicy: Redirect to: kind: Service name: hello-openshift EOF Verify it works bash curl https://hello.custom.azure.mobb.ninja bash Hello OpenShift!","title":"Adding an additional ingress controller to an ARO cluster"},{"location":"aro/additional-ingress-controller/#adding-an-additional-ingress-controller-to-an-aro-cluster","text":"Paul Czarkowski, Stuart Kirk 03/30/2022","title":"Adding an additional ingress controller to an ARO cluster"},{"location":"aro/additional-ingress-controller/#prerequisites","text":"an Azure Red Hat OpenShift cluster a DNS zone that you can easily modify","title":"Prerequisites"},{"location":"aro/additional-ingress-controller/#get-started","text":"Create some environment variables bash DOMAIN=custom.azure.mobb.ninja EMAIL=example@email.com SCRATCH_DIR=/tmp/aro Create a certificate for the ingress controller bash certbot certonly --manual \\ --preferred-challenges=dns \\ --email $EMAIL \\ --server https://acme-v02.api.letsencrypt.org/directory \\ --agree-tos \\ --manual-public-ip-logging-ok \\ -d \"*.$DOMAIN\" \\ --config-dir \"$SCRATCH_DIR/config\" \\ --work-dir \"$SCRATCH_DIR/work\" \\ --logs-dir \"$SCRATCH_DIR/logs\" Create a secret for the certificate bash oc create secret tls custom-tls \\ -n openshift-ingress \\ --cert=$SCRATCH_DIR/config/live/$DOMAIN/fullchain.pem \\ --key=$SCRATCH_DIR/config/live/$DOMAIN/privkey.pem Create an ingress controller bash cat <<EOF | oc apply -f - apiVersion: operator.openshift.io/v1 kind: IngressController metadata: name: custom namespace: openshift-ingress-operator spec: domain: $DOMAIN nodePlacement: nodeSelector: matchLabels: node-role.kubernetes.io/worker: \"\" routeSelector: matchLabels: type: custom defaultCertificate: name: custom-tls httpEmptyRequestsPolicy: Respond httpErrorCodePages: name: \"\" replicas: 3 EOF Wait a few moments then get the EXTERNAL-IP of the new ingress controller bash oc get -n openshift-ingress svc router-custom The output should look like: NAME TYPE CLUSTER-IP EXTERNAL-IP PORT(S) AGE router-custom LoadBalancer 172.30.90.84 20.120.48.78 80:32160/TCP,443:32511/TCP 49s Create a wildcard DNS record pointing at the EXTERNAL-IP Test that the Ingress is working bash curl -s https://test.$DOMAIN | head <html> <head> <meta name=\"viewport\" content=\"width=device-width, initial-scale=1\"> Create a new project to deploy an application to bash oc new-project demo Create a new application bash oc new-app --docker-image=docker.io/openshift/hello-openshift Expose bash cat << EOF | oc apply -f - apiVersion: route.openshift.io/v1 kind: Route metadata: labels: app: hello-openshift app.kubernetes.io/component: hello-openshift app.kubernetes.io/instance: hello-openshift type: custom name: hello-openshift-tls spec: host: hello.$DOMAIN port: targetPort: 8080-tcp tls: termination: edge insecureEdgeTerminationPolicy: Redirect to: kind: Service name: hello-openshift EOF Verify it works bash curl https://hello.custom.azure.mobb.ninja bash Hello OpenShift!","title":"Get Started"},{"location":"aro/astronomer/","text":"Installing Astronomer on a private ARO cluster see here for public clusters. This assumes you've already got a private ARO cluster installed. You could also follow the same instructions to create a public Astronomer, just use a regular DNS zone and skip the private parts. A default 3-node cluster is a bit small for Astronomer, If you have a three node cluster you can increase it by updating the replicas count machinesets in the openshift-machine-api namespace. Create a private DNS Log into Azure and click to private dns Click + Add Set the Resource Group to match your ARO Resource Group Set Name to your TLD (astro.mobb.ninja in the example) Click Review and Create and create the Zone Inside the Domain settings click Virtual network links -> + Add Link Name : astro-aro Select the correct Subscription and Network from the dropdown boxes Click OK Create TLS Secret Next we need a TLS Secret to use. You could create a self-signed certificate using a CA that you own, or use certbot (if you have a valid DNS provider, note records don't need to be public) certbot certonly --manual \\ --preferred-challenges=dns \\ --email username.taken@gmail.com \\ --server https://acme-v02.api.letsencrypt.org/directory \\ --agree-tos \\ --manual-public-ip-logging-ok \\ -d \"*.astro.mobb.ninja\" Follow certbot's instructions (something like ): ``` Please deploy a DNS TXT record under the name _acme-challenge.astro.mobb.ninja with the following value: 8d2HNuZ8rn9McPTzpo2evJsAJI8K4eJuVLaZlz6d-kc Before continuing, verify the record is deployed. ``` Create a Secret from the Cert (use the paths provided from the above command): oc new-project astronomer oc create secret tls astronomer-tls --cert=/etc/letsencrypt/live/astro.mobb.ninja/fullchain.pem --key=/etc/letsencrypt/live/astro.mobb.ninja/privkey.pem Deploy Astronomer update the values.yaml and set baseDomain: astro.mobb.ninja Install ``` helm repo add astronomer https://helm.astronomer.io/ helm repo update helm install -f values.yaml --version=0.25.2 \\ --namespace=astronomer astronomer \\ astronomer/astronomer ``` While that's running add our DNS In another shell run kubectl get svc -n astronomer astronomer-nginx Go back to your private DNS zone in Azure and create a record set * and copy the contents of EXTERNAL-IP from the above command. Fix SCCs for elasticsearch oc adm policy add-scc-to-user privileged -z astronomer-elasticsearch oc patch deployment astronomer-elasticsearch-client -p '{\"spec\":{\"template\":{\"spec\":{ \"containers\": [{\"name\": \"es-client\",\"securityContext\":{\"privileged\": true,\"runAsUser\": 0}}]}}}}' Validate the Install Check the Helm install has finished ``` NAME: astronomer LAST DEPLOYED: Mon May 24 18:03:05 2021 NAMESPACE: astronomer STATUS: deployed REVISION: 1 TEST SUITE: None NOTES: Thank you for installing Astronomer! Your release is named astronomer. The platform components may take a few minutes to spin up. You can access the platform at: Astronomer dashboard: https://app.astro.mobb.ninja Grafana dashboard: https://grafana.astro.mobb.ninja Kibana dashboard: https://kibana.astro.mobb.ninja Now that you've installed the platform, you are ready to get started and create your first airflow deployment. Download the CLI: curl -sSL https://install.astro.mobb.ninja | sudo bash We have guides available at https://www.astronomer.io/guides/ and are always available to help. ``` Since this is a private LB you'll need to access it from inside the network. The quick hacky way to do this is kubectl exec -ti astronomer-cli-install-6f899c87d5-2c84f -- wget -O - https://install.astro.mobb.ninja and you should see ``` ! /usr/bin/env bash TAG=${1:-v0.20.0} if (( EUID != 0 )); then echo \"Please run command as root.\" exit fi DOWNLOADER=\"https://raw.githubusercontent.com/astronomer/astro-cli/main/godownloader.sh\" ```","title":"Installing Astronomer on a private ARO cluster"},{"location":"aro/astronomer/#installing-astronomer-on-a-private-aro-cluster","text":"see here for public clusters. This assumes you've already got a private ARO cluster installed. You could also follow the same instructions to create a public Astronomer, just use a regular DNS zone and skip the private parts. A default 3-node cluster is a bit small for Astronomer, If you have a three node cluster you can increase it by updating the replicas count machinesets in the openshift-machine-api namespace.","title":"Installing Astronomer on a private ARO cluster"},{"location":"aro/astronomer/#create-a-private-dns","text":"Log into Azure and click to private dns Click + Add Set the Resource Group to match your ARO Resource Group Set Name to your TLD (astro.mobb.ninja in the example) Click Review and Create and create the Zone Inside the Domain settings click Virtual network links -> + Add Link Name : astro-aro Select the correct Subscription and Network from the dropdown boxes Click OK","title":"Create a private DNS"},{"location":"aro/astronomer/#create-tls-secret","text":"Next we need a TLS Secret to use. You could create a self-signed certificate using a CA that you own, or use certbot (if you have a valid DNS provider, note records don't need to be public) certbot certonly --manual \\ --preferred-challenges=dns \\ --email username.taken@gmail.com \\ --server https://acme-v02.api.letsencrypt.org/directory \\ --agree-tos \\ --manual-public-ip-logging-ok \\ -d \"*.astro.mobb.ninja\" Follow certbot's instructions (something like ): ``` Please deploy a DNS TXT record under the name _acme-challenge.astro.mobb.ninja with the following value: 8d2HNuZ8rn9McPTzpo2evJsAJI8K4eJuVLaZlz6d-kc Before continuing, verify the record is deployed. ``` Create a Secret from the Cert (use the paths provided from the above command): oc new-project astronomer oc create secret tls astronomer-tls --cert=/etc/letsencrypt/live/astro.mobb.ninja/fullchain.pem --key=/etc/letsencrypt/live/astro.mobb.ninja/privkey.pem","title":"Create TLS Secret"},{"location":"aro/astronomer/#deploy-astronomer","text":"update the values.yaml and set baseDomain: astro.mobb.ninja Install ``` helm repo add astronomer https://helm.astronomer.io/ helm repo update helm install -f values.yaml --version=0.25.2 \\ --namespace=astronomer astronomer \\ astronomer/astronomer ```","title":"Deploy Astronomer"},{"location":"aro/astronomer/#while-thats-running-add-our-dns","text":"In another shell run kubectl get svc -n astronomer astronomer-nginx Go back to your private DNS zone in Azure and create a record set * and copy the contents of EXTERNAL-IP from the above command.","title":"While that's running add our DNS"},{"location":"aro/astronomer/#fix-sccs-for-elasticsearch","text":"oc adm policy add-scc-to-user privileged -z astronomer-elasticsearch oc patch deployment astronomer-elasticsearch-client -p '{\"spec\":{\"template\":{\"spec\":{ \"containers\": [{\"name\": \"es-client\",\"securityContext\":{\"privileged\": true,\"runAsUser\": 0}}]}}}}'","title":"Fix SCCs for elasticsearch"},{"location":"aro/astronomer/#validate-the-install","text":"Check the Helm install has finished ``` NAME: astronomer LAST DEPLOYED: Mon May 24 18:03:05 2021 NAMESPACE: astronomer STATUS: deployed REVISION: 1 TEST SUITE: None NOTES: Thank you for installing Astronomer! Your release is named astronomer. The platform components may take a few minutes to spin up. You can access the platform at: Astronomer dashboard: https://app.astro.mobb.ninja Grafana dashboard: https://grafana.astro.mobb.ninja Kibana dashboard: https://kibana.astro.mobb.ninja Now that you've installed the platform, you are ready to get started and create your first airflow deployment. Download the CLI: curl -sSL https://install.astro.mobb.ninja | sudo bash We have guides available at https://www.astronomer.io/guides/ and are always available to help. ``` Since this is a private LB you'll need to access it from inside the network. The quick hacky way to do this is kubectl exec -ti astronomer-cli-install-6f899c87d5-2c84f -- wget -O - https://install.astro.mobb.ninja and you should see ```","title":"Validate the Install"},{"location":"aro/astronomer/#usrbinenv-bash","text":"TAG=${1:-v0.20.0} if (( EUID != 0 )); then echo \"Please run command as root.\" exit fi DOWNLOADER=\"https://raw.githubusercontent.com/astronomer/astro-cli/main/godownloader.sh\" ```","title":"! /usr/bin/env bash"},{"location":"aro/astronomer/README-public/","text":"Installing Astronomer on a public ARO cluster This assumes you've already got an ARO cluster installed. A default 3-node cluster is a bit small for Astronomer, If you have a three node cluster you can increase it by updating the replicas count machinesets in the openshift-machine-api namespace. Create TLS Secret set an environment variable containing the DNS you wish to use: ASTRO_DNS=astro.mobb.ninja We need a TLS Secret to use. You could create a self-signed certificate using a CA that you own, or use certbot (if you have a valid DNS provider, note records don't need to be public) certbot certonly --manual \\ --preferred-challenges=dns \\ --email username.taken@gmail.com \\ --server https://acme-v02.api.letsencrypt.org/directory \\ --agree-tos \\ --manual-public-ip-logging-ok \\ -d \"*.${ASTRO_DNS}\" Follow certbot's instructions (something like ): ``` Please deploy a DNS TXT record under the name _acme-challenge.astro.mobb.ninja with the following value: 8d2HNuZ8rn9McPTzpo2evJsAJI8K4eJuVLaZlz6d-kc Before continuing, verify the record is deployed. ``` Create a Secret from the Cert (use the paths provided from the above command): oc new-project astronomer oc create secret tls astronomer-tls --cert=/etc/letsencrypt/live/astro.mobb.ninja/fullchain.pem --key=/etc/letsencrypt/live/astro.mobb.ninja/privkey.pem Deploy Astronomer update the values-public.yaml and set baseDomain: astro.mobb.ninja Install ``` helm repo add astronomer https://helm.astronomer.io/ helm repo update helm install -f values-public.yaml --version=0.25.2 \\ --namespace=astronomer astronomer \\ astronomer/astronomer ``` Fix SCCs for elasticsearch In another terminal ``` oc adm policy add-scc-to-user privileged -z astronomer-elasticsearch oc patch deployment astronomer-elasticsearch-client -p '{\"spec\":{\"template\":{\"spec\":{ \"containers\": [{\"name\": \"es-client\",\"securityContext\":{\"privileged\": true,\"runAsUser\": 0}}]}}}}' ``` While that's running add our DNS In another shell run kubectl get svc -n astronomer astronomer-nginx Go back to your DNS zone in your DNS registry and create a record set * and copy the contents of EXTERNAL-IP from the above command. Validate the Install Check the Helm install has finished ``` NAME: astronomer LAST DEPLOYED: Mon May 24 18:03:05 2021 NAMESPACE: astronomer STATUS: deployed REVISION: 1 TEST SUITE: None NOTES: Thank you for installing Astronomer! Your release is named astronomer. The platform components may take a few minutes to spin up. You can access the platform at: Astronomer dashboard: https://app.astro.mobb.ninja Grafana dashboard: https://grafana.astro.mobb.ninja Kibana dashboard: https://kibana.astro.mobb.ninja Now that you've installed the platform, you are ready to get started and create your first airflow deployment. Download the CLI: curl -sSL https://install.astro.mobb.ninja | sudo bash We have guides available at https://www.astronomer.io/guides/ and are always available to help. ``` Check that you can access the service curl -sSO https://install.astro.mobb.ninja and you should see ``` ! /usr/bin/env bash TAG=${1:-v0.20.0} if (( EUID != 0 )); then echo \"Please run command as root.\" exit fi DOWNLOADER=\"https://raw.githubusercontent.com/astronomer/astro-cli/main/godownloader.sh\" ```","title":"Installing Astronomer on a public ARO cluster"},{"location":"aro/astronomer/README-public/#installing-astronomer-on-a-public-aro-cluster","text":"This assumes you've already got an ARO cluster installed. A default 3-node cluster is a bit small for Astronomer, If you have a three node cluster you can increase it by updating the replicas count machinesets in the openshift-machine-api namespace.","title":"Installing Astronomer on a public ARO cluster"},{"location":"aro/astronomer/README-public/#create-tls-secret","text":"set an environment variable containing the DNS you wish to use: ASTRO_DNS=astro.mobb.ninja We need a TLS Secret to use. You could create a self-signed certificate using a CA that you own, or use certbot (if you have a valid DNS provider, note records don't need to be public) certbot certonly --manual \\ --preferred-challenges=dns \\ --email username.taken@gmail.com \\ --server https://acme-v02.api.letsencrypt.org/directory \\ --agree-tos \\ --manual-public-ip-logging-ok \\ -d \"*.${ASTRO_DNS}\" Follow certbot's instructions (something like ): ``` Please deploy a DNS TXT record under the name _acme-challenge.astro.mobb.ninja with the following value: 8d2HNuZ8rn9McPTzpo2evJsAJI8K4eJuVLaZlz6d-kc Before continuing, verify the record is deployed. ``` Create a Secret from the Cert (use the paths provided from the above command): oc new-project astronomer oc create secret tls astronomer-tls --cert=/etc/letsencrypt/live/astro.mobb.ninja/fullchain.pem --key=/etc/letsencrypt/live/astro.mobb.ninja/privkey.pem","title":"Create TLS Secret"},{"location":"aro/astronomer/README-public/#deploy-astronomer","text":"update the values-public.yaml and set baseDomain: astro.mobb.ninja Install ``` helm repo add astronomer https://helm.astronomer.io/ helm repo update helm install -f values-public.yaml --version=0.25.2 \\ --namespace=astronomer astronomer \\ astronomer/astronomer ```","title":"Deploy Astronomer"},{"location":"aro/astronomer/README-public/#fix-sccs-for-elasticsearch","text":"In another terminal ``` oc adm policy add-scc-to-user privileged -z astronomer-elasticsearch oc patch deployment astronomer-elasticsearch-client -p '{\"spec\":{\"template\":{\"spec\":{ \"containers\": [{\"name\": \"es-client\",\"securityContext\":{\"privileged\": true,\"runAsUser\": 0}}]}}}}' ```","title":"Fix SCCs for elasticsearch"},{"location":"aro/astronomer/README-public/#while-thats-running-add-our-dns","text":"In another shell run kubectl get svc -n astronomer astronomer-nginx Go back to your DNS zone in your DNS registry and create a record set * and copy the contents of EXTERNAL-IP from the above command.","title":"While that's running add our DNS"},{"location":"aro/astronomer/README-public/#validate-the-install","text":"Check the Helm install has finished ``` NAME: astronomer LAST DEPLOYED: Mon May 24 18:03:05 2021 NAMESPACE: astronomer STATUS: deployed REVISION: 1 TEST SUITE: None NOTES: Thank you for installing Astronomer! Your release is named astronomer. The platform components may take a few minutes to spin up. You can access the platform at: Astronomer dashboard: https://app.astro.mobb.ninja Grafana dashboard: https://grafana.astro.mobb.ninja Kibana dashboard: https://kibana.astro.mobb.ninja Now that you've installed the platform, you are ready to get started and create your first airflow deployment. Download the CLI: curl -sSL https://install.astro.mobb.ninja | sudo bash We have guides available at https://www.astronomer.io/guides/ and are always available to help. ``` Check that you can access the service curl -sSO https://install.astro.mobb.ninja and you should see ```","title":"Validate the Install"},{"location":"aro/astronomer/README-public/#usrbinenv-bash","text":"TAG=${1:-v0.20.0} if (( EUID != 0 )); then echo \"Please run command as root.\" exit fi DOWNLOADER=\"https://raw.githubusercontent.com/astronomer/astro-cli/main/godownloader.sh\" ```","title":"! /usr/bin/env bash"},{"location":"aro/azure-policy/","text":"Apply Azure Policy to Azure Policy Azure Policy helps to enforce organizational standards and to assess compliance at-scale. Azure Policy supports arc enabled kubernetes cluster with both build-in and custom policies to ensure kubernetes resources are compliant. This article demonstrates how to make Azure Redhat Openshift cluster compliant with azure policy. Prerequisites Azure CLI Openshift CLI Azure Openshift Cluster (ARO Cluster) Deploy Azure Policy Deploy Azure Arc and Enable Azure Policy Add-on az connectedk8s connect -n [Cluster_Name] -g [Resource_Group_Name] az k8s-extension create --cluster-type connectedClusters --cluster-name [Cluster_Name] --resource-group [Resource_Group_Name] --extension-type Microsoft.PolicyInsights --name azurepolicy Verify Azure Arc and Azure Policy Add-on oc get pod -n azure-arc NAME READY STATUS RESTARTS AGE cluster-metadata-operator-6d4b957d65-5ts9b 2/2 Running 0 3h31m clusterconnect-agent-d5d6c6848-kbmfc 3/3 Running 0 3h31m clusteridentityoperator-6f5bf5c94-6qxlm 2/2 Running 0 3h31m config-agent-54b48fb5d9-wll42 2/2 Running 0 3h31m controller-manager-69fd59cf7-lf2mf 2/2 Running 0 3h31m extension-manager-695f99c94d-q6zmw 2/2 Running 0 3h31m flux-logs-agent-88588c88-j9xf8 1/1 Running 0 3h31m kube-aad-proxy-74d5747967-jhxq2 2/2 Running 0 3h31m metrics-agent-854dfbdc74-948bn 2/2 Running 0 3h31m resource-sync-agent-77f8bb95d4-94dpm 2/2 Running 0 3h31m oc get pod -n gatekeeper-system NAME READY STATUS RESTARTS AGE gatekeeper-audit-bbdd45779-lbsmf 1/1 Running 0 3h25m gatekeeper-controller-manager-59864c84f4-25pt2 1/1 Running 0 3h25m gatekeeper-controller-manager-59864c84f4-sztck 1/1 Running 0 3h25m Demo a simple policy This policy will allow only images from a specific registry. Open Azure Portal Policy Services Click on Assign Policy Select the subscription and ARO cluster resource group as the scope Select \"Kubernetes cluster containers should only use allowed images\" in the \"policy definition\" field Click Next -> fill out namespace inclusion as [\"test-policy\"] -> Allowed Registry Regex as \"index.docker.io.+$\" Save the result. The policy will take effect after around 30 minutes. oc get K8sAzureContainerAllowedImages #Get the latest policy oc get K8sAzureContainerAllowedImages azurepolicy-k8sazurecontainerallowedimages-5a0572fab674dfdfb636 -o yaml apiVersion: constraints.gatekeeper.sh/v1beta1 kind: K8sAzureContainerAllowedImages metadata: annotations: azure-policy-assignment-id: /subscriptions/${subscription_id}/resourceGroups/shaozhen-tf-rg/providers/Microsoft.Authorization/policyAssignments/9f9d73056d5f422bb3bbbc5f azure-policy-definition-id: /providers/Microsoft.Authorization/policyDefinitions/febd0533-8e55-448f-b837-bd0e06f16469 azure-policy-definition-reference-id: \"\" azure-policy-setdefinition-id: \"\" constraint-installed-by: azure-policy-addon creationTimestamp: \"2022-07-25T16:19:12Z\" generation: 2 labels: managed-by: azure-policy-addon name: azurepolicy-k8sazurecontainerallowedimages-5a0572fab674dfdfb636 resourceVersion: \"169521\" uid: 0e25efc6-0099-4e3c-86a9-a223dd01e13d spec: enforcementAction: deny match: excludedNamespaces: - kube-system - gatekeeper-system - azure-arc kinds: - apiGroups: - \"\" kinds: - Pod namespaces: - test-policy parameters: excludedContainers: [] imageRegex: index.docker.io.+$ Policy Engine denies images from quay.io oc run -ti --image quay.io/alpine test -- /bin/sh Error from server ([azurepolicy-k8sazurecontainerallowedimages-5a0572fab674dfdfb636] Container image quay.io/alpine for container test has not been allowed.): admission webhook \"validation.gatekeeper.sh\" denied the request: [azurepolicy-k8sazurecontainerallowedimages-5a0572fab674dfdfb636] Container image quay.io/alpine for container test has not been allowed. References Azure Policy Overview Azure Arc-enabled kubernetes Understand Azure Policy for Kubernetes Azure Arc-enabled kubernetes built-in policy","title":"Apply Azure Policy to Azure Policy"},{"location":"aro/azure-policy/#apply-azure-policy-to-azure-policy","text":"Azure Policy helps to enforce organizational standards and to assess compliance at-scale. Azure Policy supports arc enabled kubernetes cluster with both build-in and custom policies to ensure kubernetes resources are compliant. This article demonstrates how to make Azure Redhat Openshift cluster compliant with azure policy.","title":"Apply Azure Policy to Azure Policy"},{"location":"aro/azure-policy/#prerequisites","text":"Azure CLI Openshift CLI Azure Openshift Cluster (ARO Cluster)","title":"Prerequisites"},{"location":"aro/azure-policy/#deploy-azure-policy","text":"Deploy Azure Arc and Enable Azure Policy Add-on az connectedk8s connect -n [Cluster_Name] -g [Resource_Group_Name] az k8s-extension create --cluster-type connectedClusters --cluster-name [Cluster_Name] --resource-group [Resource_Group_Name] --extension-type Microsoft.PolicyInsights --name azurepolicy Verify Azure Arc and Azure Policy Add-on oc get pod -n azure-arc NAME READY STATUS RESTARTS AGE cluster-metadata-operator-6d4b957d65-5ts9b 2/2 Running 0 3h31m clusterconnect-agent-d5d6c6848-kbmfc 3/3 Running 0 3h31m clusteridentityoperator-6f5bf5c94-6qxlm 2/2 Running 0 3h31m config-agent-54b48fb5d9-wll42 2/2 Running 0 3h31m controller-manager-69fd59cf7-lf2mf 2/2 Running 0 3h31m extension-manager-695f99c94d-q6zmw 2/2 Running 0 3h31m flux-logs-agent-88588c88-j9xf8 1/1 Running 0 3h31m kube-aad-proxy-74d5747967-jhxq2 2/2 Running 0 3h31m metrics-agent-854dfbdc74-948bn 2/2 Running 0 3h31m resource-sync-agent-77f8bb95d4-94dpm 2/2 Running 0 3h31m oc get pod -n gatekeeper-system NAME READY STATUS RESTARTS AGE gatekeeper-audit-bbdd45779-lbsmf 1/1 Running 0 3h25m gatekeeper-controller-manager-59864c84f4-25pt2 1/1 Running 0 3h25m gatekeeper-controller-manager-59864c84f4-sztck 1/1 Running 0 3h25m","title":"Deploy Azure Policy"},{"location":"aro/azure-policy/#demo-a-simple-policy","text":"This policy will allow only images from a specific registry. Open Azure Portal Policy Services Click on Assign Policy Select the subscription and ARO cluster resource group as the scope Select \"Kubernetes cluster containers should only use allowed images\" in the \"policy definition\" field Click Next -> fill out namespace inclusion as [\"test-policy\"] -> Allowed Registry Regex as \"index.docker.io.+$\" Save the result. The policy will take effect after around 30 minutes. oc get K8sAzureContainerAllowedImages #Get the latest policy oc get K8sAzureContainerAllowedImages azurepolicy-k8sazurecontainerallowedimages-5a0572fab674dfdfb636 -o yaml apiVersion: constraints.gatekeeper.sh/v1beta1 kind: K8sAzureContainerAllowedImages metadata: annotations: azure-policy-assignment-id: /subscriptions/${subscription_id}/resourceGroups/shaozhen-tf-rg/providers/Microsoft.Authorization/policyAssignments/9f9d73056d5f422bb3bbbc5f azure-policy-definition-id: /providers/Microsoft.Authorization/policyDefinitions/febd0533-8e55-448f-b837-bd0e06f16469 azure-policy-definition-reference-id: \"\" azure-policy-setdefinition-id: \"\" constraint-installed-by: azure-policy-addon creationTimestamp: \"2022-07-25T16:19:12Z\" generation: 2 labels: managed-by: azure-policy-addon name: azurepolicy-k8sazurecontainerallowedimages-5a0572fab674dfdfb636 resourceVersion: \"169521\" uid: 0e25efc6-0099-4e3c-86a9-a223dd01e13d spec: enforcementAction: deny match: excludedNamespaces: - kube-system - gatekeeper-system - azure-arc kinds: - apiGroups: - \"\" kinds: - Pod namespaces: - test-policy parameters: excludedContainers: [] imageRegex: index.docker.io.+$ Policy Engine denies images from quay.io oc run -ti --image quay.io/alpine test -- /bin/sh Error from server ([azurepolicy-k8sazurecontainerallowedimages-5a0572fab674dfdfb636] Container image quay.io/alpine for container test has not been allowed.): admission webhook \"validation.gatekeeper.sh\" denied the request: [azurepolicy-k8sazurecontainerallowedimages-5a0572fab674dfdfb636] Container image quay.io/alpine for container test has not been allowed.","title":"Demo a simple policy"},{"location":"aro/azure-policy/#references","text":"Azure Policy Overview Azure Arc-enabled kubernetes Understand Azure Policy for Kubernetes Azure Arc-enabled kubernetes built-in policy","title":"References"},{"location":"aro/azure-service-operator-v1/","text":"Installing and Using the Azure Service Operator (ASO) V1 in Azure Red Hat OpenShift (ARO) Paul Czarkowski last edit - 02/16/2022 The Azure Service Operator (ASO) provides Custom Resource Definitions (CRDs) for Azure resources that can be used to create, update, and delete Azure services from an OpenShift cluster. This example uses ASO V1, which has now been replaced by ASO V2. ASO V2 does not (as of 5/19/2022) yet have an entry in the OCP OperatorHub, but is functional and should be preferred for use, especially if V1 isn't already installed on a cluster. MOBB has documented the [install of ASO V2 on ROSA]. MOBB has not tested running the two in parallel. Prerequisites Azure CLI An Azure Red Hat OpenShift (ARO) cluster Prepare your Azure Account and ARO Cluster Set the following environment variables: Note: modify the cluster name, region and resource group to match your cluster bash AZURE_TENANT_ID=$(az account show -o tsv --query tenantId) AZURE_SUBSCRIPTION_ID=$(az account show -o tsv --query id) CLUSTER_NAME=\"openshift\" AZURE_RESOURCE_GROUP=\"openshift\" AZURE_REGION=\"eastus\" Create a Service Principal with Contributor permissions to your subscription: Note: You may want to lock this down to a specific resource group. bash read -r ASO_USER ASO_PASS < <(az ad sp create-for-rbac -n \"$CLUSTER_NAME-ASO\" \\ --role contributor --scopes /subscriptions/$AZURE_SUBSCRIPTION_ID -o tsv \\ --query \"[name,password]\" | xargs) Create a secret containing your Service Principal credentials: bash cat <<EOF | oc apply -f - apiVersion: v1 kind: Secret metadata: name: azureoperatorsettings namespace: openshift-operators stringData: AZURE_TENANT_ID: $AZURE_TENANT_ID AZURE_SUBSCRIPTION_ID: $AZURE_SUBSCRIPTION_ID AZURE_CLIENT_ID: $ASO_USER AZURE_CLIENT_SECRET: $ASO_PASS AZURE_CLOUD_ENV: AzurePublicCloud EOF Deploy the ASO Operator: bash cat <<EOF | oc apply -f - apiVersion: operators.coreos.com/v1alpha1 kind: Subscription metadata: labels: operators.coreos.com/azure-service-operator.openshift-operators: \"\" name: azure-service-operator namespace: openshift-operators spec: channel: stable installPlanApproval: Automatic name: azure-service-operator source: community-operators sourceNamespace: openshift-marketplace startingCSV: azure-service-operator.v1.0.28631 EOF Deploy an Azure Redis Cache Create a Project: bash oc new-project redis-demo Allow the redis app to run as any user: bash oc adm policy add-scc-to-user anyuid -z default Create a random string to use as the unique redis hostname: bash REDIS_HOSTNAME=$(cat /dev/urandom | tr -dc 'a-z0-9' | fold -w 8 | head -n 1) Deploy a Redis service using the ASO Operator and an example application ``` cat <<EOF | oc apply -f - apiVersion: azure.microsoft.com/v1alpha1 kind: RedisCache metadata: name: $REDIS_HOSTNAME spec: location: $AZURE_REGION resourceGroup: $AZURE_RESOURCE_GROUP properties: sku: name: Basic family: C capacity: 1 enableNonSslPort: true apiVersion: apps/v1 kind: Deployment metadata: name: azure-vote-front spec: replicas: 1 selector: matchLabels: app: azure-vote-front template: metadata: labels: app: azure-vote-front spec: containers: - name: azure-vote-front image: mcr.microsoft.com/azuredocs/azure-vote-front:v1 resources: requests: cpu: 100m memory: 128Mi limits: cpu: 250m memory: 256Mi ports: - containerPort: 80 env: - name: REDIS_NAME value: $REDIS_HOSTNAME - name: REDIS value: $REDIS_HOSTNAME.redis.cache.windows.net - name: REDIS_PWD valueFrom: secretKeyRef: name: rediscache-$REDIS_HOSTNAME key: primaryKey apiVersion: v1 kind: Service metadata: name: azure-vote-front spec: ports: - port: 80 selector: app: azure-vote-front apiVersion: route.openshift.io/v1 kind: Route metadata: name: azure-vote spec: port: targetPort: 80 tls: insecureEdgeTerminationPolicy: Redirect termination: edge to: kind: Service name: azure-vote-front EOF ``` Wait for Redis to be ready This may take 10 to 15 minutes. bash watch oc get rediscache $REDIS_HOSTNAME the output should eventually show the following: NAME PROVISIONED MESSAGE l67for49 true successfully provisioned Get the URL of the example app bash oc get route azure-vote Browse to the URL provided by the previous command and validate that the app is working Cleanup Delete the project containing the demo app bash oc delete project redis-demo","title":"v1"},{"location":"aro/azure-service-operator-v1/#installing-and-using-the-azure-service-operator-aso-v1-in-azure-red-hat-openshift-aro","text":"Paul Czarkowski last edit - 02/16/2022 The Azure Service Operator (ASO) provides Custom Resource Definitions (CRDs) for Azure resources that can be used to create, update, and delete Azure services from an OpenShift cluster. This example uses ASO V1, which has now been replaced by ASO V2. ASO V2 does not (as of 5/19/2022) yet have an entry in the OCP OperatorHub, but is functional and should be preferred for use, especially if V1 isn't already installed on a cluster. MOBB has documented the [install of ASO V2 on ROSA]. MOBB has not tested running the two in parallel.","title":"Installing and Using the Azure Service Operator (ASO) V1 in Azure Red Hat OpenShift (ARO)"},{"location":"aro/azure-service-operator-v1/#prerequisites","text":"Azure CLI An Azure Red Hat OpenShift (ARO) cluster","title":"Prerequisites"},{"location":"aro/azure-service-operator-v1/#prepare-your-azure-account-and-aro-cluster","text":"Set the following environment variables: Note: modify the cluster name, region and resource group to match your cluster bash AZURE_TENANT_ID=$(az account show -o tsv --query tenantId) AZURE_SUBSCRIPTION_ID=$(az account show -o tsv --query id) CLUSTER_NAME=\"openshift\" AZURE_RESOURCE_GROUP=\"openshift\" AZURE_REGION=\"eastus\" Create a Service Principal with Contributor permissions to your subscription: Note: You may want to lock this down to a specific resource group. bash read -r ASO_USER ASO_PASS < <(az ad sp create-for-rbac -n \"$CLUSTER_NAME-ASO\" \\ --role contributor --scopes /subscriptions/$AZURE_SUBSCRIPTION_ID -o tsv \\ --query \"[name,password]\" | xargs) Create a secret containing your Service Principal credentials: bash cat <<EOF | oc apply -f - apiVersion: v1 kind: Secret metadata: name: azureoperatorsettings namespace: openshift-operators stringData: AZURE_TENANT_ID: $AZURE_TENANT_ID AZURE_SUBSCRIPTION_ID: $AZURE_SUBSCRIPTION_ID AZURE_CLIENT_ID: $ASO_USER AZURE_CLIENT_SECRET: $ASO_PASS AZURE_CLOUD_ENV: AzurePublicCloud EOF Deploy the ASO Operator: bash cat <<EOF | oc apply -f - apiVersion: operators.coreos.com/v1alpha1 kind: Subscription metadata: labels: operators.coreos.com/azure-service-operator.openshift-operators: \"\" name: azure-service-operator namespace: openshift-operators spec: channel: stable installPlanApproval: Automatic name: azure-service-operator source: community-operators sourceNamespace: openshift-marketplace startingCSV: azure-service-operator.v1.0.28631 EOF","title":"Prepare your Azure Account and ARO Cluster"},{"location":"aro/azure-service-operator-v1/#deploy-an-azure-redis-cache","text":"Create a Project: bash oc new-project redis-demo Allow the redis app to run as any user: bash oc adm policy add-scc-to-user anyuid -z default Create a random string to use as the unique redis hostname: bash REDIS_HOSTNAME=$(cat /dev/urandom | tr -dc 'a-z0-9' | fold -w 8 | head -n 1) Deploy a Redis service using the ASO Operator and an example application ``` cat <<EOF | oc apply -f - apiVersion: azure.microsoft.com/v1alpha1 kind: RedisCache metadata: name: $REDIS_HOSTNAME spec: location: $AZURE_REGION resourceGroup: $AZURE_RESOURCE_GROUP properties: sku: name: Basic family: C capacity: 1 enableNonSslPort: true apiVersion: apps/v1 kind: Deployment metadata: name: azure-vote-front spec: replicas: 1 selector: matchLabels: app: azure-vote-front template: metadata: labels: app: azure-vote-front spec: containers: - name: azure-vote-front image: mcr.microsoft.com/azuredocs/azure-vote-front:v1 resources: requests: cpu: 100m memory: 128Mi limits: cpu: 250m memory: 256Mi ports: - containerPort: 80 env: - name: REDIS_NAME value: $REDIS_HOSTNAME - name: REDIS value: $REDIS_HOSTNAME.redis.cache.windows.net - name: REDIS_PWD valueFrom: secretKeyRef: name: rediscache-$REDIS_HOSTNAME key: primaryKey apiVersion: v1 kind: Service metadata: name: azure-vote-front spec: ports: - port: 80 selector: app: azure-vote-front apiVersion: route.openshift.io/v1 kind: Route metadata: name: azure-vote spec: port: targetPort: 80 tls: insecureEdgeTerminationPolicy: Redirect termination: edge to: kind: Service name: azure-vote-front EOF ``` Wait for Redis to be ready This may take 10 to 15 minutes. bash watch oc get rediscache $REDIS_HOSTNAME the output should eventually show the following: NAME PROVISIONED MESSAGE l67for49 true successfully provisioned Get the URL of the example app bash oc get route azure-vote Browse to the URL provided by the previous command and validate that the app is working","title":"Deploy an Azure Redis Cache"},{"location":"aro/azure-service-operator-v1/#cleanup","text":"Delete the project containing the demo app bash oc delete project redis-demo","title":"Cleanup"},{"location":"aro/azure-service-operator-v2/","text":"Installing and Using the Azure Service Operator (ASO) V2 in Azure Red Hat OpenShift (ARO) Thatcher Hubbard The Azure Service Operator (ASO) provides Custom Resource Definitions (CRDs) for Azure resources that can be used to create, update, and delete Azure services from an OpenShift cluster. This example uses ASO V2, which is a replacement for ASO V1. Equivalent documentation for ASO V1 can be found here . For new installs, V2 is recommended. MOBB has not tested running them in parallel. Prerequisites Azure CLI An Azure Red Hat OpenShift (ARO) cluster The helm CLI tool Prepare your Azure Account and ARO Cluster Install cert-manager : ASO relies on having the CRDs provided by cert-manager so it can request self-signed certificates. By default, cert-manager creates an Issuer of type SelfSigned , so it will work for ASO out-of-the-box. On an OpenShift cluster, the easiest way to do this is by using the OCP console, navigating to 'Operators | OperatorHub' and installing it from there; both the Red Hat certified and community versions will work. It's also possible to install by applying manifests directly as covered here . Set the following environment variables: Note: modify the cluster name, region and resource group to match your cluster bash AZURE_TENANT_ID=$(az account show -o tsv --query tenantId) AZURE_SUBSCRIPTION_ID=$(az account show -o tsv --query id) CLUSTER_NAME=\"test-cluster\" AZURE_RESOURCE_GROUP=\"test-rg\" AZURE_REGION=\"westus2\" Create a Service Principal with Contributor permissions to your subscription: Note: You may want to lock this down to a specific resource group. bash az ad sp create-for-rbac -n \"$CLUSTER_NAME-aso\" \\ --role contributor --scopes /subscriptions/$AZURE_SUBSCRIPTION_ID The result should look something like this: json { \"appId\": \"12f48391-31ac-4565-936a-8249232aeb18\", \"displayName\": \"test-cluster-aso\", \"password\": \"xsr5Pz3IsPnnYxhsc7LhnNkY00cYxe.IPk\", \"tenant\": \"xxxxxxxx-xxxx-xxxx-xxxx-xxxxxxxxxxxx\" } You'll need two of these values for the Helm deploy of ASO: bash AZURE_CLIENT_ID=<the_appId_from_above> AZURE_CLIENT_SECRET=<the_password_from_above> Deploy the ASO Operator using Helm: First, add the ASO repo (this may already be present, Helm will thow a status message if so): bash helm repo add aso2 https://raw.githubusercontent.com/Azure/azure-service-operator/main/v2/charts Then install the operator itself: bash helm upgrade --install --devel aso2 aso2/azure-service-operator \\ --create-namespace \\ --namespace=azureserviceoperator-system \\ --set azureSubscriptionID=$AZURE_SUBSCRIPTION_ID \\ --set azureTenantID=$AZURE_TENANT_ID \\ --set azureClientID=$AZURE_CLIENT_ID \\ --set azureClientSecret=$AZURE_CLIENT_SECRET It will typically take 2-3 minutes for resources to converge and for the controller to be read to provision Azure resources. There will be one Pod created in the azureserviceoperator-system namespace with two containers, an oc -n azureserviceoperator-system logs <pod_name> manager will likely show a string of 'TLS handshake error' messages as the operator waits for a Certificate to be issued, but when they stop, the operator will be ready. Deploy an Azure Redis Cache Create a Project: bash oc new-project redis-demo Allow the redis app to run as any user: bash oc adm policy add-scc-to-user anyuid -z redis-demo Create an Azure Resource Group to hold project resources. Make sure the namespace matches the project name, and that the location is in the same region the cluster is: cat <<EOF | oc apply -f - apiVersion: resources.azure.com/v1beta20200601 kind: ResourceGroup metadata: name: redis-demo namespace: redis-demo spec: location: westus EOF Deploy a Redis service using the ASO Operator. This also shows creating a random string as part of the hostname because the Azure DNS namespace is global, and a name like sampleredis is likely to be taken. Also make sure the location spec matches. REDIS_HOSTNAME=redis-$(head -c24 < /dev/random | base64 | LC_CTYPE=C tr -dc 'a-z0-9' | cut -c -8) cat <<EOF | oc apply -f - apiVersion: cache.azure.com/v1beta20201201 kind: Redis metadata: name: $REDIS_HOSTNAME namespace: redis-demo spec: location: westus owner: name: redis-demo sku: family: C name: Basic capacity: 0 enableNonSslPort: true redisConfiguration: maxmemory-delta: \"10\" maxmemory-policy: allkeys-lru redisVersion: \"6\" operatorSpec: secrets: primaryKey: name: redis-secret key: primaryKey secondaryKey: name: redis-secret key: secondaryKey hostName: name: redis-secret key: hostName port: name: redis-secret key: port EOF This will take a couple of minutes to complete as well. Also note that there is typically a bit of lag between a resource being created and showing up in the Azure Portal. Deploy the sample application This uses a published sample application from Microsoft: cat <<EOF | oc -n redis-demo apply -f - apiVersion: apps/v1 kind: Deployment metadata: name: azure-vote-front spec: replicas: 1 selector: matchLabels: app: azure-vote-front template: metadata: labels: app: azure-vote-front spec: containers: - name: azure-vote-front image: mcr.microsoft.com/azuredocs/azure-vote-front:v1 resources: requests: cpu: 100m memory: 128Mi limits: cpu: 250m memory: 256Mi ports: - containerPort: 80 env: - name: REDIS valueFrom: secretKeyRef: name: redis-secret key: hostName - name: REDIS_NAME value: $REDIS_HOSTNAME - name: REDIS_PWD valueFrom: secretKeyRef: name: redis-secret key: primaryKey --- apiVersion: v1 kind: Service metadata: name: azure-vote-front spec: ports: - port: 80 selector: app: azure-vote-front --- apiVersion: route.openshift.io/v1 kind: Route metadata: name: azure-vote spec: port: targetPort: 80 tls: insecureEdgeTerminationPolicy: Redirect termination: edge to: kind: Service name: azure-vote-front EOF Get the URL of the example app bash oc get route azure-vote Browse to the URL provided by the previous command and validate that the app is working Cleanup Delete the project containing the demo app bash oc delete project redis-demo Further Resources There is a library of examples for creating various Azure resource types here: https://github.com/Azure/azure-service-operator/tree/main/v2/config/samples","title":"v2"},{"location":"aro/azure-service-operator-v2/#installing-and-using-the-azure-service-operator-aso-v2-in-azure-red-hat-openshift-aro","text":"Thatcher Hubbard The Azure Service Operator (ASO) provides Custom Resource Definitions (CRDs) for Azure resources that can be used to create, update, and delete Azure services from an OpenShift cluster. This example uses ASO V2, which is a replacement for ASO V1. Equivalent documentation for ASO V1 can be found here . For new installs, V2 is recommended. MOBB has not tested running them in parallel.","title":"Installing and Using the Azure Service Operator (ASO) V2 in Azure Red Hat OpenShift (ARO)"},{"location":"aro/azure-service-operator-v2/#prerequisites","text":"Azure CLI An Azure Red Hat OpenShift (ARO) cluster The helm CLI tool","title":"Prerequisites"},{"location":"aro/azure-service-operator-v2/#prepare-your-azure-account-and-aro-cluster","text":"Install cert-manager : ASO relies on having the CRDs provided by cert-manager so it can request self-signed certificates. By default, cert-manager creates an Issuer of type SelfSigned , so it will work for ASO out-of-the-box. On an OpenShift cluster, the easiest way to do this is by using the OCP console, navigating to 'Operators | OperatorHub' and installing it from there; both the Red Hat certified and community versions will work. It's also possible to install by applying manifests directly as covered here . Set the following environment variables: Note: modify the cluster name, region and resource group to match your cluster bash AZURE_TENANT_ID=$(az account show -o tsv --query tenantId) AZURE_SUBSCRIPTION_ID=$(az account show -o tsv --query id) CLUSTER_NAME=\"test-cluster\" AZURE_RESOURCE_GROUP=\"test-rg\" AZURE_REGION=\"westus2\" Create a Service Principal with Contributor permissions to your subscription: Note: You may want to lock this down to a specific resource group. bash az ad sp create-for-rbac -n \"$CLUSTER_NAME-aso\" \\ --role contributor --scopes /subscriptions/$AZURE_SUBSCRIPTION_ID The result should look something like this: json { \"appId\": \"12f48391-31ac-4565-936a-8249232aeb18\", \"displayName\": \"test-cluster-aso\", \"password\": \"xsr5Pz3IsPnnYxhsc7LhnNkY00cYxe.IPk\", \"tenant\": \"xxxxxxxx-xxxx-xxxx-xxxx-xxxxxxxxxxxx\" } You'll need two of these values for the Helm deploy of ASO: bash AZURE_CLIENT_ID=<the_appId_from_above> AZURE_CLIENT_SECRET=<the_password_from_above> Deploy the ASO Operator using Helm: First, add the ASO repo (this may already be present, Helm will thow a status message if so): bash helm repo add aso2 https://raw.githubusercontent.com/Azure/azure-service-operator/main/v2/charts Then install the operator itself: bash helm upgrade --install --devel aso2 aso2/azure-service-operator \\ --create-namespace \\ --namespace=azureserviceoperator-system \\ --set azureSubscriptionID=$AZURE_SUBSCRIPTION_ID \\ --set azureTenantID=$AZURE_TENANT_ID \\ --set azureClientID=$AZURE_CLIENT_ID \\ --set azureClientSecret=$AZURE_CLIENT_SECRET It will typically take 2-3 minutes for resources to converge and for the controller to be read to provision Azure resources. There will be one Pod created in the azureserviceoperator-system namespace with two containers, an oc -n azureserviceoperator-system logs <pod_name> manager will likely show a string of 'TLS handshake error' messages as the operator waits for a Certificate to be issued, but when they stop, the operator will be ready.","title":"Prepare your Azure Account and ARO Cluster"},{"location":"aro/azure-service-operator-v2/#deploy-an-azure-redis-cache","text":"Create a Project: bash oc new-project redis-demo Allow the redis app to run as any user: bash oc adm policy add-scc-to-user anyuid -z redis-demo Create an Azure Resource Group to hold project resources. Make sure the namespace matches the project name, and that the location is in the same region the cluster is: cat <<EOF | oc apply -f - apiVersion: resources.azure.com/v1beta20200601 kind: ResourceGroup metadata: name: redis-demo namespace: redis-demo spec: location: westus EOF Deploy a Redis service using the ASO Operator. This also shows creating a random string as part of the hostname because the Azure DNS namespace is global, and a name like sampleredis is likely to be taken. Also make sure the location spec matches. REDIS_HOSTNAME=redis-$(head -c24 < /dev/random | base64 | LC_CTYPE=C tr -dc 'a-z0-9' | cut -c -8) cat <<EOF | oc apply -f - apiVersion: cache.azure.com/v1beta20201201 kind: Redis metadata: name: $REDIS_HOSTNAME namespace: redis-demo spec: location: westus owner: name: redis-demo sku: family: C name: Basic capacity: 0 enableNonSslPort: true redisConfiguration: maxmemory-delta: \"10\" maxmemory-policy: allkeys-lru redisVersion: \"6\" operatorSpec: secrets: primaryKey: name: redis-secret key: primaryKey secondaryKey: name: redis-secret key: secondaryKey hostName: name: redis-secret key: hostName port: name: redis-secret key: port EOF This will take a couple of minutes to complete as well. Also note that there is typically a bit of lag between a resource being created and showing up in the Azure Portal. Deploy the sample application This uses a published sample application from Microsoft: cat <<EOF | oc -n redis-demo apply -f - apiVersion: apps/v1 kind: Deployment metadata: name: azure-vote-front spec: replicas: 1 selector: matchLabels: app: azure-vote-front template: metadata: labels: app: azure-vote-front spec: containers: - name: azure-vote-front image: mcr.microsoft.com/azuredocs/azure-vote-front:v1 resources: requests: cpu: 100m memory: 128Mi limits: cpu: 250m memory: 256Mi ports: - containerPort: 80 env: - name: REDIS valueFrom: secretKeyRef: name: redis-secret key: hostName - name: REDIS_NAME value: $REDIS_HOSTNAME - name: REDIS_PWD valueFrom: secretKeyRef: name: redis-secret key: primaryKey --- apiVersion: v1 kind: Service metadata: name: azure-vote-front spec: ports: - port: 80 selector: app: azure-vote-front --- apiVersion: route.openshift.io/v1 kind: Route metadata: name: azure-vote spec: port: targetPort: 80 tls: insecureEdgeTerminationPolicy: Redirect termination: edge to: kind: Service name: azure-vote-front EOF Get the URL of the example app bash oc get route azure-vote Browse to the URL provided by the previous command and validate that the app is working","title":"Deploy an Azure Redis Cache"},{"location":"aro/azure-service-operator-v2/#cleanup","text":"Delete the project containing the demo app bash oc delete project redis-demo","title":"Cleanup"},{"location":"aro/azure-service-operator-v2/#further-resources","text":"There is a library of examples for creating various Azure resource types here: https://github.com/Azure/azure-service-operator/tree/main/v2/config/samples","title":"Further Resources"},{"location":"aro/cert-manager/","text":"ARO Custom domain with cert-manager and LetsEncrypt ARO guide to deploying an ARO cluster with custom domain and automating certificate management with cert-manager and letsencrypt certificates to manage the *.apps and api endpoints. Author: Byron Miller Table of Contents Do not remove this line (it will not be displayed) {:toc} Prerequisites az cli oc cli jq gettext OpenShift 4.10 domain name to use I'm going to be running this setup through Fedora in WSL2. Be sure to always use the same terminal/session for all commands since we'll reference environment variables set or created through the steps. Fedora Linux See Azure Docs for alternative install options. Import the Microsoft Keys bash sudo rpm --import https://packages.microsoft.com/keys/microsoft.asc Add the Microsoft Yum Repository bash cat << EOF | sudo tee /etc/yum.repos.d/azure-cli.repo [azure-cli] name=Azure CLI baseurl=https://packages.microsoft.com/yumrepos/azure-cli enabled=1 gpgcheck=1 gpgkey=https://packages.microsoft.com/keys/microsoft.asc EOF Install Azure CLI bash sudo dnf install -y azure-cli Install jq & gettext I'm going to rely on using shell variables interpolated into Kubernetes config and jq to build variables. Installing or ensuring the gettext & jq package is installed will allow us to use envsubst to simplify some of our configuration so we can use output of CLI's as input into Yamls to reduce the complexity of manual editing. bash sudo dnf install gettext jq Prepare Azure Account for Azure OpenShift Log into the Azure CLI by running the following and then authorizing through your Web Browser bash az login Register resource providers bash az provider register -n Microsoft.RedHatOpenShift --wait az provider register -n Microsoft.Compute --wait az provider register -n Microsoft.Storage --wait az provider register -n Microsoft.Authorization --wait Get Red Hat pull secret Log into cloud.redhat.com Browse to https://cloud.redhat.com/openshift/install/azure/aro-provisioned click the Download pull secret button and remember where you saved it, you'll reference it later. Deploy Azure OpenShift Variables and Resource Group Set some environment variables to use later, and create an Azure Resource Group. Set the following environment variables Change the values to suit your environment bash PULL_SECRET=./pull-secret.txt # the path to pull-secret LOCATION=southcentralus # the location of your cluster RESOURCEGROUP=aro-rg # the name of the resource group where you want to create your cluster CLUSTER=aro-cluster # the name of your cluster DOMAIN=lab.domain.com # Domain or subdomain zone for cluster & cluster api Create an Azure resource group bash az group create \\ --name $RESOURCEGROUP \\ --location $LOCATION Networking Create a virtual network with two empty subnets Create virtual network bash az network vnet create \\ --resource-group $RESOURCEGROUP \\ --name aro-vnet \\ --address-prefixes 10.0.0.0/22 Create control plane subnet bash az network vnet subnet create \\ --resource-group $RESOURCEGROUP \\ --vnet-name aro-vnet \\ --name master-subnet \\ --address-prefixes 10.0.0.0/23 \\ --service-endpoints Microsoft.ContainerRegistry Create machine subnet bash az network vnet subnet create \\ --resource-group $RESOURCEGROUP \\ --vnet-name aro-vnet \\ --name worker-subnet \\ --address-prefixes 10.0.2.0/23 \\ --service-endpoints Microsoft.ContainerRegistry Disable network policies on the control plane subnet This is required for the service to be able to connect to and manage the cluster. bash az network vnet subnet update \\ --name master-subnet \\ --resource-group $RESOURCEGROUP \\ --vnet-name aro-vnet \\ --disable-private-link-service-network-policies true Create the cluster This will take between 30 and 45 minutes. bash az aro create \\ --resource-group $RESOURCEGROUP \\ --name $CLUSTER \\ --vnet aro-vnet \\ --master-subnet master-subnet \\ --worker-subnet worker-subnet \\ --pull-secret @$PULL_SECRET \\ --domain $DOMAIN Wait until the ARO cluster is fully provisioned. Get OpenShift console URL You can use these to login to the web console (will get a cert warning that you can bypass with typing \"thisisunsafe\" in a chrome browser or login with oc) bash az aro show -g $RESOURCEGROUP -n $CLUSTER --query \"consoleProfile.url\" -o tsv Get OpenShift credentials bash az aro list-credentials --name $CLUSTER --resource-group $RESOURCEGROUP Create DNS Zones & Service Principal In order for cert-manager to work with AzureDNS, we need to create the zone and add a CAA record as well as create a Service Principal that we can use to manage records in this zone so CertManager can use DNS01 authentication for validating requests. This zone should be a public zone since letsencrypt will need to be able to read records created here. If you use a subdomain, please be sure to create the NS records in your primary domain to the subdomain. For ease of management, we're using the same resource group for domain as we have the cluster in. Create Zone bash az network dns zone create -g $RESOURCEGROUP -n $DOMAIN >You will need to configure your nameservers to point to azure. The output of running this zone create will show you the nameservers for this record that you will need to set up within your domain registrar. Create API DNS record bash APIREC=$(az aro show -g $RESOURCEGROUP -n $CLUSTER --query apiserverProfile.ip -o tsv) az network dns record-set a add-record -g $RESOURCEGROUP -z $DOMAIN \\ -n api -a $APIREC Create Wildcard DNS record bash WILDCARDREC=$(az aro show -n $CLUSTER -g $RESOURCEGROUP --query '{ingress:ingressProfiles[0].ip}' -o tsv) az network dns record-set a add-record -g $RESOURCEGROUP -z $DOMAIN \\ -n \"*.apps\" -a $WILDCARDREC Add CAA Record CAA is a type of DNS record that allows owners to specify which Certificate Authorities are allowed to issue certificates containing their domain names. bash az network dns record-set caa add-record -g $RESOURCEGROUP -z $DOMAIN \\ -n @ --flags 0 --tag \"issuewild\" --value \"letsencrypt.org\" You should be able to view the records in your console Note - You may have to create NS records in your root zone for a subdomain if you use a subdomain zone to point to the subdomains name servers. Set environment variables to build new service principal and credentials to allow cert-manager to create records in this zone. AZURE_CERT_MANAGER_NEW_SP_NAME = the name of the service principal to create that will manage the DNS zone automation for cert-manager. bash AZURE_CERT_MANAGER_NEW_SP_NAME=aro-dns-sp LETSENCRYPTEMAIL=youremail@work.com DNS_SP=$(az ad sp create-for-rbac --name $AZURE_CERT_MANAGER_NEW_SP_NAME --output json) AZURE_CERT_MANAGER_SP_APP_ID=$(echo $DNS_SP | jq -r '.appId') AZURE_CERT_MANAGER_SP_PASSWORD=$(echo $DNS_SP | jq -r '.password') AZURE_TENANT_ID=$(echo $DNS_SP | jq -r '.tenant') AZURE_SUBSCRIPTION_ID=$(az account show --output json | jq -r '.id') Restrict service principal - remove contributor role. Note: This may not exist, safe to proceed. We're going to grant the DNS Management Role to it next. bash az role assignment delete --assignee $AZURE_CERT_MANAGER_SP_APP_ID --role Contributor Grant DNS Zone Contributor to our Service Principal We'll grant DNS Zone Contributor to our DNS Service principal. bash az role assignment create --assignee $AZURE_CERT_MANAGER_SP_APP_ID --role befefa01-2a29-4197-83a8-272ff33ce314 Assign service principal to DNS zone bash DNS_ID=$(az network dns zone show --name $DOMAIN --resource-group $RESOURCEGROUP --query \"id\" --output tsv) az role assignment create --assignee $AZURE_CERT_MANAGER_SP_APP_ID --role \"DNS Zone Contributor\" --scope $DNS_ID Login to Cluster Login to your cluster through oc cli You may need to flush your local dns resolver/cache before you can see the DNS/Hostnames. On Windows you can open up a command prompt as administrator and type \"ipconfig /flushdns\" bash apiServer=$(az aro show -g $RESOURCEGROUP -n $CLUSTER --query apiserverProfile.url -o tsv) loginCred=$(az aro list-credentials --name $CLUSTER --resource-group $RESOURCEGROUP --query \"kubeadminPassword\" -o tsv) oc login $apiServer -u kubeadmin -p $loginCred You may get a warning that the certificate isn't trusted. We can ignore that now since we're in the process of adding a trusted certificate. Set up Cert-Manager We'll install cert-manager from operatorhub. If you experience any issues installing here, it probably means that you didn't provide a pull-secret when you installed your ARO cluster. Create namespace bash cat <<EOF | oc apply -f - apiVersion: v1 kind: Namespace metadata: annotations: openshift.io/display-name: Red Hat Certificate Manager Operator labels: openshift.io/cluster-monitoring: 'true' name: openshift-cert-manager-operator EOF Switch openshift-cert-manager-operator project (namespace) bash oc project openshift-cert-manager-operator Create OperatorGroup bash cat <<EOF | oc apply -f - apiVersion: operators.coreos.com/v1 kind: OperatorGroup metadata: name: openshift-cert-manager-operator spec: {} EOF Create subscription for cert-manager operator yaml cat <<EOF | oc apply -f - apiVersion: operators.coreos.com/v1alpha1 kind: Subscription metadata: name: openshift-cert-manager-operator namespace: openshift-cert-manager-operator spec: channel: tech-preview installPlanApproval: Automatic name: openshift-cert-manager-operator source: redhat-operators sourceNamespace: openshift-marketplace EOF It will take a few minutes for this operator to install and complete it's set up. May be a good time to take a coffee break :) Wait for cert-manager operator to finish installing. Our next steps can't complete until the operator has finished installing. I recommend that you login to your cluster with the URL and credentials you captured after you ran the az aro create and view the installed operators to see that everything is complete and successful. Configure cert-manager LetsEncrypt We're going to set up cert-manager to use DNS verification for letsencrypt certificates. We'll need to generate a service principal that can update the DNS zone and create short term records needed to validate certificate requests and associate this service principal with the cluster issuer. Configure Certificate Requestor Switch openshift-cert-manager project (namespace) bash oc project openshift-cert-manager Create azuredns-config secret for storing service principal credentials to manage zone. bash oc create secret generic azuredns-config --from-literal=client-secret=$AZURE_CERT_MANAGER_SP_PASSWORD -n openshift-cert-manager Create Cluster Issuer yaml envsubst <<EOF | oc apply -f - apiVersion: cert-manager.io/v1 kind: ClusterIssuer metadata: name: letsencrypt-prod spec: acme: server: https://acme-v02.api.letsencrypt.org/directory email: $LETSENCRYPTEMAIL # This key doesn't exist, cert-manager creates it privateKeySecretRef: name: prod-letsencrypt-issuer-account-key solvers: - dns01: azureDNS: clientID: $AZURE_CERT_MANAGER_SP_APP_ID clientSecretSecretRef: name: azuredns-config key: client-secret subscriptionID: $AZURE_SUBSCRIPTION_ID tenantID: $AZURE_TENANT_ID resourceGroupName: $RESOURCEGROUP hostedZoneName: $DOMAIN environment: AzurePublicCloud EOF Describe issuer bash oc describe clusterissuer letsencrypt-prod You should see some output that the issuer is Registered/Ready Conditions: Last Transition Time: 2022-06-17T17:29:37Z Message: The ACME account was registered with the ACME server Observed Generation: 1 Reason: ACMEAccountRegistered Status: True Type: Ready Events: <none> Once the above command is complete, you should be able to login to openshift, click view operators and make sure you're in the \"openshift-cert-manager-operator\" project and you should see a screen like this. Again, if you have an ssl error and use a chrome browser - type \"thisisunsafe\" to get in if you get an error its an invalid cert. Create & Install API Certificate Switch openshift-config project bash oc project openshift-config Configure API certificate yaml envsubst <<EOF | oc apply -f - apiVersion: cert-manager.io/v1 kind: Certificate metadata: name: openshift-api namespace: openshift-config spec: secretName: openshift-api-certificate issuerRef: name: letsencrypt-prod kind: ClusterIssuer dnsNames: - api.$DOMAIN EOF View certificate status bash oc describe certificate openshift-api -n openshift-config Create cluster api cert job This job will install the certificate ```yaml envsubst <<EOF | oc apply -f - apiVersion: rbac.authorization.k8s.io/v1 kind: ClusterRole metadata: name: patch-cluster-api-cert rules: - apiGroups: - \"\" resources: - secrets verbs: - get - list - apiGroups: - config.openshift.io resources: - apiservers verbs: - get - list - patch - update apiVersion: rbac.authorization.k8s.io/v1 kind: ClusterRoleBinding metadata: name: patch-cluster-api-cert roleRef: apiGroup: rbac.authorization.k8s.io kind: ClusterRole name: patch-cluster-api-cert subjects: - kind: ServiceAccount name: patch-cluster-api-cert namespace: openshift-config apiVersion: v1 kind: ServiceAccount metadata: name: patch-cluster-api-cert apiVersion: batch/v1 kind: Job metadata: name: patch-cluster-api-cert annotations: argocd.argoproj.io/hook: PostSync argocd.argoproj.io/hook-delete-policy: HookSucceeded spec: template: spec: containers: - image: image-registry.openshift-image-registry.svc:5000/openshift/cli:latest env: - name: API_HOST_NAME value: api.$DOMAIN command: - /bin/bash - -c - | #!/usr/bin/env bash if oc get secret openshift-api-certificate -n openshift-config; then oc patch apiserver cluster --type=merge -p '{\"spec\":{\"servingCerts\": {\"namedCertificates\": [{\"names\": [\"'$API_HOST_NAME'\"], \"servingCertificate\": {\"name\": \"openshift-api-certificate\"}}]}}}' else echo \"Could not execute sync as secret 'openshift-api-certificate' in namespace 'openshift-config' does not exist, check status of CertificationRequest\" exit 1 fi name: patch-cluster-api-cert dnsPolicy: ClusterFirst restartPolicy: Never terminationGracePeriodSeconds: 30 serviceAccount: patch-cluster-api-cert serviceAccountName: patch-cluster-api-cert EOF ``` Create & Install APPS Wildcard Certificate Switch openshift-ingress project (namespace) bash oc project openshift-ingress Configure Wildcard Certificate yaml envsubst <<EOF | oc apply -f - apiVersion: cert-manager.io/v1 kind: Certificate metadata: name: openshift-wildcard namespace: openshift-ingress spec: secretName: openshift-wildcard-certificate issuerRef: name: letsencrypt-prod kind: ClusterIssuer commonName: \"*.apps.$DOMAIN\" dnsNames: - \"*.apps.$DOMAIN\" EOF This will generate our API and wildcard certificate requests. We'll now create two jobs that will install these certificates. View certificate status bash oc describe certificate openshift-wildcard -n openshift-ingress Install Wildcard Certificate Job ```yaml cat <<EOF | oc apply -f - apiVersion: rbac.authorization.k8s.io/v1 kind: ClusterRole metadata: name: patch-cluster-wildcard-cert rules: - apiGroups: - operator.openshift.io resources: - ingresscontrollers verbs: - get - list - patch - apiGroups: - \"\" resources: - secrets verbs: - get - list apiVersion: rbac.authorization.k8s.io/v1 kind: ClusterRoleBinding metadata: name: patch-cluster-wildcard-cert roleRef: apiGroup: rbac.authorization.k8s.io kind: ClusterRole name: patch-cluster-wildcard-cert subjects: - kind: ServiceAccount name: patch-cluster-wildcard-cert namespace: openshift-ingress apiVersion: v1 kind: ServiceAccount metadata: name: patch-cluster-wildcard-cert apiVersion: batch/v1 kind: Job metadata: name: patch-cluster-wildcard-cert annotations: argocd.argoproj.io/hook: PostSync argocd.argoproj.io/hook-delete-policy: HookSucceeded spec: template: spec: containers: - image: image-registry.openshift-image-registry.svc:5000/openshift/cli:latest command: - /bin/bash - -c - | #!/usr/bin/env bash if oc get secret openshift-wildcard-certificate -n openshift-ingress; then oc patch ingresscontroller default -n openshift-ingress-operator --type=merge --patch='{\"spec\": { \"defaultCertificate\": { \"name\": \"openshift-wildcard-certificate\" }}}' else echo \"Could not execute sync as secret 'openshift-wildcard-certificate' in namespace 'openshift-ingress' does not exist, check status of CertificationRequest\" exit 1 fi name: patch-cluster-wildcard-cert dnsPolicy: ClusterFirst restartPolicy: Never terminationGracePeriodSeconds: 30 serviceAccount: patch-cluster-wildcard-cert serviceAccountName: patch-cluster-wildcard-cert EOF ``` Debugging One of the most helpful commands i've seen for debugging is in regards to challenges failing. The order says pending in perpetuity and you can run this to see why. oc describe challenges This is a very helpful guide in debugging certificates as well. Validate Certificates It will take a few minutes for the jobs to successfully complete. Once the certificate requests are complete, you should no longer see a browser security warning and you should have a valid SSL lock in your browser and no more warnings about SSL on cli. You may want to open an InPrivate/Private browser tab to visit the console/api so you can see the new SSL cert without having to expire your cache. Delete Cluster Once you're done its a good idea to delete the cluster to ensure that you don't get a surprise bill. Delete the cluster bash az aro delete -y \\ --resource-group $RESOURCEGROUP \\ --name $CLUSTER Delete the Azure resource group Only do this if there's nothing else in the resource group. bash az group delete -y \\ --name $RESOURCEGROUP","title":"ARO Custom domain with cert-manager and LetsEncrypt"},{"location":"aro/cert-manager/#aro-custom-domain-with-cert-manager-and-letsencrypt","text":"ARO guide to deploying an ARO cluster with custom domain and automating certificate management with cert-manager and letsencrypt certificates to manage the *.apps and api endpoints. Author: Byron Miller","title":"ARO Custom domain with cert-manager and LetsEncrypt"},{"location":"aro/cert-manager/#table-of-contents","text":"Do not remove this line (it will not be displayed) {:toc}","title":"Table of Contents"},{"location":"aro/cert-manager/#prerequisites","text":"az cli oc cli jq gettext OpenShift 4.10 domain name to use I'm going to be running this setup through Fedora in WSL2. Be sure to always use the same terminal/session for all commands since we'll reference environment variables set or created through the steps. Fedora Linux See Azure Docs for alternative install options. Import the Microsoft Keys bash sudo rpm --import https://packages.microsoft.com/keys/microsoft.asc Add the Microsoft Yum Repository bash cat << EOF | sudo tee /etc/yum.repos.d/azure-cli.repo [azure-cli] name=Azure CLI baseurl=https://packages.microsoft.com/yumrepos/azure-cli enabled=1 gpgcheck=1 gpgkey=https://packages.microsoft.com/keys/microsoft.asc EOF Install Azure CLI bash sudo dnf install -y azure-cli Install jq & gettext I'm going to rely on using shell variables interpolated into Kubernetes config and jq to build variables. Installing or ensuring the gettext & jq package is installed will allow us to use envsubst to simplify some of our configuration so we can use output of CLI's as input into Yamls to reduce the complexity of manual editing. bash sudo dnf install gettext jq","title":"Prerequisites"},{"location":"aro/cert-manager/#prepare-azure-account-for-azure-openshift","text":"Log into the Azure CLI by running the following and then authorizing through your Web Browser bash az login Register resource providers bash az provider register -n Microsoft.RedHatOpenShift --wait az provider register -n Microsoft.Compute --wait az provider register -n Microsoft.Storage --wait az provider register -n Microsoft.Authorization --wait","title":"Prepare Azure Account for Azure OpenShift"},{"location":"aro/cert-manager/#get-red-hat-pull-secret","text":"Log into cloud.redhat.com Browse to https://cloud.redhat.com/openshift/install/azure/aro-provisioned click the Download pull secret button and remember where you saved it, you'll reference it later.","title":"Get Red Hat pull secret"},{"location":"aro/cert-manager/#deploy-azure-openshift","text":"","title":"Deploy Azure OpenShift"},{"location":"aro/cert-manager/#variables-and-resource-group","text":"Set some environment variables to use later, and create an Azure Resource Group. Set the following environment variables Change the values to suit your environment bash PULL_SECRET=./pull-secret.txt # the path to pull-secret LOCATION=southcentralus # the location of your cluster RESOURCEGROUP=aro-rg # the name of the resource group where you want to create your cluster CLUSTER=aro-cluster # the name of your cluster DOMAIN=lab.domain.com # Domain or subdomain zone for cluster & cluster api Create an Azure resource group bash az group create \\ --name $RESOURCEGROUP \\ --location $LOCATION","title":"Variables and Resource Group"},{"location":"aro/cert-manager/#networking","text":"Create a virtual network with two empty subnets Create virtual network bash az network vnet create \\ --resource-group $RESOURCEGROUP \\ --name aro-vnet \\ --address-prefixes 10.0.0.0/22 Create control plane subnet bash az network vnet subnet create \\ --resource-group $RESOURCEGROUP \\ --vnet-name aro-vnet \\ --name master-subnet \\ --address-prefixes 10.0.0.0/23 \\ --service-endpoints Microsoft.ContainerRegistry Create machine subnet bash az network vnet subnet create \\ --resource-group $RESOURCEGROUP \\ --vnet-name aro-vnet \\ --name worker-subnet \\ --address-prefixes 10.0.2.0/23 \\ --service-endpoints Microsoft.ContainerRegistry Disable network policies on the control plane subnet This is required for the service to be able to connect to and manage the cluster. bash az network vnet subnet update \\ --name master-subnet \\ --resource-group $RESOURCEGROUP \\ --vnet-name aro-vnet \\ --disable-private-link-service-network-policies true Create the cluster This will take between 30 and 45 minutes. bash az aro create \\ --resource-group $RESOURCEGROUP \\ --name $CLUSTER \\ --vnet aro-vnet \\ --master-subnet master-subnet \\ --worker-subnet worker-subnet \\ --pull-secret @$PULL_SECRET \\ --domain $DOMAIN Wait until the ARO cluster is fully provisioned. Get OpenShift console URL You can use these to login to the web console (will get a cert warning that you can bypass with typing \"thisisunsafe\" in a chrome browser or login with oc) bash az aro show -g $RESOURCEGROUP -n $CLUSTER --query \"consoleProfile.url\" -o tsv Get OpenShift credentials bash az aro list-credentials --name $CLUSTER --resource-group $RESOURCEGROUP","title":"Networking"},{"location":"aro/cert-manager/#create-dns-zones-service-principal","text":"In order for cert-manager to work with AzureDNS, we need to create the zone and add a CAA record as well as create a Service Principal that we can use to manage records in this zone so CertManager can use DNS01 authentication for validating requests. This zone should be a public zone since letsencrypt will need to be able to read records created here. If you use a subdomain, please be sure to create the NS records in your primary domain to the subdomain. For ease of management, we're using the same resource group for domain as we have the cluster in. Create Zone bash az network dns zone create -g $RESOURCEGROUP -n $DOMAIN >You will need to configure your nameservers to point to azure. The output of running this zone create will show you the nameservers for this record that you will need to set up within your domain registrar. Create API DNS record bash APIREC=$(az aro show -g $RESOURCEGROUP -n $CLUSTER --query apiserverProfile.ip -o tsv) az network dns record-set a add-record -g $RESOURCEGROUP -z $DOMAIN \\ -n api -a $APIREC Create Wildcard DNS record bash WILDCARDREC=$(az aro show -n $CLUSTER -g $RESOURCEGROUP --query '{ingress:ingressProfiles[0].ip}' -o tsv) az network dns record-set a add-record -g $RESOURCEGROUP -z $DOMAIN \\ -n \"*.apps\" -a $WILDCARDREC Add CAA Record CAA is a type of DNS record that allows owners to specify which Certificate Authorities are allowed to issue certificates containing their domain names. bash az network dns record-set caa add-record -g $RESOURCEGROUP -z $DOMAIN \\ -n @ --flags 0 --tag \"issuewild\" --value \"letsencrypt.org\" You should be able to view the records in your console Note - You may have to create NS records in your root zone for a subdomain if you use a subdomain zone to point to the subdomains name servers. Set environment variables to build new service principal and credentials to allow cert-manager to create records in this zone. AZURE_CERT_MANAGER_NEW_SP_NAME = the name of the service principal to create that will manage the DNS zone automation for cert-manager. bash AZURE_CERT_MANAGER_NEW_SP_NAME=aro-dns-sp LETSENCRYPTEMAIL=youremail@work.com DNS_SP=$(az ad sp create-for-rbac --name $AZURE_CERT_MANAGER_NEW_SP_NAME --output json) AZURE_CERT_MANAGER_SP_APP_ID=$(echo $DNS_SP | jq -r '.appId') AZURE_CERT_MANAGER_SP_PASSWORD=$(echo $DNS_SP | jq -r '.password') AZURE_TENANT_ID=$(echo $DNS_SP | jq -r '.tenant') AZURE_SUBSCRIPTION_ID=$(az account show --output json | jq -r '.id') Restrict service principal - remove contributor role. Note: This may not exist, safe to proceed. We're going to grant the DNS Management Role to it next. bash az role assignment delete --assignee $AZURE_CERT_MANAGER_SP_APP_ID --role Contributor Grant DNS Zone Contributor to our Service Principal We'll grant DNS Zone Contributor to our DNS Service principal. bash az role assignment create --assignee $AZURE_CERT_MANAGER_SP_APP_ID --role befefa01-2a29-4197-83a8-272ff33ce314 Assign service principal to DNS zone bash DNS_ID=$(az network dns zone show --name $DOMAIN --resource-group $RESOURCEGROUP --query \"id\" --output tsv) az role assignment create --assignee $AZURE_CERT_MANAGER_SP_APP_ID --role \"DNS Zone Contributor\" --scope $DNS_ID","title":"Create DNS Zones &amp; Service Principal"},{"location":"aro/cert-manager/#login-to-cluster","text":"Login to your cluster through oc cli You may need to flush your local dns resolver/cache before you can see the DNS/Hostnames. On Windows you can open up a command prompt as administrator and type \"ipconfig /flushdns\" bash apiServer=$(az aro show -g $RESOURCEGROUP -n $CLUSTER --query apiserverProfile.url -o tsv) loginCred=$(az aro list-credentials --name $CLUSTER --resource-group $RESOURCEGROUP --query \"kubeadminPassword\" -o tsv) oc login $apiServer -u kubeadmin -p $loginCred You may get a warning that the certificate isn't trusted. We can ignore that now since we're in the process of adding a trusted certificate.","title":"Login to Cluster"},{"location":"aro/cert-manager/#set-up-cert-manager","text":"We'll install cert-manager from operatorhub. If you experience any issues installing here, it probably means that you didn't provide a pull-secret when you installed your ARO cluster. Create namespace bash cat <<EOF | oc apply -f - apiVersion: v1 kind: Namespace metadata: annotations: openshift.io/display-name: Red Hat Certificate Manager Operator labels: openshift.io/cluster-monitoring: 'true' name: openshift-cert-manager-operator EOF Switch openshift-cert-manager-operator project (namespace) bash oc project openshift-cert-manager-operator Create OperatorGroup bash cat <<EOF | oc apply -f - apiVersion: operators.coreos.com/v1 kind: OperatorGroup metadata: name: openshift-cert-manager-operator spec: {} EOF Create subscription for cert-manager operator yaml cat <<EOF | oc apply -f - apiVersion: operators.coreos.com/v1alpha1 kind: Subscription metadata: name: openshift-cert-manager-operator namespace: openshift-cert-manager-operator spec: channel: tech-preview installPlanApproval: Automatic name: openshift-cert-manager-operator source: redhat-operators sourceNamespace: openshift-marketplace EOF It will take a few minutes for this operator to install and complete it's set up. May be a good time to take a coffee break :) Wait for cert-manager operator to finish installing. Our next steps can't complete until the operator has finished installing. I recommend that you login to your cluster with the URL and credentials you captured after you ran the az aro create and view the installed operators to see that everything is complete and successful.","title":"Set up Cert-Manager"},{"location":"aro/cert-manager/#configure-cert-manager-letsencrypt","text":"We're going to set up cert-manager to use DNS verification for letsencrypt certificates. We'll need to generate a service principal that can update the DNS zone and create short term records needed to validate certificate requests and associate this service principal with the cluster issuer.","title":"Configure cert-manager LetsEncrypt"},{"location":"aro/cert-manager/#configure-certificate-requestor","text":"Switch openshift-cert-manager project (namespace) bash oc project openshift-cert-manager Create azuredns-config secret for storing service principal credentials to manage zone. bash oc create secret generic azuredns-config --from-literal=client-secret=$AZURE_CERT_MANAGER_SP_PASSWORD -n openshift-cert-manager Create Cluster Issuer yaml envsubst <<EOF | oc apply -f - apiVersion: cert-manager.io/v1 kind: ClusterIssuer metadata: name: letsencrypt-prod spec: acme: server: https://acme-v02.api.letsencrypt.org/directory email: $LETSENCRYPTEMAIL # This key doesn't exist, cert-manager creates it privateKeySecretRef: name: prod-letsencrypt-issuer-account-key solvers: - dns01: azureDNS: clientID: $AZURE_CERT_MANAGER_SP_APP_ID clientSecretSecretRef: name: azuredns-config key: client-secret subscriptionID: $AZURE_SUBSCRIPTION_ID tenantID: $AZURE_TENANT_ID resourceGroupName: $RESOURCEGROUP hostedZoneName: $DOMAIN environment: AzurePublicCloud EOF Describe issuer bash oc describe clusterissuer letsencrypt-prod You should see some output that the issuer is Registered/Ready Conditions: Last Transition Time: 2022-06-17T17:29:37Z Message: The ACME account was registered with the ACME server Observed Generation: 1 Reason: ACMEAccountRegistered Status: True Type: Ready Events: <none> Once the above command is complete, you should be able to login to openshift, click view operators and make sure you're in the \"openshift-cert-manager-operator\" project and you should see a screen like this. Again, if you have an ssl error and use a chrome browser - type \"thisisunsafe\" to get in if you get an error its an invalid cert.","title":"Configure Certificate Requestor"},{"location":"aro/cert-manager/#create-install-api-certificate","text":"Switch openshift-config project bash oc project openshift-config Configure API certificate yaml envsubst <<EOF | oc apply -f - apiVersion: cert-manager.io/v1 kind: Certificate metadata: name: openshift-api namespace: openshift-config spec: secretName: openshift-api-certificate issuerRef: name: letsencrypt-prod kind: ClusterIssuer dnsNames: - api.$DOMAIN EOF View certificate status bash oc describe certificate openshift-api -n openshift-config Create cluster api cert job This job will install the certificate ```yaml envsubst <<EOF | oc apply -f - apiVersion: rbac.authorization.k8s.io/v1 kind: ClusterRole metadata: name: patch-cluster-api-cert rules: - apiGroups: - \"\" resources: - secrets verbs: - get - list - apiGroups: - config.openshift.io resources: - apiservers verbs: - get - list - patch - update apiVersion: rbac.authorization.k8s.io/v1 kind: ClusterRoleBinding metadata: name: patch-cluster-api-cert roleRef: apiGroup: rbac.authorization.k8s.io kind: ClusterRole name: patch-cluster-api-cert subjects: - kind: ServiceAccount name: patch-cluster-api-cert namespace: openshift-config apiVersion: v1 kind: ServiceAccount metadata: name: patch-cluster-api-cert apiVersion: batch/v1 kind: Job metadata: name: patch-cluster-api-cert annotations: argocd.argoproj.io/hook: PostSync argocd.argoproj.io/hook-delete-policy: HookSucceeded spec: template: spec: containers: - image: image-registry.openshift-image-registry.svc:5000/openshift/cli:latest env: - name: API_HOST_NAME value: api.$DOMAIN command: - /bin/bash - -c - | #!/usr/bin/env bash if oc get secret openshift-api-certificate -n openshift-config; then oc patch apiserver cluster --type=merge -p '{\"spec\":{\"servingCerts\": {\"namedCertificates\": [{\"names\": [\"'$API_HOST_NAME'\"], \"servingCertificate\": {\"name\": \"openshift-api-certificate\"}}]}}}' else echo \"Could not execute sync as secret 'openshift-api-certificate' in namespace 'openshift-config' does not exist, check status of CertificationRequest\" exit 1 fi name: patch-cluster-api-cert dnsPolicy: ClusterFirst restartPolicy: Never terminationGracePeriodSeconds: 30 serviceAccount: patch-cluster-api-cert serviceAccountName: patch-cluster-api-cert EOF ```","title":"Create &amp; Install API Certificate"},{"location":"aro/cert-manager/#create-install-apps-wildcard-certificate","text":"Switch openshift-ingress project (namespace) bash oc project openshift-ingress Configure Wildcard Certificate yaml envsubst <<EOF | oc apply -f - apiVersion: cert-manager.io/v1 kind: Certificate metadata: name: openshift-wildcard namespace: openshift-ingress spec: secretName: openshift-wildcard-certificate issuerRef: name: letsencrypt-prod kind: ClusterIssuer commonName: \"*.apps.$DOMAIN\" dnsNames: - \"*.apps.$DOMAIN\" EOF This will generate our API and wildcard certificate requests. We'll now create two jobs that will install these certificates. View certificate status bash oc describe certificate openshift-wildcard -n openshift-ingress Install Wildcard Certificate Job ```yaml cat <<EOF | oc apply -f - apiVersion: rbac.authorization.k8s.io/v1 kind: ClusterRole metadata: name: patch-cluster-wildcard-cert rules: - apiGroups: - operator.openshift.io resources: - ingresscontrollers verbs: - get - list - patch - apiGroups: - \"\" resources: - secrets verbs: - get - list apiVersion: rbac.authorization.k8s.io/v1 kind: ClusterRoleBinding metadata: name: patch-cluster-wildcard-cert roleRef: apiGroup: rbac.authorization.k8s.io kind: ClusterRole name: patch-cluster-wildcard-cert subjects: - kind: ServiceAccount name: patch-cluster-wildcard-cert namespace: openshift-ingress apiVersion: v1 kind: ServiceAccount metadata: name: patch-cluster-wildcard-cert apiVersion: batch/v1 kind: Job metadata: name: patch-cluster-wildcard-cert annotations: argocd.argoproj.io/hook: PostSync argocd.argoproj.io/hook-delete-policy: HookSucceeded spec: template: spec: containers: - image: image-registry.openshift-image-registry.svc:5000/openshift/cli:latest command: - /bin/bash - -c - | #!/usr/bin/env bash if oc get secret openshift-wildcard-certificate -n openshift-ingress; then oc patch ingresscontroller default -n openshift-ingress-operator --type=merge --patch='{\"spec\": { \"defaultCertificate\": { \"name\": \"openshift-wildcard-certificate\" }}}' else echo \"Could not execute sync as secret 'openshift-wildcard-certificate' in namespace 'openshift-ingress' does not exist, check status of CertificationRequest\" exit 1 fi name: patch-cluster-wildcard-cert dnsPolicy: ClusterFirst restartPolicy: Never terminationGracePeriodSeconds: 30 serviceAccount: patch-cluster-wildcard-cert serviceAccountName: patch-cluster-wildcard-cert EOF ```","title":"Create &amp; Install APPS Wildcard Certificate"},{"location":"aro/cert-manager/#debugging","text":"One of the most helpful commands i've seen for debugging is in regards to challenges failing. The order says pending in perpetuity and you can run this to see why. oc describe challenges This is a very helpful guide in debugging certificates as well.","title":"Debugging"},{"location":"aro/cert-manager/#validate-certificates","text":"It will take a few minutes for the jobs to successfully complete. Once the certificate requests are complete, you should no longer see a browser security warning and you should have a valid SSL lock in your browser and no more warnings about SSL on cli. You may want to open an InPrivate/Private browser tab to visit the console/api so you can see the new SSL cert without having to expire your cache.","title":"Validate Certificates"},{"location":"aro/cert-manager/#delete-cluster","text":"Once you're done its a good idea to delete the cluster to ensure that you don't get a surprise bill. Delete the cluster bash az aro delete -y \\ --resource-group $RESOURCEGROUP \\ --name $CLUSTER Delete the Azure resource group Only do this if there's nothing else in the resource group. bash az group delete -y \\ --name $RESOURCEGROUP","title":"Delete Cluster"},{"location":"aro/clf-to-azure/","text":"Using Cluster Logging Forwarder in ARO with Azure Monitor Paul Czarkowski, Steve Mirman 08/19/2021 In Azure Red Hat OpenShift (ARO) you can fairly easily set up cluster logging to an in-cluster Elasticsearch using the OpenShift Elasticsearch Operator and the Cluster Logging Operator, but what if you want to use the Azure native Log Analytics service? There's a number of ways to do this, for example installing agents onto the VMs (in this case, it would be a DaemonSet with hostvar mounts) but that isn't ideal in a managed system like ARO. Fluentd is the log collection and forwarding tool used by OpenShift, however it does not have native support for Azure Log Analytics. However Fluent-bit which supports many of the same protocols as Fluentd does have native support for Azure Log Analytics. Armed with this knowledge we can create a fluent-bit service on the cluster to accept logs from fluentd and forward them to Azure Log Analytics. Prepare your ARO cluster Deploy an ARO cluster Set some environment variables ```bash export NAMESPACE=aro-clf-am export AZR_RESOURCE_LOCATION=eastus export AZR_RESOURCE_GROUP=openshift this value must be unique export AZR_LOG_APP_NAME=$AZR_RESOURCE_GROUP-$AZR_RESOURCE_LOCATION ``` Set up ARO Monitor workspace Add the Azure CLI log extensions bash az extension add --name log-analytics Create resource group If you plan to reuse the same group as your cluster skip this step bash az group create -n $AZR_RESOURCE_GROUP -l $AZR_RESOURCE_LOCATION Create workspace bash az monitor log-analytics workspace create \\ -g $AZR_RESOURCE_GROUP -n $AZR_LOG_APP_NAME \\ -l $AZR_RESOURCE_LOCATION Create a secret for your Azure workspace bash WORKSPACE_ID=$(az monitor log-analytics workspace show \\ -g $AZR_RESOURCE_GROUP -n $AZR_LOG_APP_NAME \\ --query customerId -o tsv) SHARED_KEY=$(az monitor log-analytics workspace get-shared-keys \\ -g $AZR_RESOURCE_GROUP -n $AZR_LOG_APP_NAME \\ --query primarySharedKey -o tsv) Configure OpenShift Create a Project to run the log forwarding in bash oc new-project $NAMESPACE Create namespaces for logging operators bash kubectl create ns openshift-logging kubectl create ns openshift-operators-redhat Add the MOBB chart repository to Helm bash helm repo add mobb https://rh-mobb.github.io/helm-charts/ Update your Helm repositories bash helm repo update Deploy the OpenShift Elasticsearch Operator and the Red Hat OpenShift Logging Operator > Note: You can skip this if you already have them installed, or install them via the OpenShift Console. bash helm upgrade -n $NAMESPACE clf-operators \\ mobb/operatorhub --version 0.1.1 --install \\ --values https://raw.githubusercontent.com/rh-mobb/helm-charts/main/charts/aro-clf-am/files/operators.yaml Configure cluster logging forwarder bash helm upgrade -n $NAMESPACE clf \\ mobb/aro-clf-am --version 0.1.0 --install \\ --set \"azure.workspaceId=$WORKSPACE_ID\" --set \"azure.sharedKey=$SHARED_KEY\" Check for logs in Azure Wait 5 to 15 minutes Query our new Workspace bash az monitor log-analytics query -w $WORKSPACE_ID \\ --analytics-query \"openshift_CL | take 10\" --output tsv or Log into Azure Log Insights Select your workspace Run the Query openshift_CL | take 10","title":"Using Cluster Logging Forwarder in ARO with Azure Monitor"},{"location":"aro/clf-to-azure/#using-cluster-logging-forwarder-in-aro-with-azure-monitor","text":"Paul Czarkowski, Steve Mirman 08/19/2021 In Azure Red Hat OpenShift (ARO) you can fairly easily set up cluster logging to an in-cluster Elasticsearch using the OpenShift Elasticsearch Operator and the Cluster Logging Operator, but what if you want to use the Azure native Log Analytics service? There's a number of ways to do this, for example installing agents onto the VMs (in this case, it would be a DaemonSet with hostvar mounts) but that isn't ideal in a managed system like ARO. Fluentd is the log collection and forwarding tool used by OpenShift, however it does not have native support for Azure Log Analytics. However Fluent-bit which supports many of the same protocols as Fluentd does have native support for Azure Log Analytics. Armed with this knowledge we can create a fluent-bit service on the cluster to accept logs from fluentd and forward them to Azure Log Analytics.","title":"Using Cluster Logging Forwarder in ARO with Azure Monitor"},{"location":"aro/clf-to-azure/#prepare-your-aro-cluster","text":"Deploy an ARO cluster Set some environment variables ```bash export NAMESPACE=aro-clf-am export AZR_RESOURCE_LOCATION=eastus export AZR_RESOURCE_GROUP=openshift","title":"Prepare your ARO cluster"},{"location":"aro/clf-to-azure/#this-value-must-be-unique","text":"export AZR_LOG_APP_NAME=$AZR_RESOURCE_GROUP-$AZR_RESOURCE_LOCATION ```","title":"this value must be unique"},{"location":"aro/clf-to-azure/#set-up-aro-monitor-workspace","text":"Add the Azure CLI log extensions bash az extension add --name log-analytics Create resource group If you plan to reuse the same group as your cluster skip this step bash az group create -n $AZR_RESOURCE_GROUP -l $AZR_RESOURCE_LOCATION Create workspace bash az monitor log-analytics workspace create \\ -g $AZR_RESOURCE_GROUP -n $AZR_LOG_APP_NAME \\ -l $AZR_RESOURCE_LOCATION Create a secret for your Azure workspace bash WORKSPACE_ID=$(az monitor log-analytics workspace show \\ -g $AZR_RESOURCE_GROUP -n $AZR_LOG_APP_NAME \\ --query customerId -o tsv) SHARED_KEY=$(az monitor log-analytics workspace get-shared-keys \\ -g $AZR_RESOURCE_GROUP -n $AZR_LOG_APP_NAME \\ --query primarySharedKey -o tsv)","title":"Set up ARO Monitor workspace"},{"location":"aro/clf-to-azure/#configure-openshift","text":"Create a Project to run the log forwarding in bash oc new-project $NAMESPACE Create namespaces for logging operators bash kubectl create ns openshift-logging kubectl create ns openshift-operators-redhat Add the MOBB chart repository to Helm bash helm repo add mobb https://rh-mobb.github.io/helm-charts/ Update your Helm repositories bash helm repo update Deploy the OpenShift Elasticsearch Operator and the Red Hat OpenShift Logging Operator > Note: You can skip this if you already have them installed, or install them via the OpenShift Console. bash helm upgrade -n $NAMESPACE clf-operators \\ mobb/operatorhub --version 0.1.1 --install \\ --values https://raw.githubusercontent.com/rh-mobb/helm-charts/main/charts/aro-clf-am/files/operators.yaml Configure cluster logging forwarder bash helm upgrade -n $NAMESPACE clf \\ mobb/aro-clf-am --version 0.1.0 --install \\ --set \"azure.workspaceId=$WORKSPACE_ID\" --set \"azure.sharedKey=$SHARED_KEY\"","title":"Configure OpenShift"},{"location":"aro/clf-to-azure/#check-for-logs-in-azure","text":"Wait 5 to 15 minutes Query our new Workspace bash az monitor log-analytics query -w $WORKSPACE_ID \\ --analytics-query \"openshift_CL | take 10\" --output tsv or Log into Azure Log Insights Select your workspace Run the Query openshift_CL | take 10","title":"Check for logs in Azure"},{"location":"aro/disaster-recovery/","text":"ARO - Considerations for Diasaster Recovery This is a high level overview of disaster recovery options for Azure Red Hat OpenShift. It is not a detailed design, but rather a starting point for a more detailed design. What is Disaster Recovery (DR) Disaster Recovery is an umbrella term that includes the following: Backup (and restore!) Failover (and failback!) High Availability Disaster Avoidence The most important part of Disaster Recovery is the \"Recovery\". Whatever your DR plan it must be tested and ideally performed on a semi-regular basis. You can use RTO (Recovery Time Objective) and RPO (Recovery Point Objective) to help determine what level of DR is right for your company. These Objectives are often application dependent and may mean choosing full HA for one application, and Backup/Restore for another even if they're both on the same OpenShift cluster. Recovery Time Objective (RTO) How long can your application be down without causing significant issues for your business. This can differ from application to application. Some applications may only affect internal staff while others may affect customers and revenue. In general you will categorize your applications by priority and potential damage to the business and match your DR plans accordingly. Recovery Point Objective (RPO) How much data can you lose before significant damage is done to your business. The traditional backup strategy is Daily. If you can survive a loss of 24 hours of data, or you have an alternative way to restore that data then this is often good enough. Combined RTO / RPO When combined you will account for \"how long can the application be offline\" and \"how much data can I lose\". If the answer zero or approaching zero for both then your DR strategy must be focussed around High Availabily and real time data replication. Backup In OpenShift it is not necessary to back up the cluster itself, but instead you back up the \"active state\" of your resources, any Persistent Volumes, and any backing services. Azure provides documentation on the basic Backup and Restore of the applications running on your ARO cluster. Azure also provides documentation on Backing up the various PaaS backing services that you may have connected to your applications such as Azure PostgreSQL . Failover An ARO cluster can be deployed into Multiple Availability Zones (AZs) in a single region. To protect your applications from region failure you must deploy your application into multiple ARO clusters across different regions. Here are some considerations: Start with a solid and tested Backup/Restore manual cutover DR solution Decide on RPO/RTO (Recovery Point Objective / Recovery Time Objective) for DR Decide whether your regions should be hot/hot, hot/warm, or hot/cold. Choose regions close to your consumers. choose two \"paired\" regions . Use global virtual network peering to connect your networks together. Use Front Door or Traffic Manager to route public traffic to the correct region. Enable geo-replication for container images (If using ACR). Remove service state from inside containers. Create a storage migration plan. Backup and Restore - Manual Cutover ( Hot / Cold ) Do you currently have the ability to do a point in time restore of Backups of your applications? Create a backup of your Kubernetes cluster If you restore these backups to a new cluster and manually cutover the DNS, will your applications be full functional? Create backups of any regionally co-located resources (like Redis, Postgres, etc.). Some Azure PaaS services such as Azure Container Registry can replicate to another region which may assist in performing backups or restore. This replication is often one way, therefore a new replication relationship must be created from the new region to another for the next DR event. If using DNS based failover, make sure TTLs are set to a suitable value. Determine if Non-regionally co-located resources (such as SaaS products) have appropriate failover plans and ensure that any special networking arrangements are available at the DR region. Failover to an existing cluster in the DR region (Hot / Warm) In a Hot / Warm situation the destination cluster should be similar to the the source cluster, but for financial reasons may be smaller, or be single AZ. If this is the case you may either run the DR cluster with lower expectations on performance and resiliance with the idea of failing back to the original cluster ASAP, or you will expand the DR cluster to match the original cluster and turn the original cluster into the next DR site. Ideally Your applications and data should be replicated to the DR site and should be ready to switch over within a very short window. High Availability ( Hot / Hot ) In a Hot / Hot scenario you have two-way synchronous replication of your data. The end user can access the application in either site and have the exact same experience. ARO Specific Example This following example will create two ARO clusters, each in its own Region. Virtual Network Peering is used to make it easier for resources to communicate for replication. Create a Primary Cluster Set the following environment variables: ``` AZR_RESOURCE_LOCATION=eastus AZR_RESOURCE_GROUP=ARO-DR-1 AZR_CLUSTER=ARODR1 AZR_PULL_SECRET=~/Downloads/pull-secret.txt NETWORK_SUBNET=10.0.0.0/20 CONTROL_SUBNET=10.0.0.0/24 MACHINE_SUBNET=10.0.1.0/24 FIREWALL_SUBNET=10.0.2.0/24 JUMPHOST_SUBNET=10.0.3.0/24 ``` Complete the rest of the step to create networks and cluster following the Private ARO cluster Create a Secondary Cluster Set the following environment variables: AZR_RESOURCE_LOCATION=centralus AZR_RESOURCE_GROUP=ARO-DR-2 AZR_CLUSTER=ARODR2 AZR_PULL_SECRET=~/Downloads/pull-secret.txt NETWORK_SUBNET=10.1.0.0/20 CONTROL_SUBNET=10.1.0.0/24 MACHINE_SUBNET=10.1.1.0/24 FIREWALL_SUBNET=10.1.2.0/24 JUMPHOST_SUBNET=10.1.3.0/24 Complete the rest of the step to create networks and cluster following the Private ARO cluster Connect the clusters via Virtual Network Peering Virtual network peering allows two Azure regions to connect to each other via a virtual network. Ideally you will use a Hub-Spoke topology and create appropriate firewalling in the Hub network but that is an excercise left for the reader and here we're creating a simple open peering between the two networks. Get the ID of the two networks you created in the previous step. ``` DR1_VNET=$(az network vnet show \\ --resource-group ARO-DR-1 \\ --name ARODR1-aro-vnet-eastus \\ --query id --out tsv) echo $DR1_VNET DR2_VNET=$(az network vnet show \\ --resource-group ARO-DR-2 \\ --name ARODR2-aro-vnet-centralus \\ --query id --out tsv) echo $DR2_VNET ``` Create peering from the Primary network to the Secondary network. az network vnet peering create \\ --name primary-to-secondary \\ --resource-group ARO-DR-1 \\ --vnet-name ARODR1-aro-vnet-eastus \\ --remote-vnet $DR2_VNET \\ --allow-vnet-access Create peering from the Secondary network to the Primary network. az network vnet peering create \\ --name secondary-to-primary \\ --resource-group ARO-DR-2 \\ --vnet-name ARODR2-aro-vnet-centralus \\ --remote-vnet $DR1_VNET \\ --allow-vnet-access Verify that the Jump Host in the Primary region is able to reach the Jump Host in the Secondary region. ssh -i $HOME/.ssh/id_rsa aro@$JUMP_IP ping 10.1.3.4 PING 10.1.3.4 (10.1.3.4) 56(84) bytes of data. 64 bytes from 10.1.3.4: icmp_seq=1 ttl=64 time=23.8 ms 64 bytes from 10.1.3.4: icmp_seq=2 ttl=64 time=23.10 ms ssh to jump host forwarding port 1337 as a socks proxy. ssh -D 1337 -C -i $HOME/.ssh/id_rsa aro@$JUMP_IP configure localhost:1337 as a socks proxy in your browser and access the two consoles. From here the two clusters are visible to each other via their frontends. This means they can access eachother's ingress endpoints, routes and Load Balancers, but not pod-to-pod. A PostgreSQL pod in the primary cluster could replicate to a PostgreSQL pod in the secondary cluster via a service of type LoadBalancer. Cross Region Registry Replication Openshift comes with a local registry that is used for local builds etc, but it is likely that you use a centralized registry for your own applications and images. Ensure that your registry supports replication to the DR region. Ensure that you understand if it supports active/active replication or if its a one way replication. In a Hot/Warm scenario where you'll only ever use the DR region as a backup to the primary region its likely okay for one-way replication to be used. Redhat Quay Azure Container Registry (must use Premium SKU for geo-replication) Example - Create a ACR in the Primary Region Create a new ACR in the primary region. az acr create --resource-group ARO-DR-1 \\ --name acrdr1 --sku Premium Log into and push an Image to the ACR. bash az acr login --name acrdr1 docker pull mcr.microsoft.com/hello-world docker tag mcr.microsoft.com/hello-world acrdr1.azurecr.io/hello-world:v1 docker push acrdr1.azurecr.io/hello-world:v1 Replicate the registry to the DR2 region. az acr replication create --location centralus --registry acrdr1 Wait a few moments and then check the replication status. az acr replication show --name centralus --registry acrdr1 --query status Red Hat Advanced Cluster Management Advanced Cluster Management (ACM) is a set of tools that can be used to manage the lifecycle of multiple OpenShift clusters. ACM gives you a single view into your clusters and provides gitops style management of you workloads and also has compliance features. You can run ACM from a central infrastructure (or your Primary DR) cluster and connect your ARO clusters to it. Failover for Application Ingress If you want to expose your Applications to the internet you can use Azure's Front Door or Traffic Manager resources which you can use to fail the routing over to the DR site. However if you are running private clusters your choices are a bit more limited. You could provide people with the application ingress URL for each cluster and expect them to know to use the DR one if the Primary is down You can add a custom domain (and TLS certificate) and use your internal DNS to switch from the Primary to the DR site. You can provision a Load Balancer in your network that you can point the custom domain at and use that to switch from the Primary to the DR site as needed. Example using simple DNS: Create a new wildcard DNS record with a low TTL pointing to the Primary Cluster's Ingress/Route ExternalIP in your private DNS zone. (in our case it was *.aro-dr.mobb.ninja) Modify the route for both apache examples to use the new wildcard DNS record. Test access Update the DNS record to point to the DR site's Ingress/Route ExternalIP. Test access","title":"ARO - Considerations for Diasaster Recovery"},{"location":"aro/disaster-recovery/#aro-considerations-for-diasaster-recovery","text":"This is a high level overview of disaster recovery options for Azure Red Hat OpenShift. It is not a detailed design, but rather a starting point for a more detailed design.","title":"ARO - Considerations for Diasaster Recovery"},{"location":"aro/disaster-recovery/#what-is-disaster-recovery-dr","text":"Disaster Recovery is an umbrella term that includes the following: Backup (and restore!) Failover (and failback!) High Availability Disaster Avoidence The most important part of Disaster Recovery is the \"Recovery\". Whatever your DR plan it must be tested and ideally performed on a semi-regular basis. You can use RTO (Recovery Time Objective) and RPO (Recovery Point Objective) to help determine what level of DR is right for your company. These Objectives are often application dependent and may mean choosing full HA for one application, and Backup/Restore for another even if they're both on the same OpenShift cluster.","title":"What is Disaster Recovery (DR)"},{"location":"aro/disaster-recovery/#recovery-time-objective-rto","text":"How long can your application be down without causing significant issues for your business. This can differ from application to application. Some applications may only affect internal staff while others may affect customers and revenue. In general you will categorize your applications by priority and potential damage to the business and match your DR plans accordingly.","title":"Recovery Time Objective (RTO)"},{"location":"aro/disaster-recovery/#recovery-point-objective-rpo","text":"How much data can you lose before significant damage is done to your business. The traditional backup strategy is Daily. If you can survive a loss of 24 hours of data, or you have an alternative way to restore that data then this is often good enough.","title":"Recovery Point Objective (RPO)"},{"location":"aro/disaster-recovery/#combined-rto-rpo","text":"When combined you will account for \"how long can the application be offline\" and \"how much data can I lose\". If the answer zero or approaching zero for both then your DR strategy must be focussed around High Availabily and real time data replication.","title":"Combined RTO / RPO"},{"location":"aro/disaster-recovery/#backup","text":"In OpenShift it is not necessary to back up the cluster itself, but instead you back up the \"active state\" of your resources, any Persistent Volumes, and any backing services. Azure provides documentation on the basic Backup and Restore of the applications running on your ARO cluster. Azure also provides documentation on Backing up the various PaaS backing services that you may have connected to your applications such as Azure PostgreSQL .","title":"Backup"},{"location":"aro/disaster-recovery/#failover","text":"An ARO cluster can be deployed into Multiple Availability Zones (AZs) in a single region. To protect your applications from region failure you must deploy your application into multiple ARO clusters across different regions. Here are some considerations: Start with a solid and tested Backup/Restore manual cutover DR solution Decide on RPO/RTO (Recovery Point Objective / Recovery Time Objective) for DR Decide whether your regions should be hot/hot, hot/warm, or hot/cold. Choose regions close to your consumers. choose two \"paired\" regions . Use global virtual network peering to connect your networks together. Use Front Door or Traffic Manager to route public traffic to the correct region. Enable geo-replication for container images (If using ACR). Remove service state from inside containers. Create a storage migration plan.","title":"Failover"},{"location":"aro/disaster-recovery/#backup-and-restore-manual-cutover-hot-cold","text":"Do you currently have the ability to do a point in time restore of Backups of your applications? Create a backup of your Kubernetes cluster If you restore these backups to a new cluster and manually cutover the DNS, will your applications be full functional? Create backups of any regionally co-located resources (like Redis, Postgres, etc.). Some Azure PaaS services such as Azure Container Registry can replicate to another region which may assist in performing backups or restore. This replication is often one way, therefore a new replication relationship must be created from the new region to another for the next DR event. If using DNS based failover, make sure TTLs are set to a suitable value. Determine if Non-regionally co-located resources (such as SaaS products) have appropriate failover plans and ensure that any special networking arrangements are available at the DR region.","title":"Backup and Restore - Manual Cutover ( Hot / Cold )"},{"location":"aro/disaster-recovery/#failover-to-an-existing-cluster-in-the-dr-region-hot-warm","text":"In a Hot / Warm situation the destination cluster should be similar to the the source cluster, but for financial reasons may be smaller, or be single AZ. If this is the case you may either run the DR cluster with lower expectations on performance and resiliance with the idea of failing back to the original cluster ASAP, or you will expand the DR cluster to match the original cluster and turn the original cluster into the next DR site. Ideally Your applications and data should be replicated to the DR site and should be ready to switch over within a very short window.","title":"Failover to an existing cluster in the DR region (Hot / Warm)"},{"location":"aro/disaster-recovery/#high-availability-hot-hot","text":"In a Hot / Hot scenario you have two-way synchronous replication of your data. The end user can access the application in either site and have the exact same experience.","title":"High Availability ( Hot / Hot )"},{"location":"aro/disaster-recovery/#aro-specific-example","text":"This following example will create two ARO clusters, each in its own Region. Virtual Network Peering is used to make it easier for resources to communicate for replication.","title":"ARO Specific Example"},{"location":"aro/disaster-recovery/#create-a-primary-cluster","text":"Set the following environment variables: ``` AZR_RESOURCE_LOCATION=eastus AZR_RESOURCE_GROUP=ARO-DR-1 AZR_CLUSTER=ARODR1 AZR_PULL_SECRET=~/Downloads/pull-secret.txt NETWORK_SUBNET=10.0.0.0/20 CONTROL_SUBNET=10.0.0.0/24 MACHINE_SUBNET=10.0.1.0/24 FIREWALL_SUBNET=10.0.2.0/24 JUMPHOST_SUBNET=10.0.3.0/24 ``` Complete the rest of the step to create networks and cluster following the Private ARO cluster","title":"Create a Primary Cluster"},{"location":"aro/disaster-recovery/#create-a-secondary-cluster","text":"Set the following environment variables: AZR_RESOURCE_LOCATION=centralus AZR_RESOURCE_GROUP=ARO-DR-2 AZR_CLUSTER=ARODR2 AZR_PULL_SECRET=~/Downloads/pull-secret.txt NETWORK_SUBNET=10.1.0.0/20 CONTROL_SUBNET=10.1.0.0/24 MACHINE_SUBNET=10.1.1.0/24 FIREWALL_SUBNET=10.1.2.0/24 JUMPHOST_SUBNET=10.1.3.0/24 Complete the rest of the step to create networks and cluster following the Private ARO cluster","title":"Create a Secondary Cluster"},{"location":"aro/disaster-recovery/#connect-the-clusters-via-virtual-network-peering","text":"Virtual network peering allows two Azure regions to connect to each other via a virtual network. Ideally you will use a Hub-Spoke topology and create appropriate firewalling in the Hub network but that is an excercise left for the reader and here we're creating a simple open peering between the two networks. Get the ID of the two networks you created in the previous step. ``` DR1_VNET=$(az network vnet show \\ --resource-group ARO-DR-1 \\ --name ARODR1-aro-vnet-eastus \\ --query id --out tsv) echo $DR1_VNET DR2_VNET=$(az network vnet show \\ --resource-group ARO-DR-2 \\ --name ARODR2-aro-vnet-centralus \\ --query id --out tsv) echo $DR2_VNET ``` Create peering from the Primary network to the Secondary network. az network vnet peering create \\ --name primary-to-secondary \\ --resource-group ARO-DR-1 \\ --vnet-name ARODR1-aro-vnet-eastus \\ --remote-vnet $DR2_VNET \\ --allow-vnet-access Create peering from the Secondary network to the Primary network. az network vnet peering create \\ --name secondary-to-primary \\ --resource-group ARO-DR-2 \\ --vnet-name ARODR2-aro-vnet-centralus \\ --remote-vnet $DR1_VNET \\ --allow-vnet-access Verify that the Jump Host in the Primary region is able to reach the Jump Host in the Secondary region. ssh -i $HOME/.ssh/id_rsa aro@$JUMP_IP ping 10.1.3.4 PING 10.1.3.4 (10.1.3.4) 56(84) bytes of data. 64 bytes from 10.1.3.4: icmp_seq=1 ttl=64 time=23.8 ms 64 bytes from 10.1.3.4: icmp_seq=2 ttl=64 time=23.10 ms ssh to jump host forwarding port 1337 as a socks proxy. ssh -D 1337 -C -i $HOME/.ssh/id_rsa aro@$JUMP_IP configure localhost:1337 as a socks proxy in your browser and access the two consoles. From here the two clusters are visible to each other via their frontends. This means they can access eachother's ingress endpoints, routes and Load Balancers, but not pod-to-pod. A PostgreSQL pod in the primary cluster could replicate to a PostgreSQL pod in the secondary cluster via a service of type LoadBalancer.","title":"Connect the clusters via Virtual Network Peering"},{"location":"aro/disaster-recovery/#cross-region-registry-replication","text":"Openshift comes with a local registry that is used for local builds etc, but it is likely that you use a centralized registry for your own applications and images. Ensure that your registry supports replication to the DR region. Ensure that you understand if it supports active/active replication or if its a one way replication. In a Hot/Warm scenario where you'll only ever use the DR region as a backup to the primary region its likely okay for one-way replication to be used. Redhat Quay Azure Container Registry (must use Premium SKU for geo-replication)","title":"Cross Region Registry Replication"},{"location":"aro/disaster-recovery/#example-create-a-acr-in-the-primary-region","text":"Create a new ACR in the primary region. az acr create --resource-group ARO-DR-1 \\ --name acrdr1 --sku Premium Log into and push an Image to the ACR. bash az acr login --name acrdr1 docker pull mcr.microsoft.com/hello-world docker tag mcr.microsoft.com/hello-world acrdr1.azurecr.io/hello-world:v1 docker push acrdr1.azurecr.io/hello-world:v1 Replicate the registry to the DR2 region. az acr replication create --location centralus --registry acrdr1 Wait a few moments and then check the replication status. az acr replication show --name centralus --registry acrdr1 --query status","title":"Example - Create a ACR in the Primary Region"},{"location":"aro/disaster-recovery/#red-hat-advanced-cluster-management","text":"Advanced Cluster Management (ACM) is a set of tools that can be used to manage the lifecycle of multiple OpenShift clusters. ACM gives you a single view into your clusters and provides gitops style management of you workloads and also has compliance features. You can run ACM from a central infrastructure (or your Primary DR) cluster and connect your ARO clusters to it.","title":"Red Hat Advanced Cluster Management"},{"location":"aro/disaster-recovery/#failover-for-application-ingress","text":"If you want to expose your Applications to the internet you can use Azure's Front Door or Traffic Manager resources which you can use to fail the routing over to the DR site. However if you are running private clusters your choices are a bit more limited. You could provide people with the application ingress URL for each cluster and expect them to know to use the DR one if the Primary is down You can add a custom domain (and TLS certificate) and use your internal DNS to switch from the Primary to the DR site. You can provision a Load Balancer in your network that you can point the custom domain at and use that to switch from the Primary to the DR site as needed. Example using simple DNS: Create a new wildcard DNS record with a low TTL pointing to the Primary Cluster's Ingress/Route ExternalIP in your private DNS zone. (in our case it was *.aro-dr.mobb.ninja) Modify the route for both apache examples to use the new wildcard DNS record. Test access Update the DNS record to point to the DR site's Ingress/Route ExternalIP. Test access","title":"Failover for Application Ingress"},{"location":"aro/federated-metrics/","text":"Federating System and User metrics to Azure Files in Azure Red Hat OpenShift Paul Czarkowski 06/04/2021 By default Azure Red Hat OpenShift (ARO) stores metrics in Ephemeral volumes, and its advised that users do not change this setting. However its not unreasonable to expect that metrics should be persisted for a set amount of time. This guide shows how to set up Thanos to federate both System and User Workload Metrics to a Thanos gateway that stores the metrics in Azure Files and makes them available via a Grafana instance (managed by the Grafana Operator). ToDo - Add Authorization in front of Thanos APIs Pre-Prequsites An ARO cluster Set some environment variables to use throughout to suit your environment Note: AZR_STORAGE_ACCOUNT_NAME must be unique bash export AZR_RESOURCE_LOCATION=eastus export AZR_RESOURCE_GROUP=openshift export AZR_STORAGE_ACCOUNT_NAME=arofederatedmetrics export CLUSTER_NAME=openshift export NAMESPACE=aro-thanos-af Set some environment variables to use throughout Note: AZR_STORAGE_ACCOUNT_NAME must be unique bash export AZR_RESOURCE_LOCATION=eastus export AZR_RESOURCE_GROUP=openshift export AZR_STORAGE_ACCOUNT_NAME=arofederatedmetrics export NAMESPACE=aro-thanos-af Azure Preperation Create an Azure storage account modify the arguments to suit your environment bash az storage account create \\ --name $AZR_STORAGE_ACCOUNT_NAME \\ --resource-group $AZR_RESOURCE_GROUP \\ --location $AZR_RESOURCE_LOCATION \\ --sku Standard_RAGRS \\ --kind StorageV2 Get the account key and update the secret in thanos-store-credentials.yaml bash AZR_STORAGE_KEY=$(az storage account keys list -g $AZR_RESOURCE_GROUP \\ -n $AZR_STORAGE_ACCOUNT_NAME --query \"[0].value\" -o tsv) Create a namespace to use bash oc new-project $NAMESPACE Add the MOBB chart repository to your Helm bash helm repo add mobb https://rh-mobb.github.io/helm-charts/ Update your repositories bash helm repo update Use the mobb/operatorhub chart to deploy the grafana operator bash helm upgrade -n $NAMESPACE $NAMESPACE-operators \\ mobb/operatorhub --version 0.1.1 --install \\ --values ./files/grafana-operator.yaml --values https://raw.githubusercontent.com/rh-mobb/helm-charts/main/charts/aro-thanos-af/files/grafana-operator.yaml Use the mobb/operatorhub chart to deploy the resource-locker operator > Note: Skip this if you already have the resource-locker operator installed, or if you do not plan to use User Workload Metrics bash helm upgrade -n resource-locker-operator resource-locker-operator \\ mobb/operatorhub --version 0.1.1 --create-namespace --install \\ --values https://raw.githubusercontent.com/rh-mobb/helm-charts/main/charts/aro-thanos-af/files/resourcelocker-operator.yaml Deploy ARO Thanos Azure Files Helm Chart (mobb/aro-thanos-af) > Note: enableUserWorkloadMetrics=true will overwrite configs for cluster and userworkload metrics, remove it from the helm command below if you already have custom settings. The Addendum at the end of this doc will explain the changes you'll need to make instead. bash helm upgrade -n $NAMESPACE aro-thanos-af --install mobb/aro-thanos-af --version 0.2.0 \\ --set \"aro.storageAccount=$AZR_STORAGE_ACCOUNT_NAME\" \\ --set \"aro.storageAccountKey=$AZR_STORAGE_KEY\" \\ --set \"aro.storageContainer=$CLUSTER_NAME\" \\ --set \"enableUserWorkloadMetrics=true\" Validate Grafana is installed and seeing metrics from Azure Files get the Route URL for Grafana (remember its https) and login using username root and the password you updated to (or the default of secret ). bash oc -n $NAMESPACE get route grafana-route Once logged in go to Dashboards->Manage and expand the thanos-receiver group and you should see the cluster metrics dashboards. Click on the Use Method / Cluster Dashboard and you should see metrics. \\o/. Note: If it complains about a missing datasource run the following: oc annotate -n $NAMESPACE grafanadatasource aro-thanos-af-prometheus \"retry=1\" Cleanup Uninstall the aro-thanos-af chart bash helm delete -n $NAMESPACE aro-thanos-af Uninstall the federated-metrics-operators chart bash helm delete -n $NAMESPACE federated-metrics-operators Delete the aro-thanos-af namespace bash oc delete namespace $NAMESPACE Delete the storage account bash az storage account delete \\ --name $AZR_STORAGE_ACCOUNT_NAME \\ --resource-group $AZR_RESOURCE_GROUP Addendum Enabling User Workload Monitoring See docs for more indepth details. Check the cluster-monitoring-config ConfigMap object bash oc -n openshift-monitoring get configmap cluster-monitoring-config -o yaml Enable User Workload Monitoring by doing one of the following If the data.config.yaml is not {} you should edit it and add the enableUserWorkload: true line manually. bash oc -n openshift-monitoring edit configmap cluster-monitoring-config Otherwise if its {} then you can run the following command safely. bash oc patch configmap cluster-monitoring-config -n openshift-monitoring \\ -p='{\"data\":{\"config.yaml\": \"enableUserWorkload: true\\n\"}}' Check that the User workload monitoring is starting up bash oc -n openshift-user-workload-monitoring get pods Append remoteWrite settings to the user-workload-monitoring config to forward user workload metrics to Thanos. Check if the User Workload Config Map exists: bash oc -n openshift-user-workload-monitoring get \\ configmaps user-workload-monitoring-config If the config doesn't exist run: bash cat << EOF | kubectl apply -f - apiVersion: v1 kind: ConfigMap metadata: name: user-workload-monitoring-config namespace: openshift-user-workload-monitoring data: config.yaml: | prometheus: remoteWrite: - url: \"http://thanos-receive.$NAMESPACE.svc.cluster.local:9091/api/v1/receive\" EOF Otherwise update it with the following: bash oc -n openshift-user-workload-monitoring edit \\ configmaps user-workload-monitoring-config yaml data: config.yaml: | ... prometheus: ... remoteWrite: - url: \"http://thanos-receive.thanos-receiver.svc.cluster.local:9091/api/v1/receive\"","title":"Federating System and User metrics to Azure Files in Azure Red Hat OpenShift"},{"location":"aro/federated-metrics/#federating-system-and-user-metrics-to-azure-files-in-azure-red-hat-openshift","text":"Paul Czarkowski 06/04/2021 By default Azure Red Hat OpenShift (ARO) stores metrics in Ephemeral volumes, and its advised that users do not change this setting. However its not unreasonable to expect that metrics should be persisted for a set amount of time. This guide shows how to set up Thanos to federate both System and User Workload Metrics to a Thanos gateway that stores the metrics in Azure Files and makes them available via a Grafana instance (managed by the Grafana Operator). ToDo - Add Authorization in front of Thanos APIs","title":"Federating System and User metrics to Azure Files in Azure Red Hat OpenShift"},{"location":"aro/federated-metrics/#pre-prequsites","text":"An ARO cluster Set some environment variables to use throughout to suit your environment Note: AZR_STORAGE_ACCOUNT_NAME must be unique bash export AZR_RESOURCE_LOCATION=eastus export AZR_RESOURCE_GROUP=openshift export AZR_STORAGE_ACCOUNT_NAME=arofederatedmetrics export CLUSTER_NAME=openshift export NAMESPACE=aro-thanos-af Set some environment variables to use throughout Note: AZR_STORAGE_ACCOUNT_NAME must be unique bash export AZR_RESOURCE_LOCATION=eastus export AZR_RESOURCE_GROUP=openshift export AZR_STORAGE_ACCOUNT_NAME=arofederatedmetrics export NAMESPACE=aro-thanos-af","title":"Pre-Prequsites"},{"location":"aro/federated-metrics/#azure-preperation","text":"Create an Azure storage account modify the arguments to suit your environment bash az storage account create \\ --name $AZR_STORAGE_ACCOUNT_NAME \\ --resource-group $AZR_RESOURCE_GROUP \\ --location $AZR_RESOURCE_LOCATION \\ --sku Standard_RAGRS \\ --kind StorageV2 Get the account key and update the secret in thanos-store-credentials.yaml bash AZR_STORAGE_KEY=$(az storage account keys list -g $AZR_RESOURCE_GROUP \\ -n $AZR_STORAGE_ACCOUNT_NAME --query \"[0].value\" -o tsv) Create a namespace to use bash oc new-project $NAMESPACE Add the MOBB chart repository to your Helm bash helm repo add mobb https://rh-mobb.github.io/helm-charts/ Update your repositories bash helm repo update Use the mobb/operatorhub chart to deploy the grafana operator bash helm upgrade -n $NAMESPACE $NAMESPACE-operators \\ mobb/operatorhub --version 0.1.1 --install \\ --values ./files/grafana-operator.yaml --values https://raw.githubusercontent.com/rh-mobb/helm-charts/main/charts/aro-thanos-af/files/grafana-operator.yaml Use the mobb/operatorhub chart to deploy the resource-locker operator > Note: Skip this if you already have the resource-locker operator installed, or if you do not plan to use User Workload Metrics bash helm upgrade -n resource-locker-operator resource-locker-operator \\ mobb/operatorhub --version 0.1.1 --create-namespace --install \\ --values https://raw.githubusercontent.com/rh-mobb/helm-charts/main/charts/aro-thanos-af/files/resourcelocker-operator.yaml Deploy ARO Thanos Azure Files Helm Chart (mobb/aro-thanos-af) > Note: enableUserWorkloadMetrics=true will overwrite configs for cluster and userworkload metrics, remove it from the helm command below if you already have custom settings. The Addendum at the end of this doc will explain the changes you'll need to make instead. bash helm upgrade -n $NAMESPACE aro-thanos-af --install mobb/aro-thanos-af --version 0.2.0 \\ --set \"aro.storageAccount=$AZR_STORAGE_ACCOUNT_NAME\" \\ --set \"aro.storageAccountKey=$AZR_STORAGE_KEY\" \\ --set \"aro.storageContainer=$CLUSTER_NAME\" \\ --set \"enableUserWorkloadMetrics=true\"","title":"Azure Preperation"},{"location":"aro/federated-metrics/#validate-grafana-is-installed-and-seeing-metrics-from-azure-files","text":"get the Route URL for Grafana (remember its https) and login using username root and the password you updated to (or the default of secret ). bash oc -n $NAMESPACE get route grafana-route Once logged in go to Dashboards->Manage and expand the thanos-receiver group and you should see the cluster metrics dashboards. Click on the Use Method / Cluster Dashboard and you should see metrics. \\o/. Note: If it complains about a missing datasource run the following: oc annotate -n $NAMESPACE grafanadatasource aro-thanos-af-prometheus \"retry=1\"","title":"Validate Grafana is installed and seeing metrics from Azure Files"},{"location":"aro/federated-metrics/#cleanup","text":"Uninstall the aro-thanos-af chart bash helm delete -n $NAMESPACE aro-thanos-af Uninstall the federated-metrics-operators chart bash helm delete -n $NAMESPACE federated-metrics-operators Delete the aro-thanos-af namespace bash oc delete namespace $NAMESPACE Delete the storage account bash az storage account delete \\ --name $AZR_STORAGE_ACCOUNT_NAME \\ --resource-group $AZR_RESOURCE_GROUP","title":"Cleanup"},{"location":"aro/federated-metrics/#addendum","text":"","title":"Addendum"},{"location":"aro/federated-metrics/#enabling-user-workload-monitoring","text":"See docs for more indepth details. Check the cluster-monitoring-config ConfigMap object bash oc -n openshift-monitoring get configmap cluster-monitoring-config -o yaml Enable User Workload Monitoring by doing one of the following If the data.config.yaml is not {} you should edit it and add the enableUserWorkload: true line manually. bash oc -n openshift-monitoring edit configmap cluster-monitoring-config Otherwise if its {} then you can run the following command safely. bash oc patch configmap cluster-monitoring-config -n openshift-monitoring \\ -p='{\"data\":{\"config.yaml\": \"enableUserWorkload: true\\n\"}}' Check that the User workload monitoring is starting up bash oc -n openshift-user-workload-monitoring get pods Append remoteWrite settings to the user-workload-monitoring config to forward user workload metrics to Thanos. Check if the User Workload Config Map exists: bash oc -n openshift-user-workload-monitoring get \\ configmaps user-workload-monitoring-config If the config doesn't exist run: bash cat << EOF | kubectl apply -f - apiVersion: v1 kind: ConfigMap metadata: name: user-workload-monitoring-config namespace: openshift-user-workload-monitoring data: config.yaml: | prometheus: remoteWrite: - url: \"http://thanos-receive.$NAMESPACE.svc.cluster.local:9091/api/v1/receive\" EOF Otherwise update it with the following: bash oc -n openshift-user-workload-monitoring edit \\ configmaps user-workload-monitoring-config yaml data: config.yaml: | ... prometheus: ... remoteWrite: - url: \"http://thanos-receive.thanos-receiver.svc.cluster.local:9091/api/v1/receive\"","title":"Enabling User Workload Monitoring"},{"location":"aro/federated-metrics/user-defined/","text":"User Workload Monitoring on Azure Red Hat OpenShift In Azure Red Hat OpenShift (ARO) Monitoring for User Defined Projects is disabled by default. Follow these instructions to enable it. Enabling See docs for more indepth details. Check the cluster-monitoring-config ConfigMap object bash oc -n openshift-monitoring get configmap cluster-monitoring-config -o yaml Enable User Workload Monitoring by doing one of the following If the data.config.yaml is not {} you should edit it and add the enableUserWorkload: true line manually. bash oc -n openshift-monitoring edit configmap cluster-monitoring-config Otherwise if its {} then you can run the following command safely. bash cat << EOF | oc apply -f - apiVersion: v1 kind: ConfigMap metadata: name: cluster-monitoring-config namespace: openshift-monitoring data: config.yaml: | enableUserWorkload: true EOF Create a config for User Workload Monitoring to set retention and This will configure the user workload instance to have PVC storage and will set basic data retention values. Feel free to edit it to suit your needs. Remember if you're going to have PVCs enabled they are tied to an AZ, to for a multi-AZ cluster you should ensure you have at least 2 workers per AZ so that they can failover. bash cat << EOF | oc apply -f - apiVersion: v1 kind: ConfigMap metadata: name: user-workload-monitoring-config namespace: openshift-user-workload-monitoring data: config.yaml: | prometheus: volumeClaimTemplate: spec: storageClassName: managed-premium volumeMode: Filesystem resources: requests: storage: 40Gi retention: 24h resources: requests: cpu: 200m memory: 2Gi EOF Deploy an example application with a service monitor resource bash oc apply -f example-app.yaml Wait a few minutes and then check your cluster metrics. Switch to Developer mode Change the Project to ns1 Click the Monitoring button Grafana Create a Project for the Grafana Operator + Application bash oc new-project custom-grafana Install the Grafana Operator (or via the OperatorHub in the GUI) bash cat << EOF | oc apply -f - apiVersion: operators.coreos.com/v1alpha1 kind: Subscription metadata: name: grafana-operator namespace: custom-grafana labels: operators.coreos.com/grafana-operator.custom-grafana: '' spec: channel: alpha installPlanApproval: Automatic name: grafana-operator source: community-operators sourceNamespace: openshift-marketplace startingCSV: grafana-operator.v3.10.1 EOF Once the Grafana Operator is running create a Grafana Instance cat << EOF | oc apply -f - apiVersion: integreatly.org/v1alpha1 kind: Grafana metadata: name: custom-grafana namespace: custom-grafana spec: adminPassword: bad-password adminUser: admin basicAuth: true config: auth: disable_signout_menu: false auth.anonymous: enabled: false log: level: warn mode: console security: admin_password: secret admin_user: root dashboardLabelSelector: - matchExpressions: - key: app operator: In values: - grafana ingress: enabled: true EOF Once the instance has been created you should be able to log in by getting the route and using the admin user/pass from above. bash oc -n custom-grafana get routes The output should look like NAME HOST/PORT PATH SERVICES PORT TERMINATION WILDCARD grafana-route grafana-route-custom-grafana.apps.w4l8w924.eastus.aroapp.io grafana-service 3000 edge None Copy and paste the host into your browser and log in to verify its working. Grant the grafana instance access to cluster-metrics bash oc adm policy add-cluster-role-to-user \\ cluster-monitoring-view -z grafana-serviceaccount Save the service accounts bearer token as a variable bash BEARER_TOKEN=`oc serviceaccounts get-token grafana-serviceaccount -n custom-grafana` Create a datasource to access the Thanos Querier bash cat << EOF | oc apply -f - apiVersion: integreatly.org/v1alpha1 kind: GrafanaDataSource metadata: name: prometheus-grafanadatasource namespace: custom-grafana spec: datasources: - access: proxy editable: true isDefault: true jsonData: httpHeaderName1: 'Authorization' timeInterval: 5s tlsSkipVerify: true name: Prometheus secureJsonData: httpHeaderValue1: 'Bearer ${BEARER_TOKEN}' type: prometheus url: 'https://thanos-querier.openshift-monitoring.svc.cluster.local:9091' name: prometheus-grafanadatasource.yaml EOF Add system dashboards to Grafana The dashboards.yaml file was created by running the script generate-dashboards.sh which fetches the dashboard json files from the openshift-monitoring namespace. oc apply -f dashboards.yaml","title":"User Workload Monitoring on Azure Red Hat OpenShift"},{"location":"aro/federated-metrics/user-defined/#user-workload-monitoring-on-azure-red-hat-openshift","text":"In Azure Red Hat OpenShift (ARO) Monitoring for User Defined Projects is disabled by default. Follow these instructions to enable it.","title":"User Workload Monitoring on Azure Red Hat OpenShift"},{"location":"aro/federated-metrics/user-defined/#enabling","text":"See docs for more indepth details. Check the cluster-monitoring-config ConfigMap object bash oc -n openshift-monitoring get configmap cluster-monitoring-config -o yaml Enable User Workload Monitoring by doing one of the following If the data.config.yaml is not {} you should edit it and add the enableUserWorkload: true line manually. bash oc -n openshift-monitoring edit configmap cluster-monitoring-config Otherwise if its {} then you can run the following command safely. bash cat << EOF | oc apply -f - apiVersion: v1 kind: ConfigMap metadata: name: cluster-monitoring-config namespace: openshift-monitoring data: config.yaml: | enableUserWorkload: true EOF Create a config for User Workload Monitoring to set retention and This will configure the user workload instance to have PVC storage and will set basic data retention values. Feel free to edit it to suit your needs. Remember if you're going to have PVCs enabled they are tied to an AZ, to for a multi-AZ cluster you should ensure you have at least 2 workers per AZ so that they can failover. bash cat << EOF | oc apply -f - apiVersion: v1 kind: ConfigMap metadata: name: user-workload-monitoring-config namespace: openshift-user-workload-monitoring data: config.yaml: | prometheus: volumeClaimTemplate: spec: storageClassName: managed-premium volumeMode: Filesystem resources: requests: storage: 40Gi retention: 24h resources: requests: cpu: 200m memory: 2Gi EOF Deploy an example application with a service monitor resource bash oc apply -f example-app.yaml Wait a few minutes and then check your cluster metrics. Switch to Developer mode Change the Project to ns1 Click the Monitoring button","title":"Enabling"},{"location":"aro/federated-metrics/user-defined/#grafana","text":"Create a Project for the Grafana Operator + Application bash oc new-project custom-grafana Install the Grafana Operator (or via the OperatorHub in the GUI) bash cat << EOF | oc apply -f - apiVersion: operators.coreos.com/v1alpha1 kind: Subscription metadata: name: grafana-operator namespace: custom-grafana labels: operators.coreos.com/grafana-operator.custom-grafana: '' spec: channel: alpha installPlanApproval: Automatic name: grafana-operator source: community-operators sourceNamespace: openshift-marketplace startingCSV: grafana-operator.v3.10.1 EOF Once the Grafana Operator is running create a Grafana Instance cat << EOF | oc apply -f - apiVersion: integreatly.org/v1alpha1 kind: Grafana metadata: name: custom-grafana namespace: custom-grafana spec: adminPassword: bad-password adminUser: admin basicAuth: true config: auth: disable_signout_menu: false auth.anonymous: enabled: false log: level: warn mode: console security: admin_password: secret admin_user: root dashboardLabelSelector: - matchExpressions: - key: app operator: In values: - grafana ingress: enabled: true EOF Once the instance has been created you should be able to log in by getting the route and using the admin user/pass from above. bash oc -n custom-grafana get routes The output should look like NAME HOST/PORT PATH SERVICES PORT TERMINATION WILDCARD grafana-route grafana-route-custom-grafana.apps.w4l8w924.eastus.aroapp.io grafana-service 3000 edge None Copy and paste the host into your browser and log in to verify its working. Grant the grafana instance access to cluster-metrics bash oc adm policy add-cluster-role-to-user \\ cluster-monitoring-view -z grafana-serviceaccount Save the service accounts bearer token as a variable bash BEARER_TOKEN=`oc serviceaccounts get-token grafana-serviceaccount -n custom-grafana` Create a datasource to access the Thanos Querier bash cat << EOF | oc apply -f - apiVersion: integreatly.org/v1alpha1 kind: GrafanaDataSource metadata: name: prometheus-grafanadatasource namespace: custom-grafana spec: datasources: - access: proxy editable: true isDefault: true jsonData: httpHeaderName1: 'Authorization' timeInterval: 5s tlsSkipVerify: true name: Prometheus secureJsonData: httpHeaderValue1: 'Bearer ${BEARER_TOKEN}' type: prometheus url: 'https://thanos-querier.openshift-monitoring.svc.cluster.local:9091' name: prometheus-grafanadatasource.yaml EOF Add system dashboards to Grafana The dashboards.yaml file was created by running the script generate-dashboards.sh which fetches the dashboard json files from the openshift-monitoring namespace. oc apply -f dashboards.yaml","title":"Grafana"},{"location":"aro/frontdoor/","text":"Azure Front Door with ARO ( Azure Red Hat OpenShift ) Securing exposing an Internet facing application with a private ARO Cluster. When you create a cluster on ARO you have several options in making the cluster public or private. With a public cluster you are allowing Internet traffic to the api and *.apps endpoints. With a private cluster you can make either or both the api and .apps endpoints private. How can you allow Internet access to an application running on your private cluster where the .apps endpoint is private? This document will guide you through using Azure Frontdoor to expose your applications to the Internet. There are several advantages of this approach, namely your cluster and all the resources in your Azure account can remain private, providing you an extra layer of security. Azure FrontDoor operates at the edge so we are controlling traffic before it even gets into your Azure account. On top of that, Azure FrontDoor also offers WAF and DDoS protection, certificate management and SSL Offloading just to name a few benefits. Kevin Collins *Adopted from ARO Reference Architecture 06/16/2022 Prerequisites az cli oc cli a custom domain a DNS zone that you can easily modify To build and deploy the application * maven cli * quarkus cli * OpenJDK Java 8 Make sure to use the same terminal session while going through guide for all commands as we will reference envrionment variables set or created through the guide. Get Started Create a private ARO cluster. Follow this guide to Create a private ARO cluster or simply run this bash script Set Evironment Variables Manually set environment variables ``` AROCLUSTER= ARORG= AFD_NAME= DOMAIN='e.g. aro.kmobb.com' This is the domain that you will be adding to Azure DNS to manage. ARO_APP_FQDN='e.g. minesweeper.aro.kmobb.com' (note - we will be deploying an application called minesweeper to test front door. Select a domain you would like to use for the application. For example minesweeper.aro.kmobb.com ... where aro.kmobb.com is the domain you manage and have DNS access to.) AFD_MINE_CUSTOM_DOMAIN_NAME='minesweeper-aro-kmobb-com' (note - this should be your domain name without and .'s for example minesweeper-aro-kmobb-com) PRIVATEENDPOINTSUBNET_PREFIX= subnet in the VNET you cluster is in. If you following the example above to create a custer where you virtual network is 10.0.0.0/20 then you can use '10.0.6.0/24' PRIVATEENDPOINTSUBNET_NAME='PrivateEndpoint-subnet' ``` Set environment variables with Bash ```bash UNIQUEID=$RANDOM ARO_RGNAME=$(az aro show -n $AROCLUSTER -g $ARORG --query \"clusterProfile.resourceGroupId\" -o tsv | sed 's/.*\\///') LOCATION=$(az aro show --name $AROCLUSTER --resource-group $ARORG --query location -o tsv) INTERNAL_LBNAME=$(az network lb list --resource-group $ARO_RGNAME --query \"[? contains(name, 'internal')].name\" -o tsv) WORKER_SUBNET_NAME=$(az aro show --name $AROCLUSTER --resource-group $ARORG --query 'workerProfiles[0].subnetId' -o tsv | sed 's/.*\\///') WORKER_SUBNET_ID=$(az aro show --name $AROCLUSTER --resource-group $ARORG --query 'workerProfiles[0].subnetId' -o tsv) VNET_NAME=$(az network vnet list -g $ARORG --query '[0].name' -o tsv) LBCONFIG_ID=$(az network lb frontend-ip list -g $ARO_RGNAME --lb-name $INTERNAL_LBNAME --query \"[? contains(subnet.id,'$WORKER_SUBNET_ID')].id\" -o tsv) LBCONFIG_IP=$(az network lb frontend-ip list -g $ARO_RGNAME --lb-name $INTERNAL_LBNAME --query \"[? contains(subnet.id,'$WORKER_SUBNET_ID')].privateIpAddress\" -o tsv) ``` Create a Private Link Service After we have the cluster up and running, we need to create a private link service. The private link service will provide private and secure connectivity between the Front Door Service and our cluster. Disable the worker subnet private link service network policy for the worker subnet bash az network vnet subnet update \\ --disable-private-link-service-network-policies true \\ --name $WORKER_SUBNET_NAME \\ --resource-group $ARORG \\ --vnet-name $VNET_NAME Create a private link service targeting the worker subnets ```bash az network private-link-service create \\ --name $AROCLUSTER-pls \\ --resource-group $ARORG \\ --private-ip-address-version IPv4 \\ --private-ip-allocation-method Dynamic \\ --vnet-name $VNET_NAME \\ --subnet $WORKER_SUBNET_NAME \\ --lb-frontend-ip-configs $LBCONFIG_ID privatelink_id=$(az network private-link-service show -n $AROCLUSTER-pls -g $ARORG --query 'id' -o tsv) ``` Create and Configure an instance of Azure Front Door Create a Front Door Instance ```bash az afd profile create \\ --resource-group $ARORG \\ --profile-name $AFD_NAME \\ --sku Premium_AzureFrontDoor afd_id=$(az afd profile show -g $ARORG --profile-name $AFD_NAME --query 'id' -o tsv) ``` Create an endpoint for the ARO Internal Load Balancer bash az afd endpoint create \\ --resource-group $ARORG \\ --enabled-state Enabled \\ --endpoint-name 'aro-ilb'$UNIQUEID \\ --profile-name $AFD_NAME Create a Front Door Origin Group that will point to the ARO Internal Loadbalancer bash az afd origin-group create \\ --origin-group-name 'afdorigin' \\ --probe-path '/' \\ --probe-protocol Http \\ --probe-request-type GET \\ --probe-interval-in-seconds 100 \\ --profile-name $AFD_NAME \\ --resource-group $ARORG \\ --probe-interval-in-seconds 120 \\ --sample-size 4 \\ --successful-samples-required 3 \\ --additional-latency-in-milliseconds 50 Create a Front Door Origin with the above Origin Group that will point to the ARO Internal Loadbalancer bash az afd origin create \\ --enable-private-link true \\ --private-link-resource $privatelink_id \\ --private-link-location $LOCATION \\ --private-link-request-message 'Private link service from AFD' \\ --weight 1000 \\ --priority 1 \\ --http-port 80 \\ --https-port 443 \\ --origin-group-name 'afdorigin' \\ --enabled-state Enabled \\ --host-name $LBCONFIG_IP \\ --origin-name 'afdorigin' \\ --profile-name $AFD_NAME \\ --resource-group $ARORG Approve the private link connection ```bash privatelink_pe_id=$(az network private-link-service show -n $AROCLUSTER-pls -g $ARORG --query 'privateEndpointConnections[0].id' -o tsv) az network private-endpoint-connection approve \\ --description 'Approved' \\ --id $privatelink_pe_id ``` Add your custom domain to Azure Front Door bash az afd custom-domain create \\ --certificate-type ManagedCertificate \\ --custom-domain-name $AFD_MINE_CUSTOM_DOMAIN_NAME \\ --host-name $ARO_APP_FQDN \\ --minimum-tls-version TLS12 \\ --profile-name $AFD_NAME \\ --resource-group $ARORG Create an Azure Front Door endpoint for your custom domain bash az afd endpoint create \\ --resource-group $ARORG \\ --enabled-state Enabled \\ --endpoint-name 'aro-mine-'$UNIQUEID \\ --profile-name $AFD_NAME Add an Azure Front Door route for your custom domain bash az afd route create \\ --endpoint-name 'aro-mine-'$UNIQUEID \\ --forwarding-protocol HttpOnly \\ --https-redirect Disabled \\ --origin-group 'afdorigin' \\ --profile-name $AFD_NAME \\ --resource-group $ARORG \\ --route-name 'aro-mine-route' \\ --supported-protocols Http Https \\ --patterns-to-match '/*' \\ --custom-domains $AFD_MINE_CUSTOM_DOMAIN_NAME Update DNS Get a validation token from Front Door so Front Door can validate your domain bash afdToken=$(az afd custom-domain show \\ --resource-group $ARORG \\ --profile-name $AFD_NAME \\ --custom-domain-name $AFD_MINE_CUSTOM_DOMAIN_NAME \\ --query \"validationProperties.validationToken\") Create a DNS Zone bash az network dns zone create -g $ARORG -n $DOMAIN >You will need to configure your nameservers to point to azure. The output of running this zone create will show you the nameservers for this record that you will need to set up within your domain registrar. Create a new text record in your DNS server bash az network dns record-set txt add-record -g $ARORG -z $DOMAIN -n _dnsauth.$(echo $ARO_APP_FQDN | sed 's/\\..*//') --value $afdToken --record-set-name _dnsauth.$(echo $ARO_APP_FQDN | sed 's/\\..*//') Check if the domain has been validated: Note this can take several hours Your FQDN will not resolve until Front Door validates your domain. bash az afd custom-domain list -g $ARORG --profile-name $AFD_NAME --query \"[? contains(hostName, '$ARO_APP_FQDN')].domainValidationState\" Add a CNAME record to DNS Get the Azure Front Door endpoint: bash afdEndpoint=$(az afd endpoint show -g $ARORG --profile-name $AFD_NAME --endpoint-name aro-mine-$UNIQUEID --query \"hostName\" -o tsv) Create a cname record for the application bash az network dns record-set cname set-record -g $ARORG -z $DOMAIN \\ -n $(echo $ARO_APP_FQDN | sed 's/\\..*//') -z $DOMAIN -c $afdEndpoint Deploy an application Now the fun part, let's deploy an application! We will be deploying a Java based application called microsweeper . This is an application that runs on OpenShift and uses a PostgreSQL database to store scores. With ARO being a first class service on Azure, we will create an Azure Database for PostgreSQL service and connect it to our cluster with a private endpoint. Create a Azure Database for PostgreSQL servers service ```bash az postgres server create --name microsweeper-database --resource-group $ARORG --location $LOCATION --admin-user quarkus --admin-password r3dh4t1! --sku-name GP_Gen5_2 POSTGRES_ID=$(az postgres server show -n microsweeper-database -g $ARORG --query 'id' -o tsv) ``` Create a private endpoint connection for the database ```bash az network vnet subnet create \\ --resource-group $ARORG \\ --vnet-name $VNET_NAME \\ --name $PRIVATEENDPOINTSUBNET_NAME \\ --address-prefixes $PRIVATEENDPOINTSUBNET_PREFIX \\ --disable-private-endpoint-network-policies true az network private-endpoint create \\ --name 'postgresPvtEndpoint' \\ --resource-group $ARORG \\ --vnet-name $VNET_NAME \\ --subnet $PRIVATEENDPOINTSUBNET_NAME \\ --private-connection-resource-id $POSTGRES_ID \\ --group-id 'postgresqlServer' \\ --connection-name 'postgresdbConnection' ``` 1. Create and configure a private DNS Zone for the Postgres database ```bash az network private-dns zone create \\ --resource-group $ARORG \\ --name 'privatelink.postgres.database.azure.com' az network private-dns link vnet create \\ --resource-group $ARORG \\ --zone-name 'privatelink.postgres.database.azure.com' \\ --name 'PostgresDNSLink' \\ --virtual-network $VNET_NAME \\ --registration-enabled false az network private-endpoint dns-zone-group create \\ --resource-group $ARORG \\ --name 'PostgresDb-ZoneGroup' \\ --endpoint-name 'postgresPvtEndpoint' \\ --private-dns-zone 'privatelink.postgres.database.azure.com' \\ --zone-name 'postgresqlServer' NETWORK_INTERFACE_ID=$(az network private-endpoint show --name postgresPvtEndpoint --resource-group $ARORG --query 'networkInterfaces[0].id' -o tsv) POSTGRES_IP=$(az resource show --ids $NETWORK_INTERFACE_ID --api-version 2019-04-01 --query 'properties.ipConfigurations[0].properties.privateIPAddress' -o tsv) az network private-dns record-set a create --name $UNIQUEID-microsweeper-database --zone-name privatelink.postgres.database.azure.com --resource-group $ARORG az network private-dns record-set a add-record --record-set-name $UNIQUEID-microsweeper-database --zone-name privatelink.postgres.database.azure.com --resource-group $ARORG -a $POSTGRES_IP ``` Create a postgres database that will contain scores for the minesweeper application bash az postgres db create \\ --resource-group $ARORG \\ --name score \\ --server-name microsweeper-database Deploy the minesweeper application Clone the git repository bash git clone -b ARO https://github.com/redhat-mw-demos/microsweeper-quarkus.git change to the root directory bash cd microsweeper-quarkus Ensure Java 1.8 is set at your Java version bash mvn --version Look for Java version - 1.8XXXX if not set to Java 1.8 you will need to set your JAVA_HOME variable to Java 1.8 you have installed. To find your java versions run: bash java -version then export your JAVA_HOME variable bash export JAVA_HOME=`/usr/libexec/java_home -v 1.8.0_332` Log into your openshift cluster Before you deploy your application, you will need to be connected to a private network that has access to the cluster. A great way to establish this connectity is with a VPN connection. Follow this guide to setup a VPN connection with your Azure account. ```bash kubeadmin_password=$(az aro list-credentials --name $AROCLUSTER --resource-group $ARORG --query kubeadminPassword --output tsv) apiServer=$(az aro show -g $ARORG -n $AROCLUSTER --query apiserverProfile.url -o tsv) oc login $apiServer -u kubeadmin -p $kubeadmin_password ``` Create a new OpenShift Project bash oc new-project minesweeper add the openshift extension to quarkus bash quarkus ext add openshift Edit microsweeper-quarkus/src/main/resources/application.properties Make sure your file looks like the one below, changing the IP address on line 3 to the private ip address of your postgres instance. To find your Postgres private IP address run the following commands: ```bash NETWORK_INTERFACE_ID=$(az network private-endpoint show --name postgresPvtEndpoint --resource-group $ARORG --query 'networkInterfaces[0].id' -o tsv) az resource show --ids $NETWORK_INTERFACE_ID --api-version 2019-04-01 --query 'properties.ipConfigurations[0].properties.privateIPAddress' -o tsv ``` Sample microsweeper-quarkus/src/main/resources/application.properties ``` # Database configurations %prod.quarkus.datasource.db-kind=postgresql %prod.quarkus.datasource.jdbc.url=jdbc:postgresql://10.1.6.9:5432/score %prod.quarkus.datasource.jdbc.driver=org.postgresql.Driver %prod.quarkus.datasource.username=quarkus@microsweeper-database %prod.quarkus.datasource.password=r3dh4t1! %prod.quarkus.hibernate-orm.database.generation=drop-and-create %prod.quarkus.hibernate-orm.database.generation=update # OpenShift configurations %prod.quarkus.kubernetes-client.trust-certs=true %prod.quarkus.kubernetes.deploy=true %prod.quarkus.kubernetes.deployment-target=openshift #%prod.quarkus.kubernetes.deployment-target=knative %prod.quarkus.openshift.build-strategy=docker #%prod.quarkus.openshift.expose=true # Serverless configurations #%prod.quarkus.container-image.group=microsweeper-%prod.quarkus #%prod.quarkus.container-image.registry=image-registry.openshift-image-registry.svc:5000 # macOS configurations #%prod.quarkus.native.container-build=true ``` Build and deploy the quarkus application to OpenShift bash quarkus build --no-tests Create a route to your custom domain Change the snippet below replacing your hostname for the host: bash cat << EOF | oc apply -f - apiVersion: route.openshift.io/v1 kind: Route metadata: labels: app.kubernetes.io/name: microsweeper-appservice app.kubernetes.io/version: 1.0.0-SNAPSHOT app.openshift.io/runtime: quarkus name: microsweeper-appservice namespace: minesweeper spec: host: minesweeper.aro.kmobb.com to: kind: Service name: microsweeper-appservice weight: 100 targetPort: port: 8080 wildcardPolicy: None EOF Check the dns settings of your application. notice that the application URL is routed through Azure Front Door at the edge. The only way this application that is running on your cluster can be access is through Azure Front Door which is connected to your cluster through a private endpoint. bash nslookup $ARO_APP_FQDN sample output: ``` Server: 2600:1700:850:d220::1 Address: 2600:1700:850:d220::1#53 Non-authoritative answer: minesweeper.aro.kmobb.com canonical name = aro-mine-13947-dxh0ahd7fzfyexgx.z01.azurefd.net. aro-mine-13947-dxh0ahd7fzfyexgx.z01.azurefd.net canonical name = star-azurefd-prod.trafficmanager.net. star-azurefd-prod.trafficmanager.net canonical name = dual.part-0013.t-0009.t-msedge.net. dual.part-0013.t-0009.t-msedge.net canonical name = part-0013.t-0009.t-msedge.net. Name: part-0013.t-0009.t-msedge.net Address: 13.107.213.41 Name: part-0013.t-0009.t-msedge.net Address: 13.107.246.41 ``` Test the application Point your broswer to your domain!! Clean up To clean up everything you created, simply delete the resource group az group delete -g $ARORG","title":"Azure Frontdoor"},{"location":"aro/frontdoor/#azure-front-door-with-aro-azure-red-hat-openshift","text":"Securing exposing an Internet facing application with a private ARO Cluster. When you create a cluster on ARO you have several options in making the cluster public or private. With a public cluster you are allowing Internet traffic to the api and *.apps endpoints. With a private cluster you can make either or both the api and .apps endpoints private. How can you allow Internet access to an application running on your private cluster where the .apps endpoint is private? This document will guide you through using Azure Frontdoor to expose your applications to the Internet. There are several advantages of this approach, namely your cluster and all the resources in your Azure account can remain private, providing you an extra layer of security. Azure FrontDoor operates at the edge so we are controlling traffic before it even gets into your Azure account. On top of that, Azure FrontDoor also offers WAF and DDoS protection, certificate management and SSL Offloading just to name a few benefits. Kevin Collins *Adopted from ARO Reference Architecture 06/16/2022","title":"Azure Front Door with ARO ( Azure Red Hat OpenShift )"},{"location":"aro/frontdoor/#prerequisites","text":"az cli oc cli a custom domain a DNS zone that you can easily modify To build and deploy the application * maven cli * quarkus cli * OpenJDK Java 8 Make sure to use the same terminal session while going through guide for all commands as we will reference envrionment variables set or created through the guide.","title":"Prerequisites"},{"location":"aro/frontdoor/#get-started","text":"Create a private ARO cluster. Follow this guide to Create a private ARO cluster or simply run this bash script","title":"Get Started"},{"location":"aro/frontdoor/#set-evironment-variables","text":"Manually set environment variables ``` AROCLUSTER= ARORG= AFD_NAME= DOMAIN='e.g. aro.kmobb.com' This is the domain that you will be adding to Azure DNS to manage. ARO_APP_FQDN='e.g. minesweeper.aro.kmobb.com' (note - we will be deploying an application called minesweeper to test front door. Select a domain you would like to use for the application. For example minesweeper.aro.kmobb.com ... where aro.kmobb.com is the domain you manage and have DNS access to.) AFD_MINE_CUSTOM_DOMAIN_NAME='minesweeper-aro-kmobb-com' (note - this should be your domain name without and .'s for example minesweeper-aro-kmobb-com) PRIVATEENDPOINTSUBNET_PREFIX= subnet in the VNET you cluster is in. If you following the example above to create a custer where you virtual network is 10.0.0.0/20 then you can use '10.0.6.0/24' PRIVATEENDPOINTSUBNET_NAME='PrivateEndpoint-subnet' ``` Set environment variables with Bash ```bash UNIQUEID=$RANDOM ARO_RGNAME=$(az aro show -n $AROCLUSTER -g $ARORG --query \"clusterProfile.resourceGroupId\" -o tsv | sed 's/.*\\///') LOCATION=$(az aro show --name $AROCLUSTER --resource-group $ARORG --query location -o tsv) INTERNAL_LBNAME=$(az network lb list --resource-group $ARO_RGNAME --query \"[? contains(name, 'internal')].name\" -o tsv) WORKER_SUBNET_NAME=$(az aro show --name $AROCLUSTER --resource-group $ARORG --query 'workerProfiles[0].subnetId' -o tsv | sed 's/.*\\///') WORKER_SUBNET_ID=$(az aro show --name $AROCLUSTER --resource-group $ARORG --query 'workerProfiles[0].subnetId' -o tsv) VNET_NAME=$(az network vnet list -g $ARORG --query '[0].name' -o tsv) LBCONFIG_ID=$(az network lb frontend-ip list -g $ARO_RGNAME --lb-name $INTERNAL_LBNAME --query \"[? contains(subnet.id,'$WORKER_SUBNET_ID')].id\" -o tsv) LBCONFIG_IP=$(az network lb frontend-ip list -g $ARO_RGNAME --lb-name $INTERNAL_LBNAME --query \"[? contains(subnet.id,'$WORKER_SUBNET_ID')].privateIpAddress\" -o tsv) ```","title":"Set Evironment Variables"},{"location":"aro/frontdoor/#create-a-private-link-service","text":"After we have the cluster up and running, we need to create a private link service. The private link service will provide private and secure connectivity between the Front Door Service and our cluster. Disable the worker subnet private link service network policy for the worker subnet bash az network vnet subnet update \\ --disable-private-link-service-network-policies true \\ --name $WORKER_SUBNET_NAME \\ --resource-group $ARORG \\ --vnet-name $VNET_NAME Create a private link service targeting the worker subnets ```bash az network private-link-service create \\ --name $AROCLUSTER-pls \\ --resource-group $ARORG \\ --private-ip-address-version IPv4 \\ --private-ip-allocation-method Dynamic \\ --vnet-name $VNET_NAME \\ --subnet $WORKER_SUBNET_NAME \\ --lb-frontend-ip-configs $LBCONFIG_ID privatelink_id=$(az network private-link-service show -n $AROCLUSTER-pls -g $ARORG --query 'id' -o tsv) ```","title":"Create a Private Link Service"},{"location":"aro/frontdoor/#create-and-configure-an-instance-of-azure-front-door","text":"Create a Front Door Instance ```bash az afd profile create \\ --resource-group $ARORG \\ --profile-name $AFD_NAME \\ --sku Premium_AzureFrontDoor afd_id=$(az afd profile show -g $ARORG --profile-name $AFD_NAME --query 'id' -o tsv) ``` Create an endpoint for the ARO Internal Load Balancer bash az afd endpoint create \\ --resource-group $ARORG \\ --enabled-state Enabled \\ --endpoint-name 'aro-ilb'$UNIQUEID \\ --profile-name $AFD_NAME Create a Front Door Origin Group that will point to the ARO Internal Loadbalancer bash az afd origin-group create \\ --origin-group-name 'afdorigin' \\ --probe-path '/' \\ --probe-protocol Http \\ --probe-request-type GET \\ --probe-interval-in-seconds 100 \\ --profile-name $AFD_NAME \\ --resource-group $ARORG \\ --probe-interval-in-seconds 120 \\ --sample-size 4 \\ --successful-samples-required 3 \\ --additional-latency-in-milliseconds 50 Create a Front Door Origin with the above Origin Group that will point to the ARO Internal Loadbalancer bash az afd origin create \\ --enable-private-link true \\ --private-link-resource $privatelink_id \\ --private-link-location $LOCATION \\ --private-link-request-message 'Private link service from AFD' \\ --weight 1000 \\ --priority 1 \\ --http-port 80 \\ --https-port 443 \\ --origin-group-name 'afdorigin' \\ --enabled-state Enabled \\ --host-name $LBCONFIG_IP \\ --origin-name 'afdorigin' \\ --profile-name $AFD_NAME \\ --resource-group $ARORG Approve the private link connection ```bash privatelink_pe_id=$(az network private-link-service show -n $AROCLUSTER-pls -g $ARORG --query 'privateEndpointConnections[0].id' -o tsv) az network private-endpoint-connection approve \\ --description 'Approved' \\ --id $privatelink_pe_id ``` Add your custom domain to Azure Front Door bash az afd custom-domain create \\ --certificate-type ManagedCertificate \\ --custom-domain-name $AFD_MINE_CUSTOM_DOMAIN_NAME \\ --host-name $ARO_APP_FQDN \\ --minimum-tls-version TLS12 \\ --profile-name $AFD_NAME \\ --resource-group $ARORG Create an Azure Front Door endpoint for your custom domain bash az afd endpoint create \\ --resource-group $ARORG \\ --enabled-state Enabled \\ --endpoint-name 'aro-mine-'$UNIQUEID \\ --profile-name $AFD_NAME Add an Azure Front Door route for your custom domain bash az afd route create \\ --endpoint-name 'aro-mine-'$UNIQUEID \\ --forwarding-protocol HttpOnly \\ --https-redirect Disabled \\ --origin-group 'afdorigin' \\ --profile-name $AFD_NAME \\ --resource-group $ARORG \\ --route-name 'aro-mine-route' \\ --supported-protocols Http Https \\ --patterns-to-match '/*' \\ --custom-domains $AFD_MINE_CUSTOM_DOMAIN_NAME Update DNS Get a validation token from Front Door so Front Door can validate your domain bash afdToken=$(az afd custom-domain show \\ --resource-group $ARORG \\ --profile-name $AFD_NAME \\ --custom-domain-name $AFD_MINE_CUSTOM_DOMAIN_NAME \\ --query \"validationProperties.validationToken\") Create a DNS Zone bash az network dns zone create -g $ARORG -n $DOMAIN >You will need to configure your nameservers to point to azure. The output of running this zone create will show you the nameservers for this record that you will need to set up within your domain registrar. Create a new text record in your DNS server bash az network dns record-set txt add-record -g $ARORG -z $DOMAIN -n _dnsauth.$(echo $ARO_APP_FQDN | sed 's/\\..*//') --value $afdToken --record-set-name _dnsauth.$(echo $ARO_APP_FQDN | sed 's/\\..*//') Check if the domain has been validated: Note this can take several hours Your FQDN will not resolve until Front Door validates your domain. bash az afd custom-domain list -g $ARORG --profile-name $AFD_NAME --query \"[? contains(hostName, '$ARO_APP_FQDN')].domainValidationState\" Add a CNAME record to DNS Get the Azure Front Door endpoint: bash afdEndpoint=$(az afd endpoint show -g $ARORG --profile-name $AFD_NAME --endpoint-name aro-mine-$UNIQUEID --query \"hostName\" -o tsv) Create a cname record for the application bash az network dns record-set cname set-record -g $ARORG -z $DOMAIN \\ -n $(echo $ARO_APP_FQDN | sed 's/\\..*//') -z $DOMAIN -c $afdEndpoint","title":"Create and Configure an instance of Azure Front Door"},{"location":"aro/frontdoor/#deploy-an-application","text":"Now the fun part, let's deploy an application! We will be deploying a Java based application called microsweeper . This is an application that runs on OpenShift and uses a PostgreSQL database to store scores. With ARO being a first class service on Azure, we will create an Azure Database for PostgreSQL service and connect it to our cluster with a private endpoint. Create a Azure Database for PostgreSQL servers service ```bash az postgres server create --name microsweeper-database --resource-group $ARORG --location $LOCATION --admin-user quarkus --admin-password r3dh4t1! --sku-name GP_Gen5_2 POSTGRES_ID=$(az postgres server show -n microsweeper-database -g $ARORG --query 'id' -o tsv) ``` Create a private endpoint connection for the database ```bash az network vnet subnet create \\ --resource-group $ARORG \\ --vnet-name $VNET_NAME \\ --name $PRIVATEENDPOINTSUBNET_NAME \\ --address-prefixes $PRIVATEENDPOINTSUBNET_PREFIX \\ --disable-private-endpoint-network-policies true az network private-endpoint create \\ --name 'postgresPvtEndpoint' \\ --resource-group $ARORG \\ --vnet-name $VNET_NAME \\ --subnet $PRIVATEENDPOINTSUBNET_NAME \\ --private-connection-resource-id $POSTGRES_ID \\ --group-id 'postgresqlServer' \\ --connection-name 'postgresdbConnection' ``` 1. Create and configure a private DNS Zone for the Postgres database ```bash az network private-dns zone create \\ --resource-group $ARORG \\ --name 'privatelink.postgres.database.azure.com' az network private-dns link vnet create \\ --resource-group $ARORG \\ --zone-name 'privatelink.postgres.database.azure.com' \\ --name 'PostgresDNSLink' \\ --virtual-network $VNET_NAME \\ --registration-enabled false az network private-endpoint dns-zone-group create \\ --resource-group $ARORG \\ --name 'PostgresDb-ZoneGroup' \\ --endpoint-name 'postgresPvtEndpoint' \\ --private-dns-zone 'privatelink.postgres.database.azure.com' \\ --zone-name 'postgresqlServer' NETWORK_INTERFACE_ID=$(az network private-endpoint show --name postgresPvtEndpoint --resource-group $ARORG --query 'networkInterfaces[0].id' -o tsv) POSTGRES_IP=$(az resource show --ids $NETWORK_INTERFACE_ID --api-version 2019-04-01 --query 'properties.ipConfigurations[0].properties.privateIPAddress' -o tsv) az network private-dns record-set a create --name $UNIQUEID-microsweeper-database --zone-name privatelink.postgres.database.azure.com --resource-group $ARORG az network private-dns record-set a add-record --record-set-name $UNIQUEID-microsweeper-database --zone-name privatelink.postgres.database.azure.com --resource-group $ARORG -a $POSTGRES_IP ``` Create a postgres database that will contain scores for the minesweeper application bash az postgres db create \\ --resource-group $ARORG \\ --name score \\ --server-name microsweeper-database","title":"Deploy an application"},{"location":"aro/frontdoor/#deploy-the-minesweeper-application","text":"Clone the git repository bash git clone -b ARO https://github.com/redhat-mw-demos/microsweeper-quarkus.git change to the root directory bash cd microsweeper-quarkus Ensure Java 1.8 is set at your Java version bash mvn --version Look for Java version - 1.8XXXX if not set to Java 1.8 you will need to set your JAVA_HOME variable to Java 1.8 you have installed. To find your java versions run: bash java -version then export your JAVA_HOME variable bash export JAVA_HOME=`/usr/libexec/java_home -v 1.8.0_332` Log into your openshift cluster Before you deploy your application, you will need to be connected to a private network that has access to the cluster. A great way to establish this connectity is with a VPN connection. Follow this guide to setup a VPN connection with your Azure account. ```bash kubeadmin_password=$(az aro list-credentials --name $AROCLUSTER --resource-group $ARORG --query kubeadminPassword --output tsv) apiServer=$(az aro show -g $ARORG -n $AROCLUSTER --query apiserverProfile.url -o tsv) oc login $apiServer -u kubeadmin -p $kubeadmin_password ``` Create a new OpenShift Project bash oc new-project minesweeper add the openshift extension to quarkus bash quarkus ext add openshift Edit microsweeper-quarkus/src/main/resources/application.properties Make sure your file looks like the one below, changing the IP address on line 3 to the private ip address of your postgres instance. To find your Postgres private IP address run the following commands: ```bash NETWORK_INTERFACE_ID=$(az network private-endpoint show --name postgresPvtEndpoint --resource-group $ARORG --query 'networkInterfaces[0].id' -o tsv) az resource show --ids $NETWORK_INTERFACE_ID --api-version 2019-04-01 --query 'properties.ipConfigurations[0].properties.privateIPAddress' -o tsv ``` Sample microsweeper-quarkus/src/main/resources/application.properties ``` # Database configurations %prod.quarkus.datasource.db-kind=postgresql %prod.quarkus.datasource.jdbc.url=jdbc:postgresql://10.1.6.9:5432/score %prod.quarkus.datasource.jdbc.driver=org.postgresql.Driver %prod.quarkus.datasource.username=quarkus@microsweeper-database %prod.quarkus.datasource.password=r3dh4t1! %prod.quarkus.hibernate-orm.database.generation=drop-and-create %prod.quarkus.hibernate-orm.database.generation=update # OpenShift configurations %prod.quarkus.kubernetes-client.trust-certs=true %prod.quarkus.kubernetes.deploy=true %prod.quarkus.kubernetes.deployment-target=openshift #%prod.quarkus.kubernetes.deployment-target=knative %prod.quarkus.openshift.build-strategy=docker #%prod.quarkus.openshift.expose=true # Serverless configurations #%prod.quarkus.container-image.group=microsweeper-%prod.quarkus #%prod.quarkus.container-image.registry=image-registry.openshift-image-registry.svc:5000 # macOS configurations #%prod.quarkus.native.container-build=true ``` Build and deploy the quarkus application to OpenShift bash quarkus build --no-tests Create a route to your custom domain Change the snippet below replacing your hostname for the host: bash cat << EOF | oc apply -f - apiVersion: route.openshift.io/v1 kind: Route metadata: labels: app.kubernetes.io/name: microsweeper-appservice app.kubernetes.io/version: 1.0.0-SNAPSHOT app.openshift.io/runtime: quarkus name: microsweeper-appservice namespace: minesweeper spec: host: minesweeper.aro.kmobb.com to: kind: Service name: microsweeper-appservice weight: 100 targetPort: port: 8080 wildcardPolicy: None EOF Check the dns settings of your application. notice that the application URL is routed through Azure Front Door at the edge. The only way this application that is running on your cluster can be access is through Azure Front Door which is connected to your cluster through a private endpoint. bash nslookup $ARO_APP_FQDN sample output: ``` Server: 2600:1700:850:d220::1 Address: 2600:1700:850:d220::1#53 Non-authoritative answer: minesweeper.aro.kmobb.com canonical name = aro-mine-13947-dxh0ahd7fzfyexgx.z01.azurefd.net. aro-mine-13947-dxh0ahd7fzfyexgx.z01.azurefd.net canonical name = star-azurefd-prod.trafficmanager.net. star-azurefd-prod.trafficmanager.net canonical name = dual.part-0013.t-0009.t-msedge.net. dual.part-0013.t-0009.t-msedge.net canonical name = part-0013.t-0009.t-msedge.net. Name: part-0013.t-0009.t-msedge.net Address: 13.107.213.41 Name: part-0013.t-0009.t-msedge.net Address: 13.107.246.41 ```","title":"Deploy the minesweeper application"},{"location":"aro/frontdoor/#test-the-application","text":"Point your broswer to your domain!!","title":"Test the application"},{"location":"aro/frontdoor/#clean-up","text":"To clean up everything you created, simply delete the resource group az group delete -g $ARORG","title":"Clean up"},{"location":"aro/gpu/","text":"ARO with Nvidia GPU Workloads ARO guide to running Nvidia GPU workloads. Author: Byron Miller , Stuart Kirk Table of Contents Do not remove this line (it will not be displayed) {:toc} Prerequisites oc cli jq, moreutils, and gettext package ARO 4.10 If you need to install an ARO cluster, please read our ARO Quick start guide . Please be sure if you're installing or using an existing ARO cluster that it is 4.10.x or higher. As of OpenShift 4.10, it is no longer necessary to set up entitlements to use the nVidia Operator. This has greatly simplified the setup of the cluster for GPU workloads. Linux: sudo dnf install jq moreutils gettext MacOS brew install jq moreutils gettext Helm Prerequisites If you plan to use Helm to deploy the GPU operator, you will need do the following Add the MOBB chart repository to your Helm bash helm repo add mobb https://rh-mobb.github.io/helm-charts/ Update your repositories bash helm repo update GPU Quota All GPU quotas in Azure are 0 by default. You will need to login to the azure portal and request GPU quota. There is a lot of competition for GPU workers, so you may have to provision an ARO cluster in a region where you can actually reserve GPU. ARO supports the following GPU workers: * NC4as T4 v3 * NC8as T4 v3 * NC16as T4 v3 * NC464as T4 v3 Please remember that when you request quota that Azure is per core. To request a single NC4as T4 v3 node, you will need to request quota in groups of 4. If you wish to request an NC16as T4 v3 you will need to request quota of 16. Login to azure Login to portal.azure.com , type \"quotas\" in search by, click on Compute and in the search box type \"NCAsv3_T4\". Select the region your cluster is in (select checkbox) and then click Request quota increase and ask for quota (I chose 8 so i can build two demo clusters of NC4as T4s). Configure quota Log in to your ARO cluster Login to OpenShift - we'll use the kubeadmin account here but you can login with your user account as long as you have cluster-admin. bash oc login <apiserver> -u kubeadmin -p <kubeadminpass> Pull secret (Conditional) We'll update our pull secret to make sure that we can install operators as well as connect to cloud.redhat.com. If you have already re-created a full pull secret with cloud.redhat.com enabled you can skip this step Using Helm Before Deploying the chart you need it to adopt the existing pull secret bash kubectl -n openshift-config annotate secret \\ pull-secret meta.helm.sh/release-name=pull-secret kubectl -n openshift-config annotate secret \\ pull-secret meta.helm.sh/release-namespace=openshift-config kubectl -n openshift-config label secret \\ pull-secret app.kubernetes.io/managed-by=Helm Download your new pull secret from https://console.redhat.com/openshift/downloads -> Tokens -> Pull secret and use it to update create the pull secret in your cluster. Update the pull secret This chart will merge the in-cluster pull secret with the new pull secret. helm upgrade --install pull-secret mobb/aro-pull-secret \\ -n openshift-config --set-file pullSecret=$HOME/Downloads/pull-secret.txt Enable Operator Hub bash oc patch configs.samples.operator.openshift.io cluster --type=merge \\ -p='{\"spec\":{\"managementState\":\"Managed\"}}' oc patch operatorhub cluster --type=merge \\ -p='{\"spec\":{\"sources\":[ {\"name\":\"redhat-operators\",\"disabled\":false}, {\"name\":\"certified-operators\",\"disabled\":false}, {\"name\":\"community-operators\",\"disabled\":false}, {\"name\":\"redhat-marketplace\",\"disabled\":false} ]}}' Skip to GPU Machine Set Manually Log into cloud.redhat.com Browse to https://cloud.redhat.com/openshift/install/azure/aro-provisioned click the Download pull secret button and save it as pull-secret.txt The following steps will need to be ran in the same working directory as your pull-secret.txt Export existing pull secret bash oc get secret pull-secret -n openshift-config -o json | jq -r '.data.\".dockerconfigjson\"' | base64 --decode > export-pull.json Merge downloaded pull secret with system pull secret to add cloud.redhat.com bash jq -s '.[0] * .[1]' export-pull.json pull-secret.txt | tr -d \"\\n\\r\" > new-pull-secret.json Upload new secret file bash oc set data secret/pull-secret -n openshift-config --from-file=.dockerconfigjson=new-pull-secret.json You may need to wait for about ~1hr for everything to sync up with cloud.redhat.com. Delete secrets bash rm pull-secret.txt export-pull.json new-pull-secret.json GPU Machine Set ARO still uses Kubernetes Machinsets to create a machine set. I'm going to export the first machine set in my cluster (az 1) and use that as a template to build a single GPU machine in southcentralus region 1. Helm Create a new machine-set (replicas of 1), see the Chart's values file for configuration options helm upgrade --install -n openshift-machine-api \\ gpu mobb/aro-gpu Wait for the new GPU nodes to be available bash watch oc get machines Skip to Install Nvidia GPU Operator Manually View existing machine sets For ease of set up, I'm going to grab the first machine set and use that as the one I will clone to create our GPU machine set. bash MACHINESET=$(oc get machineset -n openshift-machine-api -o=jsonpath='{.items[0]}' | jq -r '[.metadata.name] | @tsv') Save a copy of example machine set bash oc get machineset -n openshift-machine-api $MACHINESET -o json > gpu_machineset.json Change the .metadata.name field to a new unique name I'm going to create a unique name for this single node machine set that shows nvidia-worker- that follows a similar pattern as all the other machine sets. bash jq '.metadata.name = \"nvidia-worker-southcentralus1\"' gpu_machineset.json| sponge gpu_machineset.json Ensure spec.replicas matches the desired replica count for the MachineSet bash jq '.spec.replicas = 1' gpu_machineset.json| sponge gpu_machineset.json Change the .spec.selector.matchLabels.machine.openshift.io/cluster-api-machineset field to match the .metadata.name field bash jq '.spec.selector.matchLabels.\"machine.openshift.io/cluster-api-machineset\" = \"nvidia-worker-southcentralus1\"' gpu_machineset.json| sponge gpu_machineset.json Change the .spec.template.metadata.labels.machine.openshift.io/cluster-api-machineset to match the .metadata.name field bash jq '.spec.template.metadata.labels.\"machine.openshift.io/cluster-api-machineset\" = \"nvidia-worker-southcentralus1\"' gpu_machineset.json| sponge gpu_machineset.json Change the spec.template.spec.providerSpec.value.vmSize to match the desired GPU instance type from Azure. The machine we're using is Standard_NC4as_T4_v3. bash jq '.spec.template.spec.providerSpec.value.vmSize = \"Standard_NC4as_T4_v3\"' gpu_machineset.json | sponge gpu_machineset.json Change the spec.template.spec.providerSpec.value.zone to match the desired zone from Azure bash jq '.spec.template.spec.providerSpec.value.zone = \"1\"' gpu_machineset.json | sponge gpu_machineset.json Delete the .status section of the yaml file bash jq 'del(.status)' gpu_machineset.json | sponge gpu_machineset.json Verify the other data in the yaml file. Create GPU machine set These steps will create the new GPU machine. It may take 10-15 minutes to provision a new GPU machine. If this step fails, please login to the azure portal and ensure you didn't run across availability issues. You can go \"Virtual Machines\" and search for the worker name you created above to see the status of VMs. Create GPU Machine set bash oc create -f gpu_machineset.json This command will take a few minutes to complete. Verify GPU machine set Machines should be getting deployed. You can view the status of the machine set with the following commands bash oc get machineset -n openshift-machine-api oc get machine -n openshift-machine-api Once the machines are provisioned, which could take 5-15 minutes, machines will show as nodes in the node list. bash oc get nodes You should see a node with the \"nvidia-worker-southcentralus1\" name it we created earlier. Install Nvidia GPU Operator This will create the nvidia-gpu-operator name space, set up the operator group and install the Nvidia GPU Operator. Helm Create namespaces bash oc create namespace openshift-nfd oc create namespace nvidia-gpu-operator Use the mobb/operatorhub chart to deploy the needed operators bash helm upgrade -n nvidia-gpu-operator nvidia-gpu-operator \\ mobb/operatorhub --install \\ --values https://raw.githubusercontent.com/rh-mobb/helm-charts/main/charts/nvidia-gpu/files/operatorhub.yaml Wait until the two operators are running bash watch kubectl get pods -n openshift-nfd NAME READY STATUS RESTARTS AGE nfd-controller-manager-7b66c67bd9-rk98w 2/2 Running 0 47s bash watch oc get pods -n nvidia-gpu-operator NAME READY STATUS RESTARTS AGE gpu-operator-5d8cb7dd5f-c4ljk 1/1 Running 0 87s Install the Nvidia GPU Operator chart ```bash bash helm upgrade --install -n nvidia-gpu-operator nvidia-gpu \\ mobb/nvidia-gpu --disable-openapi-validation Skip to Validate GPU Manually Create Nvidia namespace yaml cat <<EOF | oc apply -f - apiVersion: v1 kind: Namespace metadata: name: nvidia-gpu-operator EOF Create Operator Group yaml cat <<EOF | oc apply -f - apiVersion: operators.coreos.com/v1 kind: OperatorGroup metadata: name: nvidia-gpu-operator-group namespace: nvidia-gpu-operator spec: targetNamespaces: - nvidia-gpu-operator EOF Get latest nvidia channel bash CHANNEL=$(oc get packagemanifest gpu-operator-certified -n openshift-marketplace -o jsonpath='{.status.defaultChannel}') Get latest nvidia package bash PACKAGE=$(oc get packagemanifests/gpu-operator-certified -n openshift-marketplace -ojson | jq -r '.status.channels[] | select(.name == \"'$CHANNEL'\") | .currentCSV') Create Subscription yaml envsubst <<EOF | oc apply -f - apiVersion: operators.coreos.com/v1alpha1 kind: Subscription metadata: name: gpu-operator-certified namespace: nvidia-gpu-operator spec: channel: \"$CHANNEL\" installPlanApproval: Automatic name: gpu-operator-certified source: certified-operators sourceNamespace: openshift-marketplace startingCSV: \"$PACKAGE\" EOF Wait for Operator to finish installing Don't proceed until you have verified that the operator has finished installing. It's also a good point to ensure that your GPU worker is online. Install Node Feature Discovery Operator The node feature discovery operator will discover the GPU on your nodes and appropriately label the nodes so you can target them for workloads. We'll install the NFD operator into the opneshift-ndf namespace and create the \"subscription\" which is the configuration for NFD. Official Documentation for Installing Node Feature Discovery Operator Set up Name Space yaml cat <<EOF | oc apply -f - apiVersion: v1 kind: Namespace metadata: name: openshift-nfd EOF Create OperatorGroup yaml cat <<EOF | oc apply -f - apiVersion: operators.coreos.com/v1 kind: OperatorGroup metadata: generateName: openshift-nfd- name: openshift-nfd namespace: openshift-nfd EOF Create Subscription yaml cat <<EOF | oc apply -f - apiVersion: operators.coreos.com/v1alpha1 kind: Subscription metadata: name: nfd namespace: openshift-nfd spec: channel: \"stable\" installPlanApproval: Automatic name: nfd source: redhat-operators sourceNamespace: openshift-marketplace EOF 1. Wait for Node Feature discovery to complete installation You can login to your openshift console and view operators or simply wait a few minutes. The next step will error until the operator has finished installing. Create NFD Instance yaml cat <<EOF | oc apply -f - kind: NodeFeatureDiscovery apiVersion: nfd.openshift.io/v1 metadata: name: nfd-instance namespace: openshift-nfd spec: customConfig: configData: | # - name: \"more.kernel.features\" # matchOn: # - loadedKMod: [\"example_kmod3\"] # - name: \"more.features.by.nodename\" # value: customValue # matchOn: # - nodename: [\"special-.*-node-.*\"] operand: image: >- registry.redhat.io/openshift4/ose-node-feature-discovery@sha256:07658ef3df4b264b02396e67af813a52ba416b47ab6e1d2d08025a350ccd2b7b servicePort: 12000 workerConfig: configData: | core: # labelWhiteList: # noPublish: false sleepInterval: 60s # sources: [all] # klog: # addDirHeader: false # alsologtostderr: false # logBacktraceAt: # logtostderr: true # skipHeaders: false # stderrthreshold: 2 # v: 0 # vmodule: ## NOTE: the following options are not dynamically run-time ## configurable and require a nfd-worker restart to take effect ## after being changed # logDir: # logFile: # logFileMaxSize: 1800 # skipLogHeaders: false sources: # cpu: # cpuid: ## NOTE: whitelist has priority over blacklist # attributeBlacklist: # - \"BMI1\" # - \"BMI2\" # - \"CLMUL\" # - \"CMOV\" # - \"CX16\" # - \"ERMS\" # - \"F16C\" # - \"HTT\" # - \"LZCNT\" # - \"MMX\" # - \"MMXEXT\" # - \"NX\" # - \"POPCNT\" # - \"RDRAND\" # - \"RDSEED\" # - \"RDTSCP\" # - \"SGX\" # - \"SSE\" # - \"SSE2\" # - \"SSE3\" # - \"SSE4.1\" # - \"SSE4.2\" # - \"SSSE3\" # attributeWhitelist: # kernel: # kconfigFile: \"/path/to/kconfig\" # configOpts: # - \"NO_HZ\" # - \"X86\" # - \"DMI\" pci: deviceClassWhitelist: - \"0200\" - \"03\" - \"12\" deviceLabelFields: # - \"class\" - \"vendor\" # - \"device\" # - \"subsystem_vendor\" # - \"subsystem_device\" # usb: # deviceClassWhitelist: # - \"0e\" # - \"ef\" # - \"fe\" # - \"ff\" # deviceLabelFields: # - \"class\" # - \"vendor\" # - \"device\" # custom: # - name: \"my.kernel.feature\" # matchOn: # - loadedKMod: [\"example_kmod1\", \"example_kmod2\"] # - name: \"my.pci.feature\" # matchOn: # - pciId: # class: [\"0200\"] # vendor: [\"15b3\"] # device: [\"1014\", \"1017\"] # - pciId : # vendor: [\"8086\"] # device: [\"1000\", \"1100\"] # - name: \"my.usb.feature\" # matchOn: # - usbId: # class: [\"ff\"] # vendor: [\"03e7\"] # device: [\"2485\"] # - usbId: # class: [\"fe\"] # vendor: [\"1a6e\"] # device: [\"089a\"] # - name: \"my.combined.feature\" # matchOn: # - pciId: # vendor: [\"15b3\"] # device: [\"1014\", \"1017\"] # loadedKMod : [\"vendor_kmod1\", \"vendor_kmod2\"] EOF Verify NFD is ready. This operator should say Available in the status Apply nVidia Cluster Config We'll now apply the nvidia cluster config. Please read the nvidia documentation on customizing this if you have your own private repos or specific settings. This will be another process that takes a few minutes to complete. Apply cluster config yaml cat <<EOF | oc apply -f - apiVersion: nvidia.com/v1 kind: ClusterPolicy metadata: name: gpu-cluster-policy spec: migManager: enabled: true operator: defaultRuntime: crio initContainer: {} runtimeClass: nvidia deployGFD: true dcgm: enabled: true gfd: {} dcgmExporter: config: name: '' driver: licensingConfig: nlsEnabled: false configMapName: '' certConfig: name: '' kernelModuleConfig: name: '' repoConfig: configMapName: '' virtualTopology: config: '' enabled: true use_ocp_driver_toolkit: true devicePlugin: {} mig: strategy: single validator: plugin: env: - name: WITH_WORKLOAD value: 'true' nodeStatusExporter: enabled: true daemonsets: {} toolkit: enabled: true EOF Verify Cluster Policy Login to OpenShift console and browse to operators and make sure you're in nvidia-gpu-operator namespace. You should see it say State: Ready once everything is complete. Validate GPU It may take some time for the nVidia Operator and NFD to completely install and self-identify the machines. These commands can be ran to help validate that everything is running as expected. Verify NFD can see your GPU(s) bash oc describe node | egrep 'Roles|pci-10de' | grep -v master You should see output like: bash Roles: worker feature.node.kubernetes.io/pci-10de.present=true Verify node labels You can see the node labels by logging into the OpenShift console -> Compute -> Nodes -> nvidia-worker-southcentralus1- . You should see a bunch of nvidia GPU labels and the pci-10de device from above. Nvidia SMI tool verification bash oc project nvidia-gpu-operator for i in $(oc get pod -lopenshift.driver-toolkit=true --no-headers |awk '{print $1}'); do echo $i; oc exec -it $i -- nvidia-smi ; echo -e '\\n' ; done You should see output that shows the GPUs available on the host such as this example screenshot. (Varies depending on GPU worker type) Create Pod to run a GPU workload yaml oc project nvidia-gpu-operator cat <<EOF | oc apply -f - apiVersion: v1 kind: Pod metadata: name: cuda-vector-add spec: restartPolicy: OnFailure containers: - name: cuda-vector-add image: \"quay.io/giantswarm/nvidia-gpu-demo:latest\" resources: limits: nvidia.com/gpu: 1 nodeSelector: nvidia.com/gpu.present: true EOF View logs bash oc logs cuda-vector-add --tail=-1 Please note, if you get an error \"Error from server (BadRequest): container \"cuda-vector-add\" in pod \"cuda-vector-add\" is waiting to start: ContainerCreating\" try running \"oc delete pod cuda-vector-add\" and then re-run the create statement above. I've seen issues where if this step is ran before all of the operator consolidation is done it may just sit there. You should see Output like the following (mary vary depending on GPU): bash [Vector addition of 5000 elements] Copy input data from the host memory to the CUDA device CUDA kernel launch with 196 blocks of 256 threads Copy output data from the CUDA device to the host memory Test PASSED Done If successful, the pod can be deleted bash oc delete pod cuda-vector-add","title":"ARO for GPU Workloads"},{"location":"aro/gpu/#aro-with-nvidia-gpu-workloads","text":"ARO guide to running Nvidia GPU workloads. Author: Byron Miller , Stuart Kirk","title":"ARO with Nvidia GPU Workloads"},{"location":"aro/gpu/#table-of-contents","text":"Do not remove this line (it will not be displayed) {:toc}","title":"Table of Contents"},{"location":"aro/gpu/#prerequisites","text":"oc cli jq, moreutils, and gettext package ARO 4.10 If you need to install an ARO cluster, please read our ARO Quick start guide . Please be sure if you're installing or using an existing ARO cluster that it is 4.10.x or higher. As of OpenShift 4.10, it is no longer necessary to set up entitlements to use the nVidia Operator. This has greatly simplified the setup of the cluster for GPU workloads. Linux: sudo dnf install jq moreutils gettext MacOS brew install jq moreutils gettext","title":"Prerequisites"},{"location":"aro/gpu/#helm-prerequisites","text":"If you plan to use Helm to deploy the GPU operator, you will need do the following Add the MOBB chart repository to your Helm bash helm repo add mobb https://rh-mobb.github.io/helm-charts/ Update your repositories bash helm repo update","title":"Helm Prerequisites"},{"location":"aro/gpu/#gpu-quota","text":"All GPU quotas in Azure are 0 by default. You will need to login to the azure portal and request GPU quota. There is a lot of competition for GPU workers, so you may have to provision an ARO cluster in a region where you can actually reserve GPU. ARO supports the following GPU workers: * NC4as T4 v3 * NC8as T4 v3 * NC16as T4 v3 * NC464as T4 v3 Please remember that when you request quota that Azure is per core. To request a single NC4as T4 v3 node, you will need to request quota in groups of 4. If you wish to request an NC16as T4 v3 you will need to request quota of 16. Login to azure Login to portal.azure.com , type \"quotas\" in search by, click on Compute and in the search box type \"NCAsv3_T4\". Select the region your cluster is in (select checkbox) and then click Request quota increase and ask for quota (I chose 8 so i can build two demo clusters of NC4as T4s). Configure quota","title":"GPU Quota"},{"location":"aro/gpu/#log-in-to-your-aro-cluster","text":"Login to OpenShift - we'll use the kubeadmin account here but you can login with your user account as long as you have cluster-admin. bash oc login <apiserver> -u kubeadmin -p <kubeadminpass>","title":"Log in to your ARO cluster"},{"location":"aro/gpu/#pull-secret-conditional","text":"We'll update our pull secret to make sure that we can install operators as well as connect to cloud.redhat.com. If you have already re-created a full pull secret with cloud.redhat.com enabled you can skip this step","title":"Pull secret (Conditional)"},{"location":"aro/gpu/#using-helm","text":"Before Deploying the chart you need it to adopt the existing pull secret bash kubectl -n openshift-config annotate secret \\ pull-secret meta.helm.sh/release-name=pull-secret kubectl -n openshift-config annotate secret \\ pull-secret meta.helm.sh/release-namespace=openshift-config kubectl -n openshift-config label secret \\ pull-secret app.kubernetes.io/managed-by=Helm Download your new pull secret from https://console.redhat.com/openshift/downloads -> Tokens -> Pull secret and use it to update create the pull secret in your cluster. Update the pull secret This chart will merge the in-cluster pull secret with the new pull secret. helm upgrade --install pull-secret mobb/aro-pull-secret \\ -n openshift-config --set-file pullSecret=$HOME/Downloads/pull-secret.txt Enable Operator Hub bash oc patch configs.samples.operator.openshift.io cluster --type=merge \\ -p='{\"spec\":{\"managementState\":\"Managed\"}}' oc patch operatorhub cluster --type=merge \\ -p='{\"spec\":{\"sources\":[ {\"name\":\"redhat-operators\",\"disabled\":false}, {\"name\":\"certified-operators\",\"disabled\":false}, {\"name\":\"community-operators\",\"disabled\":false}, {\"name\":\"redhat-marketplace\",\"disabled\":false} ]}}' Skip to GPU Machine Set","title":"Using Helm"},{"location":"aro/gpu/#manually","text":"Log into cloud.redhat.com Browse to https://cloud.redhat.com/openshift/install/azure/aro-provisioned click the Download pull secret button and save it as pull-secret.txt The following steps will need to be ran in the same working directory as your pull-secret.txt Export existing pull secret bash oc get secret pull-secret -n openshift-config -o json | jq -r '.data.\".dockerconfigjson\"' | base64 --decode > export-pull.json Merge downloaded pull secret with system pull secret to add cloud.redhat.com bash jq -s '.[0] * .[1]' export-pull.json pull-secret.txt | tr -d \"\\n\\r\" > new-pull-secret.json Upload new secret file bash oc set data secret/pull-secret -n openshift-config --from-file=.dockerconfigjson=new-pull-secret.json You may need to wait for about ~1hr for everything to sync up with cloud.redhat.com. Delete secrets bash rm pull-secret.txt export-pull.json new-pull-secret.json","title":"Manually"},{"location":"aro/gpu/#gpu-machine-set","text":"ARO still uses Kubernetes Machinsets to create a machine set. I'm going to export the first machine set in my cluster (az 1) and use that as a template to build a single GPU machine in southcentralus region 1.","title":"GPU Machine Set"},{"location":"aro/gpu/#helm","text":"Create a new machine-set (replicas of 1), see the Chart's values file for configuration options helm upgrade --install -n openshift-machine-api \\ gpu mobb/aro-gpu Wait for the new GPU nodes to be available bash watch oc get machines Skip to Install Nvidia GPU Operator","title":"Helm"},{"location":"aro/gpu/#manually_1","text":"View existing machine sets For ease of set up, I'm going to grab the first machine set and use that as the one I will clone to create our GPU machine set. bash MACHINESET=$(oc get machineset -n openshift-machine-api -o=jsonpath='{.items[0]}' | jq -r '[.metadata.name] | @tsv') Save a copy of example machine set bash oc get machineset -n openshift-machine-api $MACHINESET -o json > gpu_machineset.json Change the .metadata.name field to a new unique name I'm going to create a unique name for this single node machine set that shows nvidia-worker- that follows a similar pattern as all the other machine sets. bash jq '.metadata.name = \"nvidia-worker-southcentralus1\"' gpu_machineset.json| sponge gpu_machineset.json Ensure spec.replicas matches the desired replica count for the MachineSet bash jq '.spec.replicas = 1' gpu_machineset.json| sponge gpu_machineset.json Change the .spec.selector.matchLabels.machine.openshift.io/cluster-api-machineset field to match the .metadata.name field bash jq '.spec.selector.matchLabels.\"machine.openshift.io/cluster-api-machineset\" = \"nvidia-worker-southcentralus1\"' gpu_machineset.json| sponge gpu_machineset.json Change the .spec.template.metadata.labels.machine.openshift.io/cluster-api-machineset to match the .metadata.name field bash jq '.spec.template.metadata.labels.\"machine.openshift.io/cluster-api-machineset\" = \"nvidia-worker-southcentralus1\"' gpu_machineset.json| sponge gpu_machineset.json Change the spec.template.spec.providerSpec.value.vmSize to match the desired GPU instance type from Azure. The machine we're using is Standard_NC4as_T4_v3. bash jq '.spec.template.spec.providerSpec.value.vmSize = \"Standard_NC4as_T4_v3\"' gpu_machineset.json | sponge gpu_machineset.json Change the spec.template.spec.providerSpec.value.zone to match the desired zone from Azure bash jq '.spec.template.spec.providerSpec.value.zone = \"1\"' gpu_machineset.json | sponge gpu_machineset.json Delete the .status section of the yaml file bash jq 'del(.status)' gpu_machineset.json | sponge gpu_machineset.json Verify the other data in the yaml file.","title":"Manually"},{"location":"aro/gpu/#create-gpu-machine-set","text":"These steps will create the new GPU machine. It may take 10-15 minutes to provision a new GPU machine. If this step fails, please login to the azure portal and ensure you didn't run across availability issues. You can go \"Virtual Machines\" and search for the worker name you created above to see the status of VMs. Create GPU Machine set bash oc create -f gpu_machineset.json This command will take a few minutes to complete. Verify GPU machine set Machines should be getting deployed. You can view the status of the machine set with the following commands bash oc get machineset -n openshift-machine-api oc get machine -n openshift-machine-api Once the machines are provisioned, which could take 5-15 minutes, machines will show as nodes in the node list. bash oc get nodes You should see a node with the \"nvidia-worker-southcentralus1\" name it we created earlier.","title":"Create GPU machine set"},{"location":"aro/gpu/#install-nvidia-gpu-operator","text":"This will create the nvidia-gpu-operator name space, set up the operator group and install the Nvidia GPU Operator.","title":"Install Nvidia GPU Operator"},{"location":"aro/gpu/#helm_1","text":"Create namespaces bash oc create namespace openshift-nfd oc create namespace nvidia-gpu-operator Use the mobb/operatorhub chart to deploy the needed operators bash helm upgrade -n nvidia-gpu-operator nvidia-gpu-operator \\ mobb/operatorhub --install \\ --values https://raw.githubusercontent.com/rh-mobb/helm-charts/main/charts/nvidia-gpu/files/operatorhub.yaml Wait until the two operators are running bash watch kubectl get pods -n openshift-nfd NAME READY STATUS RESTARTS AGE nfd-controller-manager-7b66c67bd9-rk98w 2/2 Running 0 47s bash watch oc get pods -n nvidia-gpu-operator NAME READY STATUS RESTARTS AGE gpu-operator-5d8cb7dd5f-c4ljk 1/1 Running 0 87s Install the Nvidia GPU Operator chart ```bash bash helm upgrade --install -n nvidia-gpu-operator nvidia-gpu \\ mobb/nvidia-gpu --disable-openapi-validation Skip to Validate GPU","title":"Helm"},{"location":"aro/gpu/#manually_2","text":"Create Nvidia namespace yaml cat <<EOF | oc apply -f - apiVersion: v1 kind: Namespace metadata: name: nvidia-gpu-operator EOF Create Operator Group yaml cat <<EOF | oc apply -f - apiVersion: operators.coreos.com/v1 kind: OperatorGroup metadata: name: nvidia-gpu-operator-group namespace: nvidia-gpu-operator spec: targetNamespaces: - nvidia-gpu-operator EOF Get latest nvidia channel bash CHANNEL=$(oc get packagemanifest gpu-operator-certified -n openshift-marketplace -o jsonpath='{.status.defaultChannel}') Get latest nvidia package bash PACKAGE=$(oc get packagemanifests/gpu-operator-certified -n openshift-marketplace -ojson | jq -r '.status.channels[] | select(.name == \"'$CHANNEL'\") | .currentCSV') Create Subscription yaml envsubst <<EOF | oc apply -f - apiVersion: operators.coreos.com/v1alpha1 kind: Subscription metadata: name: gpu-operator-certified namespace: nvidia-gpu-operator spec: channel: \"$CHANNEL\" installPlanApproval: Automatic name: gpu-operator-certified source: certified-operators sourceNamespace: openshift-marketplace startingCSV: \"$PACKAGE\" EOF Wait for Operator to finish installing Don't proceed until you have verified that the operator has finished installing. It's also a good point to ensure that your GPU worker is online.","title":"Manually"},{"location":"aro/gpu/#install-node-feature-discovery-operator","text":"The node feature discovery operator will discover the GPU on your nodes and appropriately label the nodes so you can target them for workloads. We'll install the NFD operator into the opneshift-ndf namespace and create the \"subscription\" which is the configuration for NFD. Official Documentation for Installing Node Feature Discovery Operator Set up Name Space yaml cat <<EOF | oc apply -f - apiVersion: v1 kind: Namespace metadata: name: openshift-nfd EOF Create OperatorGroup yaml cat <<EOF | oc apply -f - apiVersion: operators.coreos.com/v1 kind: OperatorGroup metadata: generateName: openshift-nfd- name: openshift-nfd namespace: openshift-nfd EOF Create Subscription yaml cat <<EOF | oc apply -f - apiVersion: operators.coreos.com/v1alpha1 kind: Subscription metadata: name: nfd namespace: openshift-nfd spec: channel: \"stable\" installPlanApproval: Automatic name: nfd source: redhat-operators sourceNamespace: openshift-marketplace EOF 1. Wait for Node Feature discovery to complete installation You can login to your openshift console and view operators or simply wait a few minutes. The next step will error until the operator has finished installing. Create NFD Instance yaml cat <<EOF | oc apply -f - kind: NodeFeatureDiscovery apiVersion: nfd.openshift.io/v1 metadata: name: nfd-instance namespace: openshift-nfd spec: customConfig: configData: | # - name: \"more.kernel.features\" # matchOn: # - loadedKMod: [\"example_kmod3\"] # - name: \"more.features.by.nodename\" # value: customValue # matchOn: # - nodename: [\"special-.*-node-.*\"] operand: image: >- registry.redhat.io/openshift4/ose-node-feature-discovery@sha256:07658ef3df4b264b02396e67af813a52ba416b47ab6e1d2d08025a350ccd2b7b servicePort: 12000 workerConfig: configData: | core: # labelWhiteList: # noPublish: false sleepInterval: 60s # sources: [all] # klog: # addDirHeader: false # alsologtostderr: false # logBacktraceAt: # logtostderr: true # skipHeaders: false # stderrthreshold: 2 # v: 0 # vmodule: ## NOTE: the following options are not dynamically run-time ## configurable and require a nfd-worker restart to take effect ## after being changed # logDir: # logFile: # logFileMaxSize: 1800 # skipLogHeaders: false sources: # cpu: # cpuid: ## NOTE: whitelist has priority over blacklist # attributeBlacklist: # - \"BMI1\" # - \"BMI2\" # - \"CLMUL\" # - \"CMOV\" # - \"CX16\" # - \"ERMS\" # - \"F16C\" # - \"HTT\" # - \"LZCNT\" # - \"MMX\" # - \"MMXEXT\" # - \"NX\" # - \"POPCNT\" # - \"RDRAND\" # - \"RDSEED\" # - \"RDTSCP\" # - \"SGX\" # - \"SSE\" # - \"SSE2\" # - \"SSE3\" # - \"SSE4.1\" # - \"SSE4.2\" # - \"SSSE3\" # attributeWhitelist: # kernel: # kconfigFile: \"/path/to/kconfig\" # configOpts: # - \"NO_HZ\" # - \"X86\" # - \"DMI\" pci: deviceClassWhitelist: - \"0200\" - \"03\" - \"12\" deviceLabelFields: # - \"class\" - \"vendor\" # - \"device\" # - \"subsystem_vendor\" # - \"subsystem_device\" # usb: # deviceClassWhitelist: # - \"0e\" # - \"ef\" # - \"fe\" # - \"ff\" # deviceLabelFields: # - \"class\" # - \"vendor\" # - \"device\" # custom: # - name: \"my.kernel.feature\" # matchOn: # - loadedKMod: [\"example_kmod1\", \"example_kmod2\"] # - name: \"my.pci.feature\" # matchOn: # - pciId: # class: [\"0200\"] # vendor: [\"15b3\"] # device: [\"1014\", \"1017\"] # - pciId : # vendor: [\"8086\"] # device: [\"1000\", \"1100\"] # - name: \"my.usb.feature\" # matchOn: # - usbId: # class: [\"ff\"] # vendor: [\"03e7\"] # device: [\"2485\"] # - usbId: # class: [\"fe\"] # vendor: [\"1a6e\"] # device: [\"089a\"] # - name: \"my.combined.feature\" # matchOn: # - pciId: # vendor: [\"15b3\"] # device: [\"1014\", \"1017\"] # loadedKMod : [\"vendor_kmod1\", \"vendor_kmod2\"] EOF Verify NFD is ready. This operator should say Available in the status","title":"Install Node Feature Discovery Operator"},{"location":"aro/gpu/#apply-nvidia-cluster-config","text":"We'll now apply the nvidia cluster config. Please read the nvidia documentation on customizing this if you have your own private repos or specific settings. This will be another process that takes a few minutes to complete. Apply cluster config yaml cat <<EOF | oc apply -f - apiVersion: nvidia.com/v1 kind: ClusterPolicy metadata: name: gpu-cluster-policy spec: migManager: enabled: true operator: defaultRuntime: crio initContainer: {} runtimeClass: nvidia deployGFD: true dcgm: enabled: true gfd: {} dcgmExporter: config: name: '' driver: licensingConfig: nlsEnabled: false configMapName: '' certConfig: name: '' kernelModuleConfig: name: '' repoConfig: configMapName: '' virtualTopology: config: '' enabled: true use_ocp_driver_toolkit: true devicePlugin: {} mig: strategy: single validator: plugin: env: - name: WITH_WORKLOAD value: 'true' nodeStatusExporter: enabled: true daemonsets: {} toolkit: enabled: true EOF Verify Cluster Policy Login to OpenShift console and browse to operators and make sure you're in nvidia-gpu-operator namespace. You should see it say State: Ready once everything is complete.","title":"Apply nVidia Cluster Config"},{"location":"aro/gpu/#validate-gpu","text":"It may take some time for the nVidia Operator and NFD to completely install and self-identify the machines. These commands can be ran to help validate that everything is running as expected. Verify NFD can see your GPU(s) bash oc describe node | egrep 'Roles|pci-10de' | grep -v master You should see output like: bash Roles: worker feature.node.kubernetes.io/pci-10de.present=true Verify node labels You can see the node labels by logging into the OpenShift console -> Compute -> Nodes -> nvidia-worker-southcentralus1- . You should see a bunch of nvidia GPU labels and the pci-10de device from above. Nvidia SMI tool verification bash oc project nvidia-gpu-operator for i in $(oc get pod -lopenshift.driver-toolkit=true --no-headers |awk '{print $1}'); do echo $i; oc exec -it $i -- nvidia-smi ; echo -e '\\n' ; done You should see output that shows the GPUs available on the host such as this example screenshot. (Varies depending on GPU worker type) Create Pod to run a GPU workload yaml oc project nvidia-gpu-operator cat <<EOF | oc apply -f - apiVersion: v1 kind: Pod metadata: name: cuda-vector-add spec: restartPolicy: OnFailure containers: - name: cuda-vector-add image: \"quay.io/giantswarm/nvidia-gpu-demo:latest\" resources: limits: nvidia.com/gpu: 1 nodeSelector: nvidia.com/gpu.present: true EOF View logs bash oc logs cuda-vector-add --tail=-1 Please note, if you get an error \"Error from server (BadRequest): container \"cuda-vector-add\" in pod \"cuda-vector-add\" is waiting to start: ContainerCreating\" try running \"oc delete pod cuda-vector-add\" and then re-run the create statement above. I've seen issues where if this step is ran before all of the operator consolidation is done it may just sit there. You should see Output like the following (mary vary depending on GPU): bash [Vector addition of 5000 elements] Copy input data from the host memory to the CUDA device CUDA kernel launch with 196 blocks of 256 threads Copy output data from the CUDA device to the host memory Test PASSED Done If successful, the pod can be deleted bash oc delete pod cuda-vector-add","title":"Validate GPU"},{"location":"aro/ibm-cloud-paks-for-data/","text":"ARO IBM Cloud Paks 4 Data A Quickstart guide to deploying an Azure Red Hat OpenShift cluster with IBM Cloud Paks 4 Data. Author: [Kristopher White] Video Walkthrough If you prefer a more visual medium, you can watch [Kristopher White] walk through this quickstart on YouTube . Prerequisites Azure CLI Obviously you'll need to have an Azure account to configure the CLI against. MacOS See Azure Docs for alternative install options. Install Azure CLI using homebrew bash brew update && brew install azure-cli Linux See Azure Docs for alternative install options. Import the Microsoft Keys bash sudo rpm --import https://packages.microsoft.com/keys/microsoft.asc Add the Microsoft Yum Repository bash cat << EOF | sudo tee /etc/yum.repos.d/azure-cli.repo [azure-cli] name=Azure CLI baseurl=https://packages.microsoft.com/yumrepos/azure-cli enabled=1 gpgcheck=1 gpgkey=https://packages.microsoft.com/keys/microsoft.asc EOF Install Azure CLI bash sudo dnf install -y azure-cli Prepare Azure Account for Azure OpenShift Log into the Azure CLI by running the following and then authorizing through your Web Browser bash az login Make sure you have enough Quota (change the location if you're not using East US ) bash az vm list-usage --location \"East US\" -o table see Addendum - Adding Quota to ARO account if you have less than 36 Quota left for Total Regional vCPUs . Register resource providers bash az provider register -n Microsoft.RedHatOpenShift --wait az provider register -n Microsoft.Compute --wait az provider register -n Microsoft.Storage --wait az provider register -n Microsoft.Authorization --wait Get Red Hat pull secret This step is optional, but highly recommended Log into https://console.redhat.com Browse to https://console.redhat.com/openshift/install/azure/aro-provisioned click the Download pull secret button and remember where you saved it, you'll reference it later. Deploy Azure OpenShift Variables and Resource Group Set some environment variables to use later, and create an Azure Resource Group. Set the following environment variables Change the values to suit your environment, but these defaults should work. bash AZR_RESOURCE_LOCATION=eastus AZR_RESOURCE_GROUP=openshift AZR_CLUSTER=cluster AZR_PULL_SECRET=~/Downloads/pull-secret.txt Create an Azure resource group bash az group create \\ --name $AZR_RESOURCE_GROUP \\ --location $AZR_RESOURCE_LOCATION Networking Create a virtual network with two empty subnets Create virtual network bash az network vnet create \\ --address-prefixes 10.0.0.0/22 \\ --name \"$AZR_CLUSTER-aro-vnet-$AZR_RESOURCE_LOCATION\" \\ --resource-group $AZR_RESOURCE_GROUP Create control plane subnet bash az network vnet subnet create \\ --resource-group $AZR_RESOURCE_GROUP \\ --vnet-name \"$AZR_CLUSTER-aro-vnet-$AZR_RESOURCE_LOCATION\" \\ --name \"$AZR_CLUSTER-aro-control-subnet-$AZR_RESOURCE_LOCATION\" \\ --address-prefixes 10.0.0.0/23 \\ --service-endpoints Microsoft.ContainerRegistry Create machine subnet bash az network vnet subnet create \\ --resource-group $AZR_RESOURCE_GROUP \\ --vnet-name \"$AZR_CLUSTER-aro-vnet-$AZR_RESOURCE_LOCATION\" \\ --name \"$AZR_CLUSTER-aro-machine-subnet-$AZR_RESOURCE_LOCATION\" \\ --address-prefixes 10.0.2.0/23 \\ --service-endpoints Microsoft.ContainerRegistry Disable network policies on the control plane subnet This is required for the service to be able to connect to and manage the cluster. bash az network vnet subnet update \\ --name \"$AZR_CLUSTER-aro-control-subnet-$AZR_RESOURCE_LOCATION\" \\ --resource-group $AZR_RESOURCE_GROUP \\ --vnet-name \"$AZR_CLUSTER-aro-vnet-$AZR_RESOURCE_LOCATION\" \\ --disable-private-link-service-network-policies true Create the cluster This will take between 30 and 45 minutes. bash az aro create \\ --resource-group $AZR_RESOURCE_GROUP \\ --name $AZR_CLUSTER \\ --vnet \"$AZR_CLUSTER-aro-vnet-$AZR_RESOURCE_LOCATION\" \\ --master-subnet \"$AZR_CLUSTER-aro-control-subnet-$AZR_RESOURCE_LOCATION\" \\ --worker-subnet \"$AZR_CLUSTER-aro-machine-subnet-$AZR_RESOURCE_LOCATION\" \\ --worker-vm-size Standard_D16s_v3 \\ --pull-secret @$AZR_PULL_SECRET Get OpenShift console URL bash az aro show \\ --name $AZR_CLUSTER \\ --resource-group $AZR_RESOURCE_GROUP \\ -o tsv --query consoleProfile Get OpenShift credentials bash az aro list-credentials \\ --name $AZR_CLUSTER \\ --resource-group $AZR_RESOURCE_GROUP \\ -o tsv Use the URL and the credentials provided by the output of the last two commands to log into OpenShift via a web browser. Cloud Paks 4 Data Operator Setup Adding IBM Operator Catalog to Openshift Log into the OpenShift web console with your OpenShift cluster admin credentials. In the top banner, click the plus (+) icon to open the Import YAML dialog box. Paste this resource definition into the dialog box: bash apiVersion: operators.coreos.com/v1alpha1 kind: CatalogSource metadata: name: ibm-operator-catalog namespace: openshift-marketplace spec: displayName: IBM Operator Catalog image: 'icr.io/cpopen/ibm-operator-catalog:latest' publisher: IBM sourceType: grpc updateStrategy: registryPoll: interval: 45m 1. Click Create . IBM Cloud Paks 4 Data Operator Install Log into the OpenShift web console with your OpenShift cluster admin credentials. Make sure you have selected the Administrator view. Click Operators > OperatorHub > Integration & Delivery . Search for and click the tile for the IBM Cloud Pak for Integration operator. Click Install . In the Install Operator pane: Select the latest update channel. Select the option to install Cloud Pak for Integration in one namespace or for all namespaces on your cluster . If in doubt, choose the All namespaces on the cluster installation mode, and accept the default Installed Namespace . Select the Automatic approval strategy. Click Install . Successful Install Delete Cluster Once you're done its a good idea to delete the cluster to ensure that you don't get a surprise bill. Delete the cluster bash az aro delete -y \\ --resource-group $AZR_RESOURCE_GROUP \\ --name $AZR_CLUSTER Delete the Azure resource group Only do this if there's nothing else in the resource group. bash az group delete -y \\ --name $AZR_RESOURCE_GROUP","title":"ARO IBM Cloud Paks 4 Data"},{"location":"aro/ibm-cloud-paks-for-data/#aro-ibm-cloud-paks-4-data","text":"A Quickstart guide to deploying an Azure Red Hat OpenShift cluster with IBM Cloud Paks 4 Data. Author: [Kristopher White]","title":"ARO IBM Cloud Paks 4 Data"},{"location":"aro/ibm-cloud-paks-for-data/#video-walkthrough","text":"If you prefer a more visual medium, you can watch [Kristopher White] walk through this quickstart on YouTube .","title":"Video Walkthrough"},{"location":"aro/ibm-cloud-paks-for-data/#prerequisites","text":"","title":"Prerequisites"},{"location":"aro/ibm-cloud-paks-for-data/#azure-cli","text":"Obviously you'll need to have an Azure account to configure the CLI against. MacOS See Azure Docs for alternative install options. Install Azure CLI using homebrew bash brew update && brew install azure-cli Linux See Azure Docs for alternative install options. Import the Microsoft Keys bash sudo rpm --import https://packages.microsoft.com/keys/microsoft.asc Add the Microsoft Yum Repository bash cat << EOF | sudo tee /etc/yum.repos.d/azure-cli.repo [azure-cli] name=Azure CLI baseurl=https://packages.microsoft.com/yumrepos/azure-cli enabled=1 gpgcheck=1 gpgkey=https://packages.microsoft.com/keys/microsoft.asc EOF Install Azure CLI bash sudo dnf install -y azure-cli","title":"Azure CLI"},{"location":"aro/ibm-cloud-paks-for-data/#prepare-azure-account-for-azure-openshift","text":"Log into the Azure CLI by running the following and then authorizing through your Web Browser bash az login Make sure you have enough Quota (change the location if you're not using East US ) bash az vm list-usage --location \"East US\" -o table see Addendum - Adding Quota to ARO account if you have less than 36 Quota left for Total Regional vCPUs . Register resource providers bash az provider register -n Microsoft.RedHatOpenShift --wait az provider register -n Microsoft.Compute --wait az provider register -n Microsoft.Storage --wait az provider register -n Microsoft.Authorization --wait","title":"Prepare Azure Account for Azure OpenShift"},{"location":"aro/ibm-cloud-paks-for-data/#get-red-hat-pull-secret","text":"This step is optional, but highly recommended Log into https://console.redhat.com Browse to https://console.redhat.com/openshift/install/azure/aro-provisioned click the Download pull secret button and remember where you saved it, you'll reference it later.","title":"Get Red Hat pull secret"},{"location":"aro/ibm-cloud-paks-for-data/#deploy-azure-openshift","text":"","title":"Deploy Azure OpenShift"},{"location":"aro/ibm-cloud-paks-for-data/#variables-and-resource-group","text":"Set some environment variables to use later, and create an Azure Resource Group. Set the following environment variables Change the values to suit your environment, but these defaults should work. bash AZR_RESOURCE_LOCATION=eastus AZR_RESOURCE_GROUP=openshift AZR_CLUSTER=cluster AZR_PULL_SECRET=~/Downloads/pull-secret.txt Create an Azure resource group bash az group create \\ --name $AZR_RESOURCE_GROUP \\ --location $AZR_RESOURCE_LOCATION","title":"Variables and Resource Group"},{"location":"aro/ibm-cloud-paks-for-data/#networking","text":"Create a virtual network with two empty subnets Create virtual network bash az network vnet create \\ --address-prefixes 10.0.0.0/22 \\ --name \"$AZR_CLUSTER-aro-vnet-$AZR_RESOURCE_LOCATION\" \\ --resource-group $AZR_RESOURCE_GROUP Create control plane subnet bash az network vnet subnet create \\ --resource-group $AZR_RESOURCE_GROUP \\ --vnet-name \"$AZR_CLUSTER-aro-vnet-$AZR_RESOURCE_LOCATION\" \\ --name \"$AZR_CLUSTER-aro-control-subnet-$AZR_RESOURCE_LOCATION\" \\ --address-prefixes 10.0.0.0/23 \\ --service-endpoints Microsoft.ContainerRegistry Create machine subnet bash az network vnet subnet create \\ --resource-group $AZR_RESOURCE_GROUP \\ --vnet-name \"$AZR_CLUSTER-aro-vnet-$AZR_RESOURCE_LOCATION\" \\ --name \"$AZR_CLUSTER-aro-machine-subnet-$AZR_RESOURCE_LOCATION\" \\ --address-prefixes 10.0.2.0/23 \\ --service-endpoints Microsoft.ContainerRegistry Disable network policies on the control plane subnet This is required for the service to be able to connect to and manage the cluster. bash az network vnet subnet update \\ --name \"$AZR_CLUSTER-aro-control-subnet-$AZR_RESOURCE_LOCATION\" \\ --resource-group $AZR_RESOURCE_GROUP \\ --vnet-name \"$AZR_CLUSTER-aro-vnet-$AZR_RESOURCE_LOCATION\" \\ --disable-private-link-service-network-policies true Create the cluster This will take between 30 and 45 minutes. bash az aro create \\ --resource-group $AZR_RESOURCE_GROUP \\ --name $AZR_CLUSTER \\ --vnet \"$AZR_CLUSTER-aro-vnet-$AZR_RESOURCE_LOCATION\" \\ --master-subnet \"$AZR_CLUSTER-aro-control-subnet-$AZR_RESOURCE_LOCATION\" \\ --worker-subnet \"$AZR_CLUSTER-aro-machine-subnet-$AZR_RESOURCE_LOCATION\" \\ --worker-vm-size Standard_D16s_v3 \\ --pull-secret @$AZR_PULL_SECRET Get OpenShift console URL bash az aro show \\ --name $AZR_CLUSTER \\ --resource-group $AZR_RESOURCE_GROUP \\ -o tsv --query consoleProfile Get OpenShift credentials bash az aro list-credentials \\ --name $AZR_CLUSTER \\ --resource-group $AZR_RESOURCE_GROUP \\ -o tsv Use the URL and the credentials provided by the output of the last two commands to log into OpenShift via a web browser.","title":"Networking"},{"location":"aro/ibm-cloud-paks-for-data/#cloud-paks-4-data-operator-setup","text":"","title":"Cloud Paks 4 Data Operator Setup"},{"location":"aro/ibm-cloud-paks-for-data/#adding-ibm-operator-catalog-to-openshift","text":"Log into the OpenShift web console with your OpenShift cluster admin credentials. In the top banner, click the plus (+) icon to open the Import YAML dialog box. Paste this resource definition into the dialog box: bash apiVersion: operators.coreos.com/v1alpha1 kind: CatalogSource metadata: name: ibm-operator-catalog namespace: openshift-marketplace spec: displayName: IBM Operator Catalog image: 'icr.io/cpopen/ibm-operator-catalog:latest' publisher: IBM sourceType: grpc updateStrategy: registryPoll: interval: 45m 1. Click Create .","title":"Adding IBM Operator Catalog to Openshift"},{"location":"aro/ibm-cloud-paks-for-data/#ibm-cloud-paks-4-data-operator-install","text":"Log into the OpenShift web console with your OpenShift cluster admin credentials. Make sure you have selected the Administrator view. Click Operators > OperatorHub > Integration & Delivery . Search for and click the tile for the IBM Cloud Pak for Integration operator. Click Install . In the Install Operator pane: Select the latest update channel. Select the option to install Cloud Pak for Integration in one namespace or for all namespaces on your cluster . If in doubt, choose the All namespaces on the cluster installation mode, and accept the default Installed Namespace . Select the Automatic approval strategy. Click Install .","title":"IBM Cloud Paks 4 Data Operator Install"},{"location":"aro/ibm-cloud-paks-for-data/#successful-install","text":"","title":"Successful Install"},{"location":"aro/ibm-cloud-paks-for-data/#delete-cluster","text":"Once you're done its a good idea to delete the cluster to ensure that you don't get a surprise bill. Delete the cluster bash az aro delete -y \\ --resource-group $AZR_RESOURCE_GROUP \\ --name $AZR_CLUSTER Delete the Azure resource group Only do this if there's nothing else in the resource group. bash az group delete -y \\ --name $AZR_RESOURCE_GROUP","title":"Delete Cluster"},{"location":"aro/managed-upgrade-operator/","text":"Enable the Managed Upgrade Operator in ARO and schedule Upgrades Paul Czarkowski 04/12/2022 Prerequisites an Azure Red Hat OpenShift cluster Get Started Run this oc command to enable the Managed Upgrade Operator (MUO) oc patch cluster.aro.openshift.io cluster --patch \\ '{\"spec\":{\"operatorflags\":{\"rh.srep.muo.enabled\": \"true\",\"rh.srep.muo.managed\": \"true\",\"rh.srep.muo.deploy.pullspec\":\"arosvc.azurecr.io/managed-upgrade-operator@sha256:f57615aa690580a12c1e5031ad7ea674ce249c3d0f54e6dc4d070e42a9c9a274\"}}}' \\ --type=merge Wait a few moments to ensure the Management Upgrade Operator is ready bash oc -n openshift-managed-upgrade-operator \\ get deployment managed-upgrade-operator NAME READY UP-TO-DATE AVAILABLE AGE managed-upgrade-operator 1/1 1 1 2m2s Configure the Managed Upgrade Operator cat << EOF | oc apply -f - apiVersion: v1 kind: ConfigMap metadata: name: managed-upgrade-operator-config namespace: openshift-managed-upgrade-operator data: config.yaml: | configManager: source: LOCAL localConfigName: managed-upgrade-config watchInterval: 1 maintenance: controlPlaneTime: 90 ignoredAlerts: controlPlaneCriticals: - ClusterOperatorDown - ClusterOperatorDegraded upgradeWindow: delayTrigger: 30 timeOut: 120 nodeDrain: timeOut: 45 expectedNodeDrainTime: 8 scale: timeOut: 30 healthCheck: ignoredCriticals: - PrometheusRuleFailures - CannotRetrieveUpdates - FluentdNodeDown ignoredNamespaces: - openshift-logging - openshift-redhat-marketplace - openshift-operators - openshift-user-workload-monitoring - openshift-pipelines EOF Restart the Managed Upgrade Operator oc -n openshift-managed-upgrade-operator \\ scale deployment managed-upgrade-operator --replicas=0 oc -n openshift-managed-upgrade-operator \\ scale deployment managed-upgrade-operator --replicas=1 Look for available Upgrades If there output is nil there are no available upgrades and you cannot continue. bash oc get clusterversion version -o jsonpath='{.status.availableUpdates}' Schedule an Upgrade Set the Channel and Version to the desired values from the above list of available upgrades. bash cat << EOF | oc apply -f - apiVersion: upgrade.managed.openshift.io/v1alpha1 kind: UpgradeConfig metadata: name: managed-upgrade-config namespace: openshift-managed-upgrade-operator spec: type: \"ARO\" upgradeAt: $(date -u --iso-8601=seconds --date \"+5 minutes\") PDBForceDrainTimeout: 60 capacityReservation: false desired: channel: \"stable-4.9\" version: \"4.9.27\" EOF Check the status of the scheduled upgrade bash oc -n openshift-managed-upgrade-operator get \\ upgradeconfigs.upgrade.managed.openshift.io \\ managed-upgrade-config -o jsonpath='{.status}' | jq *The output of this command should show upgrades in progress* ``` { \"history\": [ { \"conditions\": [ { \"lastProbeTime\": \"2022-04-12T14:42:02Z\", \"lastTransitionTime\": \"2022-04-12T14:16:44Z\", \"message\": \"ControlPlaneUpgraded still in progress\", \"reason\": \"ControlPlaneUpgraded not done\", \"startTime\": \"2022-04-12T14:16:44Z\", \"status\": \"False\", \"type\": \"ControlPlaneUpgraded\" }, ``` You can verify the upgrade has completed successfully via the following oc get clusterversion version NAME VERSION AVAILABLE PROGRESSING SINCE STATUS version 4.9.27 True False 161m Cluster version is 4.9.27","title":"Enable the Managed Upgrade Operator in ARO and schedule Upgrades"},{"location":"aro/managed-upgrade-operator/#enable-the-managed-upgrade-operator-in-aro-and-schedule-upgrades","text":"Paul Czarkowski 04/12/2022","title":"Enable the Managed Upgrade Operator in ARO and schedule Upgrades"},{"location":"aro/managed-upgrade-operator/#prerequisites","text":"an Azure Red Hat OpenShift cluster","title":"Prerequisites"},{"location":"aro/managed-upgrade-operator/#get-started","text":"Run this oc command to enable the Managed Upgrade Operator (MUO) oc patch cluster.aro.openshift.io cluster --patch \\ '{\"spec\":{\"operatorflags\":{\"rh.srep.muo.enabled\": \"true\",\"rh.srep.muo.managed\": \"true\",\"rh.srep.muo.deploy.pullspec\":\"arosvc.azurecr.io/managed-upgrade-operator@sha256:f57615aa690580a12c1e5031ad7ea674ce249c3d0f54e6dc4d070e42a9c9a274\"}}}' \\ --type=merge Wait a few moments to ensure the Management Upgrade Operator is ready bash oc -n openshift-managed-upgrade-operator \\ get deployment managed-upgrade-operator NAME READY UP-TO-DATE AVAILABLE AGE managed-upgrade-operator 1/1 1 1 2m2s Configure the Managed Upgrade Operator cat << EOF | oc apply -f - apiVersion: v1 kind: ConfigMap metadata: name: managed-upgrade-operator-config namespace: openshift-managed-upgrade-operator data: config.yaml: | configManager: source: LOCAL localConfigName: managed-upgrade-config watchInterval: 1 maintenance: controlPlaneTime: 90 ignoredAlerts: controlPlaneCriticals: - ClusterOperatorDown - ClusterOperatorDegraded upgradeWindow: delayTrigger: 30 timeOut: 120 nodeDrain: timeOut: 45 expectedNodeDrainTime: 8 scale: timeOut: 30 healthCheck: ignoredCriticals: - PrometheusRuleFailures - CannotRetrieveUpdates - FluentdNodeDown ignoredNamespaces: - openshift-logging - openshift-redhat-marketplace - openshift-operators - openshift-user-workload-monitoring - openshift-pipelines EOF Restart the Managed Upgrade Operator oc -n openshift-managed-upgrade-operator \\ scale deployment managed-upgrade-operator --replicas=0 oc -n openshift-managed-upgrade-operator \\ scale deployment managed-upgrade-operator --replicas=1 Look for available Upgrades If there output is nil there are no available upgrades and you cannot continue. bash oc get clusterversion version -o jsonpath='{.status.availableUpdates}' Schedule an Upgrade Set the Channel and Version to the desired values from the above list of available upgrades. bash cat << EOF | oc apply -f - apiVersion: upgrade.managed.openshift.io/v1alpha1 kind: UpgradeConfig metadata: name: managed-upgrade-config namespace: openshift-managed-upgrade-operator spec: type: \"ARO\" upgradeAt: $(date -u --iso-8601=seconds --date \"+5 minutes\") PDBForceDrainTimeout: 60 capacityReservation: false desired: channel: \"stable-4.9\" version: \"4.9.27\" EOF Check the status of the scheduled upgrade bash oc -n openshift-managed-upgrade-operator get \\ upgradeconfigs.upgrade.managed.openshift.io \\ managed-upgrade-config -o jsonpath='{.status}' | jq *The output of this command should show upgrades in progress* ``` { \"history\": [ { \"conditions\": [ { \"lastProbeTime\": \"2022-04-12T14:42:02Z\", \"lastTransitionTime\": \"2022-04-12T14:16:44Z\", \"message\": \"ControlPlaneUpgraded still in progress\", \"reason\": \"ControlPlaneUpgraded not done\", \"startTime\": \"2022-04-12T14:16:44Z\", \"status\": \"False\", \"type\": \"ControlPlaneUpgraded\" }, ``` You can verify the upgrade has completed successfully via the following oc get clusterversion version NAME VERSION AVAILABLE PROGRESSING SINCE STATUS version 4.9.27 True False 161m Cluster version is 4.9.27","title":"Get Started"},{"location":"aro/odf/","text":"OpenShift Data Foundation with ARO Kevin Collins 06/28/2022 Note: This guide demonstrates how to setup and configure self-managed OpenShift Data Foundation in Internal Mode on an ARO Cluster and test it out. Prerequisites An Azure Red Hat OpenShift cluster ( verion 4.10+ ) kubectl cli oc cli moreutils (sponge) jq Install compute nodes for ODF A best practice for optimal performance is to run ODF on dedicated nodes with a minimum of one per zone. In this guide, we will be provisioning 3 additional compute nodes, one per zone. Run the following script to create the additional nodes: Log into your ARO Cluster Create the new compute nodes bash for ZONE in 1 2 3 do item=$((ZONE-1)) MACHINESET=$(oc get machineset -n openshift-machine-api -o=jsonpath=\"{.items[$item]}\" | jq -r '[.metadata.name] | @tsv') oc get machineset -n openshift-machine-api $MACHINESET -o json > default_machineset$ZONE.json worker=odf-worker-$ZONE jq \".metadata.name = \\\"$worker\\\"\" default_machineset$ZONE.json | sponge default_machineset$ZONE.json jq '.spec.replicas = 1' default_machineset$ZONE.json| sponge default_machineset$ZONE.json jq \".spec.selector.matchLabels.\\\"machine.openshift.io/cluster-api-machineset\\\" = \\\"$worker\\\"\" default_machineset$ZONE.json| sponge default_machineset$ZONE.json jq \".spec.template.metadata.labels.\\\"machine.openshift.io/cluster-api-machineset\\\" = \\\"$worker\\\"\" default_machineset$ZONE.json| sponge default_machineset$ZONE.json jq '.spec.template.spec.providerSpec.value.vmSize = \"Standard_D16s_v3\"' default_machineset$ZONE.json | sponge default_machineset$ZONE.json jq \".spec.template.spec.providerSpec.value.zone = \\\"$ZONE\\\"\" default_machineset$ZONE.json | sponge default_machineset$ZONE.json jq 'del(.status)' default_machineset$ZONE.json | sponge default_machineset$ZONE.json oc create -f default_machineset$ZONE.json done Label new compute nodes It takes just a couple of minutes for new nodes to appear. Check if the nodes are ready: bash oc get nodes | grep odf-worker expected output: bash odf-worker-1-jg7db Ready worker 19h v1.23.5+3afdacb odf-worker-2-ktvct Ready worker 19h v1.23.5+3afdacb odf-worker-3-rk22b Ready worker 19h v1.23.5+3afdacb Once you see the three nodes, the next step we need to do is label and taint the nodes. This will ensure the OpenShift Data Foundation is installed on these nodes and no other workload will be placed on the nodes. bash for worker in $(oc get nodes | grep odf-worker | awk '{print $1}') do oc label node $worker cluster.ocs.openshift.io/openshift-storage=`` oc adm taint nodes $worker node.ocs.openshift.io/storage=true:NoSchedule done Deploy OpenShift Data Foundation Next, we will install OpenShift Data Foundation via an Operator. Create the openshift-storage namespace bash cat <<EOF | oc apply -f - apiVersion: v1 kind: Namespace metadata: labels: openshift.io/cluster-monitoring: \"true\" name: openshift-storage spec: {} EOF Create the Operator Group for openshift-storage ```bash cat <<EOF | oc apply -f - apiVersion: operators.coreos.com/v1 kind: OperatorGroup metadata: name: openshift-storage-operatorgroup namespace: openshift-storage spec: targetNamespaces: openshift-storage EOF ``` Subscribe to the ocs-operator bash cat <<EOF | oc apply -f - apiVersion: operators.coreos.com/v1alpha1 kind: Subscription metadata: name: ocs-operator namespace: openshift-storage spec: channel: \"stable-4.10\" # <-- Channel should be modified depending on the OCS version to be installed. Please ensure to maintain compatibility with OCP version installPlanApproval: Automatic name: ocs-operator source: redhat-operators # <-- Modify the name of the redhat-operators catalogsource if not default sourceNamespace: openshift-marketplace EOF Subscribe to the odf-operator bash cat <<EOF | oc apply -f - apiVersion: operators.coreos.com/v1alpha1 kind: Subscription metadata: name: odf-operator namespace: openshift-storage spec: channel: \"stable-4.10\" # <-- Channel should be modified depending on the OCS version to be installed. Please ensure to maintain compatibility with OCP version installPlanApproval: Automatic name: odf-operator source: redhat-operators # <-- Modify the name of the redhat-operators catalogsource if not default sourceNamespace: openshift-marketplace EOF Create a Storage Cluster ```bash cat <<EOF | oc apply -f - apiVersion: ocs.openshift.io/v1 kind: StorageCluster metadata: annotations: uninstall.ocs.openshift.io/cleanup-policy: delete uninstall.ocs.openshift.io/mode: graceful generation: 2 name: ocs-storagecluster namespace: openshift-storage spec: storageDeviceSets: config: {} count: 1 dataPVCTemplate: spec: accessModes: ReadWriteOnce resources: requests: storage: 2Ti storageClassName: managed-premium volumeMode: Block name: ocs-deviceset-managed-premium portable: true replica: 3 version: 4.10.0 EOF ``` Validate the install List the cluster service version for the ODF operators bash oc get csv -n openshift-storage verify that the operators below have succeeded. NAME DISPLAY VERSION REPLACES PHASE mcg-operator.v4.10.4 NooBaa Operator 4.10.4 Succeeded ocs-operator.v4.10.4 OpenShift Container Storage 4.10.4 Succeeded odf-operator.v4.10.4 OpenShift Data Foundation 4.10.4 Succeeded Check that the ocs storage classes have been created note: this can take around 5 minutes bash oc get sc NAME PROVISIONER RECLAIMPOLICY VOLUMEBINDINGMODE ALLOWVOLUMEEXPANSION AGE managed-csi disk.csi.azure.com Delete WaitForFirstConsumer true 118m managed-premium (default) kubernetes.io/azure-disk Delete WaitForFirstConsumer true 119m ocs-storagecluster-ceph-rbd openshift-storage.rbd.csi.ceph.com Delete Immediate true 7s ocs-storagecluster-cephfs openshift-storage.cephfs.csi.ceph.com Delete Immediate true 7s Test it out To test out ODF, we will create 'writer' pods on each node across all zones and then a reader pod to read the data that is written. This will prove both regional storage along with \"read write many\" mode is working correctly. Create a new project bash oc new-project odf-demo Create a RWX Persistent Volume Claim for ODF ```bash cat <<EOF | kubectl apply -f - kind: PersistentVolumeClaim apiVersion: v1 metadata: name: standard spec: accessModes: ReadWriteMany resources: requests: storage: 400Gi storageClassName: ocs-storagecluster-cephfs EOF ``` Create writer pods via a DaemonSet Using a deamonset will ensure that we have a 'writer pod' on each worker node and will also prove that we correctly set a taint on the 'ODF Workers' where which we do not want workload to be added to. The writer pods will write out which worker node the pod is running on, the data, and a hello message. bash cat <<EOF | oc apply -f - apiVersion: apps/v1 kind: DaemonSet metadata: name: test-odf labels: app: test-odf spec: selector: matchLabels: name: test-odf template: metadata: labels: name: test-odf spec: containers: - name: azodf image: centos:latest command: [\"sh\", \"-c\"] resources: limits: cpu: 1 memory: \"1Gi\" args: [ \"while true; do printenv --null NODE_NAME | tee -a /mnt/odf-data/verify-odf; echo ' says hello '$(date) | tee -a /mnt/odf-data/verify-odf; sleep 15; done;\", ] volumeMounts: - name: odf-vol mountPath: \"/mnt/odf-data\" env: - name: NODE_NAME valueFrom: fieldRef: fieldPath: spec.nodeName volumes: - name: odf-vol persistentVolumeClaim: claimName: standard EOF Check the writer pods are running. note: there should be 1 pod per non-ODF worker node bash oc get pods expected output NAME READY STATUS RESTARTS AGE IP NODE NOMINATED NODE READINESS GATES test-odf-47p2g 1/1 Running 0 107s 10.128.2.15 aro-kmobb-7zff2-worker-eastus1-xgksq <none> <none> test-odf-p5xk6 1/1 Running 0 107s 10.131.0.18 aro-kmobb-7zff2-worker-eastus3-h4gv7 <none> <none> test-odf-ss8b5 1/1 Running 0 107s 10.129.2.32 aro-kmobb-7zff2-worker-eastus2-sbfpm <none> <none> 1. Create a reader pod The reader pod will simply log data written by the writer pods. bash cat <<EOF | oc apply -f - apiVersion: v1 kind: Pod metadata: name: test-odf-read spec: containers: - name: test-odf-read image: centos:latest command: [\"/bin/bash\", \"-c\", \"--\"] resources: limits: cpu: 1 memory: \"1Gi\" args: [\"tail -f /mnt/odf-data/verify-odf\"] volumeMounts: - name: odf-vol mountPath: \"/mnt/odf-data\" volumes: - name: odf-vol persistentVolumeClaim: claimName: standard EOF Now let's verify the POD is reading from shared volume. bash oc logs test-odf-read aro-kmobb-7zff2-worker-eastus1-xgksq says hello Wed Jun 29 10:41:06 EDT 2022 aro-kmobb-7zff2-worker-eastus3-h4gv7 says hello Wed Jun 29 10:41:06 EDT 2022 aro-kmobb-7zff2-worker-eastus2-sbfpm says hello Wed Jun 29 10:41:06 EDT 2022 aro-kmobb-7zff2-worker-eastus1-xgksq says hello Wed Jun 29 10:41:06 EDT 2022 aro-kmobb-7zff2-worker-eastus3-h4gv7 says hello Wed Jun 29 10:41:06 EDT 2022 aro-kmobb-7zff2-worker-eastus2-sbfpm says hello Wed Jun 29 10:41:06 EDT 2022 aro-kmobb-7zff2-worker-eastus1-xgksq says hello Wed Jun 29 10:41:06 EDT 2022 aro-kmobb-7zff2-worker-eastus3-h4gv7 says hello Wed Jun 29 10:41:06 EDT 2022 Notice that pods in different zones are writing to the PVC which is managed by ODF.","title":"OpenShift Data Foundation with ARO"},{"location":"aro/odf/#openshift-data-foundation-with-aro","text":"Kevin Collins 06/28/2022 Note: This guide demonstrates how to setup and configure self-managed OpenShift Data Foundation in Internal Mode on an ARO Cluster and test it out.","title":"OpenShift Data Foundation with ARO"},{"location":"aro/odf/#prerequisites","text":"An Azure Red Hat OpenShift cluster ( verion 4.10+ ) kubectl cli oc cli moreutils (sponge) jq","title":"Prerequisites"},{"location":"aro/odf/#install-compute-nodes-for-odf","text":"A best practice for optimal performance is to run ODF on dedicated nodes with a minimum of one per zone. In this guide, we will be provisioning 3 additional compute nodes, one per zone. Run the following script to create the additional nodes: Log into your ARO Cluster Create the new compute nodes bash for ZONE in 1 2 3 do item=$((ZONE-1)) MACHINESET=$(oc get machineset -n openshift-machine-api -o=jsonpath=\"{.items[$item]}\" | jq -r '[.metadata.name] | @tsv') oc get machineset -n openshift-machine-api $MACHINESET -o json > default_machineset$ZONE.json worker=odf-worker-$ZONE jq \".metadata.name = \\\"$worker\\\"\" default_machineset$ZONE.json | sponge default_machineset$ZONE.json jq '.spec.replicas = 1' default_machineset$ZONE.json| sponge default_machineset$ZONE.json jq \".spec.selector.matchLabels.\\\"machine.openshift.io/cluster-api-machineset\\\" = \\\"$worker\\\"\" default_machineset$ZONE.json| sponge default_machineset$ZONE.json jq \".spec.template.metadata.labels.\\\"machine.openshift.io/cluster-api-machineset\\\" = \\\"$worker\\\"\" default_machineset$ZONE.json| sponge default_machineset$ZONE.json jq '.spec.template.spec.providerSpec.value.vmSize = \"Standard_D16s_v3\"' default_machineset$ZONE.json | sponge default_machineset$ZONE.json jq \".spec.template.spec.providerSpec.value.zone = \\\"$ZONE\\\"\" default_machineset$ZONE.json | sponge default_machineset$ZONE.json jq 'del(.status)' default_machineset$ZONE.json | sponge default_machineset$ZONE.json oc create -f default_machineset$ZONE.json done Label new compute nodes It takes just a couple of minutes for new nodes to appear. Check if the nodes are ready: bash oc get nodes | grep odf-worker expected output: bash odf-worker-1-jg7db Ready worker 19h v1.23.5+3afdacb odf-worker-2-ktvct Ready worker 19h v1.23.5+3afdacb odf-worker-3-rk22b Ready worker 19h v1.23.5+3afdacb Once you see the three nodes, the next step we need to do is label and taint the nodes. This will ensure the OpenShift Data Foundation is installed on these nodes and no other workload will be placed on the nodes. bash for worker in $(oc get nodes | grep odf-worker | awk '{print $1}') do oc label node $worker cluster.ocs.openshift.io/openshift-storage=`` oc adm taint nodes $worker node.ocs.openshift.io/storage=true:NoSchedule done","title":"Install compute nodes for ODF"},{"location":"aro/odf/#deploy-openshift-data-foundation","text":"Next, we will install OpenShift Data Foundation via an Operator. Create the openshift-storage namespace bash cat <<EOF | oc apply -f - apiVersion: v1 kind: Namespace metadata: labels: openshift.io/cluster-monitoring: \"true\" name: openshift-storage spec: {} EOF Create the Operator Group for openshift-storage ```bash cat <<EOF | oc apply -f - apiVersion: operators.coreos.com/v1 kind: OperatorGroup metadata: name: openshift-storage-operatorgroup namespace: openshift-storage spec: targetNamespaces: openshift-storage EOF ``` Subscribe to the ocs-operator bash cat <<EOF | oc apply -f - apiVersion: operators.coreos.com/v1alpha1 kind: Subscription metadata: name: ocs-operator namespace: openshift-storage spec: channel: \"stable-4.10\" # <-- Channel should be modified depending on the OCS version to be installed. Please ensure to maintain compatibility with OCP version installPlanApproval: Automatic name: ocs-operator source: redhat-operators # <-- Modify the name of the redhat-operators catalogsource if not default sourceNamespace: openshift-marketplace EOF Subscribe to the odf-operator bash cat <<EOF | oc apply -f - apiVersion: operators.coreos.com/v1alpha1 kind: Subscription metadata: name: odf-operator namespace: openshift-storage spec: channel: \"stable-4.10\" # <-- Channel should be modified depending on the OCS version to be installed. Please ensure to maintain compatibility with OCP version installPlanApproval: Automatic name: odf-operator source: redhat-operators # <-- Modify the name of the redhat-operators catalogsource if not default sourceNamespace: openshift-marketplace EOF Create a Storage Cluster ```bash cat <<EOF | oc apply -f - apiVersion: ocs.openshift.io/v1 kind: StorageCluster metadata: annotations: uninstall.ocs.openshift.io/cleanup-policy: delete uninstall.ocs.openshift.io/mode: graceful generation: 2 name: ocs-storagecluster namespace: openshift-storage spec: storageDeviceSets: config: {} count: 1 dataPVCTemplate: spec: accessModes: ReadWriteOnce resources: requests: storage: 2Ti storageClassName: managed-premium volumeMode: Block name: ocs-deviceset-managed-premium portable: true replica: 3 version: 4.10.0 EOF ```","title":"Deploy OpenShift Data Foundation"},{"location":"aro/odf/#validate-the-install","text":"List the cluster service version for the ODF operators bash oc get csv -n openshift-storage verify that the operators below have succeeded. NAME DISPLAY VERSION REPLACES PHASE mcg-operator.v4.10.4 NooBaa Operator 4.10.4 Succeeded ocs-operator.v4.10.4 OpenShift Container Storage 4.10.4 Succeeded odf-operator.v4.10.4 OpenShift Data Foundation 4.10.4 Succeeded Check that the ocs storage classes have been created note: this can take around 5 minutes bash oc get sc NAME PROVISIONER RECLAIMPOLICY VOLUMEBINDINGMODE ALLOWVOLUMEEXPANSION AGE managed-csi disk.csi.azure.com Delete WaitForFirstConsumer true 118m managed-premium (default) kubernetes.io/azure-disk Delete WaitForFirstConsumer true 119m ocs-storagecluster-ceph-rbd openshift-storage.rbd.csi.ceph.com Delete Immediate true 7s ocs-storagecluster-cephfs openshift-storage.cephfs.csi.ceph.com Delete Immediate true 7s","title":"Validate the install"},{"location":"aro/odf/#test-it-out","text":"To test out ODF, we will create 'writer' pods on each node across all zones and then a reader pod to read the data that is written. This will prove both regional storage along with \"read write many\" mode is working correctly. Create a new project bash oc new-project odf-demo Create a RWX Persistent Volume Claim for ODF ```bash cat <<EOF | kubectl apply -f - kind: PersistentVolumeClaim apiVersion: v1 metadata: name: standard spec: accessModes: ReadWriteMany resources: requests: storage: 400Gi storageClassName: ocs-storagecluster-cephfs EOF ``` Create writer pods via a DaemonSet Using a deamonset will ensure that we have a 'writer pod' on each worker node and will also prove that we correctly set a taint on the 'ODF Workers' where which we do not want workload to be added to. The writer pods will write out which worker node the pod is running on, the data, and a hello message. bash cat <<EOF | oc apply -f - apiVersion: apps/v1 kind: DaemonSet metadata: name: test-odf labels: app: test-odf spec: selector: matchLabels: name: test-odf template: metadata: labels: name: test-odf spec: containers: - name: azodf image: centos:latest command: [\"sh\", \"-c\"] resources: limits: cpu: 1 memory: \"1Gi\" args: [ \"while true; do printenv --null NODE_NAME | tee -a /mnt/odf-data/verify-odf; echo ' says hello '$(date) | tee -a /mnt/odf-data/verify-odf; sleep 15; done;\", ] volumeMounts: - name: odf-vol mountPath: \"/mnt/odf-data\" env: - name: NODE_NAME valueFrom: fieldRef: fieldPath: spec.nodeName volumes: - name: odf-vol persistentVolumeClaim: claimName: standard EOF Check the writer pods are running. note: there should be 1 pod per non-ODF worker node bash oc get pods expected output NAME READY STATUS RESTARTS AGE IP NODE NOMINATED NODE READINESS GATES test-odf-47p2g 1/1 Running 0 107s 10.128.2.15 aro-kmobb-7zff2-worker-eastus1-xgksq <none> <none> test-odf-p5xk6 1/1 Running 0 107s 10.131.0.18 aro-kmobb-7zff2-worker-eastus3-h4gv7 <none> <none> test-odf-ss8b5 1/1 Running 0 107s 10.129.2.32 aro-kmobb-7zff2-worker-eastus2-sbfpm <none> <none> 1. Create a reader pod The reader pod will simply log data written by the writer pods. bash cat <<EOF | oc apply -f - apiVersion: v1 kind: Pod metadata: name: test-odf-read spec: containers: - name: test-odf-read image: centos:latest command: [\"/bin/bash\", \"-c\", \"--\"] resources: limits: cpu: 1 memory: \"1Gi\" args: [\"tail -f /mnt/odf-data/verify-odf\"] volumeMounts: - name: odf-vol mountPath: \"/mnt/odf-data\" volumes: - name: odf-vol persistentVolumeClaim: claimName: standard EOF Now let's verify the POD is reading from shared volume. bash oc logs test-odf-read aro-kmobb-7zff2-worker-eastus1-xgksq says hello Wed Jun 29 10:41:06 EDT 2022 aro-kmobb-7zff2-worker-eastus3-h4gv7 says hello Wed Jun 29 10:41:06 EDT 2022 aro-kmobb-7zff2-worker-eastus2-sbfpm says hello Wed Jun 29 10:41:06 EDT 2022 aro-kmobb-7zff2-worker-eastus1-xgksq says hello Wed Jun 29 10:41:06 EDT 2022 aro-kmobb-7zff2-worker-eastus3-h4gv7 says hello Wed Jun 29 10:41:06 EDT 2022 aro-kmobb-7zff2-worker-eastus2-sbfpm says hello Wed Jun 29 10:41:06 EDT 2022 aro-kmobb-7zff2-worker-eastus1-xgksq says hello Wed Jun 29 10:41:06 EDT 2022 aro-kmobb-7zff2-worker-eastus3-h4gv7 says hello Wed Jun 29 10:41:06 EDT 2022 Notice that pods in different zones are writing to the PVC which is managed by ODF.","title":"Test it out"},{"location":"aro/registry/","text":"Accessing the Internal Registry from ARO Kevin Collins 06/28/2022 One of the advantages of using OpenShift is the internal registry that comes with OpenShfit to build, deploy and manage container images locally. By default, access to the registry is limited to the cluster ( by design ) but can be extended to usage outside of the cluster. This guide will go through the steps required to access the OpenShift Registry on an ARO cluster outside of the cluster. Prerequisites an ARO Cluster oc cli podman or docker cli Expose the Registry Expose the registry service bash oc create route reencrypt --service=image-registry -n openshift-image-registry Annotate the route bash oc annotate route image-registry haproxy.router.openshift.io/balance=source -n openshift-image-registry Get the route host name bash HOST=$(oc get route image-registry -n openshift-image-registry --template='{{ .spec.host }}') Log into the image registry bash podman docker login -u $(oc whoami) -p $(oc whoami -t) $HOST Test it out ```bash podman pull openshift/hello-openshift podman images ``` expected output bash openshift/hello-openshift latest 7af3297a3fb4 4 years ago 6.09MB","title":"Accessing the Internal Registry from ARO"},{"location":"aro/registry/#accessing-the-internal-registry-from-aro","text":"Kevin Collins 06/28/2022 One of the advantages of using OpenShift is the internal registry that comes with OpenShfit to build, deploy and manage container images locally. By default, access to the registry is limited to the cluster ( by design ) but can be extended to usage outside of the cluster. This guide will go through the steps required to access the OpenShift Registry on an ARO cluster outside of the cluster.","title":"Accessing the Internal Registry from ARO"},{"location":"aro/registry/#prerequisites","text":"an ARO Cluster oc cli podman or docker cli","title":"Prerequisites"},{"location":"aro/registry/#expose-the-registry","text":"Expose the registry service bash oc create route reencrypt --service=image-registry -n openshift-image-registry Annotate the route bash oc annotate route image-registry haproxy.router.openshift.io/balance=source -n openshift-image-registry Get the route host name bash HOST=$(oc get route image-registry -n openshift-image-registry --template='{{ .spec.host }}') Log into the image registry bash podman docker login -u $(oc whoami) -p $(oc whoami -t) $HOST","title":"Expose the Registry"},{"location":"aro/registry/#test-it-out","text":"```bash podman pull openshift/hello-openshift podman images ``` expected output bash openshift/hello-openshift latest 7af3297a3fb4 4 years ago 6.09MB","title":"Test it out"},{"location":"aro/setup-quay/quay-cli/","text":"Setting up Quay on an ARO cluster using Azure Container Storage Kristopher White x Connor Wooley 07/25/2022 Pre Requisites An ARO cluster oc cli azure cli Steps Create Azure Resources Create Storage Account bash az login az group create --name <resource-group> --location <location> az storage account create --name <storage-account> --resource-group <resource-group> \\ --location eastus --sku Standard_LRS --kind StorageV2 Create Storage Container bash az storage account keys list --account-name <storage_account_name> --resource-group <resource_group> --output yaml Note: this command returns a json by default with your keyName and Values, command above specifies yaml bash az storage container create --name <container_name> --public-access blob \\ --account-name <AZURE_STORAGE_ACCOUNT> --account-key <AZURE_STORAGE_ACCOUNT_KEY> Note: Will need the storage container creds for later use Install Quay-Operator and Create Quay Registry Login to your cluster's OCM Create a sub.yaml file with this template to install the quay operator yaml apiVersion: operators.coreos.com/v1alpha1 kind: Subscription metadata: name: quay-operator namespace: <namespace> spec: channel: <release_channel> name: quay-operator source: redhat-operators sourceNamespace: openshift-marketplace startingCSV: quay-operator.<version> bash oc apply -f sub.yaml 3. Create the Quay Registry 1. Create the Azure Storage Secret Bundle - Create a config.yaml file that injects the azure resource info from the storage container created in step 2 of Create Azure Resources yaml DISTRIBUTED_STORAGE_CONFIG: local_us: - AzureStorage - azure_account_key: <AZURE_STORAGE_ACCOUNT_KEY> azure_account_name: <AZURE_STORAGE_ACCOUNT> azure_container: <AZURE_CONTAINER_NAME> storage_path: /datastorage/registry DISTRIBUTED_STORAGE_DEFAULT_LOCATIONS: - local_us DISTRIBUTED_STORAGE_PREFERENCE: - local_us ```bash oc create secret generic --from-file config.yaml=./config.yaml -n <namespace> <config_bundle_secret_name> ``` Create the Quay Registry with the Secret Create a quayregistry.yaml file with this format yaml apiVersion: quay.redhat.com/v1 kind: QuayRegistry metadata: name: <registry_name> namespace: <namespace> finalizers: - quay-operator/finalizer generation: 3 spec: configBundleSecret: <config_bundle_secret_name> components: - kind: clair managed: true - kind: postgres managed: true - kind: objectstorage managed: false - kind: redis managed: true - kind: horizontalpodautoscaler managed: true - kind: route managed: true - kind: mirror managed: true - kind: monitoring managed: true - kind: tls managed: true - kind: quay managed: true - kind: clairpostgres managed: true bash oc create -n <namespace> -f quayregistry.yaml Login to your Quay Registry and begin pushing images to it! Note: This configuration does not support in-cluster authentication integration with the quay deployment. User Management with the registry is handled by the registry.","title":"Setting up Quay on an ARO cluster using Azure Container Storage"},{"location":"aro/setup-quay/quay-cli/#setting-up-quay-on-an-aro-cluster-using-azure-container-storage","text":"Kristopher White x Connor Wooley 07/25/2022","title":"Setting up Quay on an ARO cluster using Azure Container Storage"},{"location":"aro/setup-quay/quay-cli/#pre-requisites","text":"An ARO cluster oc cli azure cli","title":"Pre Requisites"},{"location":"aro/setup-quay/quay-cli/#steps","text":"","title":"Steps"},{"location":"aro/setup-quay/quay-cli/#create-azure-resources","text":"Create Storage Account bash az login az group create --name <resource-group> --location <location> az storage account create --name <storage-account> --resource-group <resource-group> \\ --location eastus --sku Standard_LRS --kind StorageV2 Create Storage Container bash az storage account keys list --account-name <storage_account_name> --resource-group <resource_group> --output yaml Note: this command returns a json by default with your keyName and Values, command above specifies yaml bash az storage container create --name <container_name> --public-access blob \\ --account-name <AZURE_STORAGE_ACCOUNT> --account-key <AZURE_STORAGE_ACCOUNT_KEY> Note: Will need the storage container creds for later use","title":"Create Azure Resources"},{"location":"aro/setup-quay/quay-cli/#install-quay-operator-and-create-quay-registry","text":"Login to your cluster's OCM Create a sub.yaml file with this template to install the quay operator yaml apiVersion: operators.coreos.com/v1alpha1 kind: Subscription metadata: name: quay-operator namespace: <namespace> spec: channel: <release_channel> name: quay-operator source: redhat-operators sourceNamespace: openshift-marketplace startingCSV: quay-operator.<version> bash oc apply -f sub.yaml 3. Create the Quay Registry 1. Create the Azure Storage Secret Bundle - Create a config.yaml file that injects the azure resource info from the storage container created in step 2 of Create Azure Resources yaml DISTRIBUTED_STORAGE_CONFIG: local_us: - AzureStorage - azure_account_key: <AZURE_STORAGE_ACCOUNT_KEY> azure_account_name: <AZURE_STORAGE_ACCOUNT> azure_container: <AZURE_CONTAINER_NAME> storage_path: /datastorage/registry DISTRIBUTED_STORAGE_DEFAULT_LOCATIONS: - local_us DISTRIBUTED_STORAGE_PREFERENCE: - local_us ```bash oc create secret generic --from-file config.yaml=./config.yaml -n <namespace> <config_bundle_secret_name> ``` Create the Quay Registry with the Secret Create a quayregistry.yaml file with this format yaml apiVersion: quay.redhat.com/v1 kind: QuayRegistry metadata: name: <registry_name> namespace: <namespace> finalizers: - quay-operator/finalizer generation: 3 spec: configBundleSecret: <config_bundle_secret_name> components: - kind: clair managed: true - kind: postgres managed: true - kind: objectstorage managed: false - kind: redis managed: true - kind: horizontalpodautoscaler managed: true - kind: route managed: true - kind: mirror managed: true - kind: monitoring managed: true - kind: tls managed: true - kind: quay managed: true - kind: clairpostgres managed: true bash oc create -n <namespace> -f quayregistry.yaml Login to your Quay Registry and begin pushing images to it! Note: This configuration does not support in-cluster authentication integration with the quay deployment. User Management with the registry is handled by the registry.","title":"Install Quay-Operator and Create Quay Registry"},{"location":"aro/setup-quay/quay-console/","text":"Red Hat Quay setup on ARO (Azure Openshift) A guide to deploying an Azure Red Hat OpenShift Cluster with Red Hat Quay. Author: [Kristopher White x Connor Wooley] Video Walkthrough If you prefer a more visual medium, you can watch [Kristopher White] walk through Quay Registry Storage Setup on YouTube . Red Hat Quay Setup Backend Storage Setup Login to Azure Search/Click Create Resource Groups Name Resource Group > Click Review + Create > Click Create Search/Click Create Storage Accounts Choose Resource Group > Name Storage Account > Choose Region > Choose Performance > Choose Redundancy > Click Review + Create > Click Create Click Go To Resource Go to Data Storage > Click Container > Click New Container > Name Container > Set Privacy to Public Access Blob > Click Create Go to Storage Account > Click Access Keys > Go to key 1 > Click Show Key Storage Account Name , Container Name , and Access Keys will be used to configure quay registry storage. Red Hat Quay Operator Install Log into the OpenShift web console with your OpenShift cluster admin credentials. Make sure you have selected the Administrator view. Click Operators > OperatorHub > Red Hat Quay . Search for and click the tile for the Red Hat Quay operator. Click Install . In the Install Operator pane: Select the latest update channel. Select the option to install Red Hat Quay in one namespace or for all namespaces on your cluster . If in doubt, choose the All namespaces on the cluster installation mode, and accept the default Installed Namespace . Select the Automatic approval strategy. Click Install . Successful Install Redhat Quay Registry Deployment Make sure you have selected the Administrator view. Click Operators > Installed Operators > Red Hat Quay > Quay Registry > Create QuayRegistry . Form View YAML View Click Create > Click Registry Successful Registry Deployment Click Config Editor Credentials Secret Go to Data > Reveal Values (These values are used to login to Config Editor Endpoint ) Go to Registry Console > Click Config Editor Endpoint > Scroll down to Registry Storage > Click Edit Fields > Go to Storage Engine click the drop down and select Azure Blob Storage > Fill in Azure Storage Container with Storage Container Name > Fill in Azure Account Name with Azure Storage Account Name > Fill in Azure Account Key with Azure Storage Account Access Key Click Validate Configuration Changes Click Reconfigure Quay Go to Registry Console > Click Registry Endpoint Click Create Account Login to Quay . Click Create Repository","title":"Quay console"},{"location":"aro/setup-quay/quay-console/#red-hat-quay-setup-on-aro-azure-openshift","text":"A guide to deploying an Azure Red Hat OpenShift Cluster with Red Hat Quay. Author: [Kristopher White x Connor Wooley]","title":"Red Hat Quay setup on ARO (Azure Openshift)"},{"location":"aro/setup-quay/quay-console/#video-walkthrough","text":"If you prefer a more visual medium, you can watch [Kristopher White] walk through Quay Registry Storage Setup on YouTube .","title":"Video Walkthrough"},{"location":"aro/setup-quay/quay-console/#red-hat-quay-setup","text":"","title":"Red Hat Quay Setup"},{"location":"aro/setup-quay/quay-console/#backend-storage-setup","text":"Login to Azure Search/Click Create Resource Groups Name Resource Group > Click Review + Create > Click Create Search/Click Create Storage Accounts Choose Resource Group > Name Storage Account > Choose Region > Choose Performance > Choose Redundancy > Click Review + Create > Click Create Click Go To Resource Go to Data Storage > Click Container > Click New Container > Name Container > Set Privacy to Public Access Blob > Click Create Go to Storage Account > Click Access Keys > Go to key 1 > Click Show Key Storage Account Name , Container Name , and Access Keys will be used to configure quay registry storage.","title":"Backend Storage Setup"},{"location":"aro/setup-quay/quay-console/#red-hat-quay-operator-install","text":"Log into the OpenShift web console with your OpenShift cluster admin credentials. Make sure you have selected the Administrator view. Click Operators > OperatorHub > Red Hat Quay . Search for and click the tile for the Red Hat Quay operator. Click Install . In the Install Operator pane: Select the latest update channel. Select the option to install Red Hat Quay in one namespace or for all namespaces on your cluster . If in doubt, choose the All namespaces on the cluster installation mode, and accept the default Installed Namespace . Select the Automatic approval strategy. Click Install .","title":"Red Hat Quay Operator Install"},{"location":"aro/setup-quay/quay-console/#successful-install","text":"","title":"Successful Install"},{"location":"aro/setup-quay/quay-console/#redhat-quay-registry-deployment","text":"Make sure you have selected the Administrator view. Click Operators > Installed Operators > Red Hat Quay > Quay Registry > Create QuayRegistry . Form View YAML View Click Create > Click Registry Successful Registry Deployment Click Config Editor Credentials Secret Go to Data > Reveal Values (These values are used to login to Config Editor Endpoint ) Go to Registry Console > Click Config Editor Endpoint > Scroll down to Registry Storage > Click Edit Fields > Go to Storage Engine click the drop down and select Azure Blob Storage > Fill in Azure Storage Container with Storage Container Name > Fill in Azure Account Name with Azure Storage Account Name > Fill in Azure Account Key with Azure Storage Account Access Key Click Validate Configuration Changes Click Reconfigure Quay Go to Registry Console > Click Registry Endpoint Click Create Account Login to Quay . Click Create Repository","title":"Redhat Quay Registry Deployment"},{"location":"aro/trident/","text":"Trident NetApp operator setup for Azure NetApp files Byron Miller 05/23/2022 Note: This guide a simple \"happy path\" to show the path of least friction to showcasing how to use NetApp files with Azure Red Hat OpenShift. This may not be the best behavior for any system beyond demonstration purposes. Prerequisites An Azure Red Hat OpenShift cluster installed with Service Principal role/credentials. kubectl cli oc cli helm 3 cli {:target=\"_blank\"} Review official trident documentation {:target=\"_blank\"} In this guide, you will need service principal and region details. Please have these handy. Azure subscriptionID Azure tenantID Azure clientID (Service Principal) Azure clientSecret (Service Principal Secret) Azure Region If you don't have your existing ARO service principal credentials, you can create your own service principal and grant it contributor to be able to manage the required resources. Please review the official Trident documentation {:target=\"_blank\"} regarding Azure NetApp files and required permissions. Important Concepts Persistent Volume Claims are namespaced objects {:target=\"_blank\"}. Mounting RWX/ROX is only possible within the same namespace. NetApp files must be have a delegated subnet within your ARO Vnet's and you must assign it to the Microsoft.Netapp/volumes service. Configure Azure You must first register the Microsoft.NetApp provider and Create a NetApp account on Azure before you can use Azure NetApp Files. Register NetApp files Azure Console {:target=\"_blank\"} or az cli az provider register --namespace Microsoft.NetApp --wait Create storage account Again, for brevity I am using the same RESOURCE_GROUP and Service Principal that the cluster was created with. Azure Console {:target=\"_blank\"} or az cli RESOURCE_GROUP=\"myresourcegroup\" LOCATION=\"southcentralus\" ANF_ACCOUNT_NAME=\"netappfiles\" az netappfiles account create \\ --resource-group $RESOURCE_GROUP \\ --location $LOCATION \\ --account-name $ANF_ACCOUNT_NAME Create capacity pool Creating one pool for now. The common pattern is to expose all three levels with unique pool names respective of each service level. Azure Console {:target=\"_blank\"} or az cli: POOL_NAME=\"Standard\" POOL_SIZE_TiB=4 # Size in Azure CLI needs to be in TiB unit (minimum 4 TiB) SERVICE_LEVEL=\"Standard\" # Valid values are Standard, Premium and Ultra az netappfiles pool create \\ --resource-group $RESOURCE_GROUP \\ --location $LOCATION \\ --account-name $ANF_ACCOUNT_NAME \\ --pool-name $POOL_NAME \\ --size $POOL_SIZE_TiB \\ --service-level $SERVICE_LEVEL Delegate subnet to ARO Login to azure console, find the subnets for your ARO cluster and click add subnet. We need to call this subnet anf.subnet since that is the name we refer to in later configuration. Install Trident Operator Login/Authenticate to ARO Login to your ARO cluster. You can create a token to login via cli straight from the web gui oc login --token=sha256~abcdefghijklmnopqrstuvwxyz --server=https://api.randomseq.eastus.aroapp.io:6443 Helm Install Download latest Trident package wget https://github.com/NetApp/trident/releases/download/v22.04.0/trident-installer-22.04.0.tar.gz Extract tar.gz into working director tar -xzvf trident-installer-22.04.0.tar.gz cd into installer cd trident-installer/helm Helm install helm install trident-operator trident-operator-22.04.0.tgz Example output from installation: W0523 17:45:22.189592 30478 warnings.go:70] policy/v1beta1 PodSecurityPolicy is deprecated in v1.21+, unavailable in v1.25+ W0523 17:45:22.484071 30478 warnings.go:70] policy/v1beta1 PodSecurityPolicy is deprecated in v1.21+, unavailable in v1.25+ NAME: trident-operator LAST DEPLOYED: Mon May 23 17:45:20 2022 NAMESPACE: openshift STATUS: deployed REVISION: 1 TEST SUITE: None NOTES: Thank you for installing trident-operator, which will deploy and manage NetApp's Trident CSI storage provisioner for Kubernetes. Your release is named 'trident-operator' and is installed into the 'openshift' namespace. Please note that there must be only one instance of Trident (and trident-operator) in a Kubernetes cluster. To configure Trident to manage storage resources, you will need a copy of tridentctl, which is available in pre-packaged Trident releases. You may find all Trident releases and source code online at https://github.com/NetApp/trident. To learn more about the release, try: $ helm status trident-operator $ helm get all trident-operator Validate cd .. ./tridentctl -n openshift version +----------------+----------------+ | SERVER VERSION | CLIENT VERSION | +----------------+----------------+ | 22.04.0 | 22.04.0 | +----------------+----------------+ Install tridentctl I put all my cli's in /usr/local/bin sudo install tridentctl /usr/local/bin example output: which tridentctl /usr/local/bin/tridentctl Create trident backend FYI - Sample files for review are in sample-input/backends-samples/azure-netapp-files directory from the trident tgz we extracted earlier. Replace client ID with service principal ID Replace clientSecret with Service Principal Secret Replace tenantID with your account tenant ID Replace subscriptionID with your azure SubscriptionID Ensure location matches your Azure Region Note: I have used nfsv3 for basic compatibility. You can remove that line and use NetApp files defaults. vi backend.json Add the following snippet: { \"version\": 1, \"nfsMountOptions\": \"nfsvers=3\", \"storageDriverName\": \"azure-netapp-files\", \"subscriptionID\": \"12abc678-4774-fake-a1b2-a7abcde39312\", \"tenantID\": \"a7abcde3-edc1-fake-b111-a7abcde356cf\", \"clientID\": \"abcde356-bf8e-fake-c111-abcde35613aa\", \"clientSecret\": \"rR0rUmWXfNioN1KhtHisiSAnoTherboGuskey6pU\", \"location\": \"southcentralus\", \"subnet\": \"anf.subnet\", \"labels\": { \"cloud\": \"azure\" }, \"storage\": [ { \"labels\": { \"performance\": \"Standard\" }, \"serviceLevel\": \"Standard\" } ] } run tridentctl -n openshift create backend -f backend.json example output: +------------------------+--------------------+--------------------------------------+--------+---------+ | NAME | STORAGE DRIVER | UUID | STATE | VOLUMES | +------------------------+--------------------+--------------------------------------+--------+---------+ | azurenetappfiles_eb177 | azure-netapp-files | f7f211afe-d7f5-41a5-a356-fa67f25ee96b | online | 0 | +------------------------+--------------------+--------------------------------------+--------+---------+ if you get a failure here, you can run to following command to review logs: tridentctl logs ``` To view log output that may help steer you in the right direction. ### Create storage class ```bash cat <<EOF | kubectl apply -f - apiVersion: storage.k8s.io/v1 kind: StorageClass metadata: name: standard provisioner: csi.trident.netapp.io parameters: backendType: \"azure-netapp-files\" fsType: \"nfs\" selector: \"performance=Standard\" # Matching labels in the backends... allowVolumeExpansion: true # To allow volume resizing. This parameter is optional mountOptions: - nconnect=16 EOF output: storageclass.storage.k8s.io/standard created Provision volume Let's create a new project and set up a persistent volume claim. Remember that PV Claims are namespaced objects and you must create the pvc in the namespace where it will be allocated. I'll use the project \"netappdemo\". oc new-project netappdemo Now we'll create a PV claim in the \"netappdemo\" project we just created. cat <<EOF | kubectl apply -f - kind: PersistentVolumeClaim apiVersion: v1 metadata: name: standard spec: accessModes: - ReadWriteMany resources: requests: storage: 4000Gi storageClassName: standard EOF output: persistentvolumeclaim/standard created Verify Quick verification of storage, volumes and services. Verify Kubectl \u279c kubectl get storageclass NAME PROVISIONER RECLAIMPOLICY VOLUMEBINDINGMODE ALLOWVOLUMEEXPANSION AGEmanaged-premium (default) kubernetes.io/azure-disk Delete WaitForFirstConsumer true 3h26m standard csi.trident.netapp.io Delete Immediate true 5m5s \u279c Verify OpenShift Login to your cluster as cluster-admin and verify your storage classes and persistent volumes. Storage Class Persisent Volumes Create Pods to test Azure NetApp We'll create two pods here to exercise the Azure NetApp file mount. One to write data and another to read data to show that it is mounted as \"read write many\" and correctly working. Writer Pod This pod will write \"hello netapp\" to a shared NetApp mount. cat <<EOF | oc apply -f - apiVersion: v1 kind: Pod metadata: name: test-netapp labels: app: test-aznetapp deploymethod: trident spec: containers: - name: aznetapp image: centos:latest command: [\"/bin/bash\", \"-c\", \"--\"] resources: limits: cpu: 1 memory: \"1Gi\" args: [ \"while true; do echo 'hello netapp' | tee -a /mnt/netapp-data/verify-netapp && sleep 5; done;\", ] volumeMounts: - name: disk01 mountPath: \"/mnt/netapp-data\" volumes: - name: disk01 persistentVolumeClaim: claimName: standard EOF You can watch for this container to be ready: watch oc get pod test-netapp Or view it in the OpenShift Pod console for the netappdemo project. Reader Pod This pod will read back the data from the shared NetApp mount. cat <<EOF | oc apply -f - apiVersion: v1 kind: Pod metadata: name: test-netapp-read spec: containers: - name: test-netapp-read image: centos:latest command: [\"/bin/bash\", \"-c\", \"--\"] resources: limits: cpu: 1 memory: \"1Gi\" args: [\"tail -f /mnt/netapp-data/verify-netapp\"] volumeMounts: - name: disk01 mountPath: \"/mnt/netapp-data\" volumes: - name: disk01 persistentVolumeClaim: claimName: standard EOF Now let's verify the POD is reading from shared volume. oc logs test-netapp-read hello netapp hello netapp hello netapp hello netapp hello netapp hello netapp hello netapp hello netapp hello netapp hello netapp You can also see the pod details in OpenShift for the reader:","title":"Trident NetApp operator setup for Azure NetApp files"},{"location":"aro/trident/#trident-netapp-operator-setup-for-azure-netapp-files","text":"Byron Miller 05/23/2022 Note: This guide a simple \"happy path\" to show the path of least friction to showcasing how to use NetApp files with Azure Red Hat OpenShift. This may not be the best behavior for any system beyond demonstration purposes.","title":"Trident NetApp operator setup for Azure NetApp files"},{"location":"aro/trident/#prerequisites","text":"An Azure Red Hat OpenShift cluster installed with Service Principal role/credentials. kubectl cli oc cli helm 3 cli {:target=\"_blank\"} Review official trident documentation {:target=\"_blank\"} In this guide, you will need service principal and region details. Please have these handy. Azure subscriptionID Azure tenantID Azure clientID (Service Principal) Azure clientSecret (Service Principal Secret) Azure Region If you don't have your existing ARO service principal credentials, you can create your own service principal and grant it contributor to be able to manage the required resources. Please review the official Trident documentation {:target=\"_blank\"} regarding Azure NetApp files and required permissions.","title":"Prerequisites"},{"location":"aro/trident/#important-concepts","text":"Persistent Volume Claims are namespaced objects {:target=\"_blank\"}. Mounting RWX/ROX is only possible within the same namespace. NetApp files must be have a delegated subnet within your ARO Vnet's and you must assign it to the Microsoft.Netapp/volumes service.","title":"Important Concepts"},{"location":"aro/trident/#configure-azure","text":"You must first register the Microsoft.NetApp provider and Create a NetApp account on Azure before you can use Azure NetApp Files.","title":"Configure Azure"},{"location":"aro/trident/#register-netapp-files","text":"Azure Console {:target=\"_blank\"} or az cli az provider register --namespace Microsoft.NetApp --wait","title":"Register NetApp files"},{"location":"aro/trident/#create-storage-account","text":"Again, for brevity I am using the same RESOURCE_GROUP and Service Principal that the cluster was created with. Azure Console {:target=\"_blank\"} or az cli RESOURCE_GROUP=\"myresourcegroup\" LOCATION=\"southcentralus\" ANF_ACCOUNT_NAME=\"netappfiles\" az netappfiles account create \\ --resource-group $RESOURCE_GROUP \\ --location $LOCATION \\ --account-name $ANF_ACCOUNT_NAME","title":"Create storage account"},{"location":"aro/trident/#create-capacity-pool","text":"Creating one pool for now. The common pattern is to expose all three levels with unique pool names respective of each service level. Azure Console {:target=\"_blank\"} or az cli: POOL_NAME=\"Standard\" POOL_SIZE_TiB=4 # Size in Azure CLI needs to be in TiB unit (minimum 4 TiB) SERVICE_LEVEL=\"Standard\" # Valid values are Standard, Premium and Ultra az netappfiles pool create \\ --resource-group $RESOURCE_GROUP \\ --location $LOCATION \\ --account-name $ANF_ACCOUNT_NAME \\ --pool-name $POOL_NAME \\ --size $POOL_SIZE_TiB \\ --service-level $SERVICE_LEVEL","title":"Create capacity pool"},{"location":"aro/trident/#delegate-subnet-to-aro","text":"Login to azure console, find the subnets for your ARO cluster and click add subnet. We need to call this subnet anf.subnet since that is the name we refer to in later configuration.","title":"Delegate subnet to ARO"},{"location":"aro/trident/#install-trident-operator","text":"","title":"Install Trident Operator"},{"location":"aro/trident/#loginauthenticate-to-aro","text":"Login to your ARO cluster. You can create a token to login via cli straight from the web gui oc login --token=sha256~abcdefghijklmnopqrstuvwxyz --server=https://api.randomseq.eastus.aroapp.io:6443","title":"Login/Authenticate to ARO"},{"location":"aro/trident/#helm-install","text":"Download latest Trident package wget https://github.com/NetApp/trident/releases/download/v22.04.0/trident-installer-22.04.0.tar.gz Extract tar.gz into working director tar -xzvf trident-installer-22.04.0.tar.gz cd into installer cd trident-installer/helm Helm install helm install trident-operator trident-operator-22.04.0.tgz Example output from installation: W0523 17:45:22.189592 30478 warnings.go:70] policy/v1beta1 PodSecurityPolicy is deprecated in v1.21+, unavailable in v1.25+ W0523 17:45:22.484071 30478 warnings.go:70] policy/v1beta1 PodSecurityPolicy is deprecated in v1.21+, unavailable in v1.25+ NAME: trident-operator LAST DEPLOYED: Mon May 23 17:45:20 2022 NAMESPACE: openshift STATUS: deployed REVISION: 1 TEST SUITE: None NOTES: Thank you for installing trident-operator, which will deploy and manage NetApp's Trident CSI storage provisioner for Kubernetes. Your release is named 'trident-operator' and is installed into the 'openshift' namespace. Please note that there must be only one instance of Trident (and trident-operator) in a Kubernetes cluster. To configure Trident to manage storage resources, you will need a copy of tridentctl, which is available in pre-packaged Trident releases. You may find all Trident releases and source code online at https://github.com/NetApp/trident. To learn more about the release, try: $ helm status trident-operator $ helm get all trident-operator Validate cd .. ./tridentctl -n openshift version +----------------+----------------+ | SERVER VERSION | CLIENT VERSION | +----------------+----------------+ | 22.04.0 | 22.04.0 | +----------------+----------------+","title":"Helm Install"},{"location":"aro/trident/#install-tridentctl","text":"I put all my cli's in /usr/local/bin sudo install tridentctl /usr/local/bin example output: which tridentctl /usr/local/bin/tridentctl","title":"Install tridentctl"},{"location":"aro/trident/#create-trident-backend","text":"FYI - Sample files for review are in sample-input/backends-samples/azure-netapp-files directory from the trident tgz we extracted earlier. Replace client ID with service principal ID Replace clientSecret with Service Principal Secret Replace tenantID with your account tenant ID Replace subscriptionID with your azure SubscriptionID Ensure location matches your Azure Region Note: I have used nfsv3 for basic compatibility. You can remove that line and use NetApp files defaults. vi backend.json Add the following snippet: { \"version\": 1, \"nfsMountOptions\": \"nfsvers=3\", \"storageDriverName\": \"azure-netapp-files\", \"subscriptionID\": \"12abc678-4774-fake-a1b2-a7abcde39312\", \"tenantID\": \"a7abcde3-edc1-fake-b111-a7abcde356cf\", \"clientID\": \"abcde356-bf8e-fake-c111-abcde35613aa\", \"clientSecret\": \"rR0rUmWXfNioN1KhtHisiSAnoTherboGuskey6pU\", \"location\": \"southcentralus\", \"subnet\": \"anf.subnet\", \"labels\": { \"cloud\": \"azure\" }, \"storage\": [ { \"labels\": { \"performance\": \"Standard\" }, \"serviceLevel\": \"Standard\" } ] } run tridentctl -n openshift create backend -f backend.json example output: +------------------------+--------------------+--------------------------------------+--------+---------+ | NAME | STORAGE DRIVER | UUID | STATE | VOLUMES | +------------------------+--------------------+--------------------------------------+--------+---------+ | azurenetappfiles_eb177 | azure-netapp-files | f7f211afe-d7f5-41a5-a356-fa67f25ee96b | online | 0 | +------------------------+--------------------+--------------------------------------+--------+---------+ if you get a failure here, you can run to following command to review logs: tridentctl logs ``` To view log output that may help steer you in the right direction. ### Create storage class ```bash cat <<EOF | kubectl apply -f - apiVersion: storage.k8s.io/v1 kind: StorageClass metadata: name: standard provisioner: csi.trident.netapp.io parameters: backendType: \"azure-netapp-files\" fsType: \"nfs\" selector: \"performance=Standard\" # Matching labels in the backends... allowVolumeExpansion: true # To allow volume resizing. This parameter is optional mountOptions: - nconnect=16 EOF output: storageclass.storage.k8s.io/standard created","title":"Create trident backend"},{"location":"aro/trident/#provision-volume","text":"Let's create a new project and set up a persistent volume claim. Remember that PV Claims are namespaced objects and you must create the pvc in the namespace where it will be allocated. I'll use the project \"netappdemo\". oc new-project netappdemo Now we'll create a PV claim in the \"netappdemo\" project we just created. cat <<EOF | kubectl apply -f - kind: PersistentVolumeClaim apiVersion: v1 metadata: name: standard spec: accessModes: - ReadWriteMany resources: requests: storage: 4000Gi storageClassName: standard EOF output: persistentvolumeclaim/standard created","title":"Provision volume"},{"location":"aro/trident/#verify","text":"Quick verification of storage, volumes and services.","title":"Verify"},{"location":"aro/trident/#verify-kubectl","text":"\u279c kubectl get storageclass NAME PROVISIONER RECLAIMPOLICY VOLUMEBINDINGMODE ALLOWVOLUMEEXPANSION AGEmanaged-premium (default) kubernetes.io/azure-disk Delete WaitForFirstConsumer true 3h26m standard csi.trident.netapp.io Delete Immediate true 5m5s \u279c","title":"Verify Kubectl"},{"location":"aro/trident/#verify-openshift","text":"Login to your cluster as cluster-admin and verify your storage classes and persistent volumes. Storage Class Persisent Volumes","title":"Verify OpenShift"},{"location":"aro/trident/#create-pods-to-test-azure-netapp","text":"We'll create two pods here to exercise the Azure NetApp file mount. One to write data and another to read data to show that it is mounted as \"read write many\" and correctly working.","title":"Create Pods to test Azure NetApp"},{"location":"aro/trident/#writer-pod","text":"This pod will write \"hello netapp\" to a shared NetApp mount. cat <<EOF | oc apply -f - apiVersion: v1 kind: Pod metadata: name: test-netapp labels: app: test-aznetapp deploymethod: trident spec: containers: - name: aznetapp image: centos:latest command: [\"/bin/bash\", \"-c\", \"--\"] resources: limits: cpu: 1 memory: \"1Gi\" args: [ \"while true; do echo 'hello netapp' | tee -a /mnt/netapp-data/verify-netapp && sleep 5; done;\", ] volumeMounts: - name: disk01 mountPath: \"/mnt/netapp-data\" volumes: - name: disk01 persistentVolumeClaim: claimName: standard EOF You can watch for this container to be ready: watch oc get pod test-netapp Or view it in the OpenShift Pod console for the netappdemo project.","title":"Writer Pod"},{"location":"aro/trident/#reader-pod","text":"This pod will read back the data from the shared NetApp mount. cat <<EOF | oc apply -f - apiVersion: v1 kind: Pod metadata: name: test-netapp-read spec: containers: - name: test-netapp-read image: centos:latest command: [\"/bin/bash\", \"-c\", \"--\"] resources: limits: cpu: 1 memory: \"1Gi\" args: [\"tail -f /mnt/netapp-data/verify-netapp\"] volumeMounts: - name: disk01 mountPath: \"/mnt/netapp-data\" volumes: - name: disk01 persistentVolumeClaim: claimName: standard EOF Now let's verify the POD is reading from shared volume. oc logs test-netapp-read hello netapp hello netapp hello netapp hello netapp hello netapp hello netapp hello netapp hello netapp hello netapp hello netapp You can also see the pod details in OpenShift for the reader:","title":"Reader Pod"},{"location":"aro/vpn/","text":"Setup a VPN Connection into an ARO Cluster with OpenVPN Kevin Collins 06/20/2022 When you configure an Azure Red Hat OpenShift (ARO) cluster with a private only configuration, you will need connectivity to this private network in order to access your cluster. This guide will show you how to configute a point-to0-site VPN connection so you won't need to setup and configure Jump Boxes. Prerequisites a private ARO Cluster git openssl Create certificates to use for your VPN Connection There are many ways and methods to create certificates for VPN, the guide below is one of the ways that works well. Note, that whatever method you use, make sure it supports \"X509v3 Extended Key Usage\". Clone OpenVPN/easy-rsa bash git clone https://github.com/OpenVPN/easy-rsa.git Change to the easyrsa directory bash cd easy-rsa/easyrsa3 Initialize the PKI bash ./easyrsa init-pki Edit certificate parameters Uncomment and edit the copied template with your values bash vim pki/vars set_var EASYRSA_REQ_COUNTRY \"US\" set_var EASYRSA_REQ_PROVINCE \"California\" set_var EASYRSA_REQ_CITY \"San Francisco\" set_var EASYRSA_REQ_ORG \"Copyleft Certificate Co\" set_var EASYRSA_REQ_EMAIL \"me@example.net\" set_var EASYRSA_REQ_OU \"My Organizational Unit\" Uncomment (remove the #) the folowing field #set_var EASYRSA_KEY_SIZE 2048 Create the CA: bash ./easyrsa build-ca nopass Generate the Server Certificate and Key bash ./easyrsa build-server-full server nopass Generate Diffie-Hellman (DH) parameters bash ./easyrsa gen-dh Generate client credentials bash ./easyrsa build-client-full azure nopass Set environment variables for the CA certificate you just created. bash CACERT=$(openssl x509 -in pki/ca.crt -outform der | base64) Set Envrionment Variables AROCLUSTER=<cluster name> ARORG=<resource group the cluster is in> UNIQUEID=$RANDOM LOCATION=$(az aro show --name $AROCLUSTER --resource-group $ARORG --query location -o tsv) VNET_NAME=$(az network vnet list -g $ARORG --query '[0].name' -o tsv) GW_NAME=${USER}_${VNET_NAME} GW_SUBNET_PREFIX=e.g. 10.0.7.0/24 choose a new available subnet in the VNET your cluster is in. VPN_PREFIX=172.18.0.0/24 Create an Azure Virtual Network Gateway Request a public IP Address ```bash az network public-ip create \\ -n $USER-pip-$UNIQUEID \\ -g $ARORG \\ --allocation-method Static \\ --sku Standard \\ --zone 1 2 3 pip=$(az network public-ip show -g $ARORG --name $USER-pip-$UNIQUEID --query \"ipAddress\" -o tsv) ``` Create a Gateway Subnet bash az network vnet subnet create \\ --vnet-name $VNET_NAME \\ -n GatewaySubnet \\ -g $ARORG \\ --address-prefix $GW_SUBNET_PREFIX Create a virtual network gateway bash az network vnet-gateway create \\ --name $GW_NAME \\ --location $LOCATION \\ --public-ip-address $USER-pip-$UNIQUEID \\ --resource-group $ARORG \\ --vnet $VNET_NAME \\ --gateway-type Vpn \\ --sku VpnGw3AZ \\ --address-prefixes $VPN_PREFIX \\ --root-cert-data $CACERT \\ --root-cert-name $USER-p2s \\ --vpn-type RouteBased \\ --vpn-gateway-generation Generation2 \\ --client-protocol IkeV2 OpenVPN go grab a coffee, this takes about 15 - 20 minutes Configure your OpenVPN Client Retrieve the VPN Settings From the Azure Portal - navigate to your Virtual Network Gateway, point to site configuration, and then click Download VPN Client. This will download a zip file containing the VPN Client Create a VPN Client Configuration Uncompress the file you downloaded in the previous step and edit the OpenVPN\\vpnconfig.ovpn file. Note: The next two commands assume you are still in the easyrsa3 directory. In the vpnconfig.ovpn replace the $CLIENTCERTIFICATE line with the entire contents of: bash openssl x509 -in pki/issued/azure.crt Make sure to copy the -----BEGIN CERTIFICATE----- and the -----END CERTIFICATE----- lines. also replace $PRIVATEKEY line with the output of: bash cat pki/private/azure.key Make sure to copy the -----BEGIN PRIVATE KEY----- and the -----END PRIVATE KEY----- lines. add the new OpenVPN configuration file to your OpenVPN client. mac users - just double click on the vpnserver.ovpn file and it will be automatically imported. Connect your VPN.","title":"Setup a VPN Connection into an ARO Cluster with OpenVPN"},{"location":"aro/vpn/#setup-a-vpn-connection-into-an-aro-cluster-with-openvpn","text":"Kevin Collins 06/20/2022 When you configure an Azure Red Hat OpenShift (ARO) cluster with a private only configuration, you will need connectivity to this private network in order to access your cluster. This guide will show you how to configute a point-to0-site VPN connection so you won't need to setup and configure Jump Boxes.","title":"Setup a VPN Connection into an ARO Cluster with OpenVPN"},{"location":"aro/vpn/#prerequisites","text":"a private ARO Cluster git openssl","title":"Prerequisites"},{"location":"aro/vpn/#create-certificates-to-use-for-your-vpn-connection","text":"There are many ways and methods to create certificates for VPN, the guide below is one of the ways that works well. Note, that whatever method you use, make sure it supports \"X509v3 Extended Key Usage\". Clone OpenVPN/easy-rsa bash git clone https://github.com/OpenVPN/easy-rsa.git Change to the easyrsa directory bash cd easy-rsa/easyrsa3 Initialize the PKI bash ./easyrsa init-pki Edit certificate parameters Uncomment and edit the copied template with your values bash vim pki/vars set_var EASYRSA_REQ_COUNTRY \"US\" set_var EASYRSA_REQ_PROVINCE \"California\" set_var EASYRSA_REQ_CITY \"San Francisco\" set_var EASYRSA_REQ_ORG \"Copyleft Certificate Co\" set_var EASYRSA_REQ_EMAIL \"me@example.net\" set_var EASYRSA_REQ_OU \"My Organizational Unit\" Uncomment (remove the #) the folowing field #set_var EASYRSA_KEY_SIZE 2048 Create the CA: bash ./easyrsa build-ca nopass Generate the Server Certificate and Key bash ./easyrsa build-server-full server nopass Generate Diffie-Hellman (DH) parameters bash ./easyrsa gen-dh Generate client credentials bash ./easyrsa build-client-full azure nopass Set environment variables for the CA certificate you just created. bash CACERT=$(openssl x509 -in pki/ca.crt -outform der | base64)","title":"Create certificates to use for your VPN Connection"},{"location":"aro/vpn/#set-envrionment-variables","text":"AROCLUSTER=<cluster name> ARORG=<resource group the cluster is in> UNIQUEID=$RANDOM LOCATION=$(az aro show --name $AROCLUSTER --resource-group $ARORG --query location -o tsv) VNET_NAME=$(az network vnet list -g $ARORG --query '[0].name' -o tsv) GW_NAME=${USER}_${VNET_NAME} GW_SUBNET_PREFIX=e.g. 10.0.7.0/24 choose a new available subnet in the VNET your cluster is in. VPN_PREFIX=172.18.0.0/24","title":"Set Envrionment Variables"},{"location":"aro/vpn/#create-an-azure-virtual-network-gateway","text":"Request a public IP Address ```bash az network public-ip create \\ -n $USER-pip-$UNIQUEID \\ -g $ARORG \\ --allocation-method Static \\ --sku Standard \\ --zone 1 2 3 pip=$(az network public-ip show -g $ARORG --name $USER-pip-$UNIQUEID --query \"ipAddress\" -o tsv) ``` Create a Gateway Subnet bash az network vnet subnet create \\ --vnet-name $VNET_NAME \\ -n GatewaySubnet \\ -g $ARORG \\ --address-prefix $GW_SUBNET_PREFIX Create a virtual network gateway bash az network vnet-gateway create \\ --name $GW_NAME \\ --location $LOCATION \\ --public-ip-address $USER-pip-$UNIQUEID \\ --resource-group $ARORG \\ --vnet $VNET_NAME \\ --gateway-type Vpn \\ --sku VpnGw3AZ \\ --address-prefixes $VPN_PREFIX \\ --root-cert-data $CACERT \\ --root-cert-name $USER-p2s \\ --vpn-type RouteBased \\ --vpn-gateway-generation Generation2 \\ --client-protocol IkeV2 OpenVPN go grab a coffee, this takes about 15 - 20 minutes","title":"Create an Azure Virtual Network Gateway"},{"location":"aro/vpn/#configure-your-openvpn-client","text":"Retrieve the VPN Settings From the Azure Portal - navigate to your Virtual Network Gateway, point to site configuration, and then click Download VPN Client. This will download a zip file containing the VPN Client Create a VPN Client Configuration Uncompress the file you downloaded in the previous step and edit the OpenVPN\\vpnconfig.ovpn file. Note: The next two commands assume you are still in the easyrsa3 directory. In the vpnconfig.ovpn replace the $CLIENTCERTIFICATE line with the entire contents of: bash openssl x509 -in pki/issued/azure.crt Make sure to copy the -----BEGIN CERTIFICATE----- and the -----END CERTIFICATE----- lines. also replace $PRIVATEKEY line with the output of: bash cat pki/private/azure.key Make sure to copy the -----BEGIN PRIVATE KEY----- and the -----END PRIVATE KEY----- lines. add the new OpenVPN configuration file to your OpenVPN client. mac users - just double click on the vpnserver.ovpn file and it will be automatically imported. Connect your VPN.","title":"Configure your OpenVPN Client"},{"location":"aws/waf/","text":"Examples of using a WAF in front of ROSA / OSD on AWS / OCP on AWS Problem Statement Operator requires WAF (Web Application Firewall) in front of their workloads running on OpenShift (ROSA) Operator does not want WAF running on OpenShift to ensure that OCP resources do not experience Denial of Service through handling the WAF Solutions Cloud Front -> WAF -> CustomDomain -> $APP This is the preferred method and can also work with most third party WAF systems that act as a reverse proxy Uses a custom domain, custom route, LE cert. CloudFront and WAF Using Cloud Front Application Load Balancer -> ALB Operator -> $APP Installs the ALB Operator, and uses the ALB to route via WAF, one ALB per app though! Application Load Balancer","title":"Examples of using a WAF in front of ROSA / OSD on AWS / OCP on AWS"},{"location":"aws/waf/#examples-of-using-a-waf-in-front-of-rosa-osd-on-aws-ocp-on-aws","text":"","title":"Examples of using a WAF in front of ROSA / OSD on AWS / OCP on AWS"},{"location":"aws/waf/#problem-statement","text":"Operator requires WAF (Web Application Firewall) in front of their workloads running on OpenShift (ROSA) Operator does not want WAF running on OpenShift to ensure that OCP resources do not experience Denial of Service through handling the WAF","title":"Problem Statement"},{"location":"aws/waf/#solutions","text":"","title":"Solutions"},{"location":"aws/waf/#cloud-front-waf-customdomain-app","text":"This is the preferred method and can also work with most third party WAF systems that act as a reverse proxy Uses a custom domain, custom route, LE cert. CloudFront and WAF Using Cloud Front","title":"Cloud Front -&gt; WAF -&gt; CustomDomain -&gt; $APP"},{"location":"aws/waf/#application-load-balancer-alb-operator-app","text":"Installs the ALB Operator, and uses the ALB to route via WAF, one ALB per app though! Application Load Balancer","title":"Application Load Balancer -&gt; ALB Operator -&gt; $APP"},{"location":"aws/waf/README-complex/","text":"This is a POC of ROSA with a AWS WAF service Non working (yet) instructions for using STS to manage the creds for ALB See https://issues.redhat.com/browse/CTONET-858 for a similar request Here 's a good overview of AWS LB types and what they support Problem Statement Customer requires WAF (Web Application Firewall) in front of their workloads running on OpenShift (ROSA) Customer does not want WAF running on OpenShift to ensure that OCP resources do not experience Denial of Service through handling the WAF Proposed Solution Loosely based off EKS instructions here - https://aws.amazon.com/premiumsupport/knowledge-center/eks-alb-ingress-aws-waf/ Deploy secondary Ingress solution (+TLS +DNS) that uses an AWS ALB Configure TLS + DNS for that Ingress Lets Encrypt + WildCard DNS Deployment AWS Load Balancer Controller AWS Load Balancer controller manages the following AWS resources Application Load Balancers to satisfy Kubernetes ingress objects Network Load Balancers in IP mode to satisfy Kubernetes service objects of type LoadBalancer with NLB IP mode annotation Configure STS Make sure your cluster has the pod identity webhook bash kubectl get mutatingwebhookconfigurations.admissionregistration.k8s.io pod-identity-webhook Create AWS Policy and Service Account bash POLICY_ARN=$(aws iam create-policy --policy-name \"AWSLoadBalancerControllerIAMPolicy\" --policy-document file://iam-policy.json --query Policy.Arn --output text) echo $POLICY_ARN Create service account Note I had issues with the policy, and for now just gave this user admin creds. Need to revisit and figure out. SA_ARN=$(aws iam create-user --user-name aws-lb-controller --permissions-boundary=$POLICY_ARN --query User.Arn --output text) Create access key ACCESS_KEY=$(aws iam create-access-key --user-name aws-lb-controller) Attach policy to user ```bash Paste the AccessKeyId and SecretAccessKey into values.yaml tag your public subnet with `` Create a namespace for the controller bash kubectl create ns aws-load-balancer-controller Apply CRDs bash kubectl apply -k \"github.com/aws/eks-charts/stable/aws-load-balancer-controller//crds?ref=master\" Add the helm repo and install the controller (install helm3 if not already) bash helm repo add eks https://aws.github.io/eks-charts helm install -n aws-load-balancer-controller \\ aws-load-balancer-controller eks/aws-load-balancer-controller \\ --values=./helm/values.yaml --create-namespace Deploy Sample Application oc new-project demo oc new-app https://github.com/sclorg/django-ex.git kubectl -n demo patch service django-ex -p '{\"spec\":{\"type\":\"NodePort\"}}' kubectl apply -f ingress.yaml","title":"This is a POC of ROSA with a AWS WAF service"},{"location":"aws/waf/README-complex/#this-is-a-poc-of-rosa-with-a-aws-waf-service","text":"Non working (yet) instructions for using STS to manage the creds for ALB See https://issues.redhat.com/browse/CTONET-858 for a similar request Here 's a good overview of AWS LB types and what they support","title":"This is a POC of ROSA with a AWS WAF service"},{"location":"aws/waf/README-complex/#problem-statement","text":"Customer requires WAF (Web Application Firewall) in front of their workloads running on OpenShift (ROSA) Customer does not want WAF running on OpenShift to ensure that OCP resources do not experience Denial of Service through handling the WAF","title":"Problem Statement"},{"location":"aws/waf/README-complex/#proposed-solution","text":"Loosely based off EKS instructions here - https://aws.amazon.com/premiumsupport/knowledge-center/eks-alb-ingress-aws-waf/ Deploy secondary Ingress solution (+TLS +DNS) that uses an AWS ALB Configure TLS + DNS for that Ingress Lets Encrypt + WildCard DNS","title":"Proposed Solution"},{"location":"aws/waf/README-complex/#deployment","text":"","title":"Deployment"},{"location":"aws/waf/README-complex/#aws-load-balancer-controller","text":"AWS Load Balancer controller manages the following AWS resources Application Load Balancers to satisfy Kubernetes ingress objects Network Load Balancers in IP mode to satisfy Kubernetes service objects of type LoadBalancer with NLB IP mode annotation","title":"AWS Load Balancer Controller"},{"location":"aws/waf/README-complex/#configure-sts","text":"Make sure your cluster has the pod identity webhook bash kubectl get mutatingwebhookconfigurations.admissionregistration.k8s.io pod-identity-webhook Create AWS Policy and Service Account bash POLICY_ARN=$(aws iam create-policy --policy-name \"AWSLoadBalancerControllerIAMPolicy\" --policy-document file://iam-policy.json --query Policy.Arn --output text) echo $POLICY_ARN Create service account Note I had issues with the policy, and for now just gave this user admin creds. Need to revisit and figure out. SA_ARN=$(aws iam create-user --user-name aws-lb-controller --permissions-boundary=$POLICY_ARN --query User.Arn --output text) Create access key ACCESS_KEY=$(aws iam create-access-key --user-name aws-lb-controller) Attach policy to user ```bash Paste the AccessKeyId and SecretAccessKey into values.yaml tag your public subnet with `` Create a namespace for the controller bash kubectl create ns aws-load-balancer-controller Apply CRDs bash kubectl apply -k \"github.com/aws/eks-charts/stable/aws-load-balancer-controller//crds?ref=master\" Add the helm repo and install the controller (install helm3 if not already) bash helm repo add eks https://aws.github.io/eks-charts helm install -n aws-load-balancer-controller \\ aws-load-balancer-controller eks/aws-load-balancer-controller \\ --values=./helm/values.yaml --create-namespace","title":"Configure STS"},{"location":"aws/waf/README-complex/#deploy-sample-application","text":"oc new-project demo oc new-app https://github.com/sclorg/django-ex.git kubectl -n demo patch service django-ex -p '{\"spec\":{\"type\":\"NodePort\"}}' kubectl apply -f ingress.yaml","title":"Deploy Sample Application"},{"location":"aws/waf/alb/","text":"This is a POC of ROSA with a AWS WAF service Note: It is recommended that you use the Cloud Front based guide unless you absolutely must use an ALB based solution. Here 's a good overview of AWS LB types and what they support Problem Statement Operator requires WAF (Web Application Firewall) in front of their workloads running on OpenShift (ROSA) Operator does not want WAF running on OpenShift to ensure that OCP resources do not experience Denial of Service through handling the WAF Proposed Solution Loosely based off EKS instructions here - https://aws.amazon.com/premiumsupport/knowledge-center/eks-alb-ingress-aws-waf/ Deploy secondary Ingress solution (+TLS +DNS) that uses an AWS ALB Todo Configure TLS + DNS for that Ingress (Lets Encrypt + WildCard DNS) Pre Requisites A ROSA / OSD on AWS cluster Helm 3 cli oc / kubectl AWS cli Disable AWS cli output paging bash export AWS_PAGER=\"\" Set the ALB Controller version bash export ALB_VERSION=\"v2.2.0\" Set the name of your cluster for lookup bash export CLUSTER_NAME=\"waf-demo\" Deployment Create a new public ROSA cluster called waf-demo and make sure to set it to be multi-AZ enabled, or replace the cluster name variable with your own cluster name. AWS Load Balancer Controller AWS Load Balancer controller manages the following AWS resources Application Load Balancers to satisfy Kubernetes ingress objects Network Load Balancers in IP mode to satisfy Kubernetes service objects of type LoadBalancer with NLB IP mode annotation Create AWS Policy and Service Account ```bash curl -so iam-policy.json https://raw.githubusercontent.com/kubernetes-sigs/aws-load-balancer-controller/${ALB_VERSION}/docs/install/iam_policy.json POLICY_ARN=$(aws iam create-policy --policy-name \"AWSLoadBalancerControllerIAMPolicy\" --policy-document file://iam-policy.json --query Policy.Arn --output text) echo $POLICY_ARN ``` Create service account bash aws iam create-user --user-name aws-lb-controller \\ --query User.Arn --output text Attach policy to user bash aws iam attach-user-policy --user-name aws-lb-controller \\ --policy-arn ${POLICY_ARN} Create access key and save the output (Paste the AccessKeyId and SecretAccessKey into values.yaml ) bash aws iam create-access-key --user-name aws-lb-controller bash export AWS_ID=<from above> export AWS_KEY=<from above> Modify the VPC ID and cluster name in the values.yaml with the output from (replace poc-waf with your cluster name) : bash VPC_ID=$(aws ec2 describe-vpcs --output json --filters \\ Name=tag-value,Values=\"${CLUSTER_NAME}*\" \\ --query \"Vpcs[].VpcId\" --output text) echo ${VPC_ID} Modify the subnet list in ingress.yaml with the output from: (replace poc-waf with your cluster name) bash SUBNET_IDS=$(aws ec2 describe-subnets --output json \\ --filters Name=tag-value,Values=\"${CLUSTER_NAME}-*public*\" \\ --query \"Subnets[].SubnetId\" --output text | sed 's/\\t/ /g') echo ${SUBNET_IDS} Add tags to those subnets (change the subnet ids in the resources line) bash aws ec2 create-tags \\ --resources $(echo ${SUBNET_IDS}) \\ --tags Key=kubernetes.io/role/elb,Value= Key=kubernetes.io/cluster/${CLUSTER_NAME},Value=shared 1. Create a namespace for the controller bash kubectl create ns aws-load-balancer-controller Apply CRDs bash kubectl apply -k \"github.com/aws/eks-charts/stable/aws-load-balancer-controller//crds?ref=master\" Add the helm repo and install the controller (install helm3 if not already) bash helm repo add eks https://aws.github.io/eks-charts helm install -n aws-load-balancer-controller \\ aws-load-balancer-controller eks/aws-load-balancer-controller \\ --set \"env.AWS_ACCESS_KEY_ID=${AWS_ID}\" \\ --set \"env.AWS_SECRET_ACCESS_KEY=${AWS_KEY}\" \\ --set \"vpcID=${VPC_ID}\" \\ --set \"clusterName=${CLUSTER_NAME}\" \\ --set \"image.tag=${ALB_VERSION}\" \\ --create-namespace Deploy Sample Application Create a new application in OpenShift bash oc new-project demo oc new-app https://github.com/sclorg/django-ex.git kubectl -n demo patch service django-ex -p '{\"spec\":{\"type\":\"NodePort\"}}' Create an Ingress to trigger an ALB bash cat << EOF | kubectl apply -f - apiVersion: networking.k8s.io/v1 kind: Ingress metadata: name: django-ex namespace: demo annotations: kubernetes.io/ingress.class: alb alb.ingress.kubernetes.io/scheme: internet-facing alb.ingress.kubernetes.io/target-type: instance # alb.ingress.kubernetes.io/subnets: subnet-0982bb73ca67d61de,subnet-0aa9967e8767d792f,subnet-0fd57669a80eb7596 alb.ingress.kubernetes.io/shield-advanced-protection: \"true\" # wafv2 arn to use # alb.ingress.kubernetes.io/wafv2-acl-arn: arn:aws:wafv2:us-east-2:660250927410:regional/webacl/waf-demo/6565d2a1-6d26-4b6b-b56f-1e996c7e9e8f labels: app: django-ex spec: rules: - host: foo.bar http: paths: - pathType: Prefix path: /* backend: service: name: django-ex port: number: 8080 Check the logs of the ALB controller kubectl logs -f deployment/aws-load-balancer-controller use the second address from the ingress to browse to the app kubectl -n demo get ingress bash curl -s --header \"Host: foo.bar\" k8s-demo-djangoex-49f31c1921-782305710.us-east-2.elb.amazonaws.com | head WAF time Create a WAF rule here https://console.aws.amazon.com/wafv2/homev2/web-acls/new and use the Core and SQL Injection rules. (make sure region matches us-east-2) View your WAF bash aws wafv2 list-web-acls --scope REGIONAL --region us-east-2 | jq . set the waf annotation to match the ARN provided above (and uncomment it) then re-apply the ingress bash kubectl apply -f ingress.yaml test the app still works bash curl -s --header \"Host: foo.bar\" --location \"k8s-demo-djangoex-49f31c1921-782305710.us-east-2.elb.amazonaws.com\" test the WAF denies a bad request You should get a 403 Forbidden error bash curl -X POST http://k8s-demo-djangoex-49f31c1921-782305710.us-east-2.elb.amazonaws.com -F \"user='<script><alert>Hello></alert></script>'\"","title":"This is a POC of ROSA with a AWS WAF service"},{"location":"aws/waf/alb/#this-is-a-poc-of-rosa-with-a-aws-waf-service","text":"Note: It is recommended that you use the Cloud Front based guide unless you absolutely must use an ALB based solution. Here 's a good overview of AWS LB types and what they support","title":"This is a POC of ROSA with a AWS WAF service"},{"location":"aws/waf/alb/#problem-statement","text":"Operator requires WAF (Web Application Firewall) in front of their workloads running on OpenShift (ROSA) Operator does not want WAF running on OpenShift to ensure that OCP resources do not experience Denial of Service through handling the WAF","title":"Problem Statement"},{"location":"aws/waf/alb/#proposed-solution","text":"Loosely based off EKS instructions here - https://aws.amazon.com/premiumsupport/knowledge-center/eks-alb-ingress-aws-waf/ Deploy secondary Ingress solution (+TLS +DNS) that uses an AWS ALB Todo Configure TLS + DNS for that Ingress (Lets Encrypt + WildCard DNS)","title":"Proposed Solution"},{"location":"aws/waf/alb/#pre-requisites","text":"A ROSA / OSD on AWS cluster Helm 3 cli oc / kubectl AWS cli Disable AWS cli output paging bash export AWS_PAGER=\"\" Set the ALB Controller version bash export ALB_VERSION=\"v2.2.0\" Set the name of your cluster for lookup bash export CLUSTER_NAME=\"waf-demo\"","title":"Pre Requisites"},{"location":"aws/waf/alb/#deployment","text":"Create a new public ROSA cluster called waf-demo and make sure to set it to be multi-AZ enabled, or replace the cluster name variable with your own cluster name.","title":"Deployment"},{"location":"aws/waf/alb/#aws-load-balancer-controller","text":"AWS Load Balancer controller manages the following AWS resources Application Load Balancers to satisfy Kubernetes ingress objects Network Load Balancers in IP mode to satisfy Kubernetes service objects of type LoadBalancer with NLB IP mode annotation Create AWS Policy and Service Account ```bash curl -so iam-policy.json https://raw.githubusercontent.com/kubernetes-sigs/aws-load-balancer-controller/${ALB_VERSION}/docs/install/iam_policy.json POLICY_ARN=$(aws iam create-policy --policy-name \"AWSLoadBalancerControllerIAMPolicy\" --policy-document file://iam-policy.json --query Policy.Arn --output text) echo $POLICY_ARN ``` Create service account bash aws iam create-user --user-name aws-lb-controller \\ --query User.Arn --output text Attach policy to user bash aws iam attach-user-policy --user-name aws-lb-controller \\ --policy-arn ${POLICY_ARN} Create access key and save the output (Paste the AccessKeyId and SecretAccessKey into values.yaml ) bash aws iam create-access-key --user-name aws-lb-controller bash export AWS_ID=<from above> export AWS_KEY=<from above> Modify the VPC ID and cluster name in the values.yaml with the output from (replace poc-waf with your cluster name) : bash VPC_ID=$(aws ec2 describe-vpcs --output json --filters \\ Name=tag-value,Values=\"${CLUSTER_NAME}*\" \\ --query \"Vpcs[].VpcId\" --output text) echo ${VPC_ID} Modify the subnet list in ingress.yaml with the output from: (replace poc-waf with your cluster name) bash SUBNET_IDS=$(aws ec2 describe-subnets --output json \\ --filters Name=tag-value,Values=\"${CLUSTER_NAME}-*public*\" \\ --query \"Subnets[].SubnetId\" --output text | sed 's/\\t/ /g') echo ${SUBNET_IDS} Add tags to those subnets (change the subnet ids in the resources line) bash aws ec2 create-tags \\ --resources $(echo ${SUBNET_IDS}) \\ --tags Key=kubernetes.io/role/elb,Value= Key=kubernetes.io/cluster/${CLUSTER_NAME},Value=shared 1. Create a namespace for the controller bash kubectl create ns aws-load-balancer-controller Apply CRDs bash kubectl apply -k \"github.com/aws/eks-charts/stable/aws-load-balancer-controller//crds?ref=master\" Add the helm repo and install the controller (install helm3 if not already) bash helm repo add eks https://aws.github.io/eks-charts helm install -n aws-load-balancer-controller \\ aws-load-balancer-controller eks/aws-load-balancer-controller \\ --set \"env.AWS_ACCESS_KEY_ID=${AWS_ID}\" \\ --set \"env.AWS_SECRET_ACCESS_KEY=${AWS_KEY}\" \\ --set \"vpcID=${VPC_ID}\" \\ --set \"clusterName=${CLUSTER_NAME}\" \\ --set \"image.tag=${ALB_VERSION}\" \\ --create-namespace","title":"AWS Load Balancer Controller"},{"location":"aws/waf/alb/#deploy-sample-application","text":"Create a new application in OpenShift bash oc new-project demo oc new-app https://github.com/sclorg/django-ex.git kubectl -n demo patch service django-ex -p '{\"spec\":{\"type\":\"NodePort\"}}' Create an Ingress to trigger an ALB bash cat << EOF | kubectl apply -f - apiVersion: networking.k8s.io/v1 kind: Ingress metadata: name: django-ex namespace: demo annotations: kubernetes.io/ingress.class: alb alb.ingress.kubernetes.io/scheme: internet-facing alb.ingress.kubernetes.io/target-type: instance # alb.ingress.kubernetes.io/subnets: subnet-0982bb73ca67d61de,subnet-0aa9967e8767d792f,subnet-0fd57669a80eb7596 alb.ingress.kubernetes.io/shield-advanced-protection: \"true\" # wafv2 arn to use # alb.ingress.kubernetes.io/wafv2-acl-arn: arn:aws:wafv2:us-east-2:660250927410:regional/webacl/waf-demo/6565d2a1-6d26-4b6b-b56f-1e996c7e9e8f labels: app: django-ex spec: rules: - host: foo.bar http: paths: - pathType: Prefix path: /* backend: service: name: django-ex port: number: 8080 Check the logs of the ALB controller kubectl logs -f deployment/aws-load-balancer-controller use the second address from the ingress to browse to the app kubectl -n demo get ingress bash curl -s --header \"Host: foo.bar\" k8s-demo-djangoex-49f31c1921-782305710.us-east-2.elb.amazonaws.com | head","title":"Deploy Sample Application"},{"location":"aws/waf/alb/#waf-time","text":"Create a WAF rule here https://console.aws.amazon.com/wafv2/homev2/web-acls/new and use the Core and SQL Injection rules. (make sure region matches us-east-2) View your WAF bash aws wafv2 list-web-acls --scope REGIONAL --region us-east-2 | jq . set the waf annotation to match the ARN provided above (and uncomment it) then re-apply the ingress bash kubectl apply -f ingress.yaml test the app still works bash curl -s --header \"Host: foo.bar\" --location \"k8s-demo-djangoex-49f31c1921-782305710.us-east-2.elb.amazonaws.com\" test the WAF denies a bad request You should get a 403 Forbidden error bash curl -X POST http://k8s-demo-djangoex-49f31c1921-782305710.us-east-2.elb.amazonaws.com -F \"user='<script><alert>Hello></alert></script>'\"","title":"WAF time"},{"location":"aws/waf/cloud-front/","text":"Using CloudFront + WAF Problem Statement Operator requires WAF (Web Application Firewall) in front of their workloads running on OpenShift (ROSA) Operator does not want WAF running on OpenShift to ensure that OCP resources do not experience Denial of Service through handling the WAF Proposed Solution Add a CustomDomain resource to the cluster using a wildcard DNS and TLS certificate. Set the Wildcard DNS CNAME's to CloudFront and enable the CloudFront + WAF services to reverse proxy and inspect the traffic before sending it to the cluster. Preparation Create a cluster bash rosa create cluster --cluster-name poc-waf --multi-az \\ --region us-east-2 --version 4.7.9 --compute-nodes 3 \\ --machine-cidr 10.0.0.0/16 --service-cidr 172.30.0.0/16 \\ --pod-cidr 10.128.0.0/14 --host-prefix 23 When its ready create a admin user and follow the instructions to log in bash rosa create admin -c poc-waf Set some environment variables bash EMAIL=username.taken@gmail.com DOMAIN=waf.mobb.ninja Certificate and DNS Use certbot to create a wildcard cert bash certbot certonly --manual \\ --preferred-challenges=dns \\ --email $EMAIL \\ --server https://acme-v02.api.letsencrypt.org/directory \\ --agree-tos \\ --manual-public-ip-logging-ok \\ -d \"*.$DOMAIN\" Follow Certbot's instructions to create a DNS TXT record. certificate records will be saved on your system, in my case in /etc/letsencrypt/live/waf.mobb.ninja/ . set that as an enviroment variable. bash CERTS=/etc/letsencrypt/live/waf.mobb.ninja Custom OpenShift Domain Create a project and add the certs as a secret bash oc new-project my-custom-route oc create secret tls acme-tls --cert=$CERTS/fullchain1.pem --key=$CERTS/privkey1.pem Create a Custom Domain resource bash cat << EOF | oc apply -f - apiVersion: managed.openshift.io/v1alpha1 kind: CustomDomain metadata: name: acme spec: domain: $DOMAIN certificate: name: acme-tls namespace: my-custom-route EOF Wait until your Custom Domain has an Endpoint bash watch oc get customdomains AWS WAF + CloudFront Create a WAF rule here https://console.aws.amazon.com/wafv2/homev2/web-acls/new?region=us-east-2 and use the Core and SQL Injection rules and set it as a CloudFront distribution resource type. View your WAF bash aws wafv2 list-web-acls --scope REGIONAL --region us-east-2 | jq . Add a certificate to ACM - https://us-east-2.console.aws.amazon.com/acm/home?region=us-east-1#/importwizard/, Paste in the cert, key, certchain from the files certbot game you. Make sure you create it in the US-EAST-1 region (otherwise cloud front can't use it) Log into the AWS console and Create a Cloud Front distribution (make sure its the same region as your cluster). Origin Domain Name: Origin Protocol Policy: HTTPS only Viewer Protocol Policy: Redirect HTTP to HTTPS Allowed HTTP Methods: GET, HEAD, OPTIONS, PUT, POST, PATCH, DELETE AWS WAF Web ACL: demo-waf-acl Alternate Domain Names: *. Custom SSL Certificate: Origin Request Policy: create a new policy whitelist: Origin, user-agent, referer, host ( IMPORTANT ) Hit Create then wait until the Status is Ready . DNS CNAME Create a CNAME in your DNS provider for *.<$DOMAIN> that points at the endpoint from the above status page. It should look something like d1vm7mfs9sc24l.cloudfront.net . Deploy an Application Create a new application bash oc new-app --docker-image=docker.io/openshift/hello-openshift Create a route for the application bash oc create route edge --service=hello-openshift hello-openshift-tls \\ --hostname hello.waf.mobb.ninja Test the WAF Make sure you can access your application with curl bash curl https://hello.waf.mobb.ninja You should get a simple hello response Hello OpenShift! Try do a XSS injection bash curl -X POST https://hello.waf.mobb.ninja \\ -F \"user='<script><alert>Hello></alert></script>'\" you should see this html <!DOCTYPE HTML PUBLIC \"-//W3C//DTD HTML 4.01 Transitional//EN\" \"http://www.w3.org/TR/html4/loose.dtd\"> <HTML><HEAD><META HTTP-EQUIV=\"Content-Type\" CONTENT=\"text/html; charset=iso-8859-1\"> <TITLE>ERROR: The request could not be satisfied</TITLE> </HEAD><BODY> <H1>403 ERROR</H1>","title":"Using CloudFront + WAF"},{"location":"aws/waf/cloud-front/#using-cloudfront-waf","text":"","title":"Using CloudFront + WAF"},{"location":"aws/waf/cloud-front/#problem-statement","text":"Operator requires WAF (Web Application Firewall) in front of their workloads running on OpenShift (ROSA) Operator does not want WAF running on OpenShift to ensure that OCP resources do not experience Denial of Service through handling the WAF","title":"Problem Statement"},{"location":"aws/waf/cloud-front/#proposed-solution","text":"Add a CustomDomain resource to the cluster using a wildcard DNS and TLS certificate. Set the Wildcard DNS CNAME's to CloudFront and enable the CloudFront + WAF services to reverse proxy and inspect the traffic before sending it to the cluster.","title":"Proposed Solution"},{"location":"aws/waf/cloud-front/#preparation","text":"Create a cluster bash rosa create cluster --cluster-name poc-waf --multi-az \\ --region us-east-2 --version 4.7.9 --compute-nodes 3 \\ --machine-cidr 10.0.0.0/16 --service-cidr 172.30.0.0/16 \\ --pod-cidr 10.128.0.0/14 --host-prefix 23 When its ready create a admin user and follow the instructions to log in bash rosa create admin -c poc-waf Set some environment variables bash EMAIL=username.taken@gmail.com DOMAIN=waf.mobb.ninja","title":"Preparation"},{"location":"aws/waf/cloud-front/#certificate-and-dns","text":"Use certbot to create a wildcard cert bash certbot certonly --manual \\ --preferred-challenges=dns \\ --email $EMAIL \\ --server https://acme-v02.api.letsencrypt.org/directory \\ --agree-tos \\ --manual-public-ip-logging-ok \\ -d \"*.$DOMAIN\" Follow Certbot's instructions to create a DNS TXT record. certificate records will be saved on your system, in my case in /etc/letsencrypt/live/waf.mobb.ninja/ . set that as an enviroment variable. bash CERTS=/etc/letsencrypt/live/waf.mobb.ninja","title":"Certificate and DNS"},{"location":"aws/waf/cloud-front/#custom-openshift-domain","text":"Create a project and add the certs as a secret bash oc new-project my-custom-route oc create secret tls acme-tls --cert=$CERTS/fullchain1.pem --key=$CERTS/privkey1.pem Create a Custom Domain resource bash cat << EOF | oc apply -f - apiVersion: managed.openshift.io/v1alpha1 kind: CustomDomain metadata: name: acme spec: domain: $DOMAIN certificate: name: acme-tls namespace: my-custom-route EOF Wait until your Custom Domain has an Endpoint bash watch oc get customdomains","title":"Custom OpenShift Domain"},{"location":"aws/waf/cloud-front/#aws-waf-cloudfront","text":"Create a WAF rule here https://console.aws.amazon.com/wafv2/homev2/web-acls/new?region=us-east-2 and use the Core and SQL Injection rules and set it as a CloudFront distribution resource type. View your WAF bash aws wafv2 list-web-acls --scope REGIONAL --region us-east-2 | jq . Add a certificate to ACM - https://us-east-2.console.aws.amazon.com/acm/home?region=us-east-1#/importwizard/, Paste in the cert, key, certchain from the files certbot game you. Make sure you create it in the US-EAST-1 region (otherwise cloud front can't use it) Log into the AWS console and Create a Cloud Front distribution (make sure its the same region as your cluster). Origin Domain Name: Origin Protocol Policy: HTTPS only Viewer Protocol Policy: Redirect HTTP to HTTPS Allowed HTTP Methods: GET, HEAD, OPTIONS, PUT, POST, PATCH, DELETE AWS WAF Web ACL: demo-waf-acl Alternate Domain Names: *. Custom SSL Certificate: Origin Request Policy: create a new policy whitelist: Origin, user-agent, referer, host ( IMPORTANT ) Hit Create then wait until the Status is Ready .","title":"AWS WAF + CloudFront"},{"location":"aws/waf/cloud-front/#dns-cname","text":"Create a CNAME in your DNS provider for *.<$DOMAIN> that points at the endpoint from the above status page. It should look something like d1vm7mfs9sc24l.cloudfront.net .","title":"DNS CNAME"},{"location":"aws/waf/cloud-front/#deploy-an-application","text":"Create a new application bash oc new-app --docker-image=docker.io/openshift/hello-openshift Create a route for the application bash oc create route edge --service=hello-openshift hello-openshift-tls \\ --hostname hello.waf.mobb.ninja","title":"Deploy an Application"},{"location":"aws/waf/cloud-front/#test-the-waf","text":"Make sure you can access your application with curl bash curl https://hello.waf.mobb.ninja You should get a simple hello response Hello OpenShift! Try do a XSS injection bash curl -X POST https://hello.waf.mobb.ninja \\ -F \"user='<script><alert>Hello></alert></script>'\" you should see this html <!DOCTYPE HTML PUBLIC \"-//W3C//DTD HTML 4.01 Transitional//EN\" \"http://www.w3.org/TR/html4/loose.dtd\"> <HTML><HEAD><META HTTP-EQUIV=\"Content-Type\" CONTENT=\"text/html; charset=iso-8859-1\"> <TITLE>ERROR: The request could not be satisfied</TITLE> </HEAD><BODY> <H1>403 ERROR</H1>","title":"Test the WAF"},{"location":"blog/","text":"MOBB Blogs {{ blog_content }}","title":"Blogs"},{"location":"blog/#mobb-blogs","text":"{{ blog_content }}","title":"MOBB Blogs"},{"location":"blog/posts/2nd_blog/","tags":["blogging"],"text":"Content Title This is my blog content","title":"The Title of My Second Blog"},{"location":"blog/posts/2nd_blog/#content-title","text":"This is my blog content","title":"Content Title"},{"location":"blog/posts/hello-world/","tags":["blogging"],"text":"Hello World This is my blog content","title":"Hello World"},{"location":"blog/posts/hello-world/#hello-world","text":"This is my blog content","title":"Hello World"},{"location":"demos/crane/","text":"Migrate Kubernetes Applications with Konveyer Crane 7/29/2021 by Paul Czarkowski and JJ Asghar Introduction Occasionally when you're moving between major version of Kubernetes or Red Hat OpenShift, you'll want to migrate your applications between clusters. Or if you're moving between two clouds, you'll want an easy way to migrate your workloads from one platform to another. The Crane operator from the open source Konveyer project automates this migration process for you. The Konveyer site offers a selection of helpful projects to administer your cluster. Crane is designed to automate migration from one cluster to another and is surprisingly easy to get working. This article shows you how we moved a default sample application from a Red Hat OpenShift on AWS (ROSA) to a Red Hat OpenShift on IBM Cloud (ROIC) cluster. To see how it's done, watch the video or read the steps below. Install the Crane operator First, log into the cluster console where your original application is hosted and also log into the console of the destination where you want to migrate your application. In our example, we logged into the OpenShift Service on AWS as our origin console and, in another tab, logged into to the Red Hat OpenShift on IBM Cloud console as our destination console. From the Operator Hub in both consoles, search for \"Crane Operator\" and follow the default prompts to install the operator. Set up your sample application From your origin cluster, choose the Developer profile and then click +Add to add a project where you will deploy your application into. Choose a sample app to play with. In our example, we chose the Python application. Name it and then click \u201cCreate\u201d. It will pull the source information from GitHub and build an image, deploy the image, and expose it as a PHP endpoint. You can change back from a Developer profile to the Admin profile in order to see if the operator has been installed correctly. Create a migration (MIG) controller Now it\u2019s time to create your migration controller. Go to your m migration cluster (in this example, our IBM console), select the Crane operator, and select Create migration controller . Do the same on the origin cluster (in our example, AWS). Switch to the openshift-migration namespace. Update the host MIG controller. yaml apiVersion: migration.openshift.io/v1alpha1 kind: MigCluster metadata: name: host namespace: openshift-migration spec: isHostCluster: true Then you can apply your migration cluster. bash kubectl apply -f origin-migcluster.yaml NOTE : Run only this following command on the remote cluster Save your service account secret for the destination cluster. bash oc sa get-token migration-controller -n openshift-migration | base64 -w 0 Write this into sa-secret-remote.yaml on your origin cluster: yaml apiVersion: v1 kind: Secret metadata: name: sa-token-remote namespace: openshift-config type: Opaque data: # [!] Change saToken to contain a base64 encoded SA token with cluster-admin # privileges on the remote cluster. # `oc sa get-token migration-controller -n openshift-migration | base64 -w 0` saToken: <your-base64-encoded-aws-sa-token-here> bash kubectl apply -f sa-secret-remote.yaml Add your destination cluster: yaml apiVersion: migration.openshift.io/v1alpha1 kind: MigCluster metadata: name: src-ocp-3-cluster namespace: openshift-migration spec: insecure: true isHostCluster: false serviceAccountSecretRef: name: sa-token-remote namespace: openshift-config url: 'https://master.ocp3.mycluster.com/' bash kubectl apply -f dest-migcluster.yaml Configure s3 credentials to host migration storage. Included here is the correct access key for my files. You'll need to have that handy. yaml apiVersion: v1 kind: Secret metadata: namespace: openshift-config name: migstorage-creds type: Opaque data: aws-access-key-id: aGVsbG8K aws-secret-access-key: aGVsbG8K bash kubectl apply -f mig-storage-creds.yaml Configure MIG storage to use s3 yaml apiVersion: migration.openshift.io/v1alpha1 kind: MigStorage metadata: name: aws-s3 namespace: openshift-migration spec: backupStorageConfig: awsBucketName: konveyer-jj-migration # You need to change this for your s3 bucket credsSecretRef: name: migstorage-creds namespace: openshift-config backupStorageProvider: aws volumeSnapshotConfig: credsSecretRef: name: migstorage-creds namespace: openshift-config volumeSnapshotProvider: aws bash kubectl apply -f migstorage.yaml Create the migration plan. THe plan is essentially saying: \"This is what I want\". The plan says what namespace you want to move. In this example, it\u2019s project-a as referenced below. yaml apiVersion: migration.openshift.io/v1alpha1 kind: MigPlan metadata: name: migrate-project-a namespace: openshift-migration spec: destMigClusterRef: name: destination namespace: openshift-migration indirectImageMigration: true indirectVolumeMigration: true srcMigClusterRef: name: host namespace: openshift-migration migStorageRef: name: aws-s3 namespace: openshift-migration namespaces: - project-a # notice this is where you'd add other projects persistentVolumes: [] bash kubectl apply -f migplan.yaml Execute your migration plan. yaml apiVersion: migration.openshift.io/v1alpha1 kind: MigMigration metadata: name: migrate-project-a-execute namespace: openshift-migration spec: migPlanRef: name: migrate-project-a namespace: openshift-migration quiescePods: true stage: false bash kubectl apply -f migplanexecute.yaml Watch the magic Back in your original console, you can find the correct namespace and see that things have moved. Go back to the origin OpenShift console (AWS in our example). Bring up the migration GUI from the openshift-migration namespace. Check through the migration plans which show you the migration or watch the logs to see the migration happening. Or, watch the progress via the CLI if you prefer. kubectl logs -f migration-log-reader-<hash> color You can also open your destination console (Red Hat OpenShift on IBM Cloud in our example) and see if the new cluster has migrated. Conclusion Hopefully walking through these steps has helped you understand the power that Crane can offer you when migrating workloads between clusters. This was only a sample application. With a little work and testing, you should be able to leverage Crane for your applications. If you have any questions or thoughts, come around to #konveyer on the Kubernetes public Slack channel, and the team would me more then willing to help advise you. Happy migrating your apps!","title":"Migrate Kubernetes Applications with Konveyer Crane"},{"location":"demos/crane/#migrate-kubernetes-applications-with-konveyer-crane","text":"7/29/2021 by Paul Czarkowski and JJ Asghar","title":"Migrate Kubernetes Applications with Konveyer Crane"},{"location":"demos/crane/#introduction","text":"Occasionally when you're moving between major version of Kubernetes or Red Hat OpenShift, you'll want to migrate your applications between clusters. Or if you're moving between two clouds, you'll want an easy way to migrate your workloads from one platform to another. The Crane operator from the open source Konveyer project automates this migration process for you. The Konveyer site offers a selection of helpful projects to administer your cluster. Crane is designed to automate migration from one cluster to another and is surprisingly easy to get working. This article shows you how we moved a default sample application from a Red Hat OpenShift on AWS (ROSA) to a Red Hat OpenShift on IBM Cloud (ROIC) cluster. To see how it's done, watch the video or read the steps below.","title":"Introduction"},{"location":"demos/crane/#install-the-crane-operator","text":"First, log into the cluster console where your original application is hosted and also log into the console of the destination where you want to migrate your application. In our example, we logged into the OpenShift Service on AWS as our origin console and, in another tab, logged into to the Red Hat OpenShift on IBM Cloud console as our destination console. From the Operator Hub in both consoles, search for \"Crane Operator\" and follow the default prompts to install the operator.","title":"Install the Crane operator"},{"location":"demos/crane/#set-up-your-sample-application","text":"From your origin cluster, choose the Developer profile and then click +Add to add a project where you will deploy your application into. Choose a sample app to play with. In our example, we chose the Python application. Name it and then click \u201cCreate\u201d. It will pull the source information from GitHub and build an image, deploy the image, and expose it as a PHP endpoint. You can change back from a Developer profile to the Admin profile in order to see if the operator has been installed correctly.","title":"Set up your sample application"},{"location":"demos/crane/#create-a-migration-mig-controller","text":"Now it\u2019s time to create your migration controller. Go to your m migration cluster (in this example, our IBM console), select the Crane operator, and select Create migration controller . Do the same on the origin cluster (in our example, AWS). Switch to the openshift-migration namespace. Update the host MIG controller. yaml apiVersion: migration.openshift.io/v1alpha1 kind: MigCluster metadata: name: host namespace: openshift-migration spec: isHostCluster: true Then you can apply your migration cluster. bash kubectl apply -f origin-migcluster.yaml NOTE : Run only this following command on the remote cluster Save your service account secret for the destination cluster. bash oc sa get-token migration-controller -n openshift-migration | base64 -w 0 Write this into sa-secret-remote.yaml on your origin cluster: yaml apiVersion: v1 kind: Secret metadata: name: sa-token-remote namespace: openshift-config type: Opaque data: # [!] Change saToken to contain a base64 encoded SA token with cluster-admin # privileges on the remote cluster. # `oc sa get-token migration-controller -n openshift-migration | base64 -w 0` saToken: <your-base64-encoded-aws-sa-token-here> bash kubectl apply -f sa-secret-remote.yaml Add your destination cluster: yaml apiVersion: migration.openshift.io/v1alpha1 kind: MigCluster metadata: name: src-ocp-3-cluster namespace: openshift-migration spec: insecure: true isHostCluster: false serviceAccountSecretRef: name: sa-token-remote namespace: openshift-config url: 'https://master.ocp3.mycluster.com/' bash kubectl apply -f dest-migcluster.yaml Configure s3 credentials to host migration storage. Included here is the correct access key for my files. You'll need to have that handy. yaml apiVersion: v1 kind: Secret metadata: namespace: openshift-config name: migstorage-creds type: Opaque data: aws-access-key-id: aGVsbG8K aws-secret-access-key: aGVsbG8K bash kubectl apply -f mig-storage-creds.yaml Configure MIG storage to use s3 yaml apiVersion: migration.openshift.io/v1alpha1 kind: MigStorage metadata: name: aws-s3 namespace: openshift-migration spec: backupStorageConfig: awsBucketName: konveyer-jj-migration # You need to change this for your s3 bucket credsSecretRef: name: migstorage-creds namespace: openshift-config backupStorageProvider: aws volumeSnapshotConfig: credsSecretRef: name: migstorage-creds namespace: openshift-config volumeSnapshotProvider: aws bash kubectl apply -f migstorage.yaml Create the migration plan. THe plan is essentially saying: \"This is what I want\". The plan says what namespace you want to move. In this example, it\u2019s project-a as referenced below. yaml apiVersion: migration.openshift.io/v1alpha1 kind: MigPlan metadata: name: migrate-project-a namespace: openshift-migration spec: destMigClusterRef: name: destination namespace: openshift-migration indirectImageMigration: true indirectVolumeMigration: true srcMigClusterRef: name: host namespace: openshift-migration migStorageRef: name: aws-s3 namespace: openshift-migration namespaces: - project-a # notice this is where you'd add other projects persistentVolumes: [] bash kubectl apply -f migplan.yaml Execute your migration plan. yaml apiVersion: migration.openshift.io/v1alpha1 kind: MigMigration metadata: name: migrate-project-a-execute namespace: openshift-migration spec: migPlanRef: name: migrate-project-a namespace: openshift-migration quiescePods: true stage: false bash kubectl apply -f migplanexecute.yaml","title":"Create a migration (MIG) controller"},{"location":"demos/crane/#watch-the-magic","text":"Back in your original console, you can find the correct namespace and see that things have moved. Go back to the origin OpenShift console (AWS in our example). Bring up the migration GUI from the openshift-migration namespace. Check through the migration plans which show you the migration or watch the logs to see the migration happening. Or, watch the progress via the CLI if you prefer. kubectl logs -f migration-log-reader-<hash> color You can also open your destination console (Red Hat OpenShift on IBM Cloud in our example) and see if the new cluster has migrated.","title":"Watch the magic"},{"location":"demos/crane/#conclusion","text":"Hopefully walking through these steps has helped you understand the power that Crane can offer you when migrating workloads between clusters. This was only a sample application. With a little work and testing, you should be able to leverage Crane for your applications. If you have any questions or thoughts, come around to #konveyer on the Kubernetes public Slack channel, and the team would me more then willing to help advise you. Happy migrating your apps!","title":"Conclusion"},{"location":"demos/gitops/","text":"Demonstrate GitOps on Managed OpenShift with ArgoCD Author: Steve Mirman Video Walkthrough If you prefer a more visual medium, you can watch Steve Mirman walk through this quickstart on YouTube . The purpose of this document is to help you get OpenShift GitOps running in your cluster, including deploying a sample application and demonstrating how ArgoCD ensures environment consistency. This demo assumes you have a Managed OpenShift Cluster available and cluster-admin rights. GitHub resources referenced in the demo: BGD Application: gitops-bgd-app OpenShift / ArgoCD configuration: gitops-demo Required command line (CLI) tools GitHub: git OpenShift: oc ArgoCD: argocd Kustomize: kam Environment Set Up Install the OpenShift GitOps operator Install the OpenShift GitOps operator from the Operator Hub Pull files from GitHub Clone the gitops-demo GitHub repository to your local machine git clone https://github.com/rh-mobb/gitops-demo gitops Export your local path to the GitHub files export GITOPS_HOME=\"$(pwd)/gitops\" cd $GITOPS_HOME Log in to OpenShift via the CLI Retrieve the login command from the OpenShift console Enter the command in your terminal to authenticate with the OpenShift CLI (oc) >Output should appear similar to: Logged into \"https://<YOUR-INSTANCE>.openshiftapps.com:6443\" as \"<YOUR-ID>\" using the token provided. Deploy the ArgoCD Project Create a new OpenShift project Create a new OpenShift project called gitops oc new-project gitops Edit service account permissions Add cluster-admin rights to the openshift-gitops-argocd-application-controller service account in the openshift-gitops namespace oc adm policy add-cluster-role-to-user cluster-admin -z openshift-gitops-argocd-application-controller -n openshift-gitops Log in to ArgoCD Retrieve ArgoCD URL: argoURL=$(oc get route openshift-gitops-server -n openshift-gitops -o jsonpath='{.spec.host}{\"\\n\"}') echo $argoURL Retrieve ArgoCD Password: argoPass=$(oc get secret/openshift-gitops-cluster -n openshift-gitops -o jsonpath='{.data.admin\\.password}' | base64 -d) echo $argoPass In a browser, navigate to the ArgoCD console using the $argoURL value returned above Log in with the user name admin and the password returned as $argoPass above Optional step if you prefer CLI access Login to the CLI: argocd login --insecure --grpc-web $argoURL --username admin --password $argoPass Deploy the ArgoCD project Use kubectl to apply the bgd-app.yaml file kubectl apply -f documentation/modules/ROOT/examples/bgd-app/bgd-app.yaml >The bgd-app.yaml file defines several things, including the repo location for the gitops-bgd-app application Check the rollout running the following command: kubectl rollout status deploy/bgd -n bgd Once the rollout is complete get the route to the application oc get route bgd -n bgd -o jsonpath='{.spec.host}{\"\\n\"}' In your browser, paste the route to open the application Go back to your ArgoCD window and verify the configuration shows there as well Exploring the application in ArgoCD, you can see all the components are green (synchronized) Deploy a change to the application In the terminal, enter the following command which will introduce a chance into the bgd application kubectl -n bgd patch deploy/bgd --type='json' -p='[{\"op\": \"replace\", \"path\": \"/spec/template/spec/containers/0/env/0/value\", \"value\":\"green\"}]' Go back to your ArgoCD window. The application should no longer be synchronized Refresh the bgd application window and notice the change in box color > The new deployment changed the box from blue to green, but only within OpenShift, not in the source code repository Synchronize the application In the ArgoCD console, click the SYNC button to re-synchronize the bgd application with the approved configuration in the source code repository Refresh the bgd application window and notice the change in box color Details from GitHub perspective TBD","title":"Demonstrate GitOps on Managed OpenShift with ArgoCD"},{"location":"demos/gitops/#demonstrate-gitops-on-managed-openshift-with-argocd","text":"Author: Steve Mirman","title":"Demonstrate GitOps on Managed OpenShift with ArgoCD"},{"location":"demos/gitops/#video-walkthrough","text":"If you prefer a more visual medium, you can watch Steve Mirman walk through this quickstart on YouTube . The purpose of this document is to help you get OpenShift GitOps running in your cluster, including deploying a sample application and demonstrating how ArgoCD ensures environment consistency. This demo assumes you have a Managed OpenShift Cluster available and cluster-admin rights.","title":"Video Walkthrough"},{"location":"demos/gitops/#github-resources-referenced-in-the-demo","text":"BGD Application: gitops-bgd-app OpenShift / ArgoCD configuration: gitops-demo","title":"GitHub resources referenced in the demo:"},{"location":"demos/gitops/#required-command-line-cli-tools","text":"GitHub: git OpenShift: oc ArgoCD: argocd Kustomize: kam","title":"Required command line (CLI) tools"},{"location":"demos/gitops/#environment-set-up","text":"","title":"Environment Set Up"},{"location":"demos/gitops/#install-the-openshift-gitops-operator","text":"Install the OpenShift GitOps operator from the Operator Hub","title":"Install the OpenShift GitOps operator"},{"location":"demos/gitops/#pull-files-from-github","text":"Clone the gitops-demo GitHub repository to your local machine git clone https://github.com/rh-mobb/gitops-demo gitops Export your local path to the GitHub files export GITOPS_HOME=\"$(pwd)/gitops\" cd $GITOPS_HOME","title":"Pull files from GitHub"},{"location":"demos/gitops/#log-in-to-openshift-via-the-cli","text":"Retrieve the login command from the OpenShift console Enter the command in your terminal to authenticate with the OpenShift CLI (oc) >Output should appear similar to: Logged into \"https://<YOUR-INSTANCE>.openshiftapps.com:6443\" as \"<YOUR-ID>\" using the token provided.","title":"Log in to OpenShift via the CLI"},{"location":"demos/gitops/#deploy-the-argocd-project","text":"","title":"Deploy the ArgoCD Project"},{"location":"demos/gitops/#create-a-new-openshift-project","text":"Create a new OpenShift project called gitops oc new-project gitops","title":"Create a new OpenShift project"},{"location":"demos/gitops/#edit-service-account-permissions","text":"Add cluster-admin rights to the openshift-gitops-argocd-application-controller service account in the openshift-gitops namespace oc adm policy add-cluster-role-to-user cluster-admin -z openshift-gitops-argocd-application-controller -n openshift-gitops","title":"Edit service account permissions"},{"location":"demos/gitops/#log-in-to-argocd","text":"Retrieve ArgoCD URL: argoURL=$(oc get route openshift-gitops-server -n openshift-gitops -o jsonpath='{.spec.host}{\"\\n\"}') echo $argoURL Retrieve ArgoCD Password: argoPass=$(oc get secret/openshift-gitops-cluster -n openshift-gitops -o jsonpath='{.data.admin\\.password}' | base64 -d) echo $argoPass In a browser, navigate to the ArgoCD console using the $argoURL value returned above Log in with the user name admin and the password returned as $argoPass above Optional step if you prefer CLI access Login to the CLI: argocd login --insecure --grpc-web $argoURL --username admin --password $argoPass","title":"Log in to ArgoCD"},{"location":"demos/gitops/#deploy-the-argocd-project_1","text":"Use kubectl to apply the bgd-app.yaml file kubectl apply -f documentation/modules/ROOT/examples/bgd-app/bgd-app.yaml >The bgd-app.yaml file defines several things, including the repo location for the gitops-bgd-app application Check the rollout running the following command: kubectl rollout status deploy/bgd -n bgd Once the rollout is complete get the route to the application oc get route bgd -n bgd -o jsonpath='{.spec.host}{\"\\n\"}' In your browser, paste the route to open the application Go back to your ArgoCD window and verify the configuration shows there as well Exploring the application in ArgoCD, you can see all the components are green (synchronized)","title":"Deploy the ArgoCD project"},{"location":"demos/gitops/#deploy-a-change-to-the-application","text":"In the terminal, enter the following command which will introduce a chance into the bgd application kubectl -n bgd patch deploy/bgd --type='json' -p='[{\"op\": \"replace\", \"path\": \"/spec/template/spec/containers/0/env/0/value\", \"value\":\"green\"}]' Go back to your ArgoCD window. The application should no longer be synchronized Refresh the bgd application window and notice the change in box color > The new deployment changed the box from blue to green, but only within OpenShift, not in the source code repository","title":"Deploy a change to the application"},{"location":"demos/gitops/#synchronize-the-application","text":"In the ArgoCD console, click the SYNC button to re-synchronize the bgd application with the approved configuration in the source code repository Refresh the bgd application window and notice the change in box color","title":"Synchronize the application"},{"location":"demos/gitops/#details-from-github-perspective","text":"TBD","title":"Details from GitHub perspective"},{"location":"gcp/filestore/","text":"Create Filestore Storage for OSD in GCP Author: Roberto Carratal\u00e1 , Paul Czarkowski , Andrea Bozzoni By default, within OSD in GCP only the GCE-PD StorageClass is available in the cluster. With this StorageClass, only ReadWriteOnce mode is permitted, and the gcePersistentDisks can only be mounted by a single consumer in read-write mode . Because of that, and for provide Storage with Shared Access (RWX) Access Mode to our OpenShift clusters a GCP Filestore could be used. GCP Filestore is not managed neither supported by Red Hat or Red Hat SRE team. Prerequisites gcloud CLI jq oc CLI The GCP Cloud Shell can be used as well and have all the prerequisites installed already. Steps From the CLI or GCP Cloud Shell, login within your account and your GCP project: sh gcloud auth login <google account user> gcloud config set project <google project name> Create a Filestore instance in GCP: ```sh export ZONE_FS=\"us-west1-a\" export NAME_FS=\"nfs-server\" export TIER_FS=\"BASIC_HDD\" export VOL_NAME_FS=\"osd4\" export CAPACITY=\"1TB\" export VPC_NETWORK=\"projects/my-project/global/networks/demo-vpc\" gcloud filestore instances create $NAME_FS --zone=$ZONE_FS --tier=$TIER_FS --file-share=name=\"$VOL_NAME_FS\",capacity=$CAPACITY --network=name=\"$VPC_NETWORK\" ``` Due to the Static Provisioning through the creation of the PV/PVC the Filestore for the RWX storage needs to be created upfront. After the creation, check the Filestore instance generated in the GCP project: sh gcloud filestore instances describe $NAME_FS --zone=$ZONE_FS Extract the ipAddresses from the NFS share for use them into the PV definition: ```sh NFS_IP=$(gcloud filestore instances describe $NAME_FS --zone=$ZONE_FS --format=json | jq -r .networks[0].ipAddresses[0]) echo $NFS_IP ``` Login your OSD in GCP cluster Create a Persistent Volume using the NFS_IP of the Filestore as the nfs server into the PV definition, specifying the path of the shared Filestore: sh cat <<EOF | oc apply -f - apiVersion: v1 kind: PersistentVolume metadata: name: nfs spec: capacity: storage: 500Gi accessModes: - ReadWriteMany nfs: server: $NFS_IP path: \"/$VOL_NAME_FS\" EOF As you can check the PV is generated with the accessMode of ReadWriteMany (RWX) Check that the PV is generated properly: sh $ oc get pv nfs NAME CAPACITY ACCESS MODES RECLAIM POLICY STATUS CLAIM STORAGECLASS REASON AGE nfs 500Gi RWX Retain Available 12s Create a PersistentVolumeClaim for this PersistentVolume: sh cat <<EOF | oc apply -f - apiVersion: v1 kind: PersistentVolumeClaim metadata: name: nfs spec: accessModes: - ReadWriteMany storageClassName: \"\" resources: requests: storage: 500Gi EOF As we can check the storageClassName is empty because we're using the Static Provisioning in this case. Check that the PVC is generated properly and with the Bound status: sh oc get pvc nfs NAME STATUS VOLUME CAPACITY ACCESS MODES STORAGECLASS AGE nfs Bound nfs 500Gi RWX 7s Generate an example app with more than replicas sharing the same Filestore NFS volume share: sh cat <<EOF | oc apply -f - apiVersion: apps/v1 kind: Deployment metadata: creationTimestamp: null labels: app: nfs-web2 name: nfs-web spec: replicas: 2 selector: matchLabels: app: nfs-web strategy: {} template: metadata: creationTimestamp: null labels: app: nfs-web spec: containers: - image: nginxinc/nginx-unprivileged name: nginx-unprivileged ports: - name: web containerPort: 8080 volumeMounts: - name: nfs mountPath: \"/usr/share/nginx/html\" volumes: - name: nfs persistentVolumeClaim: claimName: nfs EOF Check that the pods are up && running: sh oc get pod NAME READY STATUS RESTARTS AGE nfs-web2-54f9fb5cd8-8dcgh 1/1 Running 0 118s nfs-web2-54f9fb5cd8-bhmkw 1/1 Running 0 118s Check that the pods mount the same volume provided by the Filestore NFS share: ```sh for i in $(oc get pod --no-headers | awk '{ print $1 }'); do echo \"POD -> $i\"; oc exec -ti $i -- df -h | grep nginx; echo \"\"; done POD -> nfs-web2-54f9fb5cd8-8dcgh 10.124.186.98:/osd4 1007G 0 956G 0% /usr/share/nginx/html POD -> nfs-web2-54f9fb5cd8-bhmkw 10.124.186.98:/osd4 1007G 0 956G 0% /usr/share/nginx/html ```","title":"Using Filestore w/ OSD"},{"location":"gcp/filestore/#create-filestore-storage-for-osd-in-gcp","text":"Author: Roberto Carratal\u00e1 , Paul Czarkowski , Andrea Bozzoni By default, within OSD in GCP only the GCE-PD StorageClass is available in the cluster. With this StorageClass, only ReadWriteOnce mode is permitted, and the gcePersistentDisks can only be mounted by a single consumer in read-write mode . Because of that, and for provide Storage with Shared Access (RWX) Access Mode to our OpenShift clusters a GCP Filestore could be used. GCP Filestore is not managed neither supported by Red Hat or Red Hat SRE team.","title":"Create Filestore Storage for OSD in GCP"},{"location":"gcp/filestore/#prerequisites","text":"gcloud CLI jq oc CLI The GCP Cloud Shell can be used as well and have all the prerequisites installed already.","title":"Prerequisites"},{"location":"gcp/filestore/#steps","text":"From the CLI or GCP Cloud Shell, login within your account and your GCP project: sh gcloud auth login <google account user> gcloud config set project <google project name> Create a Filestore instance in GCP: ```sh export ZONE_FS=\"us-west1-a\" export NAME_FS=\"nfs-server\" export TIER_FS=\"BASIC_HDD\" export VOL_NAME_FS=\"osd4\" export CAPACITY=\"1TB\" export VPC_NETWORK=\"projects/my-project/global/networks/demo-vpc\" gcloud filestore instances create $NAME_FS --zone=$ZONE_FS --tier=$TIER_FS --file-share=name=\"$VOL_NAME_FS\",capacity=$CAPACITY --network=name=\"$VPC_NETWORK\" ``` Due to the Static Provisioning through the creation of the PV/PVC the Filestore for the RWX storage needs to be created upfront. After the creation, check the Filestore instance generated in the GCP project: sh gcloud filestore instances describe $NAME_FS --zone=$ZONE_FS Extract the ipAddresses from the NFS share for use them into the PV definition: ```sh NFS_IP=$(gcloud filestore instances describe $NAME_FS --zone=$ZONE_FS --format=json | jq -r .networks[0].ipAddresses[0]) echo $NFS_IP ``` Login your OSD in GCP cluster Create a Persistent Volume using the NFS_IP of the Filestore as the nfs server into the PV definition, specifying the path of the shared Filestore: sh cat <<EOF | oc apply -f - apiVersion: v1 kind: PersistentVolume metadata: name: nfs spec: capacity: storage: 500Gi accessModes: - ReadWriteMany nfs: server: $NFS_IP path: \"/$VOL_NAME_FS\" EOF As you can check the PV is generated with the accessMode of ReadWriteMany (RWX) Check that the PV is generated properly: sh $ oc get pv nfs NAME CAPACITY ACCESS MODES RECLAIM POLICY STATUS CLAIM STORAGECLASS REASON AGE nfs 500Gi RWX Retain Available 12s Create a PersistentVolumeClaim for this PersistentVolume: sh cat <<EOF | oc apply -f - apiVersion: v1 kind: PersistentVolumeClaim metadata: name: nfs spec: accessModes: - ReadWriteMany storageClassName: \"\" resources: requests: storage: 500Gi EOF As we can check the storageClassName is empty because we're using the Static Provisioning in this case. Check that the PVC is generated properly and with the Bound status: sh oc get pvc nfs NAME STATUS VOLUME CAPACITY ACCESS MODES STORAGECLASS AGE nfs Bound nfs 500Gi RWX 7s Generate an example app with more than replicas sharing the same Filestore NFS volume share: sh cat <<EOF | oc apply -f - apiVersion: apps/v1 kind: Deployment metadata: creationTimestamp: null labels: app: nfs-web2 name: nfs-web spec: replicas: 2 selector: matchLabels: app: nfs-web strategy: {} template: metadata: creationTimestamp: null labels: app: nfs-web spec: containers: - image: nginxinc/nginx-unprivileged name: nginx-unprivileged ports: - name: web containerPort: 8080 volumeMounts: - name: nfs mountPath: \"/usr/share/nginx/html\" volumes: - name: nfs persistentVolumeClaim: claimName: nfs EOF Check that the pods are up && running: sh oc get pod NAME READY STATUS RESTARTS AGE nfs-web2-54f9fb5cd8-8dcgh 1/1 Running 0 118s nfs-web2-54f9fb5cd8-bhmkw 1/1 Running 0 118s Check that the pods mount the same volume provided by the Filestore NFS share: ```sh for i in $(oc get pod --no-headers | awk '{ print $1 }'); do echo \"POD -> $i\"; oc exec -ti $i -- df -h | grep nginx; echo \"\"; done POD -> nfs-web2-54f9fb5cd8-8dcgh 10.124.186.98:/osd4 1007G 0 956G 0% /usr/share/nginx/html POD -> nfs-web2-54f9fb5cd8-bhmkw 10.124.186.98:/osd4 1007G 0 956G 0% /usr/share/nginx/html ```","title":"Steps"},{"location":"gcp/osd_preexisting_vpc/","text":"Creating a OSD in GCP with Existing VPCs Roberto Carratal\u00e1, Andrea Bozzoni Last updated 07/06/2022 Tip The official documentation for installing a OSD cluster in GCP can be found here . For deploy an OSD cluster in GCP using existing Virtual Private Cloud (VPC) you need to implement some prerequisites that you must create before starting the OpenShift Dedicated installation though the OCM. Prerequisites gcloud CLI jq NOTE: Also the GCloud Shell can be used, and have the gcloud cli among other tools preinstalled. Generate GCP VPC and Subnets This is a diagram showing the GCP infra prerequisites that are needed for the OSD installation: {: width=\"750\" } To deploy the GCP VPC and subnets among other prerequisites for install the OSD in GCP using the preexisting VPCs you have two options: Option 1 - GCloud CLI Option 2 - Terraform Automation Please select one of these two options and proceed with the OSD install steps. Option 1 - Generate OSD VPC and Subnets using GCloud CLI As mentioned before, for deploy OSD in GCP using existing GCP VPC, you need to provide and create beforehand a GCP VPC and two subnets (one for the masters and another for the workers nodes). Login and configure the proper GCP project where the OSD will be deployed: sh export PROJECT_NAME=<google project name> gcloud auth list gcloud config set project $PROJECT_NAME gcloud config list project Export the names of the vpc and subnets: sh export REGION=<region name> export OSD_VPC=<vpc name> export MASTER_SUBNET=<master subnet name> export WORKER_SUBNET=<worker subnet name> Create a custom mode VPC network: sh gcloud compute networks create $OSD_VPC --subnet-mode=custom gcloud compute networks describe $OSD_VPC NOTE: we need to create the mode custom for the VPC network, because the auto mode generates automatically the subnets with IPv4 ranges with predetermined set of ranges . This example is using the standard configuration for these two subnets: md master-subnet - CIDR 10.0.0.0/17 - Gateway 10.0.0.1 worker-subnet - CIDR 10.0.128.0/17 - Gateway 10.0.128.1 Create the GCP Subnets for the masters and workers within the previous GCP VPC network: ```sh gcloud compute networks subnets create $MASTER_SUBNET \\ --network=$OSD_VPC --range=10.0.0.0/17 --region=$REGION gcloud compute networks subnets create $WORKER_SUBNET \\ --network=$OSD_VPC --range=10.0.128.0/17 --region=$REGION ``` {: width=\"750\" } Once the VPC and the two subnets are provided it is needed to create one GCP Cloud Router : ```sh export OSD_ROUTER= gcloud compute routers create $OSD_ROUTER \\ --project=$PROJECT_NAME --network=$OSD_VPC --region=$REGION ``` {:style=\"display:block; margin-left:auto; margin-right:auto\"}{: width=\"350\" } Then, we will deploy two GCP Cloud NATs and attach them within the GCP Router: Generate the GCP Cloud Nat for the Master Subnets: ```sh export NAT_MASTER= gcloud compute routers nats create $NAT_MASTER \\ --region=$REGION \\ --router=$OSD_ROUTER \\ --auto-allocate-nat-external-ips \\ --nat-custom-subnet-ip-ranges=$MASTER_SUBNET ``` {:style=\"display:block; margin-left:auto; margin-right:auto\"}{: width=\"350\" } * Generate the GCP Cloud NAT for the Worker Subnets: ```sh export NAT_WORKER=<worker subnet name> gcloud compute routers nats create $NAT_WORKER \\ --region=$REGION \\ --router=$OSD_ROUTER \\ --auto-allocate-nat-external-ips \\ --nat-custom-subnet-ip-ranges=$WORKER_SUBNET ``` {:style=\"display:block; margin-left:auto; margin-right:auto\"}{: width=\"350\" } As you can check the Cloud NATs GW are attached now to the Cloud Router: {:style=\"display:block; margin-left:auto; margin-right:auto\"}{: width=\"350\" } Option 2 - Deploy OSD VPC and Subnets using Terraform You can use also automation code in Terraform to deploy all the GCP infrastructure required to deploy the OSD in preexistent VPCs. Clone the tf-osd-gcp repository: git clone https://github.com/rh-mobb/tf-osd-gcp.git cd tf-osd-gcp Copy and modify the tfvars file in order to custom to your scenario: cp -pr terraform.tfvars.example terraform.tfvars Deploy the network infrastructure in GCP needed for deploy the OSD cluster: make all Install the OSD cluster using pre-existent VPCs These steps are based in the official OSD installation documentation . Log in to OpenShift Cluster Manager and click Create cluster. In the Cloud tab, click Create cluster in the Red Hat OpenShift Dedicated row. Under Billing model, configure the subscription type and infrastructure type {:style=\"display:block; margin-left:auto; margin-right:auto\"}{: width=\"750\" } Select Run on Google Cloud Platform. Click Prerequisites to review the prerequisites for installing OpenShift Dedicated on GCP with CCS. Provide your GCP service account private key in JSON format. You can either click Browse to locate and attach a JSON file or add the details in the Service account JSON field. {:style=\"display:block; margin-left:auto; margin-right:auto\"}{: width=\"750\" } Validate your cloud provider account and then click Next. On the Cluster details page, provide a name for your cluster and specify the cluster details: {:style=\"display:block; margin-left:auto; margin-right:auto\"}{: width=\"750\" } NOTE: the Region used to be installed needs to be the same as the VPC and Subnets deployed in the early step. On the Default machine pool page, select a Compute node instance type and a Compute node count: {:style=\"display:block; margin-left:auto; margin-right:auto\"}{: width=\"600\" } In the Cluster privacy section, select Public endpoints and application routes for your cluster. Select Install into an existing VPC to install the cluster in an existing GCP Virtual Private Cloud (VPC): {:style=\"display:block; margin-left:auto; margin-right:auto\"}{: width=\"600\" } Provide your Virtual Private Cloud (VPC) subnet settings, that you deployed as prerequisites in the previous section: {:style=\"display:block; margin-left:auto; margin-right:auto\"}{: width=\"600\" } In the CIDR ranges dialog, configure custom classless inter-domain routing (CIDR) ranges or use the defaults that are provided: {:style=\"display:block; margin-left:auto; margin-right:auto\"}{: width=\"600\" } On the Cluster update strategy page, configure your update preferences. Review the summary of your selections and click Create cluster to start the cluster installation. Check that the Install into Existing VPC is enabled and the VPC and Subnets are properly selected and defined: {:style=\"display:block; margin-left:auto; margin-right:auto\"}{: width=\"600\" } Cleanup Deleting a ROSA cluster consists of two parts: Deleting the OSD cluster can be done using the OCM console described in the official OSD docs . Deleting the GCP infrastructure resources (VPC, Subnets, Cloud NAT, Cloud Router). Depending of which option you selected you must perform: Option 1 : Delete GCP resources using GCloud CLI: ```sh gcloud compute routers nats delete $NAT_WORKER \\ --region=$REGION --router=$OSD_ROUTER --quiet gcloud compute routers nats delete $NAT_MASTER \\ --region=$REGION --router=$OSD_ROUTER --quiet gcloud compute routers delete $OSD_ROUTER --region=$REGION --quiet gcloud compute networks subnets delete $MASTER_SUBNET --region=$REGION --quiet gcloud compute networks subnets delete $WORKER_SUBNET --region=$REGION --quiet gcloud compute networks delete $OSD_VPC --quiet ``` Option 2 : Delete GCP resources using Terraform: sh make destroy","title":"Deploy OSD in GCP w/ BYO VPC"},{"location":"gcp/osd_preexisting_vpc/#creating-a-osd-in-gcp-with-existing-vpcs","text":"Roberto Carratal\u00e1, Andrea Bozzoni Last updated 07/06/2022 Tip The official documentation for installing a OSD cluster in GCP can be found here . For deploy an OSD cluster in GCP using existing Virtual Private Cloud (VPC) you need to implement some prerequisites that you must create before starting the OpenShift Dedicated installation though the OCM.","title":"Creating a OSD in GCP with Existing VPCs"},{"location":"gcp/osd_preexisting_vpc/#prerequisites","text":"gcloud CLI jq NOTE: Also the GCloud Shell can be used, and have the gcloud cli among other tools preinstalled.","title":"Prerequisites"},{"location":"gcp/osd_preexisting_vpc/#generate-gcp-vpc-and-subnets","text":"This is a diagram showing the GCP infra prerequisites that are needed for the OSD installation: {: width=\"750\" } To deploy the GCP VPC and subnets among other prerequisites for install the OSD in GCP using the preexisting VPCs you have two options: Option 1 - GCloud CLI Option 2 - Terraform Automation Please select one of these two options and proceed with the OSD install steps.","title":"Generate GCP VPC and Subnets"},{"location":"gcp/osd_preexisting_vpc/#option-1-generate-osd-vpc-and-subnets-using-gcloud-cli","text":"As mentioned before, for deploy OSD in GCP using existing GCP VPC, you need to provide and create beforehand a GCP VPC and two subnets (one for the masters and another for the workers nodes). Login and configure the proper GCP project where the OSD will be deployed: sh export PROJECT_NAME=<google project name> gcloud auth list gcloud config set project $PROJECT_NAME gcloud config list project Export the names of the vpc and subnets: sh export REGION=<region name> export OSD_VPC=<vpc name> export MASTER_SUBNET=<master subnet name> export WORKER_SUBNET=<worker subnet name> Create a custom mode VPC network: sh gcloud compute networks create $OSD_VPC --subnet-mode=custom gcloud compute networks describe $OSD_VPC NOTE: we need to create the mode custom for the VPC network, because the auto mode generates automatically the subnets with IPv4 ranges with predetermined set of ranges . This example is using the standard configuration for these two subnets: md master-subnet - CIDR 10.0.0.0/17 - Gateway 10.0.0.1 worker-subnet - CIDR 10.0.128.0/17 - Gateway 10.0.128.1 Create the GCP Subnets for the masters and workers within the previous GCP VPC network: ```sh gcloud compute networks subnets create $MASTER_SUBNET \\ --network=$OSD_VPC --range=10.0.0.0/17 --region=$REGION gcloud compute networks subnets create $WORKER_SUBNET \\ --network=$OSD_VPC --range=10.0.128.0/17 --region=$REGION ``` {: width=\"750\" } Once the VPC and the two subnets are provided it is needed to create one GCP Cloud Router : ```sh export OSD_ROUTER= gcloud compute routers create $OSD_ROUTER \\ --project=$PROJECT_NAME --network=$OSD_VPC --region=$REGION ``` {:style=\"display:block; margin-left:auto; margin-right:auto\"}{: width=\"350\" } Then, we will deploy two GCP Cloud NATs and attach them within the GCP Router: Generate the GCP Cloud Nat for the Master Subnets: ```sh export NAT_MASTER= gcloud compute routers nats create $NAT_MASTER \\ --region=$REGION \\ --router=$OSD_ROUTER \\ --auto-allocate-nat-external-ips \\ --nat-custom-subnet-ip-ranges=$MASTER_SUBNET ``` {:style=\"display:block; margin-left:auto; margin-right:auto\"}{: width=\"350\" } * Generate the GCP Cloud NAT for the Worker Subnets: ```sh export NAT_WORKER=<worker subnet name> gcloud compute routers nats create $NAT_WORKER \\ --region=$REGION \\ --router=$OSD_ROUTER \\ --auto-allocate-nat-external-ips \\ --nat-custom-subnet-ip-ranges=$WORKER_SUBNET ``` {:style=\"display:block; margin-left:auto; margin-right:auto\"}{: width=\"350\" } As you can check the Cloud NATs GW are attached now to the Cloud Router: {:style=\"display:block; margin-left:auto; margin-right:auto\"}{: width=\"350\" }","title":"Option 1 - Generate OSD VPC and Subnets using GCloud CLI"},{"location":"gcp/osd_preexisting_vpc/#option-2-deploy-osd-vpc-and-subnets-using-terraform","text":"You can use also automation code in Terraform to deploy all the GCP infrastructure required to deploy the OSD in preexistent VPCs. Clone the tf-osd-gcp repository: git clone https://github.com/rh-mobb/tf-osd-gcp.git cd tf-osd-gcp Copy and modify the tfvars file in order to custom to your scenario: cp -pr terraform.tfvars.example terraform.tfvars Deploy the network infrastructure in GCP needed for deploy the OSD cluster: make all","title":"Option 2 - Deploy OSD VPC and Subnets using Terraform"},{"location":"gcp/osd_preexisting_vpc/#install-the-osd-cluster-using-pre-existent-vpcs","text":"These steps are based in the official OSD installation documentation . Log in to OpenShift Cluster Manager and click Create cluster. In the Cloud tab, click Create cluster in the Red Hat OpenShift Dedicated row. Under Billing model, configure the subscription type and infrastructure type {:style=\"display:block; margin-left:auto; margin-right:auto\"}{: width=\"750\" } Select Run on Google Cloud Platform. Click Prerequisites to review the prerequisites for installing OpenShift Dedicated on GCP with CCS. Provide your GCP service account private key in JSON format. You can either click Browse to locate and attach a JSON file or add the details in the Service account JSON field. {:style=\"display:block; margin-left:auto; margin-right:auto\"}{: width=\"750\" } Validate your cloud provider account and then click Next. On the Cluster details page, provide a name for your cluster and specify the cluster details: {:style=\"display:block; margin-left:auto; margin-right:auto\"}{: width=\"750\" } NOTE: the Region used to be installed needs to be the same as the VPC and Subnets deployed in the early step. On the Default machine pool page, select a Compute node instance type and a Compute node count: {:style=\"display:block; margin-left:auto; margin-right:auto\"}{: width=\"600\" } In the Cluster privacy section, select Public endpoints and application routes for your cluster. Select Install into an existing VPC to install the cluster in an existing GCP Virtual Private Cloud (VPC): {:style=\"display:block; margin-left:auto; margin-right:auto\"}{: width=\"600\" } Provide your Virtual Private Cloud (VPC) subnet settings, that you deployed as prerequisites in the previous section: {:style=\"display:block; margin-left:auto; margin-right:auto\"}{: width=\"600\" } In the CIDR ranges dialog, configure custom classless inter-domain routing (CIDR) ranges or use the defaults that are provided: {:style=\"display:block; margin-left:auto; margin-right:auto\"}{: width=\"600\" } On the Cluster update strategy page, configure your update preferences. Review the summary of your selections and click Create cluster to start the cluster installation. Check that the Install into Existing VPC is enabled and the VPC and Subnets are properly selected and defined: {:style=\"display:block; margin-left:auto; margin-right:auto\"}{: width=\"600\" }","title":"Install the OSD cluster using pre-existent VPCs"},{"location":"gcp/osd_preexisting_vpc/#cleanup","text":"Deleting a ROSA cluster consists of two parts: Deleting the OSD cluster can be done using the OCM console described in the official OSD docs . Deleting the GCP infrastructure resources (VPC, Subnets, Cloud NAT, Cloud Router). Depending of which option you selected you must perform: Option 1 : Delete GCP resources using GCloud CLI: ```sh gcloud compute routers nats delete $NAT_WORKER \\ --region=$REGION --router=$OSD_ROUTER --quiet gcloud compute routers nats delete $NAT_MASTER \\ --region=$REGION --router=$OSD_ROUTER --quiet gcloud compute routers delete $OSD_ROUTER --region=$REGION --quiet gcloud compute networks subnets delete $MASTER_SUBNET --region=$REGION --quiet gcloud compute networks subnets delete $WORKER_SUBNET --region=$REGION --quiet gcloud compute networks delete $OSD_VPC --quiet ``` Option 2 : Delete GCP resources using Terraform: sh make destroy","title":"Cleanup"},{"location":"idp/","text":"Configuring Identity Providers for ROSA and OSD Red Hat OpenShift on AWS (ROSA) and OpenShift Dedicated (OSD) provide a simple way for the cluster administrator to configure one or more identity providers for their cluster[s] via the OpenShift Cluster Manager (OCM) , while Azure Red Hat OpenShift relies on the internal cluster OAuth provider. The identity providers available for use are: GitHub GitLab Google LDAP OpenID HTPasswd Configuring Specific Identity Providers ARO GitLab Azure AD Azure AD with Group Claims Azure AD via CLI ROSA/OSD GitLab Azure AD Azure AD with Group Claims (ROSA Only) Configuring Group Synchronization Using Group Sync Operator with Azure Active Directory and ROSA/OSD Using Group Sync Operator with Okta and ROSA/OSD","title":"Configuring Identity Providers for ROSA and OSD #"},{"location":"idp/#configuring-identity-providers-for-rosa-and-osd","text":"Red Hat OpenShift on AWS (ROSA) and OpenShift Dedicated (OSD) provide a simple way for the cluster administrator to configure one or more identity providers for their cluster[s] via the OpenShift Cluster Manager (OCM) , while Azure Red Hat OpenShift relies on the internal cluster OAuth provider. The identity providers available for use are: GitHub GitLab Google LDAP OpenID HTPasswd","title":"Configuring Identity Providers for ROSA and OSD"},{"location":"idp/#configuring-specific-identity-providers","text":"","title":"Configuring Specific Identity Providers"},{"location":"idp/#aro","text":"GitLab Azure AD Azure AD with Group Claims Azure AD via CLI","title":"ARO"},{"location":"idp/#rosaosd","text":"GitLab Azure AD Azure AD with Group Claims (ROSA Only)","title":"ROSA/OSD"},{"location":"idp/#configuring-group-synchronization","text":"Using Group Sync Operator with Azure Active Directory and ROSA/OSD Using Group Sync Operator with Okta and ROSA/OSD","title":"Configuring Group Synchronization"},{"location":"idp/az-ad-grp-sync/","text":"Using Group Sync Operator with Azure Active Directory and ROSA/OSD Steve Mirman 8 November 2021 This guide focuses on how to synchronize Identity Provider (IDP) groups and users after configuring authentication in OpenShift Cluster Manager (OCM). For an IDP configuration example, please reference the Configure Azure AD as an OIDC identity provider for ROSA/OSD guide. To set up group synchronization from Azure Active Directory (AD) to ROSA/OSD you must: Define groups and assign users in Azure AD Add the required API permissions to the app registration in Azure AD Install the Group Sync Operator from the OpenShift Operator Hub Create and configure a new Group Sync instance Set a synchronization schedule Test the synchronization process Define groups and assign users in Azure AD To synchronize groups and users with ROSA/OSD they must exist in Azure AD Create groups to syncronize with ROSA/OSD if they do not already exist Create user IDs to synchronize with ROSA/OSD if they do not already exist Assign newly created users to the appropriate group Add API Permissions to Azure AD App Registration The GroupSync job requires permissions on the Azure AD tenant beyond those of the OIDC IdP. For it to work, add the these entries: Group.ReadAll GroupMember.ReadAll User.ReadAll ..under the 'API Permissions' menu item. These three should all be 'Application' rather than 'Delegated' and this will require clicking on 'Grant admin consent' button above the permissions list. When done, the screen should look like this: ![API permissions](./images/grp-sync-api-perm.png) Install the Group Sync Operator from the OpenShift Operator Hub In the OpenShift Operator Hub find the Group Sync Operator Install the operator in the group-sync-operator namespace Create and configure a new Group Sync instance Create a new secret named azure-group-sync in the group-sync-operator namespace. For this you will need the following values: AZURE_TENANT_ID AZURE_CLIENT_ID AZURE_CLIENT_SECRET Using the OpenShift CLI, create the secret using the following format: oc create secret generic azure-group-sync \\ --from-literal=AZURE_TENANT_ID=<insert-id> \\ --from-literal=AZURE_CLIENT_ID=<insert-id> \\ --from-literal=AZURE_CLIENT_SECRET=<insert-secret> Create a new Group Sync instance in the group-sync-operator namespace Select all the default YAML and replace is with a modified version of the the example below, customizingthe YAML to match the group names and save the configuration. Sample YAML: apiVersion: redhatcop.redhat.io/v1alpha1 kind: GroupSync metadata: name: azure-groupsync namespace: group-sync-operator spec: providers: - name: azure azure: credentialsSecret: name: azure-group-sync namespace: group-sync-operator groups: - rosa_admin - rosa_project_owner - rosa_viewer prune: false schedule: '* * * * *' Set a synchronization schedule The Group Sync Operator provides a cron-based scheduling parameter for specifying how often the groups and users should be synchronized. This can be set in the instance YAML file during initial configuration or at any time after. The schedule setting of schedule: * * * * * would result in synchronization occuring every minute. It also supports the cron \"slash\" notation (e.g., \" /5 * * *\", which would synchronize every five minutes). Testing the synchronization process Check to see if the Group Sync process has completed with a Condition: ReconcileSuccess message Check to see that all the groups specified in the configuration YAML file show up in the ROSA/OSD Groups list Validate that all users specified in Azure AD also show up as members of the associated group in ROSA/OSD Add a new user in Azure AD and assign it to the admin group Verify that the user now appears in ROSA/OSD (after the specified synchronization time) Now delete a user from the Azure AD admin group Verify the user has been deleted from the ROSA/OSD admin group Binding Groups to Roles The preceding steps provide a method to get group membership information into OpenShift, but the final step in translating that into user authorization control requires binding each group to a role or roles on the cluster. This can be done via the OCP web console by opening the Group detail, or by applying YAML via the CLI. Additional Notes The prune key in the YAML controls how the sync handles groups that are removed from Azure AD. If they key isn't present, the default value is false , which means that if a group is removed from Azure AD, it will still persist in OpenShift. If it is set to true , removal of a group from Azure AD will also remove the corresponding OpenShift Group. If there is a need to have multiple GroupSync configurations against multiple providers, note that there is no \"merge\" functionality in the operator when it comes to group membership. If a group named ocp-admins is present in two directories with sync jobs, they will effectively overwrite each other each time the sync job runs. It is recommended to name groups intended for use on OCP such that they indicate from which directory they originate (e.g., azure-ocp-admins or something like contoso_ocp_admins in the case of multiple Azure AD providers). Bind multiple groups with the same permissions needs to the same Role or ClusterRole as needed.","title":"Using Group Sync Operator with Azure Active Directory and ROSA/OSD #"},{"location":"idp/az-ad-grp-sync/#using-group-sync-operator-with-azure-active-directory-and-rosaosd","text":"Steve Mirman 8 November 2021 This guide focuses on how to synchronize Identity Provider (IDP) groups and users after configuring authentication in OpenShift Cluster Manager (OCM). For an IDP configuration example, please reference the Configure Azure AD as an OIDC identity provider for ROSA/OSD guide. To set up group synchronization from Azure Active Directory (AD) to ROSA/OSD you must: Define groups and assign users in Azure AD Add the required API permissions to the app registration in Azure AD Install the Group Sync Operator from the OpenShift Operator Hub Create and configure a new Group Sync instance Set a synchronization schedule Test the synchronization process","title":"Using Group Sync Operator with Azure Active Directory and ROSA/OSD"},{"location":"idp/az-ad-grp-sync/#define-groups-and-assign-users-in-azure-ad","text":"To synchronize groups and users with ROSA/OSD they must exist in Azure AD Create groups to syncronize with ROSA/OSD if they do not already exist Create user IDs to synchronize with ROSA/OSD if they do not already exist Assign newly created users to the appropriate group","title":"Define groups and assign users in Azure AD"},{"location":"idp/az-ad-grp-sync/#add-api-permissions-to-azure-ad-app-registration","text":"The GroupSync job requires permissions on the Azure AD tenant beyond those of the OIDC IdP. For it to work, add the these entries: Group.ReadAll GroupMember.ReadAll User.ReadAll ..under the 'API Permissions' menu item. These three should all be 'Application' rather than 'Delegated' and this will require clicking on 'Grant admin consent' button above the permissions list. When done, the screen should look like this: ![API permissions](./images/grp-sync-api-perm.png)","title":"Add API Permissions to Azure AD App Registration"},{"location":"idp/az-ad-grp-sync/#install-the-group-sync-operator-from-the-openshift-operator-hub","text":"In the OpenShift Operator Hub find the Group Sync Operator Install the operator in the group-sync-operator namespace","title":"Install the Group Sync Operator from the OpenShift Operator Hub"},{"location":"idp/az-ad-grp-sync/#create-and-configure-a-new-group-sync-instance","text":"Create a new secret named azure-group-sync in the group-sync-operator namespace. For this you will need the following values: AZURE_TENANT_ID AZURE_CLIENT_ID AZURE_CLIENT_SECRET Using the OpenShift CLI, create the secret using the following format: oc create secret generic azure-group-sync \\ --from-literal=AZURE_TENANT_ID=<insert-id> \\ --from-literal=AZURE_CLIENT_ID=<insert-id> \\ --from-literal=AZURE_CLIENT_SECRET=<insert-secret> Create a new Group Sync instance in the group-sync-operator namespace Select all the default YAML and replace is with a modified version of the the example below, customizingthe YAML to match the group names and save the configuration. Sample YAML: apiVersion: redhatcop.redhat.io/v1alpha1 kind: GroupSync metadata: name: azure-groupsync namespace: group-sync-operator spec: providers: - name: azure azure: credentialsSecret: name: azure-group-sync namespace: group-sync-operator groups: - rosa_admin - rosa_project_owner - rosa_viewer prune: false schedule: '* * * * *'","title":"Create and configure a new Group Sync instance"},{"location":"idp/az-ad-grp-sync/#set-a-synchronization-schedule","text":"The Group Sync Operator provides a cron-based scheduling parameter for specifying how often the groups and users should be synchronized. This can be set in the instance YAML file during initial configuration or at any time after. The schedule setting of schedule: * * * * * would result in synchronization occuring every minute. It also supports the cron \"slash\" notation (e.g., \" /5 * * *\", which would synchronize every five minutes).","title":"Set a synchronization schedule"},{"location":"idp/az-ad-grp-sync/#testing-the-synchronization-process","text":"Check to see if the Group Sync process has completed with a Condition: ReconcileSuccess message Check to see that all the groups specified in the configuration YAML file show up in the ROSA/OSD Groups list Validate that all users specified in Azure AD also show up as members of the associated group in ROSA/OSD Add a new user in Azure AD and assign it to the admin group Verify that the user now appears in ROSA/OSD (after the specified synchronization time) Now delete a user from the Azure AD admin group Verify the user has been deleted from the ROSA/OSD admin group","title":"Testing the synchronization process"},{"location":"idp/az-ad-grp-sync/#binding-groups-to-roles","text":"The preceding steps provide a method to get group membership information into OpenShift, but the final step in translating that into user authorization control requires binding each group to a role or roles on the cluster. This can be done via the OCP web console by opening the Group detail, or by applying YAML via the CLI.","title":"Binding Groups to Roles"},{"location":"idp/az-ad-grp-sync/#additional-notes","text":"The prune key in the YAML controls how the sync handles groups that are removed from Azure AD. If they key isn't present, the default value is false , which means that if a group is removed from Azure AD, it will still persist in OpenShift. If it is set to true , removal of a group from Azure AD will also remove the corresponding OpenShift Group. If there is a need to have multiple GroupSync configurations against multiple providers, note that there is no \"merge\" functionality in the operator when it comes to group membership. If a group named ocp-admins is present in two directories with sync jobs, they will effectively overwrite each other each time the sync job runs. It is recommended to name groups intended for use on OCP such that they indicate from which directory they originate (e.g., azure-ocp-admins or something like contoso_ocp_admins in the case of multiple Azure AD providers). Bind multiple groups with the same permissions needs to the same Role or ClusterRole as needed.","title":"Additional Notes"},{"location":"idp/azuread/","text":"Configure Azure AD as an OIDC identity provider for ROSA/OSD Andrea Bozzoni, Steve Mirman 27 October 2021 The steps to add Azure AD as an identity provider for Red Hat OpenShift on AWS (ROSA) and OpenShift Dedicated (OSD) are: Define the OAuth callback URL Register a new Webapp on Azure AD Create the client secret Configure the Token Configure the OAuth identity provider in OCM Define the OAuth callback URL You can find the callback URL in OpenShift Cluster Manager (OCM) Select your cluster in OCM and then go to the 'Access control' tab. Pick OpenID as identity provider from the identity providers list. Give a name to the identity provider that we are adding to the OCP cluster Keep the OAuth callback URL to use later. Note: the OAuth Callback has the following format: https://oauth-openshift.apps.<cluster_name>.<cluster_domain>/oauth2callback/<idp_name> Register a new Webapp on Azure AD Access your Azure account and select the Azure Active Directory service and execute the following steps: From the main menu add a new Webapp Set the Name to or something else unique to the cluster, set the Redirect URI to the callback URL from above and click 'Register' Remember Application (client) ID and Directory (tenant) ID to be used later Create the client secret Create a new Secret for the Webapp Remember the Secret Value to be used later in the OCM OAuth configuration Configure the Token Create a new token configuration Select upn and email as optional claims Specify that the claim must be returned in the token. Configure the OAuth identity provider in OCM In the OCM fill all the fields with the values collected during the registration of the new Webapp in the Azure AD and click the 'Add' button. After a few minutes the Azure AD authentication methos will be available in the OpenShift console login screen","title":"Azure AD for ROSA/OSD"},{"location":"idp/azuread/#configure-azure-ad-as-an-oidc-identity-provider-for-rosaosd","text":"Andrea Bozzoni, Steve Mirman 27 October 2021 The steps to add Azure AD as an identity provider for Red Hat OpenShift on AWS (ROSA) and OpenShift Dedicated (OSD) are: Define the OAuth callback URL Register a new Webapp on Azure AD Create the client secret Configure the Token Configure the OAuth identity provider in OCM","title":"Configure Azure AD as an OIDC identity provider for ROSA/OSD"},{"location":"idp/azuread/#define-the-oauth-callback-url","text":"You can find the callback URL in OpenShift Cluster Manager (OCM) Select your cluster in OCM and then go to the 'Access control' tab. Pick OpenID as identity provider from the identity providers list. Give a name to the identity provider that we are adding to the OCP cluster Keep the OAuth callback URL to use later. Note: the OAuth Callback has the following format: https://oauth-openshift.apps.<cluster_name>.<cluster_domain>/oauth2callback/<idp_name>","title":"Define the OAuth callback URL"},{"location":"idp/azuread/#register-a-new-webapp-on-azure-ad","text":"Access your Azure account and select the Azure Active Directory service and execute the following steps: From the main menu add a new Webapp Set the Name to or something else unique to the cluster, set the Redirect URI to the callback URL from above and click 'Register' Remember Application (client) ID and Directory (tenant) ID to be used later","title":"Register a new Webapp on Azure AD"},{"location":"idp/azuread/#create-the-client-secret","text":"Create a new Secret for the Webapp Remember the Secret Value to be used later in the OCM OAuth configuration","title":"Create the client secret"},{"location":"idp/azuread/#configure-the-token","text":"Create a new token configuration Select upn and email as optional claims Specify that the claim must be returned in the token.","title":"Configure the Token"},{"location":"idp/azuread/#configure-the-oauth-identity-provider-in-ocm","text":"In the OCM fill all the fields with the values collected during the registration of the new Webapp in the Azure AD and click the 'Add' button. After a few minutes the Azure AD authentication methos will be available in the OpenShift console login screen","title":"Configure the OAuth identity provider in OCM"},{"location":"idp/azuread-aro/","text":"Configure ARO to use Azure AD Michael McNeill, Sohaib Azed 28 July 2022 This guide demonstrates how to configure Azure AD as the cluster identity provider in Azure Red Hat OpenShift. This guide will walk through the creation of an Azure Active Directory (Azure AD) application and configure Azure Red Hat OpenShift (ARO) to authenticate using Azure AD. This guide will walk through the following steps: Register a new application in Azure AD for authentication. Configure the application registration in Azure AD to include optional claims in tokens. Configure the Azure Red Hat OpenShift (ARO) cluster to use Azure AD as the identity provider. Grant additional permissions to individual users. Before you Begin If you are using zsh as your shell (which is the default shell on macOS) you may need to run set -k to get the below commands to run without errors. This is because zsh disables comments in interactive shells from being used . 1. Register a new application in Azure AD for authenitcation Capture the OAuth callback URL First, construct the cluster's OAuth callback URL and make note of it. To do so, run the following command, making sure to replace the variables specified: The \"AAD\" directory at the end of the the OAuth callback URL should match the OAuth identity provider name you'll setup later. RESOURCE_GROUP=example-rg # Replace this with the name of your ARO cluster's resource group CLUSTER_NAME=example-cluster # Replace this with the name of your ARO cluster echo 'OAuth callback URL: '$(az aro show -g $RESOURCE_GROUP -n $CLUSTER_NAME --query consoleProfile.url -o tsv | sed 's/console-openshift-console/oauth-openshift/')'oauth2callback/AAD' Register a new application in Azure AD Second, you need to create the Azure AD application itself. To do so, login to the Azure portal, and navigate to App registrations blade , then click on \"New registration\" to create a new application. Provide a name for the application, for example openshift-auth . Select \"Web\" from the Redirect URI dropdown and fill in the Redirect URI using the value of the OAuth callback URL you retrieved in the previous step. Once you fill in the necessary information, click \"Register\" to create the application. Then, click on the \"Certificates & secrets\" sub-blade and select \"New client secret\". Fill in the details request and make note of the generated client secret value, as you'll use it in a later step. You won't be able to retrieve it again. Then, click on the \"Overview\" sub-blade and make note of the \"Application (client) ID\" and \"Directory (tenant) ID\". You'll need those values in a later step as well. 2. Configure optional claims In order to provide OpenShift with enough information about the user to create their account, we will configure Azure AD to provide two optional claims, specifically \"email\" and \"upn\" when a user logs in. For more information on optional claims in Azure AD, see the Microsoft documentation . Click on the \"Token configuration\" sub-blade and select the \"Add optional claim\" button. Select ID then check the \"email\" and \"upn\" claims and click the \"Add\" button to configure them for your Azure AD application. When prompted, follow the prompt to enable the necessary Microsoft Graph permissions. 3. Configure the OpenShift cluster to use Azure AD as the identity provider Finally, we need to configure OpenShift to use Azure AD as its identity provider. To do so, ensure you are logged in to the OpenShift command line interface ( oc ) by running the following command, making sure to replace the variables specified: RESOURCE_GROUP=example-rg # Replace this with the name of your ARO cluster's resource group CLUSTER_NAME=example-cluster # Replace this with the name of your ARO cluster oc login \\ $(az aro show -g $RESOURCE_GROUP -n $CLUSTER_NAME --query apiserverProfile.url -o tsv) \\ -u $(az aro list-credentials -g $RESOURCE_GROUP -n $CLUSTER_NAME --query kubeadminUsername -o tsv) \\ -p $(az aro list-credentials -g $RESOURCE_GROUP -n $CLUSTER_NAME --query kubeadminPassword -o tsv) Next, create a secret that contains the client secret that you captured in step 2 above. To do so, run the following command, making sure to replace the variable specified: CLIENT_SECRET=xxxxxxxx-xxxx-xxxx-xxxx-xxxxxxxxxxxx # Replace this with the Client Secret oc create secret generic openid-client-secret --from-literal=clientSecret=${CLIENT_SECRET} -n openshift-config Next, generate the necessary YAML for the cluster's OAuth provider to use Azure AD as its identity provider. To do so, run the following command, making sure to replace the variables specified: IDP_NAME=AAD # Replace this with the name you used in the OAuth callback URL APP_ID=yyyyyyyy-yyyy-yyyy-yyyy-yyyyyyyyyyyy # Replace this with the Application (client) ID TENANT_ID=zzzzzzzz-zzzz-zzzz-zzzz-zzzzzzzzzzzz # Replace this with the Directory (tenant) ID cat << EOF > cluster-oauth-config.yaml apiVersion: config.openshift.io/v1 kind: OAuth metadata: name: cluster spec: identityProviders: - mappingMethod: claim name: ${IDP_NAME} openID: claims: email: - email name: - name preferredUsername: - upn clientID: ${APP_ID} clientSecret: name: openid-client-secret extraScopes: [] issuer: https://login.microsoftonline.com/${TENANT_ID}/v2.0 type: OpenID EOF Feel free to further modify this output (which is saved in your current directory as cluster-oauth-config.yaml ). Finally, apply the new configuration to the cluster's OAuth provider by running the following command: oc apply -f ./cluster-oauth-config.yaml Note: It is normal to receive an error that says an annotation is missing when you run oc apply for the first time. This can be safely ignored. 4. Grant additional permissions to individual users Once the cluster authentication operator reconciles your changes (generally within a few minutes), you will be able to login to the cluster using Azure AD. Once you login, you will notice that you have very limited permissions. This is because, by default, OpenShift only grants you the ability to create new projects (namespaces) in the cluster. Other projects (namespaces) are restricted from view. OpenShift includes a signifcant number of pre-configured roles, including the cluster-admin role that grants full access and control over the clster. To grant your user access to the cluster-admin role, you must create a ClusterRoleBinding to your user account. USERNAME=example@redhat.com # Replace with your Azure AD username oc create clusterrolebinding cluster-admin-user \\ --clusterrole=cluster-admin \\ --user=$USERNAME For more information on how to use RBAC to define and apply permissions in OpenShift, see the OpenShift documentation .","title":"Azure AD for ARO"},{"location":"idp/azuread-aro/#configure-aro-to-use-azure-ad","text":"Michael McNeill, Sohaib Azed 28 July 2022 This guide demonstrates how to configure Azure AD as the cluster identity provider in Azure Red Hat OpenShift. This guide will walk through the creation of an Azure Active Directory (Azure AD) application and configure Azure Red Hat OpenShift (ARO) to authenticate using Azure AD. This guide will walk through the following steps: Register a new application in Azure AD for authentication. Configure the application registration in Azure AD to include optional claims in tokens. Configure the Azure Red Hat OpenShift (ARO) cluster to use Azure AD as the identity provider. Grant additional permissions to individual users.","title":"Configure ARO to use Azure AD"},{"location":"idp/azuread-aro/#before-you-begin","text":"If you are using zsh as your shell (which is the default shell on macOS) you may need to run set -k to get the below commands to run without errors. This is because zsh disables comments in interactive shells from being used .","title":"Before you Begin"},{"location":"idp/azuread-aro/#1-register-a-new-application-in-azure-ad-for-authenitcation","text":"","title":"1. Register a new application in Azure AD for authenitcation"},{"location":"idp/azuread-aro/#capture-the-oauth-callback-url","text":"First, construct the cluster's OAuth callback URL and make note of it. To do so, run the following command, making sure to replace the variables specified: The \"AAD\" directory at the end of the the OAuth callback URL should match the OAuth identity provider name you'll setup later. RESOURCE_GROUP=example-rg # Replace this with the name of your ARO cluster's resource group CLUSTER_NAME=example-cluster # Replace this with the name of your ARO cluster echo 'OAuth callback URL: '$(az aro show -g $RESOURCE_GROUP -n $CLUSTER_NAME --query consoleProfile.url -o tsv | sed 's/console-openshift-console/oauth-openshift/')'oauth2callback/AAD'","title":"Capture the OAuth callback URL"},{"location":"idp/azuread-aro/#register-a-new-application-in-azure-ad","text":"Second, you need to create the Azure AD application itself. To do so, login to the Azure portal, and navigate to App registrations blade , then click on \"New registration\" to create a new application. Provide a name for the application, for example openshift-auth . Select \"Web\" from the Redirect URI dropdown and fill in the Redirect URI using the value of the OAuth callback URL you retrieved in the previous step. Once you fill in the necessary information, click \"Register\" to create the application. Then, click on the \"Certificates & secrets\" sub-blade and select \"New client secret\". Fill in the details request and make note of the generated client secret value, as you'll use it in a later step. You won't be able to retrieve it again. Then, click on the \"Overview\" sub-blade and make note of the \"Application (client) ID\" and \"Directory (tenant) ID\". You'll need those values in a later step as well.","title":"Register a new application in Azure AD"},{"location":"idp/azuread-aro/#2-configure-optional-claims","text":"In order to provide OpenShift with enough information about the user to create their account, we will configure Azure AD to provide two optional claims, specifically \"email\" and \"upn\" when a user logs in. For more information on optional claims in Azure AD, see the Microsoft documentation . Click on the \"Token configuration\" sub-blade and select the \"Add optional claim\" button. Select ID then check the \"email\" and \"upn\" claims and click the \"Add\" button to configure them for your Azure AD application. When prompted, follow the prompt to enable the necessary Microsoft Graph permissions.","title":"2. Configure optional claims"},{"location":"idp/azuread-aro/#3-configure-the-openshift-cluster-to-use-azure-ad-as-the-identity-provider","text":"Finally, we need to configure OpenShift to use Azure AD as its identity provider. To do so, ensure you are logged in to the OpenShift command line interface ( oc ) by running the following command, making sure to replace the variables specified: RESOURCE_GROUP=example-rg # Replace this with the name of your ARO cluster's resource group CLUSTER_NAME=example-cluster # Replace this with the name of your ARO cluster oc login \\ $(az aro show -g $RESOURCE_GROUP -n $CLUSTER_NAME --query apiserverProfile.url -o tsv) \\ -u $(az aro list-credentials -g $RESOURCE_GROUP -n $CLUSTER_NAME --query kubeadminUsername -o tsv) \\ -p $(az aro list-credentials -g $RESOURCE_GROUP -n $CLUSTER_NAME --query kubeadminPassword -o tsv) Next, create a secret that contains the client secret that you captured in step 2 above. To do so, run the following command, making sure to replace the variable specified: CLIENT_SECRET=xxxxxxxx-xxxx-xxxx-xxxx-xxxxxxxxxxxx # Replace this with the Client Secret oc create secret generic openid-client-secret --from-literal=clientSecret=${CLIENT_SECRET} -n openshift-config Next, generate the necessary YAML for the cluster's OAuth provider to use Azure AD as its identity provider. To do so, run the following command, making sure to replace the variables specified: IDP_NAME=AAD # Replace this with the name you used in the OAuth callback URL APP_ID=yyyyyyyy-yyyy-yyyy-yyyy-yyyyyyyyyyyy # Replace this with the Application (client) ID TENANT_ID=zzzzzzzz-zzzz-zzzz-zzzz-zzzzzzzzzzzz # Replace this with the Directory (tenant) ID cat << EOF > cluster-oauth-config.yaml apiVersion: config.openshift.io/v1 kind: OAuth metadata: name: cluster spec: identityProviders: - mappingMethod: claim name: ${IDP_NAME} openID: claims: email: - email name: - name preferredUsername: - upn clientID: ${APP_ID} clientSecret: name: openid-client-secret extraScopes: [] issuer: https://login.microsoftonline.com/${TENANT_ID}/v2.0 type: OpenID EOF Feel free to further modify this output (which is saved in your current directory as cluster-oauth-config.yaml ). Finally, apply the new configuration to the cluster's OAuth provider by running the following command: oc apply -f ./cluster-oauth-config.yaml Note: It is normal to receive an error that says an annotation is missing when you run oc apply for the first time. This can be safely ignored.","title":"3. Configure the OpenShift cluster to use Azure AD as the identity provider"},{"location":"idp/azuread-aro/#4-grant-additional-permissions-to-individual-users","text":"Once the cluster authentication operator reconciles your changes (generally within a few minutes), you will be able to login to the cluster using Azure AD. Once you login, you will notice that you have very limited permissions. This is because, by default, OpenShift only grants you the ability to create new projects (namespaces) in the cluster. Other projects (namespaces) are restricted from view. OpenShift includes a signifcant number of pre-configured roles, including the cluster-admin role that grants full access and control over the clster. To grant your user access to the cluster-admin role, you must create a ClusterRoleBinding to your user account. USERNAME=example@redhat.com # Replace with your Azure AD username oc create clusterrolebinding cluster-admin-user \\ --clusterrole=cluster-admin \\ --user=$USERNAME For more information on how to use RBAC to define and apply permissions in OpenShift, see the OpenShift documentation .","title":"4. Grant additional permissions to individual users"},{"location":"idp/azuread-aro-cli/","text":"Configure Azure AD as an OIDC identity provider for ARO with cli Daniel Moessner 26 June 2022 The steps to add Azure AD as an identity provider for Azure Red Hat OpenShift (ARO) via cli are: Prerequisites Have Azure cli installed Login to Azure Azure Define needed variables Get oauthCallbackURL Create manifest.json file to configure the Azure Active Directory application Register/create app Add Servive Principal for the new app Make Service Principal an Enterprise Application Create the client secret Update the Azure AD application scope permissions Get Tenant ID OpenShift Login to OpenShift as kubeadmin Create an OpenShift secret Apply OpenShift OpenID authentication Wait for authentication operator to roll out Verify login through Azure Active Directory Last steps Prerequisites Have Azure cli installed Follow the Microsoft instuctions: https://docs.microsoft.com/en-us/cli/azure/install-azure-cli Note This has been written for az cli verion 2.37.0 some commands will not work with previous versions, however, there is a known issue https://github.com/Azure/azure-cli/issues/23027 where we will use an older version via podman run -it mcr.microsoft.com/azure-cli:2.36.0 . In case you're using docker , just replace podman command by docker . For podman installation on Mac, Windows & Linux, please refer to https://podman.io/getting-started/installation Login to Azure Login to Azure as follows: az login If you're logging in from a system you have no access to your browser you can authenticate, you can also use az login --use-device-code Azure Define needed variables To simplly follow along, first define the following variables according to your set-up: RESOURCEGROUP=<cluster-dmoessne-aro01> # replave with your name CLUSTERNAME=<rg-dmoessne-aro01> # replave with your name Get oauthCallbackURL To get the oauthCallbackURL for the Azure AD integration, run the following commands: ``` DOMAIN=$(az aro show -g $RESOURCEGROUP -n $CLUSTERNAME --query clusterProfile.domain -o tsv) APISERVER=$(az aro show -g $RESOURCEGROUP -n $CLUSTERNAME --query apiserverProfile.url -o tsv) oauthCallbackURL=https://oauth-openshift.apps.$DOMAIN/oauth2callback/AAD echo $oauthCallbackURL ``` Note oauthCallbackURL , in particular AAD can be changed but must match the name in the oauth providerwhen creating the OpenShift OpenID authentication Create manifest.json file to configure the Azure Active Directory application Configure OpenShift to use the email claim and fall back to upn to set the Preferred Username by adding the upn as part of the ID token returned by Azure Active Directory. Create a manifest.json file to configure the Azure Active Directory application. cat << EOF > manifest.json { \"idToken\": [ { \"name\": \"upn\", \"source\": null, \"essential\": false, \"additionalProperties\": [] }, { \"name\": \"email\", \"source\": null, \"essential\": false, \"additionalProperties\": [] } ] } EOF Register/create app Create an Azure AD application and retrieve app id: ``` DISPLAYNAME= # set you name accordingly az ad app create \\ --display-name $DISPLAYNAME \\ --web-redirect-uris $oauthCallbackURL \\ --sign-in-audience AzureADMyOrg \\ --optional-claims @manifest.json ``` APPID=$(az ad app list --display-name $DISPLAYNAME --query [].appId -o tsv) Add Servive Principal for the new app Create Pervice Principal for the app created: az ad sp create --id $APPID Make Service Principal an Enterprise Application We need this Service Principal to be an Enterprise Application to be able to add users and groups, so we add the needed tag (az cli >= 2.38.0 ) az ad sp update --id $APPID --set 'tags=[\"WindowsAzureActiveDirectoryIntegratedApp\"]' Note In case you get a trace back (az cli = 2.37.0 ) check out https://github.com/Azure/azure-cli/issues/23027 To overcome that issue, we'll do the following ``` APP_ID=$(az ad app list --display-name $DISPLAYNAME --query [].id -o tsv) az rest --method PATCH --url https://graph.microsoft.com/v1.0/applications/$APP_ID --body '{\"tags\":[\"WindowsAzureActiveDirectoryIntegratedApp\"]}' ``` Create the client secret The password for the app created is retrieved by resetting the same: PASSWD=$(az ad app credential reset --id $APPID --query password -o tsv) Note The password generated with above command is by default valid for one year and you may want to change that by adding either and end date via --end-date or set validity in years with --years . For details consult the documentation Update the Azure AD application scope permissions To be able to read the user information from Azure Active Directory, we need to add the following Azure Active Directory Graph permissions Add permission for the Azure Active Directory as follows: read email az ad app permission add \\ --api 00000003-0000-0000-c000-000000000000 \\ --api-permissions 64a6cdd6-aab1-4aaf-94b8-3cc8405e90d0=Scope \\ --id $APPID read profile az ad app permission add \\ --api 00000003-0000-0000-c000-000000000000 \\ --api-permissions 14dad69e-099b-42c9-810b-d002981feec1=Scope \\ --id $APPID User.Read az ad app permission add \\ --api 00000003-0000-0000-c000-000000000000 \\ --api-permissions e1fe6dd8-ba31-4d61-89e7-88639da4683d=Scope \\ --id $APPID Note If you see a message that you need to grant consent you can safely ignore it, unless you are authenticated as a alobal administrator for this Azure Active Directory. Standard domain users will be asked to grant consent when they first login to the cluster using their AAD credentials. Get Tenant ID We do need the Tenant ID for setting up the Oauth provider later on: TENANTID=$(az account show --query tenantId -o tsv) Note Now we can switch over to our OpenShift installation and apply the needed configuraion. Please refer to https://docs.openshift.com/container-platform/latest/cli_reference/openshift_cli/getting-started-cli.html to get the latest oc cli OpenShift Login to OpenShift as kubeadmin Fetch kubeadmin password and login to your cluster via oc cli (you can use any other cluster-admin user in case you have already created/added other oauth providers) ``` KUBEPW=$(az aro list-credentials \\ --name $CLUSTERNAME \\ --resource-group $RESOURCEGROUP \\ --query kubeadminPassword --output tsv) oc login $APISERVER -u kubeadmin -p $KUBEPW ``` Create an OpenShift secret Create an OpenShift secret to store the Azure Active Directory application secret from the application password we created/reset earlier: oc create secret generic openid-client-secret-azuread \\ -n openshift-config \\ --from-literal=clientSecret=$PASSWD Apply OpenShift OpenID authentication As a last step we need to apply the OpenShift OpenID authentication for Azure Active Directory: cat << EOF | oc apply -f - apiVersion: config.openshift.io/v1 kind: OAuth metadata: name: cluster spec: identityProviders: - name: AAD mappingMethod: claim type: OpenID openID: clientID: $APPID clientSecret: name: openid-client-secret-azuread extraScopes: - email - profile extraAuthorizeParameters: include_granted_scopes: \"true\" claims: preferredUsername: - email - upn name: - name email: - email issuer: https://login.microsoftonline.com/$TENANTID EOF Wait for authentication operator to roll out Before we move over to the OpenShift login, let's wait for the new version of the authentication cluster operator to be rolled out watch -n 5 oc get co authentication Note it may take some time until the rollout starts Verify login through Azure Active Directory Get console url to login: az aro show --name $CLUSTERNAME --resource-group $RESOURCEGROUP --query \"consoleProfile.url\" -o tsv Opening the url in a browser, we can see the login to Azure AD is available At first login you may have to accept application permissions Last steps As a last step you may want to grant a user or group cluster-admin permissions and remove kubeadmin user, see - https://docs.openshift.com/container-platform/4.10/authentication/using-rbac.html#cluster-role-binding-commands_using-rbac - https://docs.openshift.com/container-platform/4.10/authentication/remove-kubeadmin.html","title":"Azure AD for ARO via CLI"},{"location":"idp/azuread-aro-cli/#configure-azure-ad-as-an-oidc-identity-provider-for-aro-with-cli","text":"Daniel Moessner 26 June 2022 The steps to add Azure AD as an identity provider for Azure Red Hat OpenShift (ARO) via cli are: Prerequisites Have Azure cli installed Login to Azure Azure Define needed variables Get oauthCallbackURL Create manifest.json file to configure the Azure Active Directory application Register/create app Add Servive Principal for the new app Make Service Principal an Enterprise Application Create the client secret Update the Azure AD application scope permissions Get Tenant ID OpenShift Login to OpenShift as kubeadmin Create an OpenShift secret Apply OpenShift OpenID authentication Wait for authentication operator to roll out Verify login through Azure Active Directory Last steps","title":"Configure Azure AD as an OIDC identity provider for ARO with cli"},{"location":"idp/azuread-aro-cli/#prerequisites","text":"","title":"Prerequisites"},{"location":"idp/azuread-aro-cli/#have-azure-cli-installed","text":"Follow the Microsoft instuctions: https://docs.microsoft.com/en-us/cli/azure/install-azure-cli Note This has been written for az cli verion 2.37.0 some commands will not work with previous versions, however, there is a known issue https://github.com/Azure/azure-cli/issues/23027 where we will use an older version via podman run -it mcr.microsoft.com/azure-cli:2.36.0 . In case you're using docker , just replace podman command by docker . For podman installation on Mac, Windows & Linux, please refer to https://podman.io/getting-started/installation","title":"Have Azure cli installed"},{"location":"idp/azuread-aro-cli/#login-to-azure","text":"Login to Azure as follows: az login If you're logging in from a system you have no access to your browser you can authenticate, you can also use az login --use-device-code","title":"Login to Azure"},{"location":"idp/azuread-aro-cli/#azure","text":"","title":"Azure"},{"location":"idp/azuread-aro-cli/#define-needed-variables","text":"To simplly follow along, first define the following variables according to your set-up: RESOURCEGROUP=<cluster-dmoessne-aro01> # replave with your name CLUSTERNAME=<rg-dmoessne-aro01> # replave with your name","title":"Define needed variables"},{"location":"idp/azuread-aro-cli/#get-oauthcallbackurl","text":"To get the oauthCallbackURL for the Azure AD integration, run the following commands: ``` DOMAIN=$(az aro show -g $RESOURCEGROUP -n $CLUSTERNAME --query clusterProfile.domain -o tsv) APISERVER=$(az aro show -g $RESOURCEGROUP -n $CLUSTERNAME --query apiserverProfile.url -o tsv) oauthCallbackURL=https://oauth-openshift.apps.$DOMAIN/oauth2callback/AAD echo $oauthCallbackURL ``` Note oauthCallbackURL , in particular AAD can be changed but must match the name in the oauth providerwhen creating the OpenShift OpenID authentication","title":"Get oauthCallbackURL"},{"location":"idp/azuread-aro-cli/#create-manifestjson-file-to-configure-the-azure-active-directory-application","text":"Configure OpenShift to use the email claim and fall back to upn to set the Preferred Username by adding the upn as part of the ID token returned by Azure Active Directory. Create a manifest.json file to configure the Azure Active Directory application. cat << EOF > manifest.json { \"idToken\": [ { \"name\": \"upn\", \"source\": null, \"essential\": false, \"additionalProperties\": [] }, { \"name\": \"email\", \"source\": null, \"essential\": false, \"additionalProperties\": [] } ] } EOF","title":"Create manifest.json file to configure the Azure Active Directory application"},{"location":"idp/azuread-aro-cli/#registercreate-app","text":"Create an Azure AD application and retrieve app id: ``` DISPLAYNAME= # set you name accordingly az ad app create \\ --display-name $DISPLAYNAME \\ --web-redirect-uris $oauthCallbackURL \\ --sign-in-audience AzureADMyOrg \\ --optional-claims @manifest.json ``` APPID=$(az ad app list --display-name $DISPLAYNAME --query [].appId -o tsv)","title":"Register/create app"},{"location":"idp/azuread-aro-cli/#add-servive-principal-for-the-new-app","text":"Create Pervice Principal for the app created: az ad sp create --id $APPID","title":"Add Servive Principal for the new app"},{"location":"idp/azuread-aro-cli/#make-service-principal-an-enterprise-application","text":"We need this Service Principal to be an Enterprise Application to be able to add users and groups, so we add the needed tag (az cli >= 2.38.0 ) az ad sp update --id $APPID --set 'tags=[\"WindowsAzureActiveDirectoryIntegratedApp\"]' Note In case you get a trace back (az cli = 2.37.0 ) check out https://github.com/Azure/azure-cli/issues/23027 To overcome that issue, we'll do the following ```","title":"Make Service Principal an Enterprise Application"},{"location":"idp/azuread-aro-cli/#app_idaz-ad-app-list-display-name-displayname-query-id-o-tsv","text":"","title":"APP_ID=$(az ad app list --display-name $DISPLAYNAME --query [].id -o tsv)"},{"location":"idp/azuread-aro-cli/#az-rest-method-patch-url-httpsgraphmicrosoftcomv10applicationsapp_id-body-tagswindowsazureactivedirectoryintegratedapp","text":"```","title":"az rest --method PATCH --url https://graph.microsoft.com/v1.0/applications/$APP_ID --body '{\"tags\":[\"WindowsAzureActiveDirectoryIntegratedApp\"]}'"},{"location":"idp/azuread-aro-cli/#create-the-client-secret","text":"The password for the app created is retrieved by resetting the same: PASSWD=$(az ad app credential reset --id $APPID --query password -o tsv) Note The password generated with above command is by default valid for one year and you may want to change that by adding either and end date via --end-date or set validity in years with --years . For details consult the documentation","title":"Create the client secret"},{"location":"idp/azuread-aro-cli/#update-the-azure-ad-application-scope-permissions","text":"To be able to read the user information from Azure Active Directory, we need to add the following Azure Active Directory Graph permissions Add permission for the Azure Active Directory as follows: read email az ad app permission add \\ --api 00000003-0000-0000-c000-000000000000 \\ --api-permissions 64a6cdd6-aab1-4aaf-94b8-3cc8405e90d0=Scope \\ --id $APPID read profile az ad app permission add \\ --api 00000003-0000-0000-c000-000000000000 \\ --api-permissions 14dad69e-099b-42c9-810b-d002981feec1=Scope \\ --id $APPID User.Read az ad app permission add \\ --api 00000003-0000-0000-c000-000000000000 \\ --api-permissions e1fe6dd8-ba31-4d61-89e7-88639da4683d=Scope \\ --id $APPID Note If you see a message that you need to grant consent you can safely ignore it, unless you are authenticated as a alobal administrator for this Azure Active Directory. Standard domain users will be asked to grant consent when they first login to the cluster using their AAD credentials.","title":"Update the Azure AD application scope permissions"},{"location":"idp/azuread-aro-cli/#get-tenant-id","text":"We do need the Tenant ID for setting up the Oauth provider later on: TENANTID=$(az account show --query tenantId -o tsv) Note Now we can switch over to our OpenShift installation and apply the needed configuraion. Please refer to https://docs.openshift.com/container-platform/latest/cli_reference/openshift_cli/getting-started-cli.html to get the latest oc cli","title":"Get Tenant ID"},{"location":"idp/azuread-aro-cli/#openshift","text":"","title":"OpenShift"},{"location":"idp/azuread-aro-cli/#login-to-openshift-as-kubeadmin","text":"Fetch kubeadmin password and login to your cluster via oc cli (you can use any other cluster-admin user in case you have already created/added other oauth providers) ``` KUBEPW=$(az aro list-credentials \\ --name $CLUSTERNAME \\ --resource-group $RESOURCEGROUP \\ --query kubeadminPassword --output tsv) oc login $APISERVER -u kubeadmin -p $KUBEPW ```","title":"Login to OpenShift as kubeadmin"},{"location":"idp/azuread-aro-cli/#create-an-openshift-secret","text":"Create an OpenShift secret to store the Azure Active Directory application secret from the application password we created/reset earlier: oc create secret generic openid-client-secret-azuread \\ -n openshift-config \\ --from-literal=clientSecret=$PASSWD","title":"Create an OpenShift secret"},{"location":"idp/azuread-aro-cli/#apply-openshift-openid-authentication","text":"As a last step we need to apply the OpenShift OpenID authentication for Azure Active Directory: cat << EOF | oc apply -f - apiVersion: config.openshift.io/v1 kind: OAuth metadata: name: cluster spec: identityProviders: - name: AAD mappingMethod: claim type: OpenID openID: clientID: $APPID clientSecret: name: openid-client-secret-azuread extraScopes: - email - profile extraAuthorizeParameters: include_granted_scopes: \"true\" claims: preferredUsername: - email - upn name: - name email: - email issuer: https://login.microsoftonline.com/$TENANTID EOF","title":"Apply OpenShift OpenID authentication"},{"location":"idp/azuread-aro-cli/#wait-for-authentication-operator-to-roll-out","text":"Before we move over to the OpenShift login, let's wait for the new version of the authentication cluster operator to be rolled out watch -n 5 oc get co authentication Note it may take some time until the rollout starts","title":"Wait for authentication operator to roll out"},{"location":"idp/azuread-aro-cli/#verify-login-through-azure-active-directory","text":"Get console url to login: az aro show --name $CLUSTERNAME --resource-group $RESOURCEGROUP --query \"consoleProfile.url\" -o tsv Opening the url in a browser, we can see the login to Azure AD is available At first login you may have to accept application permissions","title":"Verify login through Azure Active Directory"},{"location":"idp/azuread-aro-cli/#last-steps","text":"As a last step you may want to grant a user or group cluster-admin permissions and remove kubeadmin user, see - https://docs.openshift.com/container-platform/4.10/authentication/using-rbac.html#cluster-role-binding-commands_using-rbac - https://docs.openshift.com/container-platform/4.10/authentication/remove-kubeadmin.html","title":"Last steps"},{"location":"idp/gitlab/","text":"Configure GitLab as an identity provider for ROSA/OSD Steve Mirman 16 February 2022 The following instructions will detail how to configure GitLab as the identity provider for Managed OpenShift through the OpenShift Cluster Manager (OCM): Create OAuth callback URL in OCM Register a new application in GitLab Configure the identity provider credentials and URL Add cluster-admin or dedicated-admin users Log in and confirm Create OAuth callback URL in OCM Log in to the OpenShift Cluster Manager (OCM) to add a GitLab identity provider Select your cluster in OCM and then go to the 'Access control' tab and select 'Identity Providers' Choose GitLab as identity provider from the identity providers list Provide a name for the new identity provider Copy the OAuth callback URL . It will be needed later Note: the OAuth Callback has the following format: https://oauth-openshift.apps.<cluster_name>.<cluster_domain>/oauth2callback/<idp_name> At this point, leave the Client ID , Client secret , and URL blank while configuring GitLab Register a new application in GitLab Log into GitLab and execute the following steps: Go to Preferences Select Applications from the left navigation bar Provide a Name and enter the OAuth Callback URL copied from OCM above and enter it as the Redirect URL in GitLab Check the openid box and save the application After saving the GitLab application you will be provided with an Application ID and a Secret Copy both the Application ID and Secret and return to the OCM console Configure the identity provider credentials and URL Returning to the OCM console, enter the Application ID and Secret obtained from GitLab in the previous step and enter them as Client ID and Client Secret respectively in the OCM console. Additionally, provide the GitLab URL where credentials were obtained and click Add The new GitLab identity provider should display in the IDP list Add cluster-admin or dedicated-admin users Now that the GitLab identity provider is configured, it is possible to add authenticated users to elevated OCM and OpenShift roles. Under Cluster Roles and Access select Add user and enter an existing GitLab user. Then choose to assign dedicated-admin or cluster-admin permissions to the user and click Add user The new user should now display, with proper permissions, in the cluster-admin or dedicated-admin user lists Log in and confirm Select the Open console button in OCM to bring up the OpenShift login page. An option for GitLab should now be available. Note: I can take 1-2 minutes for this update to occur After selecting GitLab for the first time an authorization message will appear. Click Authorize to confirm. Congratulations!","title":"ROSA/OSD"},{"location":"idp/gitlab/#configure-gitlab-as-an-identity-provider-for-rosaosd","text":"Steve Mirman 16 February 2022 The following instructions will detail how to configure GitLab as the identity provider for Managed OpenShift through the OpenShift Cluster Manager (OCM): Create OAuth callback URL in OCM Register a new application in GitLab Configure the identity provider credentials and URL Add cluster-admin or dedicated-admin users Log in and confirm","title":"Configure GitLab as an identity provider for ROSA/OSD"},{"location":"idp/gitlab/#create-oauth-callback-url-in-ocm","text":"Log in to the OpenShift Cluster Manager (OCM) to add a GitLab identity provider Select your cluster in OCM and then go to the 'Access control' tab and select 'Identity Providers' Choose GitLab as identity provider from the identity providers list Provide a name for the new identity provider Copy the OAuth callback URL . It will be needed later Note: the OAuth Callback has the following format: https://oauth-openshift.apps.<cluster_name>.<cluster_domain>/oauth2callback/<idp_name> At this point, leave the Client ID , Client secret , and URL blank while configuring GitLab","title":"Create OAuth callback URL in OCM"},{"location":"idp/gitlab/#register-a-new-application-in-gitlab","text":"Log into GitLab and execute the following steps: Go to Preferences Select Applications from the left navigation bar Provide a Name and enter the OAuth Callback URL copied from OCM above and enter it as the Redirect URL in GitLab Check the openid box and save the application After saving the GitLab application you will be provided with an Application ID and a Secret Copy both the Application ID and Secret and return to the OCM console","title":"Register a new application in GitLab"},{"location":"idp/gitlab/#configure-the-identity-provider-credentials-and-url","text":"Returning to the OCM console, enter the Application ID and Secret obtained from GitLab in the previous step and enter them as Client ID and Client Secret respectively in the OCM console. Additionally, provide the GitLab URL where credentials were obtained and click Add The new GitLab identity provider should display in the IDP list","title":"Configure the identity provider credentials and URL"},{"location":"idp/gitlab/#add-cluster-admin-or-dedicated-admin-users","text":"Now that the GitLab identity provider is configured, it is possible to add authenticated users to elevated OCM and OpenShift roles. Under Cluster Roles and Access select Add user and enter an existing GitLab user. Then choose to assign dedicated-admin or cluster-admin permissions to the user and click Add user The new user should now display, with proper permissions, in the cluster-admin or dedicated-admin user lists","title":"Add cluster-admin or dedicated-admin users"},{"location":"idp/gitlab/#log-in-and-confirm","text":"Select the Open console button in OCM to bring up the OpenShift login page. An option for GitLab should now be available. Note: I can take 1-2 minutes for this update to occur After selecting GitLab for the first time an authorization message will appear. Click Authorize to confirm. Congratulations!","title":"Log in and confirm"},{"location":"idp/gitlab-aro/","text":"Configure GitLab as an identity provider for ARO Steve Mirman 28 March 2022 The following instructions will detail how to configure GitLab as the identity provider for Azure Red Hat OpenShift: Register a new application in GitLab Create OAuth callback URL in ARO Log in and confirm Add administrative users or groups Register a new application in GitLab Log into GitLab and execute the following steps: Go to Preferences Select Applications from the left navigation bar Provide a Name and enter an OAuth Callback URL as the Redirect URI in GitLab Note: the OAuth Callback has the following format: https://oauth-openshift.apps.<cluster-id>.<region>.aroapp.io/oauth2callback/GitLab Check the openid box and save the application After saving the GitLab application you will be provided with an Application ID and a Secret Copy both the Application ID and Secret for use in the ARO console Create OAuth provider in ARO Log in to the ARO console as an administrator to add a GitLab identity provider Select the 'Administration' drop down and click 'Cluster Settings' On the 'Configuration' scroll down and click on 'OAuth' Select 'GitLab' from the Identity Providers drop down Enter a Name , the base URL of your GitLab OAuth server, and the Client ID and CLient Secret from the previous step Click Add to confirm the configuration Log in and confirm Go to the ARO console in a new browser to bring up the OpenShift login page. An option for GitLab should now be available. Note: I can take 2-3 minutes for this update to occur After selecting GitLab for the first time an authorization message will appear. Click Authorize to confirm. Once you have successfully logged in using GitLab, your userid should display under Users in the User Management section of the ARO console Note: On initial login users do NOT have elevated access Add administrative users or groups Now that the GitLab identity provider is configured, it is possible to add authenticated users to elevated OpenShift roles. This can be accomplished at the user or group level. To elevate a users permissions, select the user in the OpenShift console and click Create Binding from the RoleBindings tab Choose the scope (namespace/cluster), assign a name to the RoleBinding, and choose a role. After clicking Create the assigned user will have elevated access once they log in. To elevate a groups permissions, create a group in the OpenShift console. Edit the group YAML to specify a custom name and initial user set Create a RoleBinding for the group, similar to what was configured previously for an individual user Add additional users to the YAML file as needed and they will assume the elevated access","title":"ARO"},{"location":"idp/gitlab-aro/#configure-gitlab-as-an-identity-provider-for-aro","text":"Steve Mirman 28 March 2022 The following instructions will detail how to configure GitLab as the identity provider for Azure Red Hat OpenShift: Register a new application in GitLab Create OAuth callback URL in ARO Log in and confirm Add administrative users or groups","title":"Configure GitLab as an identity provider for ARO"},{"location":"idp/gitlab-aro/#register-a-new-application-in-gitlab","text":"Log into GitLab and execute the following steps: Go to Preferences Select Applications from the left navigation bar Provide a Name and enter an OAuth Callback URL as the Redirect URI in GitLab Note: the OAuth Callback has the following format: https://oauth-openshift.apps.<cluster-id>.<region>.aroapp.io/oauth2callback/GitLab Check the openid box and save the application After saving the GitLab application you will be provided with an Application ID and a Secret Copy both the Application ID and Secret for use in the ARO console","title":"Register a new application in GitLab"},{"location":"idp/gitlab-aro/#create-oauth-provider-in-aro","text":"Log in to the ARO console as an administrator to add a GitLab identity provider Select the 'Administration' drop down and click 'Cluster Settings' On the 'Configuration' scroll down and click on 'OAuth' Select 'GitLab' from the Identity Providers drop down Enter a Name , the base URL of your GitLab OAuth server, and the Client ID and CLient Secret from the previous step Click Add to confirm the configuration","title":"Create OAuth provider in ARO"},{"location":"idp/gitlab-aro/#log-in-and-confirm","text":"Go to the ARO console in a new browser to bring up the OpenShift login page. An option for GitLab should now be available. Note: I can take 2-3 minutes for this update to occur After selecting GitLab for the first time an authorization message will appear. Click Authorize to confirm. Once you have successfully logged in using GitLab, your userid should display under Users in the User Management section of the ARO console Note: On initial login users do NOT have elevated access","title":"Log in and confirm"},{"location":"idp/gitlab-aro/#add-administrative-users-or-groups","text":"Now that the GitLab identity provider is configured, it is possible to add authenticated users to elevated OpenShift roles. This can be accomplished at the user or group level. To elevate a users permissions, select the user in the OpenShift console and click Create Binding from the RoleBindings tab Choose the scope (namespace/cluster), assign a name to the RoleBinding, and choose a role. After clicking Create the assigned user will have elevated access once they log in. To elevate a groups permissions, create a group in the OpenShift console. Edit the group YAML to specify a custom name and initial user set Create a RoleBinding for the group, similar to what was configured previously for an individual user Add additional users to the YAML file as needed and they will assume the elevated access","title":"Add administrative users or groups"},{"location":"idp/group-claims/aro/","text":"Configure ARO to use Azure AD Group Claims Michael McNeill 28 July 2022 This guide demonstrates how to utilize the OpenID Connect group claim functionality implemented in OpenShift 4.10. This functionality allows an identity provider to provide a user's group membership for use within OpenShift. This guide will walk through the creation of an Azure Active Directory (Azure AD) application, configure the necessary Azure AD groups, and configure Azure Red Hat OpenShift (ARO) to authenticate and manage authorization using Azure AD. This guide will walk through the following steps: Register a new application in Azure AD for authentication. Configure the application registration in Azure AD to include optional and group claims in tokens. Configure the Azure Red Hat OpenShift (ARO) cluster to use Azure AD as the identity provider. Grant additional permissions to individual groups. Before you Begin Create a set of security groups and assign users by following the Microsoft documentation . In addition, if you are using zsh as your shell (which is the default shell on macOS) you may need to run set -k to get the below commands to run without errors. This is because zsh disables comments in interactive shells from being used . 1. Register a new application in Azure AD for authenitcation Capture the OAuth callback URL First, construct the cluster's OAuth callback URL and make note of it. To do so, run the following command, making sure to replace the variables specified: The \"AAD\" directory at the end of the the OAuth callback URL should match the OAuth identity provider name you'll setup later. RESOURCE_GROUP=example-rg # Replace this with the name of your ARO cluster's resource group CLUSTER_NAME=example-cluster # Replace this with the name of your ARO cluster echo 'OAuth callback URL: '$(az aro show -g $RESOURCE_GROUP -n $CLUSTER_NAME --query consoleProfile.url -o tsv | sed 's/console-openshift-console/oauth-openshift/')'oauth2callback/AAD' Register a new application in Azure AD Second, you need to create the Azure AD application itself. To do so, login to the Azure portal, and navigate to App registrations blade , then click on \"New registration\" to create a new application. Provide a name for the application, for example openshift-auth . Select \"Web\" from the Redirect URI dropdown and fill in the Redirect URI using the value of the OAuth callback URL you retrieved in the previous step. Once you fill in the necessary information, click \"Register\" to create the application. Then, click on the \"Certificates & secrets\" sub-blade and select \"New client secret\". Fill in the details request and make note of the generated client secret value, as you'll use it in a later step. You won't be able to retrieve it again. Then, click on the \"Overview\" sub-blade and make note of the \"Application (client) ID\" and \"Directory (tenant) ID\". You'll need those values in a later step as well. 2. Configure optional claims (for optional and group claims) In order to provide OpenShift with enough information about the user to create their account, we will configure Azure AD to provide two optional claims, specifically \"email\" and \"upn\", as well as a group claim when a user logs in. For more information on optional claims in Azure AD, see the Microsoft documentation . Click on the \"Token configuration\" sub-blade and select the \"Add optional claim\" button. Select ID then check the \"email\" and \"upn\" claims and click the \"Add\" button to configure them for your Azure AD application. When prompted, follow the prompt to enable the necessary Microsoft Graph permissions. Next, select the \"Add groups claim\" button. Select the \"Security groups\" option and click the \"Add\" button to configure group claims for your Azure AD application. Note: In this example, we are providing all security groups a user is a member of via the group claim. In a real production environment, we highly recommend scoping the groups provided by the group claim to _only those groups which are applicable to OpenShift . 3. Configure the OpenShift cluster to use Azure AD as the identity provider Finally, we need to configure OpenShift to use Azure AD as its identity provider. To do so, ensure you are logged in to the OpenShift command line interface ( oc ) by running the following command, making sure to replace the variables specified: RESOURCE_GROUP=example-rg # Replace this with the name of your ARO cluster's resource group CLUSTER_NAME=example-cluster # Replace this with the name of your ARO cluster oc login \\ $(az aro show -g $RESOURCE_GROUP -n $CLUSTER_NAME --query apiserverProfile.url -o tsv) \\ -u $(az aro list-credentials -g $RESOURCE_GROUP -n $CLUSTER_NAME --query kubeadminUsername -o tsv) \\ -p $(az aro list-credentials -g $RESOURCE_GROUP -n $CLUSTER_NAME --query kubeadminPassword -o tsv) Next, create a secret that contains the client secret that you captured in step 2 above. To do so, run the following command, making sure to replace the variable specified: CLIENT_SECRET=xxxxxxxx-xxxx-xxxx-xxxx-xxxxxxxxxxxx # Replace this with the Client Secret oc create secret generic openid-client-secret --from-literal=clientSecret=${CLIENT_SECRET} -n openshift-config Next, generate the necessary YAML for the cluster's OAuth provider to use Azure AD as its identity provider. To do so, run the following command, making sure to replace the variables specified: IDP_NAME=AAD # Replace this with the name you used in the OAuth callback URL APP_ID=yyyyyyyy-yyyy-yyyy-yyyy-yyyyyyyyyyyy # Replace this with the Application (client) ID TENANT_ID=zzzzzzzz-zzzz-zzzz-zzzz-zzzzzzzzzzzz # Replace this with the Directory (tenant) ID cat << EOF > cluster-oauth-config.yaml apiVersion: config.openshift.io/v1 kind: OAuth metadata: name: cluster spec: identityProviders: - mappingMethod: claim name: ${IDP_NAME} openID: claims: email: - email groups: - groups name: - name preferredUsername: - upn clientID: ${APP_ID} clientSecret: name: openid-client-secret extraScopes: [] issuer: https://login.microsoftonline.com/${TENANT_ID}/v2.0 type: OpenID EOF Feel free to further modify this output (which is saved in your current directory as cluster-oauth-config.yaml ). Finally, apply the new configuration to the cluster's OAuth provider by running the following command: oc apply -f ./cluster-oauth-config.yaml Note: It is normal to receive an error that says an annotation is missing when you run oc apply for the first time. This can be safely ignored. Once the cluster authentication operator reconciles your changes (generally within a few minutes), you will be able to login to the cluster using Azure AD. In addition, the cluster OAuth provider will automatically create or update the membership of groups the user is a member of (using the group ID). The provider does not automatically create RoleBindings and ClusterRoleBindings for the groups that are created, you are responsible for creating those via your own processes. 4. Grant additional permissions to individual groups Once the cluster authentication operator reconciles your changes (generally within a few minutes), you will be able to login to the cluster using Azure AD. In addition, the cluster OAuth provider will automatically create or update the membership of groups the user is a member of (using the group ID). Once you login, you will notice that you have very limited permissions. This is because, by default, OpenShift only grants you the ability to create new projects (namespaces) in the cluster. Other projects (namespaces) are restricted from view. The cluster OAth provider does not automatically create RoleBindings and ClusterRoleBindings for the groups that are created, you are responsible for creating those via your own processes. OpenShift includes a signifcant number of pre-configured roles, including the cluster-admin role that grants full access and control over the clster. To grant an automatically generated group access to the cluster-admin role, you must create a ClusterRoleBinding to the group ID. GROUP_ID=wwwwwwww-wwww-wwww-wwww-wwwwwwwwwwww # Replace with your Azure AD Group ID that you would like to have cluster admin permissions oc create clusterrolebinding cluster-admin-group \\ --clusterrole=cluster-admin \\ --group=$GROUP_ID Now, any user in the specified group will automatically be granted cluster-admin access. For more information on how to use RBAC to define and apply permissions in OpenShift, see the OpenShift documentation .","title":"Azure AD for ARO w/group claims"},{"location":"idp/group-claims/aro/#configure-aro-to-use-azure-ad-group-claims","text":"Michael McNeill 28 July 2022 This guide demonstrates how to utilize the OpenID Connect group claim functionality implemented in OpenShift 4.10. This functionality allows an identity provider to provide a user's group membership for use within OpenShift. This guide will walk through the creation of an Azure Active Directory (Azure AD) application, configure the necessary Azure AD groups, and configure Azure Red Hat OpenShift (ARO) to authenticate and manage authorization using Azure AD. This guide will walk through the following steps: Register a new application in Azure AD for authentication. Configure the application registration in Azure AD to include optional and group claims in tokens. Configure the Azure Red Hat OpenShift (ARO) cluster to use Azure AD as the identity provider. Grant additional permissions to individual groups.","title":"Configure ARO to use Azure AD Group Claims"},{"location":"idp/group-claims/aro/#before-you-begin","text":"Create a set of security groups and assign users by following the Microsoft documentation . In addition, if you are using zsh as your shell (which is the default shell on macOS) you may need to run set -k to get the below commands to run without errors. This is because zsh disables comments in interactive shells from being used .","title":"Before you Begin"},{"location":"idp/group-claims/aro/#1-register-a-new-application-in-azure-ad-for-authenitcation","text":"","title":"1. Register a new application in Azure AD for authenitcation"},{"location":"idp/group-claims/aro/#capture-the-oauth-callback-url","text":"First, construct the cluster's OAuth callback URL and make note of it. To do so, run the following command, making sure to replace the variables specified: The \"AAD\" directory at the end of the the OAuth callback URL should match the OAuth identity provider name you'll setup later. RESOURCE_GROUP=example-rg # Replace this with the name of your ARO cluster's resource group CLUSTER_NAME=example-cluster # Replace this with the name of your ARO cluster echo 'OAuth callback URL: '$(az aro show -g $RESOURCE_GROUP -n $CLUSTER_NAME --query consoleProfile.url -o tsv | sed 's/console-openshift-console/oauth-openshift/')'oauth2callback/AAD'","title":"Capture the OAuth callback URL"},{"location":"idp/group-claims/aro/#register-a-new-application-in-azure-ad","text":"Second, you need to create the Azure AD application itself. To do so, login to the Azure portal, and navigate to App registrations blade , then click on \"New registration\" to create a new application. Provide a name for the application, for example openshift-auth . Select \"Web\" from the Redirect URI dropdown and fill in the Redirect URI using the value of the OAuth callback URL you retrieved in the previous step. Once you fill in the necessary information, click \"Register\" to create the application. Then, click on the \"Certificates & secrets\" sub-blade and select \"New client secret\". Fill in the details request and make note of the generated client secret value, as you'll use it in a later step. You won't be able to retrieve it again. Then, click on the \"Overview\" sub-blade and make note of the \"Application (client) ID\" and \"Directory (tenant) ID\". You'll need those values in a later step as well.","title":"Register a new application in Azure AD"},{"location":"idp/group-claims/aro/#2-configure-optional-claims-for-optional-and-group-claims","text":"In order to provide OpenShift with enough information about the user to create their account, we will configure Azure AD to provide two optional claims, specifically \"email\" and \"upn\", as well as a group claim when a user logs in. For more information on optional claims in Azure AD, see the Microsoft documentation . Click on the \"Token configuration\" sub-blade and select the \"Add optional claim\" button. Select ID then check the \"email\" and \"upn\" claims and click the \"Add\" button to configure them for your Azure AD application. When prompted, follow the prompt to enable the necessary Microsoft Graph permissions. Next, select the \"Add groups claim\" button. Select the \"Security groups\" option and click the \"Add\" button to configure group claims for your Azure AD application. Note: In this example, we are providing all security groups a user is a member of via the group claim. In a real production environment, we highly recommend scoping the groups provided by the group claim to _only those groups which are applicable to OpenShift .","title":"2. Configure optional claims (for optional and group claims)"},{"location":"idp/group-claims/aro/#3-configure-the-openshift-cluster-to-use-azure-ad-as-the-identity-provider","text":"Finally, we need to configure OpenShift to use Azure AD as its identity provider. To do so, ensure you are logged in to the OpenShift command line interface ( oc ) by running the following command, making sure to replace the variables specified: RESOURCE_GROUP=example-rg # Replace this with the name of your ARO cluster's resource group CLUSTER_NAME=example-cluster # Replace this with the name of your ARO cluster oc login \\ $(az aro show -g $RESOURCE_GROUP -n $CLUSTER_NAME --query apiserverProfile.url -o tsv) \\ -u $(az aro list-credentials -g $RESOURCE_GROUP -n $CLUSTER_NAME --query kubeadminUsername -o tsv) \\ -p $(az aro list-credentials -g $RESOURCE_GROUP -n $CLUSTER_NAME --query kubeadminPassword -o tsv) Next, create a secret that contains the client secret that you captured in step 2 above. To do so, run the following command, making sure to replace the variable specified: CLIENT_SECRET=xxxxxxxx-xxxx-xxxx-xxxx-xxxxxxxxxxxx # Replace this with the Client Secret oc create secret generic openid-client-secret --from-literal=clientSecret=${CLIENT_SECRET} -n openshift-config Next, generate the necessary YAML for the cluster's OAuth provider to use Azure AD as its identity provider. To do so, run the following command, making sure to replace the variables specified: IDP_NAME=AAD # Replace this with the name you used in the OAuth callback URL APP_ID=yyyyyyyy-yyyy-yyyy-yyyy-yyyyyyyyyyyy # Replace this with the Application (client) ID TENANT_ID=zzzzzzzz-zzzz-zzzz-zzzz-zzzzzzzzzzzz # Replace this with the Directory (tenant) ID cat << EOF > cluster-oauth-config.yaml apiVersion: config.openshift.io/v1 kind: OAuth metadata: name: cluster spec: identityProviders: - mappingMethod: claim name: ${IDP_NAME} openID: claims: email: - email groups: - groups name: - name preferredUsername: - upn clientID: ${APP_ID} clientSecret: name: openid-client-secret extraScopes: [] issuer: https://login.microsoftonline.com/${TENANT_ID}/v2.0 type: OpenID EOF Feel free to further modify this output (which is saved in your current directory as cluster-oauth-config.yaml ). Finally, apply the new configuration to the cluster's OAuth provider by running the following command: oc apply -f ./cluster-oauth-config.yaml Note: It is normal to receive an error that says an annotation is missing when you run oc apply for the first time. This can be safely ignored. Once the cluster authentication operator reconciles your changes (generally within a few minutes), you will be able to login to the cluster using Azure AD. In addition, the cluster OAuth provider will automatically create or update the membership of groups the user is a member of (using the group ID). The provider does not automatically create RoleBindings and ClusterRoleBindings for the groups that are created, you are responsible for creating those via your own processes.","title":"3. Configure the OpenShift cluster to use Azure AD as the identity provider"},{"location":"idp/group-claims/aro/#4-grant-additional-permissions-to-individual-groups","text":"Once the cluster authentication operator reconciles your changes (generally within a few minutes), you will be able to login to the cluster using Azure AD. In addition, the cluster OAuth provider will automatically create or update the membership of groups the user is a member of (using the group ID). Once you login, you will notice that you have very limited permissions. This is because, by default, OpenShift only grants you the ability to create new projects (namespaces) in the cluster. Other projects (namespaces) are restricted from view. The cluster OAth provider does not automatically create RoleBindings and ClusterRoleBindings for the groups that are created, you are responsible for creating those via your own processes. OpenShift includes a signifcant number of pre-configured roles, including the cluster-admin role that grants full access and control over the clster. To grant an automatically generated group access to the cluster-admin role, you must create a ClusterRoleBinding to the group ID. GROUP_ID=wwwwwwww-wwww-wwww-wwww-wwwwwwwwwwww # Replace with your Azure AD Group ID that you would like to have cluster admin permissions oc create clusterrolebinding cluster-admin-group \\ --clusterrole=cluster-admin \\ --group=$GROUP_ID Now, any user in the specified group will automatically be granted cluster-admin access. For more information on how to use RBAC to define and apply permissions in OpenShift, see the OpenShift documentation .","title":"4. Grant additional permissions to individual groups"},{"location":"idp/group-claims/rosa/","text":"Configure ROSA to use Azure AD Group Claims Michael McNeill 28 July 2022 This guide demonstrates how to utilize the OpenID Connect group claim functionality implemented in OpenShift 4.10. This functionality allows an identity provider to provide a user's group membership for use within OpenShift. This guide will walk through the creation of an Azure Active Directory (Azure AD) application, configure the necessary Azure AD groups, and configure Red Hat OpenShift Service on AWS (ROSA) to authenticate and manage authorization using Azure AD. This guide will walk through the following steps: Register a new application in Azure AD for authentication. Configure the application registration in Azure AD to include optional and group claims in tokens. Configure the OpenShift cluster to use Azure AD as the identity provider. Grant additional permissions to individual groups. Before you Begin Create a set of security groups and assign users by following the Microsoft documentation . In addition, if you are using zsh as your shell (which is the default shell on macOS) you may need to run set -k to get the below commands to run without errors. This is because zsh disables comments in interactive shells from being used . 1. Register a new application in Azure AD for authenitcation Capture the OAuth callback URL First, construct the cluster's OAuth callback URL and make note of it. To do so, run the following command, making sure to replace the variable specified: The \"AAD\" directory at the end of the the OAuth callback URL should match the OAuth identity provider name you'll setup later. CLUSTER_NAME=example-cluster # Replace this with the name of your ROSA cluster domain=$(rosa describe cluster -c $CLUSTER_NAME | grep \"DNS\" | grep -oE '\\S+.openshiftapps.com') echo \"OAuth callback URL: https://oauth-openshift.apps.$domain/oauth2callback/AAD\" Register a new application in Azure AD Second, you need to create the Azure AD application itself. To do so, login to the Azure portal, and navigate to App registrations blade , then click on \"New registration\" to create a new application. Provide a name for the application, for example openshift-auth . Select \"Web\" from the Redirect URI dropdown and fill in the Redirect URI using the value of the OAuth callback URL you retrieved in the previous step. Once you fill in the necessary information, click \"Register\" to create the application. Then, click on the \"Certificates & secrets\" sub-blade and select \"New client secret\". Fill in the details request and make note of the generated client secret value, as you'll use it in a later step. You won't be able to retrieve it again. Then, click on the \"Overview\" sub-blade and make note of the \"Application (client) ID\" and \"Directory (tenant) ID\". You'll need those values in a later step as well. 2. Configure optional claims (for optional and group claims) In order to provide OpenShift with enough information about the user to create their account, we will configure Azure AD to provide two optional claims, specifically \"email\" and \"upn\", as well as a group claim when a user logs in. For more information on optional claims in Azure AD, see the Microsoft documentation . Click on the \"Token configuration\" sub-blade and select the \"Add optional claim\" button. Select ID then check the \"email\" and \"upn\" claims and click the \"Add\" button to configure them for your Azure AD application. When prompted, follow the prompt to enable the necessary Microsoft Graph permissions. Next, select the \"Add groups claim\" button. Select the \"Security groups\" option and click the \"Add\" button to configure group claims for your Azure AD application. Note: In this example, we are providing all security groups a user is a member of via the group claim. In a real production environment, we highly recommend scoping the groups provided by the group claim to _only those groups which are applicable to OpenShift . 3. Configure the OpenShift cluster to use Azure AD as the identity provider Finally, we need to configure OpenShift to use Azure AD as its identity provider. While Red Hat OpenShift Service on AWS (ROSA) offers the ability to configure identity providers via the OpenShift Cluster Manager (OCM), that functionality does not currently support group claims. Instead, we'll configure the cluster's OAuth provider to use Azure AD as its identity provider via the rosa CLI. To do so, run the following command, making sure to replace the variable specified: CLUSTER_NAME=example-cluster # Replace this with the name of your ROSA cluster IDP_NAME=AAD # Replace this with the name you used in the OAuth callback URL APP_ID=yyyyyyyy-yyyy-yyyy-yyyy-yyyyyyyyyyyy # Replace this with the Application (client) ID CLIENT_SECRET=xxxxxxxx-xxxx-xxxx-xxxx-xxxxxxxxxxxx # Replace this with the Client Secret TENANT_ID=zzzzzzzz-zzzz-zzzz-zzzz-zzzzzzzzzzzz # Replace this with the Directory (tenant) ID rosa create idp \\ --cluster ${CLUSTER_NAME} \\ --type openid \\ --name ${IDP_NAME} \\ --client-id ${APP_ID} \\ --client-secret ${CLIENT_SECRET} \\ --issuer-url https://login.microsoftonline.com/${TENANT_ID}/v2.0 \\ --email-claims email \\ --name-claims name \\ --username-claims upn \\ --groups-claims groups 4. Grant additional permissions to individual groups Once the cluster authentication operator reconciles your changes (generally within a few minutes), you will be able to login to the cluster using Azure AD. In addition, the cluster OAuth provider will automatically create or update the membership of groups the user is a member of (using the group ID). Once you login, you will notice that you have very limited permissions. This is because, by default, OpenShift only grants you the ability to create new projects (namespaces) in the cluster. Other projects (namespaces) are restricted from view. The cluster OAth provider does not automatically create RoleBindings and ClusterRoleBindings for the groups that are created, you are responsible for creating those via your own processes. OpenShift includes a signifcant number of pre-configured roles, including the cluster-admin role that grants full access and control over the clster. To grant an automatically generated group access to the cluster-admin role, you must create a ClusterRoleBinding to the group ID. GROUP_ID=wwwwwwww-wwww-wwww-wwww-wwwwwwwwwwww # Replace with your Azure AD Group ID that you would like to have cluster admin permissions oc create clusterrolebinding cluster-admin-group \\ --clusterrole=cluster-admin \\ --group=$GROUP_ID Now, any user in the specified group will automatically be granted cluster-admin access. For more information on how to use RBAC to define and apply permissions in OpenShift, see the OpenShift documentation .","title":"Azure AD for ROSA w/group claims"},{"location":"idp/group-claims/rosa/#configure-rosa-to-use-azure-ad-group-claims","text":"Michael McNeill 28 July 2022 This guide demonstrates how to utilize the OpenID Connect group claim functionality implemented in OpenShift 4.10. This functionality allows an identity provider to provide a user's group membership for use within OpenShift. This guide will walk through the creation of an Azure Active Directory (Azure AD) application, configure the necessary Azure AD groups, and configure Red Hat OpenShift Service on AWS (ROSA) to authenticate and manage authorization using Azure AD. This guide will walk through the following steps: Register a new application in Azure AD for authentication. Configure the application registration in Azure AD to include optional and group claims in tokens. Configure the OpenShift cluster to use Azure AD as the identity provider. Grant additional permissions to individual groups.","title":"Configure ROSA to use Azure AD Group Claims"},{"location":"idp/group-claims/rosa/#before-you-begin","text":"Create a set of security groups and assign users by following the Microsoft documentation . In addition, if you are using zsh as your shell (which is the default shell on macOS) you may need to run set -k to get the below commands to run without errors. This is because zsh disables comments in interactive shells from being used .","title":"Before you Begin"},{"location":"idp/group-claims/rosa/#1-register-a-new-application-in-azure-ad-for-authenitcation","text":"","title":"1. Register a new application in Azure AD for authenitcation"},{"location":"idp/group-claims/rosa/#capture-the-oauth-callback-url","text":"First, construct the cluster's OAuth callback URL and make note of it. To do so, run the following command, making sure to replace the variable specified: The \"AAD\" directory at the end of the the OAuth callback URL should match the OAuth identity provider name you'll setup later. CLUSTER_NAME=example-cluster # Replace this with the name of your ROSA cluster domain=$(rosa describe cluster -c $CLUSTER_NAME | grep \"DNS\" | grep -oE '\\S+.openshiftapps.com') echo \"OAuth callback URL: https://oauth-openshift.apps.$domain/oauth2callback/AAD\"","title":"Capture the OAuth callback URL"},{"location":"idp/group-claims/rosa/#register-a-new-application-in-azure-ad","text":"Second, you need to create the Azure AD application itself. To do so, login to the Azure portal, and navigate to App registrations blade , then click on \"New registration\" to create a new application. Provide a name for the application, for example openshift-auth . Select \"Web\" from the Redirect URI dropdown and fill in the Redirect URI using the value of the OAuth callback URL you retrieved in the previous step. Once you fill in the necessary information, click \"Register\" to create the application. Then, click on the \"Certificates & secrets\" sub-blade and select \"New client secret\". Fill in the details request and make note of the generated client secret value, as you'll use it in a later step. You won't be able to retrieve it again. Then, click on the \"Overview\" sub-blade and make note of the \"Application (client) ID\" and \"Directory (tenant) ID\". You'll need those values in a later step as well.","title":"Register a new application in Azure AD"},{"location":"idp/group-claims/rosa/#2-configure-optional-claims-for-optional-and-group-claims","text":"In order to provide OpenShift with enough information about the user to create their account, we will configure Azure AD to provide two optional claims, specifically \"email\" and \"upn\", as well as a group claim when a user logs in. For more information on optional claims in Azure AD, see the Microsoft documentation . Click on the \"Token configuration\" sub-blade and select the \"Add optional claim\" button. Select ID then check the \"email\" and \"upn\" claims and click the \"Add\" button to configure them for your Azure AD application. When prompted, follow the prompt to enable the necessary Microsoft Graph permissions. Next, select the \"Add groups claim\" button. Select the \"Security groups\" option and click the \"Add\" button to configure group claims for your Azure AD application. Note: In this example, we are providing all security groups a user is a member of via the group claim. In a real production environment, we highly recommend scoping the groups provided by the group claim to _only those groups which are applicable to OpenShift .","title":"2. Configure optional claims (for optional and group claims)"},{"location":"idp/group-claims/rosa/#3-configure-the-openshift-cluster-to-use-azure-ad-as-the-identity-provider","text":"Finally, we need to configure OpenShift to use Azure AD as its identity provider. While Red Hat OpenShift Service on AWS (ROSA) offers the ability to configure identity providers via the OpenShift Cluster Manager (OCM), that functionality does not currently support group claims. Instead, we'll configure the cluster's OAuth provider to use Azure AD as its identity provider via the rosa CLI. To do so, run the following command, making sure to replace the variable specified: CLUSTER_NAME=example-cluster # Replace this with the name of your ROSA cluster IDP_NAME=AAD # Replace this with the name you used in the OAuth callback URL APP_ID=yyyyyyyy-yyyy-yyyy-yyyy-yyyyyyyyyyyy # Replace this with the Application (client) ID CLIENT_SECRET=xxxxxxxx-xxxx-xxxx-xxxx-xxxxxxxxxxxx # Replace this with the Client Secret TENANT_ID=zzzzzzzz-zzzz-zzzz-zzzz-zzzzzzzzzzzz # Replace this with the Directory (tenant) ID rosa create idp \\ --cluster ${CLUSTER_NAME} \\ --type openid \\ --name ${IDP_NAME} \\ --client-id ${APP_ID} \\ --client-secret ${CLIENT_SECRET} \\ --issuer-url https://login.microsoftonline.com/${TENANT_ID}/v2.0 \\ --email-claims email \\ --name-claims name \\ --username-claims upn \\ --groups-claims groups","title":"3. Configure the OpenShift cluster to use Azure AD as the identity provider"},{"location":"idp/group-claims/rosa/#4-grant-additional-permissions-to-individual-groups","text":"Once the cluster authentication operator reconciles your changes (generally within a few minutes), you will be able to login to the cluster using Azure AD. In addition, the cluster OAuth provider will automatically create or update the membership of groups the user is a member of (using the group ID). Once you login, you will notice that you have very limited permissions. This is because, by default, OpenShift only grants you the ability to create new projects (namespaces) in the cluster. Other projects (namespaces) are restricted from view. The cluster OAth provider does not automatically create RoleBindings and ClusterRoleBindings for the groups that are created, you are responsible for creating those via your own processes. OpenShift includes a signifcant number of pre-configured roles, including the cluster-admin role that grants full access and control over the clster. To grant an automatically generated group access to the cluster-admin role, you must create a ClusterRoleBinding to the group ID. GROUP_ID=wwwwwwww-wwww-wwww-wwww-wwwwwwwwwwww # Replace with your Azure AD Group ID that you would like to have cluster admin permissions oc create clusterrolebinding cluster-admin-group \\ --clusterrole=cluster-admin \\ --group=$GROUP_ID Now, any user in the specified group will automatically be granted cluster-admin access. For more information on how to use RBAC to define and apply permissions in OpenShift, see the OpenShift documentation .","title":"4. Grant additional permissions to individual groups"},{"location":"idp/okta-grp-sync/","text":"Using Group Sync Operator with Okta and ROSA/OSD Thatcher Hubbard 15 July 2022 This guide focuses on how to synchronize Identity Provider (IDP) groups and users after configuring authentication in OpenShift Cluster Manager (OCM). For an IDP configuration example, please reference the Configure Okta as an OIDC identity provider for ROSA/OSD guide. To set up group synchronization from Okta to ROSA/OSD you must: Define groups and assign users in Okta Install the Group Sync Operator from the OpenShift Operator Hub Create and configure a new Group Sync instance Set a synchronization schedule Test the synchronization process Define groups and assign users in Okta To synchronize groups and users with ROSA/OSD they must exist in Okta Create groups to syncronize with ROSA/OSD if they do not already exist Create user IDs to synchronize with ROSA/OSD if they do not already exist Assign newly created users to the appropriate group Install the Group Sync Operator from the OpenShift Operator Hub In the OpenShift Operator Hub find the Group Sync Operator Install the operator in the group-sync-operator namespace (Optional) Create an Okta Group Sync Administrator Tokens are created with whatever permissions the currently logged-in user has, which is typically 'Super Admin' for a developer account. This is obviously not good practice for anything other than the most basic testing. Ideally, a user would be created inside the Okta organization that was specifically for group synchronizations, which should only need to be able to read groups. Creating a user with 'Read Administrator' permissions on the account would be a good place to start for following the principle of \"least privilege\". That user can then issue a token that includes only those permissions. Create an Okta API Access Token Login as a user that has minimally has Group and User read permissions (see previous section) and generate an API token in Okta Create and configure a new Group Sync instance Create a new secret named okta-group-sync in the group-sync-operator namespace. This will contain the Okta API key that was just created. Using the OpenShift CLI, create the secret using the following format: oc create secret generic okta-api-token --from-literal='okta-api-token=${API_TOKEN}' -n group-sync-operator Obtain values from Okta for the AppId and URL. The AppId is the client ID under the application created to support OpenID for the OCP Cluster(s). The URL is the same as the one used to admin Okta, without the -admin in the first term and should look something like this: https://dev-34278011.okta.com/ Create a new Group Sync instance in the group-sync-operator namespace Using the example below, customize the YAML to match the group names and save the configuration Sample YAML: apiVersion: redhatcop.redhat.io/v1alpha1 kind: GroupSync metadata: name: okta-sync spec: schedule: \"*/1 * * * *\" providers: - name: okta okta: credentialsSecret: name: okta-api-token namespace: group-sync-operator groups: - ocp-admins - ocp-restricted-users prune: true url: \"<Okta URL here>\" appId: <Okta AppID here> Set a synchronization schedule The Group Sync Operator provides a cron-based scheduling parameter for specifying how often the groups and users should be synchronized. This can be set in the instance YAML file during initial configuration or at any time after. The schedule setting of schedule: \"* * * * *\" would result in synchronization occuring every minute. It also supports the cron \"slash\" notation (e.g., \" /5 * * *\", which would synchronize every five minutes). Testing the synchronization process Check to see if the Group Sync process has completed with a Condition: ReconcileSuccess message Check to see that all the groups specified in the configuration YAML file show up in the ROSA/OSD Groups list Validate that all users specified in Okta also show up as members of the associated group in ROSA/OSD Add a new user in Okta and assign it to the admin group Verify that the user now appears in ROSA/OSD (after the specified synchronization time) Now deactivate a user from the Okta admin group Verify the user has been deleted from the ROSA/OSD admin group Binding Groups to Roles The preceding steps provide a method to get group membership information into OpenShift, but the final step in translating that into user authorization control requires binding each group to a role or roles on the cluster. This can be done via the OCP web console by opening the Group detail, or by applying YAML via the CLI. Additional Okta Config Options There are also other options that are provider-specific that are covered in the operator documentation that should be kept in mind: Pruning groups that cease to exist on Okta A numeric limit on the number of groups to sync A list of groups against which to match If there is a need to have multiple GroupSync configurations against multiple providers, note that there is no \"merge\" functionality in the operator when it comes to group membership. If a group named ocp-admins is present in two directories with sync jobs, they will effectively overwrite each other each time the sync job runs. It is recommended to name groups intended for use on OCP such that they indicate from which directory they originate (e.g., okta-ocp-admins or something like okta-contoso-ocp-admins in the case of multiple Okta providers). Bind multiple groups with the same permissions needs to the same Role or ClusterRole as needed.","title":"Using Group Sync Operator with Okta and ROSA/OSD #"},{"location":"idp/okta-grp-sync/#using-group-sync-operator-with-okta-and-rosaosd","text":"Thatcher Hubbard 15 July 2022 This guide focuses on how to synchronize Identity Provider (IDP) groups and users after configuring authentication in OpenShift Cluster Manager (OCM). For an IDP configuration example, please reference the Configure Okta as an OIDC identity provider for ROSA/OSD guide. To set up group synchronization from Okta to ROSA/OSD you must: Define groups and assign users in Okta Install the Group Sync Operator from the OpenShift Operator Hub Create and configure a new Group Sync instance Set a synchronization schedule Test the synchronization process","title":"Using Group Sync Operator with Okta and ROSA/OSD"},{"location":"idp/okta-grp-sync/#define-groups-and-assign-users-in-okta","text":"To synchronize groups and users with ROSA/OSD they must exist in Okta Create groups to syncronize with ROSA/OSD if they do not already exist Create user IDs to synchronize with ROSA/OSD if they do not already exist Assign newly created users to the appropriate group","title":"Define groups and assign users in Okta"},{"location":"idp/okta-grp-sync/#install-the-group-sync-operator-from-the-openshift-operator-hub","text":"In the OpenShift Operator Hub find the Group Sync Operator Install the operator in the group-sync-operator namespace","title":"Install the Group Sync Operator from the OpenShift Operator Hub"},{"location":"idp/okta-grp-sync/#optional-create-an-okta-group-sync-administrator","text":"Tokens are created with whatever permissions the currently logged-in user has, which is typically 'Super Admin' for a developer account. This is obviously not good practice for anything other than the most basic testing. Ideally, a user would be created inside the Okta organization that was specifically for group synchronizations, which should only need to be able to read groups. Creating a user with 'Read Administrator' permissions on the account would be a good place to start for following the principle of \"least privilege\". That user can then issue a token that includes only those permissions.","title":"(Optional) Create an Okta Group Sync Administrator"},{"location":"idp/okta-grp-sync/#create-an-okta-api-access-token","text":"Login as a user that has minimally has Group and User read permissions (see previous section) and generate an API token in Okta","title":"Create an Okta API Access Token"},{"location":"idp/okta-grp-sync/#create-and-configure-a-new-group-sync-instance","text":"Create a new secret named okta-group-sync in the group-sync-operator namespace. This will contain the Okta API key that was just created. Using the OpenShift CLI, create the secret using the following format: oc create secret generic okta-api-token --from-literal='okta-api-token=${API_TOKEN}' -n group-sync-operator Obtain values from Okta for the AppId and URL. The AppId is the client ID under the application created to support OpenID for the OCP Cluster(s). The URL is the same as the one used to admin Okta, without the -admin in the first term and should look something like this: https://dev-34278011.okta.com/ Create a new Group Sync instance in the group-sync-operator namespace Using the example below, customize the YAML to match the group names and save the configuration Sample YAML: apiVersion: redhatcop.redhat.io/v1alpha1 kind: GroupSync metadata: name: okta-sync spec: schedule: \"*/1 * * * *\" providers: - name: okta okta: credentialsSecret: name: okta-api-token namespace: group-sync-operator groups: - ocp-admins - ocp-restricted-users prune: true url: \"<Okta URL here>\" appId: <Okta AppID here>","title":"Create and configure a new Group Sync instance"},{"location":"idp/okta-grp-sync/#set-a-synchronization-schedule","text":"The Group Sync Operator provides a cron-based scheduling parameter for specifying how often the groups and users should be synchronized. This can be set in the instance YAML file during initial configuration or at any time after. The schedule setting of schedule: \"* * * * *\" would result in synchronization occuring every minute. It also supports the cron \"slash\" notation (e.g., \" /5 * * *\", which would synchronize every five minutes).","title":"Set a synchronization schedule"},{"location":"idp/okta-grp-sync/#testing-the-synchronization-process","text":"Check to see if the Group Sync process has completed with a Condition: ReconcileSuccess message Check to see that all the groups specified in the configuration YAML file show up in the ROSA/OSD Groups list Validate that all users specified in Okta also show up as members of the associated group in ROSA/OSD Add a new user in Okta and assign it to the admin group Verify that the user now appears in ROSA/OSD (after the specified synchronization time) Now deactivate a user from the Okta admin group Verify the user has been deleted from the ROSA/OSD admin group","title":"Testing the synchronization process"},{"location":"idp/okta-grp-sync/#binding-groups-to-roles","text":"The preceding steps provide a method to get group membership information into OpenShift, but the final step in translating that into user authorization control requires binding each group to a role or roles on the cluster. This can be done via the OCP web console by opening the Group detail, or by applying YAML via the CLI.","title":"Binding Groups to Roles"},{"location":"idp/okta-grp-sync/#additional-okta-config-options","text":"There are also other options that are provider-specific that are covered in the operator documentation that should be kept in mind: Pruning groups that cease to exist on Okta A numeric limit on the number of groups to sync A list of groups against which to match If there is a need to have multiple GroupSync configurations against multiple providers, note that there is no \"merge\" functionality in the operator when it comes to group membership. If a group named ocp-admins is present in two directories with sync jobs, they will effectively overwrite each other each time the sync job runs. It is recommended to name groups intended for use on OCP such that they indicate from which directory they originate (e.g., okta-ocp-admins or something like okta-contoso-ocp-admins in the case of multiple Okta providers). Bind multiple groups with the same permissions needs to the same Role or ClusterRole as needed.","title":"Additional Okta Config Options"},{"location":"ingress/default-router-custom-domain/","text":"Stop default router from serving custom domain routes OSD and ROSA supports custom domain operator to serve application custom domain, which provisions openshift ingress controller and cloud load balancers. However, when a route with custom domain is created, both default router and custom domain router serve routes. This article describes how to use route labels to stop default router from serving custom domain routes. Prerequisites Rosa or OSD Cluster Custom Domain Deployed Problem Demo Deploy A Custom Domain oc create secret tls example-tls --cert=[cert_file] --key=[key_file] cat << EOF | oc apply -f - apiVersion: managed.openshift.io/v1alpha1 kind: CustomDomain metadata: name: example spec: domain: example.com scope: External certificate: name: example-tls namespace: default EOF Create a sample application and Route oc new-app --image=openshift/hello-openshift cat <<EOF | oc apply -f - apiVersion: route.openshift.io/v1 kind: Route metadata: labels: app: hello-openshift app.kubernetes.io/component: hello-openshift app.kubernetes.io/instance: hello-openshift name: helloworld spec: host: helloworld-openshift.example.com port: targetPort: 8080-tcp tls: termination: edge to: kind: \"\" name: hello-openshift EOF Both default router and custom router serve the routes oc get route -o yaml .... status: ingress: - conditions: - lastTransitionTime: \"2022-06-02T20:30:39Z\" status: \"True\" type: Admitted host: helloworld-openshift.example.com routerCanonicalHostname: router-default.apps.mobb-infra-gcp.e8e4.p2.openshiftapps.com routerName: default wildcardPolicy: None - conditions: - lastTransitionTime: \"2022-06-02T20:30:39Z\" status: \"True\" type: Admitted host: helloworld-openshift.example.com routerCanonicalHostname: router-example.example.mobb-infra-gcp.e8e4.p2.openshiftapps.com routerName: example wildcardPolicy: None End user can access the app from both ingress controllers' cloud load balancer oc get svc -n openshift-ingress NAME TYPE CLUSTER-IP EXTERNAL-IP PORT(S) AGE router-default LoadBalancer 172.30.39.254 34.73.154.84 80:32108/TCP,443:30332/TCP 39d router-example LoadBalancer 172.30.209.51 34.138.159.7 80:32477/TCP,443:31383/TCP 9m51s curl -k -H \"Host: helloworld-openshift.example.com\" https://34.73.154.84 Hello OpenShift! shading@shading-mac gcp_domain % curl -k -H \"Host: helloworld-openshift.example.com\" https://34.138.159.7 Hello OpenShift! Stop the default router from serving custom domain routes Delete the route oc delete route helloworld Custom Domain only serve routes with corresponding custom domain label oc patch \\ -n openshift-ingress-operator \\ IngressController/example \\ --type='merge' \\ -p '{\"spec\":{\"routeSelector\":{\"matchLabels\": {\"domain\": \"example.com\"}}}}' Exclude default router with corresponding custom domain label oc patch \\ -n openshift-ingress-operator \\ IngressController/default \\ --type='merge' \\ -p '{\"spec\":{\"routeSelector\":{\"matchExpressions\":[{\"key\":\"domain\",\"operator\":\"NotIn\",\"values\":[\"example.com\"]}]}}}' Create route with custom domain label cat <<EOF | oc apply -f - apiVersion: route.openshift.io/v1 kind: Route metadata: labels: app: hello-openshift domain: example.com app.kubernetes.io/component: hello-openshift app.kubernetes.io/instance: hello-openshift name: helloworld spec: host: helloworld-openshift.example.com port: targetPort: 8080-tcp tls: termination: edge to: kind: \"\" name: hello-openshift EOF Only Custom Domain router route the traffic oc get route -o yaml .... status: ingress: - conditions: - lastTransitionTime: \"2022-06-02T20:30:39Z\" status: \"True\" type: Admitted host: helloworld-openshift.example.com routerCanonicalHostname: router-example.example.mobb-infra-gcp.e8e4.p2.openshiftapps.com routerName: example wildcardPolicy: None oc get svc -n openshift-ingress NAME TYPE CLUSTER-IP EXTERNAL-IP PORT(S) AGE router-default LoadBalancer 172.30.39.254 34.73.154.84 80:32108/TCP,443:30332/TCP 39d router-example LoadBalancer 172.30.209.51 34.138.159.7 80:32477/TCP,443:31383/TCP 9m51s curl -k -H \"Host: helloworld-openshift.example.com\" https://34.73.154.84 .... The application is currently not serving requests at this endpoint. It may not have been started or is still starting. ... curl -k -H \"Host: helloworld-openshift.example.com\" https://34.138.159.7 Hello OpenShift!","title":"Stop default router from serving custom domain routes"},{"location":"ingress/default-router-custom-domain/#stop-default-router-from-serving-custom-domain-routes","text":"OSD and ROSA supports custom domain operator to serve application custom domain, which provisions openshift ingress controller and cloud load balancers. However, when a route with custom domain is created, both default router and custom domain router serve routes. This article describes how to use route labels to stop default router from serving custom domain routes.","title":"Stop default router from serving custom domain routes"},{"location":"ingress/default-router-custom-domain/#prerequisites","text":"Rosa or OSD Cluster Custom Domain Deployed","title":"Prerequisites"},{"location":"ingress/default-router-custom-domain/#problem-demo","text":"","title":"Problem Demo"},{"location":"ingress/default-router-custom-domain/#deploy-a-custom-domain","text":"oc create secret tls example-tls --cert=[cert_file] --key=[key_file] cat << EOF | oc apply -f - apiVersion: managed.openshift.io/v1alpha1 kind: CustomDomain metadata: name: example spec: domain: example.com scope: External certificate: name: example-tls namespace: default EOF","title":"Deploy A Custom Domain"},{"location":"ingress/default-router-custom-domain/#create-a-sample-application-and-route","text":"oc new-app --image=openshift/hello-openshift cat <<EOF | oc apply -f - apiVersion: route.openshift.io/v1 kind: Route metadata: labels: app: hello-openshift app.kubernetes.io/component: hello-openshift app.kubernetes.io/instance: hello-openshift name: helloworld spec: host: helloworld-openshift.example.com port: targetPort: 8080-tcp tls: termination: edge to: kind: \"\" name: hello-openshift EOF","title":"Create a sample application and Route"},{"location":"ingress/default-router-custom-domain/#both-default-router-and-custom-router-serve-the-routes","text":"oc get route -o yaml .... status: ingress: - conditions: - lastTransitionTime: \"2022-06-02T20:30:39Z\" status: \"True\" type: Admitted host: helloworld-openshift.example.com routerCanonicalHostname: router-default.apps.mobb-infra-gcp.e8e4.p2.openshiftapps.com routerName: default wildcardPolicy: None - conditions: - lastTransitionTime: \"2022-06-02T20:30:39Z\" status: \"True\" type: Admitted host: helloworld-openshift.example.com routerCanonicalHostname: router-example.example.mobb-infra-gcp.e8e4.p2.openshiftapps.com routerName: example wildcardPolicy: None","title":"Both default router and custom router serve the routes"},{"location":"ingress/default-router-custom-domain/#end-user-can-access-the-app-from-both-ingress-controllers-cloud-load-balancer","text":"oc get svc -n openshift-ingress NAME TYPE CLUSTER-IP EXTERNAL-IP PORT(S) AGE router-default LoadBalancer 172.30.39.254 34.73.154.84 80:32108/TCP,443:30332/TCP 39d router-example LoadBalancer 172.30.209.51 34.138.159.7 80:32477/TCP,443:31383/TCP 9m51s curl -k -H \"Host: helloworld-openshift.example.com\" https://34.73.154.84 Hello OpenShift! shading@shading-mac gcp_domain % curl -k -H \"Host: helloworld-openshift.example.com\" https://34.138.159.7 Hello OpenShift!","title":"End user can access the app from both ingress controllers' cloud load balancer"},{"location":"ingress/default-router-custom-domain/#stop-the-default-router-from-serving-custom-domain-routes","text":"","title":"Stop the default router from serving custom domain routes"},{"location":"ingress/default-router-custom-domain/#delete-the-route","text":"oc delete route helloworld","title":"Delete the route"},{"location":"ingress/default-router-custom-domain/#custom-domain-only-serve-routes-with-corresponding-custom-domain-label","text":"oc patch \\ -n openshift-ingress-operator \\ IngressController/example \\ --type='merge' \\ -p '{\"spec\":{\"routeSelector\":{\"matchLabels\": {\"domain\": \"example.com\"}}}}'","title":"Custom Domain only serve routes with corresponding custom domain label"},{"location":"ingress/default-router-custom-domain/#exclude-default-router-with-corresponding-custom-domain-label","text":"oc patch \\ -n openshift-ingress-operator \\ IngressController/default \\ --type='merge' \\ -p '{\"spec\":{\"routeSelector\":{\"matchExpressions\":[{\"key\":\"domain\",\"operator\":\"NotIn\",\"values\":[\"example.com\"]}]}}}'","title":"Exclude default router with corresponding custom domain label"},{"location":"ingress/default-router-custom-domain/#create-route-with-custom-domain-label","text":"cat <<EOF | oc apply -f - apiVersion: route.openshift.io/v1 kind: Route metadata: labels: app: hello-openshift domain: example.com app.kubernetes.io/component: hello-openshift app.kubernetes.io/instance: hello-openshift name: helloworld spec: host: helloworld-openshift.example.com port: targetPort: 8080-tcp tls: termination: edge to: kind: \"\" name: hello-openshift EOF","title":"Create route with custom domain label"},{"location":"ingress/default-router-custom-domain/#only-custom-domain-router-route-the-traffic","text":"oc get route -o yaml .... status: ingress: - conditions: - lastTransitionTime: \"2022-06-02T20:30:39Z\" status: \"True\" type: Admitted host: helloworld-openshift.example.com routerCanonicalHostname: router-example.example.mobb-infra-gcp.e8e4.p2.openshiftapps.com routerName: example wildcardPolicy: None oc get svc -n openshift-ingress NAME TYPE CLUSTER-IP EXTERNAL-IP PORT(S) AGE router-default LoadBalancer 172.30.39.254 34.73.154.84 80:32108/TCP,443:30332/TCP 39d router-example LoadBalancer 172.30.209.51 34.138.159.7 80:32477/TCP,443:31383/TCP 9m51s curl -k -H \"Host: helloworld-openshift.example.com\" https://34.73.154.84 .... The application is currently not serving requests at this endpoint. It may not have been started or is still starting. ... curl -k -H \"Host: helloworld-openshift.example.com\" https://34.138.159.7 Hello OpenShift!","title":"Only Custom Domain router route the traffic"},{"location":"ingress/tls-cipher-customization/","text":"Configure ROSA/OSD to use custom TLS ciphers on the ingress controllers Michael McNeill 24 August 2022 This guide demonstrates how to properly patch the cluster ingress controllers, as well as ingress controllers created by the Custom Domain Operator. This functionality allows customers to modify the tlsSecurityProfile value on cluster ingress controllers. This guide will demonstrate how to apply a custom tlsSecurityProfile , a scoped service account (with the associated role and role binding), and a CronJob that the cipher changes are reapplied with 60 minutes (in the event that an ingress controller is recreated or modified). Before you Begin Review the OpenShift Documentation that explains the options for the tlsSecurityProfile . By default, ingress controllers are configured to use the Intermediate profile, which corresponds to the Intermediate Mozilla profile . 1. Create a service account for the CronJob to use A service account allows our CronJob to directly access the cluster API, without using a regular user's credentials. To create a service account, run the following command: oc create sa cron-ingress-patch-sa -n openshift-ingress-operator 2. Create a role and role binding that allows limited access to patch the ingress controllers Role-based access control (RBAC) is critical to ensuring security inside your cluster. Creating a role allows us to provide scoped access to only the API resources we need within the cluster. To create the role, run the following command: oc create role cron-ingress-patch-role --verb=get,patch,update --resource=ingresscontroller.operator.openshift.io -n openshift-ingress-operator Once the role has been created, you need to bind the role to the service account using a role binding. To create the role binding, run the following command: oc create rolebinding cron-ingress-patch-rolebinding --role=cron-ingress-patch-role --serviceaccount=openshift-ingress-operator:cron-ingress-patch-sa -n openshift-ingress-operator 3. Patch the ingress controller Important note: The examples provided below add an additional cipher to the ingress controller's tlsSecurityProfile to allow IE 11 access from Windows Server 2008 R2. You should modify this command to meet your specific business requirements. Before we create the CronJob, we first want to apply the tlsSecurityProfile configuration to validate our changes. This process depends on if you are using the Custom Domain Operator . Clusters not using the Custom Domain Operator If you are only using the default ingress controller, and not using the Custom Domain Operator , you will run the following command to patch the ingress controller: oc patch ingresscontroller/default -n openshift-ingress-operator --type=merge -p '{\"spec\":{\"tlsSecurityProfile\":{\"type\":\"Custom\",\"custom\":{\"ciphers\":[\"TLS_AES_128_GCM_SHA256\",\"TLS_AES_256_GCM_SHA384\",\"ECDHE-ECDSA-AES128-GCM-SHA256\",\"ECDHE-RSA-AES128-GCM-SHA256\",\"ECDHE-ECDSA-AES256-GCM-SHA384\",\"ECDHE-RSA-AES256-GCM-SHA384\",\"ECDHE-ECDSA-CHACHA20-POLY1305\",\"ECDHE-RSA-CHACHA20-POLY1305\",\"DHE-RSA-AES128-GCM-SHA256\",\"DHE-RSA-AES256-GCM-SHA384\",\"TLS_CHACHA20_POLY1305_SHA256\",\"TLS_ECDHE_RSA_WITH_AES_128_CBC_SHA\"],\"minTLSVersion\":\"VersionTLS12\"}}}}' This patch will add the TLS_ECDHE_RSA_WITH_AES_128_CBC_SHA cipher which allows access from IE 11 on Windows Server 2008 R2 when using RSA certificates. Once you've run the command, you'll receive a response that looks like this: ingresscontroller.operator.openshift.io/default patched Clusters using the Custom Domain Operator Customers who are using the Custom Domain Operator will need to loop through each of their ingress controllers to patch each one. To patch all of your cluster's ingress controllers, run the following command: for ic in $(oc get ingresscontroller -o name -n openshift-ingress-operator); do oc patch ${ic} -n openshift-ingress-operator --type=merge -p '{\"spec\":{\"tlsSecurityProfile\":{\"type\":\"Custom\",\"custom\":{\"ciphers\":[\"TLS_AES_128_GCM_SHA256\",\"TLS_AES_256_GCM_SHA384\",\"ECDHE-ECDSA-AES128-GCM-SHA256\",\"ECDHE-RSA-AES128-GCM-SHA256\",\"ECDHE-ECDSA-AES256-GCM-SHA384\",\"ECDHE-RSA-AES256-GCM-SHA384\",\"ECDHE-ECDSA-CHACHA20-POLY1305\",\"ECDHE-RSA-CHACHA20-POLY1305\",\"DHE-RSA-AES128-GCM-SHA256\",\"DHE-RSA-AES256-GCM-SHA384\",\"TLS_CHACHA20_POLY1305_SHA256\",\"TLS_ECDHE_RSA_WITH_AES_128_CBC_SHA\"],\"minTLSVersion\":\"VersionTLS12\"}}}}'; done Once you've run the command, you'll receive a response that looks like this: ingresscontroller.operator.openshift.io/default patched ingresscontroller.operator.openshift.io/custom1 patched ingresscontroller.operator.openshift.io/custom2 patched 4. Create the CronJob to ensure the TLS configuration is not overwritten Occasionally, the cluster's ingress controller can get recreated. In these cases, the ingress controller will likely not retain the tlsSecurityProfile changes that we've made. To ensure this doesn't happen, we'll create a CronJob that goes through and updates the cluster's ingress controller(s). This process depends on if you are using the Custom Domain Operator . Clusters not using the Custom Domain Operator If you are not using the Custom Domain Operator , creating the CronJob is as simple as running the following command: cat << EOF | oc apply -f - apiVersion: batch/v1 kind: CronJob metadata: name: tls-patch namespace: openshift-ingress-operator spec: schedule: '@hourly' jobTemplate: spec: template: spec: containers: - name: tls-patch image: registry.redhat.io/openshift4/ose-tools-rhel8:latest args: - /bin/sh - '-c' - oc patch ingresscontroller/default -n openshift-ingress-operator --type=merge -p '{\"spec\":{\"tlsSecurityProfile\":{\"type\":\"Custom\",\"custom\":{\"ciphers\":[\"TLS_AES_128_GCM_SHA256\",\"TLS_AES_256_GCM_SHA384\",\"ECDHE-ECDSA-AES128-GCM-SHA256\",\"ECDHE-RSA-AES128-GCM-SHA256\",\"ECDHE-ECDSA-AES256-GCM-SHA384\",\"ECDHE-RSA-AES256-GCM-SHA384\",\"ECDHE-ECDSA-CHACHA20-POLY1305\",\"ECDHE-RSA-CHACHA20-POLY1305\",\"DHE-RSA-AES128-GCM-SHA256\",\"DHE-RSA-AES256-GCM-SHA384\",\"TLS_CHACHA20_POLY1305_SHA256\",\"TLS_ECDHE_RSA_WITH_AES_128_CBC_SHA\"],\"minTLSVersion\":\"VersionTLS12\"}}}}' restartPolicy: Never serviceAccountName: cron-ingress-patch-sa EOF Note, this CronJob will run every hour, and will patch the ingress controller, if necessary. It is important that this CronJob does not run constantly, as it can trigger reconciles that could overload the OpenShift Ingress Operator. Most of the time, the logs of the CronJob pod will look something like this, as it will not be changing anything: ingresscontroller.operator.openshift.io/default patched (no change) Clusters using the Custom Domain Operator If you are using the Custom Domain Operator the CronJob will need to loop through and patch each ingress controller. To create this CronJob, run the following command: cat << EOF | oc apply -f - apiVersion: batch/v1 kind: CronJob metadata: name: tls-patch namespace: openshift-ingress-operator spec: schedule: '@hourly' jobTemplate: spec: template: spec: containers: - name: tls-patch image: registry.redhat.io/openshift4/ose-tools-rhel8:latest args: - /bin/sh - '-c' - for ic in $(oc get ingresscontroller -o name -n openshift-ingress-operator); do oc patch ${ic} -n openshift-ingress-operator --type=merge -p '{\"spec\":{\"tlsSecurityProfile\":{\"type\":\"Custom\",\"custom\":{\"ciphers\":[\"TLS_AES_128_GCM_SHA256\",\"TLS_AES_256_GCM_SHA384\",\"ECDHE-ECDSA-AES128-GCM-SHA256\",\"ECDHE-RSA-AES128-GCM-SHA256\",\"ECDHE-ECDSA-AES256-GCM-SHA384\",\"ECDHE-RSA-AES256-GCM-SHA384\",\"ECDHE-ECDSA-CHACHA20-POLY1305\",\"ECDHE-RSA-CHACHA20-POLY1305\",\"DHE-RSA-AES128-GCM-SHA256\",\"DHE-RSA-AES256-GCM-SHA384\",\"TLS_CHACHA20_POLY1305_SHA256\",\"TLS_ECDHE_RSA_WITH_AES_128_CBC_SHA\"],\"minTLSVersion\":\"VersionTLS12\"}}}}'; done restartPolicy: Never serviceAccountName: cron-ingress-patch-sa EOF Note, this CronJob will run every hour, and will patch the ingress controller, if necessary. It is important that this CronJob does not run constantly, as it can trigger reconciles that could overload the OpenShift Ingress Operator. Most of the time, the logs of the CronJob pod will look something like this, as it will not be changing anything: ingresscontroller.operator.openshift.io/default patched (no change) ingresscontroller.operator.openshift.io/custom1 patched (no change) ingresscontroller.operator.openshift.io/custom2 patched (no change)","title":"Configure ROSA/OSD to use custom TLS ciphers on the ingress controllers #"},{"location":"ingress/tls-cipher-customization/#configure-rosaosd-to-use-custom-tls-ciphers-on-the-ingress-controllers","text":"Michael McNeill 24 August 2022 This guide demonstrates how to properly patch the cluster ingress controllers, as well as ingress controllers created by the Custom Domain Operator. This functionality allows customers to modify the tlsSecurityProfile value on cluster ingress controllers. This guide will demonstrate how to apply a custom tlsSecurityProfile , a scoped service account (with the associated role and role binding), and a CronJob that the cipher changes are reapplied with 60 minutes (in the event that an ingress controller is recreated or modified).","title":"Configure ROSA/OSD to use custom TLS ciphers on the ingress controllers"},{"location":"ingress/tls-cipher-customization/#before-you-begin","text":"Review the OpenShift Documentation that explains the options for the tlsSecurityProfile . By default, ingress controllers are configured to use the Intermediate profile, which corresponds to the Intermediate Mozilla profile .","title":"Before you Begin"},{"location":"ingress/tls-cipher-customization/#1-create-a-service-account-for-the-cronjob-to-use","text":"A service account allows our CronJob to directly access the cluster API, without using a regular user's credentials. To create a service account, run the following command: oc create sa cron-ingress-patch-sa -n openshift-ingress-operator","title":"1. Create a service account for the CronJob to use"},{"location":"ingress/tls-cipher-customization/#2-create-a-role-and-role-binding-that-allows-limited-access-to-patch-the-ingress-controllers","text":"Role-based access control (RBAC) is critical to ensuring security inside your cluster. Creating a role allows us to provide scoped access to only the API resources we need within the cluster. To create the role, run the following command: oc create role cron-ingress-patch-role --verb=get,patch,update --resource=ingresscontroller.operator.openshift.io -n openshift-ingress-operator Once the role has been created, you need to bind the role to the service account using a role binding. To create the role binding, run the following command: oc create rolebinding cron-ingress-patch-rolebinding --role=cron-ingress-patch-role --serviceaccount=openshift-ingress-operator:cron-ingress-patch-sa -n openshift-ingress-operator","title":"2. Create a role and role binding that allows limited access to patch the ingress controllers"},{"location":"ingress/tls-cipher-customization/#3-patch-the-ingress-controller","text":"Important note: The examples provided below add an additional cipher to the ingress controller's tlsSecurityProfile to allow IE 11 access from Windows Server 2008 R2. You should modify this command to meet your specific business requirements. Before we create the CronJob, we first want to apply the tlsSecurityProfile configuration to validate our changes. This process depends on if you are using the Custom Domain Operator .","title":"3. Patch the ingress controller"},{"location":"ingress/tls-cipher-customization/#clusters-not-using-the-custom-domain-operator","text":"If you are only using the default ingress controller, and not using the Custom Domain Operator , you will run the following command to patch the ingress controller: oc patch ingresscontroller/default -n openshift-ingress-operator --type=merge -p '{\"spec\":{\"tlsSecurityProfile\":{\"type\":\"Custom\",\"custom\":{\"ciphers\":[\"TLS_AES_128_GCM_SHA256\",\"TLS_AES_256_GCM_SHA384\",\"ECDHE-ECDSA-AES128-GCM-SHA256\",\"ECDHE-RSA-AES128-GCM-SHA256\",\"ECDHE-ECDSA-AES256-GCM-SHA384\",\"ECDHE-RSA-AES256-GCM-SHA384\",\"ECDHE-ECDSA-CHACHA20-POLY1305\",\"ECDHE-RSA-CHACHA20-POLY1305\",\"DHE-RSA-AES128-GCM-SHA256\",\"DHE-RSA-AES256-GCM-SHA384\",\"TLS_CHACHA20_POLY1305_SHA256\",\"TLS_ECDHE_RSA_WITH_AES_128_CBC_SHA\"],\"minTLSVersion\":\"VersionTLS12\"}}}}' This patch will add the TLS_ECDHE_RSA_WITH_AES_128_CBC_SHA cipher which allows access from IE 11 on Windows Server 2008 R2 when using RSA certificates. Once you've run the command, you'll receive a response that looks like this: ingresscontroller.operator.openshift.io/default patched","title":"Clusters not using the Custom Domain Operator"},{"location":"ingress/tls-cipher-customization/#clusters-using-the-custom-domain-operator","text":"Customers who are using the Custom Domain Operator will need to loop through each of their ingress controllers to patch each one. To patch all of your cluster's ingress controllers, run the following command: for ic in $(oc get ingresscontroller -o name -n openshift-ingress-operator); do oc patch ${ic} -n openshift-ingress-operator --type=merge -p '{\"spec\":{\"tlsSecurityProfile\":{\"type\":\"Custom\",\"custom\":{\"ciphers\":[\"TLS_AES_128_GCM_SHA256\",\"TLS_AES_256_GCM_SHA384\",\"ECDHE-ECDSA-AES128-GCM-SHA256\",\"ECDHE-RSA-AES128-GCM-SHA256\",\"ECDHE-ECDSA-AES256-GCM-SHA384\",\"ECDHE-RSA-AES256-GCM-SHA384\",\"ECDHE-ECDSA-CHACHA20-POLY1305\",\"ECDHE-RSA-CHACHA20-POLY1305\",\"DHE-RSA-AES128-GCM-SHA256\",\"DHE-RSA-AES256-GCM-SHA384\",\"TLS_CHACHA20_POLY1305_SHA256\",\"TLS_ECDHE_RSA_WITH_AES_128_CBC_SHA\"],\"minTLSVersion\":\"VersionTLS12\"}}}}'; done Once you've run the command, you'll receive a response that looks like this: ingresscontroller.operator.openshift.io/default patched ingresscontroller.operator.openshift.io/custom1 patched ingresscontroller.operator.openshift.io/custom2 patched","title":"Clusters using the Custom Domain Operator"},{"location":"ingress/tls-cipher-customization/#4-create-the-cronjob-to-ensure-the-tls-configuration-is-not-overwritten","text":"Occasionally, the cluster's ingress controller can get recreated. In these cases, the ingress controller will likely not retain the tlsSecurityProfile changes that we've made. To ensure this doesn't happen, we'll create a CronJob that goes through and updates the cluster's ingress controller(s). This process depends on if you are using the Custom Domain Operator .","title":"4. Create the CronJob to ensure the TLS configuration is not overwritten"},{"location":"ingress/tls-cipher-customization/#clusters-not-using-the-custom-domain-operator_1","text":"If you are not using the Custom Domain Operator , creating the CronJob is as simple as running the following command: cat << EOF | oc apply -f - apiVersion: batch/v1 kind: CronJob metadata: name: tls-patch namespace: openshift-ingress-operator spec: schedule: '@hourly' jobTemplate: spec: template: spec: containers: - name: tls-patch image: registry.redhat.io/openshift4/ose-tools-rhel8:latest args: - /bin/sh - '-c' - oc patch ingresscontroller/default -n openshift-ingress-operator --type=merge -p '{\"spec\":{\"tlsSecurityProfile\":{\"type\":\"Custom\",\"custom\":{\"ciphers\":[\"TLS_AES_128_GCM_SHA256\",\"TLS_AES_256_GCM_SHA384\",\"ECDHE-ECDSA-AES128-GCM-SHA256\",\"ECDHE-RSA-AES128-GCM-SHA256\",\"ECDHE-ECDSA-AES256-GCM-SHA384\",\"ECDHE-RSA-AES256-GCM-SHA384\",\"ECDHE-ECDSA-CHACHA20-POLY1305\",\"ECDHE-RSA-CHACHA20-POLY1305\",\"DHE-RSA-AES128-GCM-SHA256\",\"DHE-RSA-AES256-GCM-SHA384\",\"TLS_CHACHA20_POLY1305_SHA256\",\"TLS_ECDHE_RSA_WITH_AES_128_CBC_SHA\"],\"minTLSVersion\":\"VersionTLS12\"}}}}' restartPolicy: Never serviceAccountName: cron-ingress-patch-sa EOF Note, this CronJob will run every hour, and will patch the ingress controller, if necessary. It is important that this CronJob does not run constantly, as it can trigger reconciles that could overload the OpenShift Ingress Operator. Most of the time, the logs of the CronJob pod will look something like this, as it will not be changing anything: ingresscontroller.operator.openshift.io/default patched (no change)","title":"Clusters not using the Custom Domain Operator"},{"location":"ingress/tls-cipher-customization/#clusters-using-the-custom-domain-operator_1","text":"If you are using the Custom Domain Operator the CronJob will need to loop through and patch each ingress controller. To create this CronJob, run the following command: cat << EOF | oc apply -f - apiVersion: batch/v1 kind: CronJob metadata: name: tls-patch namespace: openshift-ingress-operator spec: schedule: '@hourly' jobTemplate: spec: template: spec: containers: - name: tls-patch image: registry.redhat.io/openshift4/ose-tools-rhel8:latest args: - /bin/sh - '-c' - for ic in $(oc get ingresscontroller -o name -n openshift-ingress-operator); do oc patch ${ic} -n openshift-ingress-operator --type=merge -p '{\"spec\":{\"tlsSecurityProfile\":{\"type\":\"Custom\",\"custom\":{\"ciphers\":[\"TLS_AES_128_GCM_SHA256\",\"TLS_AES_256_GCM_SHA384\",\"ECDHE-ECDSA-AES128-GCM-SHA256\",\"ECDHE-RSA-AES128-GCM-SHA256\",\"ECDHE-ECDSA-AES256-GCM-SHA384\",\"ECDHE-RSA-AES256-GCM-SHA384\",\"ECDHE-ECDSA-CHACHA20-POLY1305\",\"ECDHE-RSA-CHACHA20-POLY1305\",\"DHE-RSA-AES128-GCM-SHA256\",\"DHE-RSA-AES256-GCM-SHA384\",\"TLS_CHACHA20_POLY1305_SHA256\",\"TLS_ECDHE_RSA_WITH_AES_128_CBC_SHA\"],\"minTLSVersion\":\"VersionTLS12\"}}}}'; done restartPolicy: Never serviceAccountName: cron-ingress-patch-sa EOF Note, this CronJob will run every hour, and will patch the ingress controller, if necessary. It is important that this CronJob does not run constantly, as it can trigger reconciles that could overload the OpenShift Ingress Operator. Most of the time, the logs of the CronJob pod will look something like this, as it will not be changing anything: ingresscontroller.operator.openshift.io/default patched (no change) ingresscontroller.operator.openshift.io/custom1 patched (no change) ingresscontroller.operator.openshift.io/custom2 patched (no change)","title":"Clusters using the Custom Domain Operator"},{"location":"jup/1-Prereq-concept/","text":"Prerequisites You will need the following prerequistes in order to run a basic Jupyter notebook with GPU on OpenShift 1. A OpenShift Cluster This will assume you have already provisioned a OpenShift cluster succesfully and are able to use it. You will need to log in as cluster admin to deploy GPU Operator . 2. OpenShift Command Line Interface Please see the OpenShift Command Line section for more information on installing. The following guides through a step by step procedure in deploying Jupyter Notebook in OpenShift. 3. Reference images You\u2019ll be doing the majority of the labs using the OpenShift CLI, but you can also accomplish them using the OpenShift web console. You can create a minimal Jupyter notebook image using the Source-to-Image (S2I) build process. The image can be built in OpenShift, separately using the s2i tool, or using a docker build. One can deploy a custom notebook image. The Jupyter Project provides a number of images for notebooks on Docker Hub. These are: base-notebook r-notebook minimal-notebook scipy-notebook tensorflow-notebook datascience-notebook pyspark-notebook all-spark-notebook The GitHub repository used to create these is: https://github.com/jupyter/docker-stacks Basic Concepts Source-To-Image (S2I) Source-to-Image (S2I) is a toolkit and workflow for building reproducible container images from source code. S2I produces ready-to-run images by injecting source code into a container image and letting the container prepare that source code for execution. By creating self-assembling builder images, you can version and control your build environments exactly like you use container images to version your runtime environments. S2I Builds Creating Images How it works Start a container from the builder image with the application source injected into a known directory The container process transforms that source code into the appropriate runnable setup - in this case, it will create an image stream with tag corresponding to the Python version being used, with the underlying image reference referring to a specific version of the image on quay.io, rather than the latest build. This ensures that the version of the image doesn't change to a newer version of the image which you haven't tested. Goals and benefits 1. Reproducibility Allow build environments to be tightly versioned by encapsulating them within a container image and defining a simple interface (injected source code) for callers. Reproducible builds are a key requirement to enabling security updates and continuous integration in containerized infrastructure, and builder images help ensure repeatability as well as the ability to swap runtimes. 2. Flexibility Any existing build system that can run on Linux can be run inside of a container, and each individual builder can also be part of a larger pipeline. In addition, the scripts that process the application source code can be injected into the builder image, allowing authors to adapt existing images to enable source handling. 3. Speed Instead of building multiple layers in a single Dockerfile, S2I encourages authors to represent an application in a single image layer. This saves time during creation and deployment, and allows for better control over the output of the final image. 4. Security Dockerfiles are run without many of the normal operational controls of containers, usually running as root and having access to the container network. S2I can be used to control what permissions and privileges are available to the builder image since the build is launched in a single container. In concert with platforms like OpenShift, source-to-image can enable admins to tightly control what privileges developers have at build time. Routes An OpenShift Route exposes a service at a host name, like www.example.com, so that external clients can reach it by name. When a Route object is created on OpenShift, it gets picked up by the built-in HAProxy load balancer in order to expose the requested service and make it externally available with the given configuration. You might be familiar with the Kubernetes Ingress object and might already be asking \"what's the difference?\". Red Hat created the concept of Route in order to fill this need and then contributed the design principles behind this to the community; which heavily influenced the Ingress design. Though a Route does have some additional features as can be seen in the chart below. NOTE: DNS resolution for a host name is handled separately from routing; your administrator may have configured a cloud domain that will always correctly resolve to the router, or if using an unrelated host name you may need to modify its DNS records independently to resolve to the router. Also of note is that an individual route can override some defaults by providing specific configurations in its annotations. See route specific annotations for more details. ImageStreams An ImageStream stores a mapping of tags to images, metadata overrides that are applied when images are tagged in a stream, and an optional reference to a Docker image repository on a registry. What are the benefits? Using an ImageStream makes it easy to change a tag for a container image. Otherwise to change a tag you need to download the whole image, change it locally, then push it all back. Also promoting applications by having to do that to change the tag and then update the deployment object entails many steps. With ImageStreams you upload a container image once and then you manage it\u2019s virtual tags internally in OpenShift. In one project you may use the dev tag and only change reference to it internally, in prod you may use a prod tag and also manage it internally. You don't really have to deal with the registry! You can also use ImageStreams in conjunction with DeploymentConfigs to set a trigger that will start a deployment as soon as a new image appears or a tag changes its reference. See below for more details: Image Streams Blog post OpenShift Docs - Understanding containers, images, and image streams Builds A build is the process of transforming input parameters into a resulting object. Most often, the process is used to transform input parameters or source code into a runnable image. A BuildConfig object is the definition of the entire build process. OpenShift Container Platform leverages Kubernetes by creating Docker-formatted containers from build images and pushing them to a container image registry. Build objects share common characteristics: inputs for a build, the need to complete a build process, logging the build process, publishing resources from successful builds, and publishing the final status of the build. Builds take advantage of resource restrictions, specifying limitations on resources such as CPU usage, memory usage, and build or pod execution time. See Understanding image builds for more details.","title":"Prerequisites"},{"location":"jup/1-Prereq-concept/#prerequisites","text":"You will need the following prerequistes in order to run a basic Jupyter notebook with GPU on OpenShift","title":"Prerequisites"},{"location":"jup/1-Prereq-concept/#1-a-openshift-cluster","text":"This will assume you have already provisioned a OpenShift cluster succesfully and are able to use it. You will need to log in as cluster admin to deploy GPU Operator .","title":"1. A OpenShift Cluster"},{"location":"jup/1-Prereq-concept/#2-openshift-command-line-interface","text":"Please see the OpenShift Command Line section for more information on installing. The following guides through a step by step procedure in deploying Jupyter Notebook in OpenShift.","title":"2. OpenShift Command Line Interface"},{"location":"jup/1-Prereq-concept/#3-reference-images","text":"You\u2019ll be doing the majority of the labs using the OpenShift CLI, but you can also accomplish them using the OpenShift web console. You can create a minimal Jupyter notebook image using the Source-to-Image (S2I) build process. The image can be built in OpenShift, separately using the s2i tool, or using a docker build. One can deploy a custom notebook image. The Jupyter Project provides a number of images for notebooks on Docker Hub. These are: base-notebook r-notebook minimal-notebook scipy-notebook tensorflow-notebook datascience-notebook pyspark-notebook all-spark-notebook The GitHub repository used to create these is: https://github.com/jupyter/docker-stacks","title":"3. Reference images"},{"location":"jup/1-Prereq-concept/#basic-concepts","text":"","title":"Basic Concepts"},{"location":"jup/1-Prereq-concept/#source-to-image-s2i","text":"Source-to-Image (S2I) is a toolkit and workflow for building reproducible container images from source code. S2I produces ready-to-run images by injecting source code into a container image and letting the container prepare that source code for execution. By creating self-assembling builder images, you can version and control your build environments exactly like you use container images to version your runtime environments. S2I Builds Creating Images","title":"Source-To-Image (S2I)"},{"location":"jup/1-Prereq-concept/#how-it-works","text":"Start a container from the builder image with the application source injected into a known directory The container process transforms that source code into the appropriate runnable setup - in this case, it will create an image stream with tag corresponding to the Python version being used, with the underlying image reference referring to a specific version of the image on quay.io, rather than the latest build. This ensures that the version of the image doesn't change to a newer version of the image which you haven't tested.","title":"How it works"},{"location":"jup/1-Prereq-concept/#goals-and-benefits","text":"","title":"Goals and benefits"},{"location":"jup/1-Prereq-concept/#1-reproducibility","text":"Allow build environments to be tightly versioned by encapsulating them within a container image and defining a simple interface (injected source code) for callers. Reproducible builds are a key requirement to enabling security updates and continuous integration in containerized infrastructure, and builder images help ensure repeatability as well as the ability to swap runtimes.","title":"1. Reproducibility"},{"location":"jup/1-Prereq-concept/#2-flexibility","text":"Any existing build system that can run on Linux can be run inside of a container, and each individual builder can also be part of a larger pipeline. In addition, the scripts that process the application source code can be injected into the builder image, allowing authors to adapt existing images to enable source handling.","title":"2. Flexibility"},{"location":"jup/1-Prereq-concept/#3-speed","text":"Instead of building multiple layers in a single Dockerfile, S2I encourages authors to represent an application in a single image layer. This saves time during creation and deployment, and allows for better control over the output of the final image.","title":"3. Speed"},{"location":"jup/1-Prereq-concept/#4-security","text":"Dockerfiles are run without many of the normal operational controls of containers, usually running as root and having access to the container network. S2I can be used to control what permissions and privileges are available to the builder image since the build is launched in a single container. In concert with platforms like OpenShift, source-to-image can enable admins to tightly control what privileges developers have at build time.","title":"4. Security"},{"location":"jup/1-Prereq-concept/#routes","text":"An OpenShift Route exposes a service at a host name, like www.example.com, so that external clients can reach it by name. When a Route object is created on OpenShift, it gets picked up by the built-in HAProxy load balancer in order to expose the requested service and make it externally available with the given configuration. You might be familiar with the Kubernetes Ingress object and might already be asking \"what's the difference?\". Red Hat created the concept of Route in order to fill this need and then contributed the design principles behind this to the community; which heavily influenced the Ingress design. Though a Route does have some additional features as can be seen in the chart below. NOTE: DNS resolution for a host name is handled separately from routing; your administrator may have configured a cloud domain that will always correctly resolve to the router, or if using an unrelated host name you may need to modify its DNS records independently to resolve to the router. Also of note is that an individual route can override some defaults by providing specific configurations in its annotations. See route specific annotations for more details.","title":"Routes"},{"location":"jup/1-Prereq-concept/#imagestreams","text":"An ImageStream stores a mapping of tags to images, metadata overrides that are applied when images are tagged in a stream, and an optional reference to a Docker image repository on a registry.","title":"ImageStreams"},{"location":"jup/1-Prereq-concept/#what-are-the-benefits","text":"Using an ImageStream makes it easy to change a tag for a container image. Otherwise to change a tag you need to download the whole image, change it locally, then push it all back. Also promoting applications by having to do that to change the tag and then update the deployment object entails many steps. With ImageStreams you upload a container image once and then you manage it\u2019s virtual tags internally in OpenShift. In one project you may use the dev tag and only change reference to it internally, in prod you may use a prod tag and also manage it internally. You don't really have to deal with the registry! You can also use ImageStreams in conjunction with DeploymentConfigs to set a trigger that will start a deployment as soon as a new image appears or a tag changes its reference. See below for more details: Image Streams Blog post OpenShift Docs - Understanding containers, images, and image streams","title":"What are the benefits?"},{"location":"jup/1-Prereq-concept/#builds","text":"A build is the process of transforming input parameters into a resulting object. Most often, the process is used to transform input parameters or source code into a runnable image. A BuildConfig object is the definition of the entire build process. OpenShift Container Platform leverages Kubernetes by creating Docker-formatted containers from build images and pushing them to a container image registry. Build objects share common characteristics: inputs for a build, the need to complete a build process, logging the build process, publishing resources from successful builds, and publishing the final status of the build. Builds take advantage of resource restrictions, specifying limitations on resources such as CPU usage, memory usage, and build or pod execution time. See Understanding image builds for more details.","title":"Builds"},{"location":"jup/2-BuildNotebook/","text":"How to deploy Jupyter Notebook Retrieve the login command If you are not logged in via the CLI, access your cluster via the web console, then click on the dropdown arrow next to your name in the top-right and select Copy Login Command . A new tab will open and select the authentication method you are using (in our case it's github ) Click Display Token Copy the command under where it says \"Log in with this token\". Then go to your terminal and paste that command and press enter. You will see a similar confirmation message if you successfully logged in. oc login --token=RYhFlXXXXXXXXXXXX --server=https://api.osd4-demo.abc1.p1.openshiftapps.com:6443 Logged into \"https://api.osd4-demo.abc1.p1.openshiftapps.com:6443\" as \"openshiftuser\" using the token provided. You don't have any projects. You can try to create a new project, by running oc new-project <projectname> Create new project Create a new project called \"notebook-demo\" in your cluster by entering the following command: oc new-project notebook-demo You should receive the following response Now using project \"notebook-demo\" on server \"https://api.aro.openshiftdemo.dev:6443\". You can add applications to this project with the 'new-app' command. For example, try: oc new-app rails-postgresql-example to build a new example application in Ruby. Or use kubectl to deploy a simple Kubernetes application: kubectl create deployment hello-node --image=k8s.gcr.io/e2e-test-images/agnhost:2.33 -- /agnhost serve-hostname Equivalently you can also create this new project using the web console UI by clicking on \"Projects\" under \"Home\" on the left menu, and then click \"Create Project\" button on the right. Importing the Minimal Notebook A pre-built version of the minimal notebook which is based on CentOS, can be found at on quay.io at: https://quay.io/organization/jupyteronopenshift The name of the latest build version of this image is: quay.io/jupyteronopenshift/s2i-minimal-notebook-py36:latest Although this image could be imported into an OpenShift cluster using oc import-image , it is recommended instead that you load it using the supplied image stream definition, using: oc create -f https://raw.githubusercontent.com/jupyter-on-openshift/jupyter-notebooks/master/image-streams/s2i-minimal-notebook.json This is preferred, as it will create an image stream with tag corresponding to the Python version being used, with the underlying image reference referring to a specific version of the image on quay.io, rather than the latest build. This ensures that the version of the image doesn't change to a newer version of the image which you haven't tested. Once the image stream definition is loaded, the project it is loaded into should have the tagged image: s2i-minimal-notebook:3.6 Deploying the Minimal Notebook To deploy the minimal notebook image run the following commands: oc new-app s2i-minimal-notebook:3.6 --name minimal-notebook \\ --env JUPYTER_NOTEBOOK_PASSWORD=mypassword The JUPYTER_NOTEBOOK_PASSWORD environment variable will allow you to access the notebook instance with a known password. Deployment should be quick if you build the minimal notebook from source code. If you used the image stream, the first deployment may be slow as the image will need to be pulled down from quay.io. You can monitor progress of the deployment if necessary by running: oc rollout status dc/minimal-notebook Because the notebook instance is not exposed to the public network by default, you will need to expose it. To do this, and ensure that access is over a secure connection run: oc create route edge minimal-notebook --service minimal-notebook \\ --insecure-policy Redirect To see the hostname which is assigned to the notebook instance, run: oc get route/minimal-notebook Access the hostname shown using your browser and enter the password you used above. To delete the notebook instance when done, run: oc delete all --selector app=minimal-notebook Creating Custom Notebook Images To create custom notebooks images, you can use the s2i-minimal-notebook:3.6 image as an S2I builder. This repository contains two examples for extending the minimal notebook. These can be found in: scipy-notebook tensorflow-notebook These are intended to mimic the images of the same name available from the Jupyter project. In the directories you will find a requirements.txt file listing the additional Python packages that need to be installed from PyPi. You will also find a .s2i/bin/assemble script which will be triggered by the S2I build process, and which installs further packages and extensions. To use the S2I build process to create a custom image, you can then run the command: oc new-build --name custom-notebook \\ --image-stream s2i-minimal-notebook:3.6 \\ --code https://github.com/jupyter-on-openshift/jupyter-notebooks \\ --context-dir scipy-notebook If any build of a custom image fails because the default memory limit on builds in your OpenShift cluster is too small, you can increase the limit by running: oc patch bc/custom-notebook \\ --patch '{\"spec\":{\"resources\":{\"limits\":{\"memory\":\"1Gi\"}}}}' and start a new build by running: oc start-build bc/custom-notebook If using the custom notebook image with JupyterHub running in OpenShift, you may also need to set the image lookup policy on the image stream created. oc set image-lookup is/custom-notebook This is necessary so that the image stream reference in the pod definition created by JupyterHub will be able to resolve the name to that of the image stream. For the scipy-notebook and tensorflow-notebook examples provided, if you wish to use the images, instead of running the above commands, after you have loaded the image stream for, or built the minimal notebook image, you can instead run the commands: oc create -f https://raw.githubusercontent.com/jupyter-on-openshift/jupyter-notebooks/master/build-configs/s2i-scipy-notebook.json oc create -f https://raw.githubusercontent.com/jupyter-on-openshift/jupyter-notebooks/master/build-configs/s2i-tensorflow-notebook.json When creating a custom notebook image, the directory in the Git repository the S2I build is run against can contain a requirements.txt file listing the Python package to be installed in the custom notebook image. Any other files in the directory will also be copied into the image. When the notebook instance is started from the image, those files will then be present in your workspace.","title":"How to deploy Jupyter Notebook #"},{"location":"jup/2-BuildNotebook/#how-to-deploy-jupyter-notebook","text":"","title":"How to deploy Jupyter Notebook"},{"location":"jup/2-BuildNotebook/#retrieve-the-login-command","text":"If you are not logged in via the CLI, access your cluster via the web console, then click on the dropdown arrow next to your name in the top-right and select Copy Login Command . A new tab will open and select the authentication method you are using (in our case it's github ) Click Display Token Copy the command under where it says \"Log in with this token\". Then go to your terminal and paste that command and press enter. You will see a similar confirmation message if you successfully logged in. oc login --token=RYhFlXXXXXXXXXXXX --server=https://api.osd4-demo.abc1.p1.openshiftapps.com:6443 Logged into \"https://api.osd4-demo.abc1.p1.openshiftapps.com:6443\" as \"openshiftuser\" using the token provided. You don't have any projects. You can try to create a new project, by running oc new-project <projectname>","title":"Retrieve the login command"},{"location":"jup/2-BuildNotebook/#create-new-project","text":"Create a new project called \"notebook-demo\" in your cluster by entering the following command: oc new-project notebook-demo You should receive the following response Now using project \"notebook-demo\" on server \"https://api.aro.openshiftdemo.dev:6443\". You can add applications to this project with the 'new-app' command. For example, try: oc new-app rails-postgresql-example to build a new example application in Ruby. Or use kubectl to deploy a simple Kubernetes application: kubectl create deployment hello-node --image=k8s.gcr.io/e2e-test-images/agnhost:2.33 -- /agnhost serve-hostname Equivalently you can also create this new project using the web console UI by clicking on \"Projects\" under \"Home\" on the left menu, and then click \"Create Project\" button on the right. Importing the Minimal Notebook A pre-built version of the minimal notebook which is based on CentOS, can be found at on quay.io at: https://quay.io/organization/jupyteronopenshift The name of the latest build version of this image is: quay.io/jupyteronopenshift/s2i-minimal-notebook-py36:latest Although this image could be imported into an OpenShift cluster using oc import-image , it is recommended instead that you load it using the supplied image stream definition, using: oc create -f https://raw.githubusercontent.com/jupyter-on-openshift/jupyter-notebooks/master/image-streams/s2i-minimal-notebook.json This is preferred, as it will create an image stream with tag corresponding to the Python version being used, with the underlying image reference referring to a specific version of the image on quay.io, rather than the latest build. This ensures that the version of the image doesn't change to a newer version of the image which you haven't tested. Once the image stream definition is loaded, the project it is loaded into should have the tagged image: s2i-minimal-notebook:3.6","title":"Create new project"},{"location":"jup/2-BuildNotebook/#deploying-the-minimal-notebook","text":"To deploy the minimal notebook image run the following commands: oc new-app s2i-minimal-notebook:3.6 --name minimal-notebook \\ --env JUPYTER_NOTEBOOK_PASSWORD=mypassword The JUPYTER_NOTEBOOK_PASSWORD environment variable will allow you to access the notebook instance with a known password. Deployment should be quick if you build the minimal notebook from source code. If you used the image stream, the first deployment may be slow as the image will need to be pulled down from quay.io. You can monitor progress of the deployment if necessary by running: oc rollout status dc/minimal-notebook Because the notebook instance is not exposed to the public network by default, you will need to expose it. To do this, and ensure that access is over a secure connection run: oc create route edge minimal-notebook --service minimal-notebook \\ --insecure-policy Redirect To see the hostname which is assigned to the notebook instance, run: oc get route/minimal-notebook Access the hostname shown using your browser and enter the password you used above. To delete the notebook instance when done, run: oc delete all --selector app=minimal-notebook","title":"Deploying the Minimal Notebook"},{"location":"jup/2-BuildNotebook/#creating-custom-notebook-images","text":"To create custom notebooks images, you can use the s2i-minimal-notebook:3.6 image as an S2I builder. This repository contains two examples for extending the minimal notebook. These can be found in: scipy-notebook tensorflow-notebook These are intended to mimic the images of the same name available from the Jupyter project. In the directories you will find a requirements.txt file listing the additional Python packages that need to be installed from PyPi. You will also find a .s2i/bin/assemble script which will be triggered by the S2I build process, and which installs further packages and extensions. To use the S2I build process to create a custom image, you can then run the command: oc new-build --name custom-notebook \\ --image-stream s2i-minimal-notebook:3.6 \\ --code https://github.com/jupyter-on-openshift/jupyter-notebooks \\ --context-dir scipy-notebook If any build of a custom image fails because the default memory limit on builds in your OpenShift cluster is too small, you can increase the limit by running: oc patch bc/custom-notebook \\ --patch '{\"spec\":{\"resources\":{\"limits\":{\"memory\":\"1Gi\"}}}}' and start a new build by running: oc start-build bc/custom-notebook If using the custom notebook image with JupyterHub running in OpenShift, you may also need to set the image lookup policy on the image stream created. oc set image-lookup is/custom-notebook This is necessary so that the image stream reference in the pod definition created by JupyterHub will be able to resolve the name to that of the image stream. For the scipy-notebook and tensorflow-notebook examples provided, if you wish to use the images, instead of running the above commands, after you have loaded the image stream for, or built the minimal notebook image, you can instead run the commands: oc create -f https://raw.githubusercontent.com/jupyter-on-openshift/jupyter-notebooks/master/build-configs/s2i-scipy-notebook.json oc create -f https://raw.githubusercontent.com/jupyter-on-openshift/jupyter-notebooks/master/build-configs/s2i-tensorflow-notebook.json When creating a custom notebook image, the directory in the Git repository the S2I build is run against can contain a requirements.txt file listing the Python package to be installed in the custom notebook image. Any other files in the directory will also be copied into the image. When the notebook instance is started from the image, those files will then be present in your workspace.","title":"Creating Custom Notebook Images"},{"location":"jup/3-OpenDataHub-GPU/","text":"Installing the Open Data Hub Operator The Open Data Hub operator is available for deployment in the OpenShift OperatorHub as a Community Operators. You can install it from the OpenShift web console: From the OpenShift web console, log in as a user with cluster-admin privileges. For a developer installation from try.openshift.com including AWS and CRC, the kubeadmin user will work. Create a new project named \u2018jph-demo\u2019 for your installation of Open Data Hub Find Open Data Hub in the OperatorHub catalog. Select the new namespace if not already selected. Under Operators, select OperatorHub for a list of operators available for deployment. Filter for Open Data Hub or look under Big Data for the icon for Open Data Hub. Click the Install button and follow the installation instructions to install the Open Data Hub operator.(optional if operator not installed) The subscription creation view will offer a few options including Update Channel, keep the rolling channel selected. To view the status of the Open Data Hub operator installation, find the Open Data Hub Operator under Operators -> Installed Operators (inside the project you created earlier). Once the STATUS field displays InstallSucceeded, you can proceed to create a new Open Data Hub deployment. Find the Open Data Hub Operator under Installed Operators (inside the project you created earlier) Click on the Open Data Hub Operator to bring up the details for the version that is currently installed. Click Create Instance to create a new deployment. Select the YAML View radio button to be presented with a YAML file to customize your deployment. Most of the components available in ODH have been removed, and only components for JupyterHub are required for this example. apiVersion: kfdef.apps.kubeflow.org/v1 kind: KfDef metadata: creationTimestamp: '2022-06-24T18:55:12Z' finalizers: - kfdef-finalizer.kfdef.apps.kubeflow.org generation: 2 managedFields: - apiVersion: kfdef.apps.kubeflow.org/v1 fieldsType: FieldsV1 fieldsV1: 'f:spec': .: {} 'f:applications': {} 'f:repos': {} manager: Mozilla operation: Update time: '2022-06-24T18:55:12Z' - apiVersion: kfdef.apps.kubeflow.org/v1 fieldsType: FieldsV1 fieldsV1: 'f:metadata': 'f:finalizers': .: {} 'v:\"kfdef-finalizer.kfdef.apps.kubeflow.org\"': {} 'f:status': {} manager: opendatahub-operator operation: Update time: '2022-06-24T18:55:12Z' name: opendatahub namespace: jph-demo resourceVersion: '27393048' uid: f54399a6-faa7-4724-bf3d-be04a63d3120 spec: applications: - kustomizeConfig: repoRef: name: manifests path: odh-common name: odh-common - kustomizeConfig: parameters: - name: s3_endpoint_url value: s3.odh.com repoRef: name: manifests path: jupyterhub/jupyterhub name: jupyterhub - kustomizeConfig: overlays: - additional repoRef: name: manifests path: jupyterhub/notebook-images name: notebook-images repos: - name: kf-manifests uri: >- https://github.com/opendatahub-io/manifests/tarball/v1.4.0-rc.2-openshift - name: manifests uri: 'https://github.com/opendatahub-io/odh-manifests/tarball/v1.2' status: {} Update the spec of the resource to match the above and click Create. If you accepted the default name, this will trigger the creation of an Open Data Hub deployment named opendatahub with JupyterHub. Verify the installation by viewing the project workload. JupyterHub and traefik-proxy should be running. Click Routes under Networking and url to launch Jupyterhub is created Open JupyterHub on web browser Configure GPU and start server Check for GPU in notebook Reference: Check the blog on Using the NVIDIA GPU Operator to Run Distributed TensorFlow 2.4 GPU Benchmarks in OpenShift 4","title":"Installing the Open Data Hub Operator #"},{"location":"jup/3-OpenDataHub-GPU/#installing-the-open-data-hub-operator","text":"The Open Data Hub operator is available for deployment in the OpenShift OperatorHub as a Community Operators. You can install it from the OpenShift web console: From the OpenShift web console, log in as a user with cluster-admin privileges. For a developer installation from try.openshift.com including AWS and CRC, the kubeadmin user will work. Create a new project named \u2018jph-demo\u2019 for your installation of Open Data Hub Find Open Data Hub in the OperatorHub catalog. Select the new namespace if not already selected. Under Operators, select OperatorHub for a list of operators available for deployment. Filter for Open Data Hub or look under Big Data for the icon for Open Data Hub. Click the Install button and follow the installation instructions to install the Open Data Hub operator.(optional if operator not installed) The subscription creation view will offer a few options including Update Channel, keep the rolling channel selected. To view the status of the Open Data Hub operator installation, find the Open Data Hub Operator under Operators -> Installed Operators (inside the project you created earlier). Once the STATUS field displays InstallSucceeded, you can proceed to create a new Open Data Hub deployment. Find the Open Data Hub Operator under Installed Operators (inside the project you created earlier) Click on the Open Data Hub Operator to bring up the details for the version that is currently installed. Click Create Instance to create a new deployment. Select the YAML View radio button to be presented with a YAML file to customize your deployment. Most of the components available in ODH have been removed, and only components for JupyterHub are required for this example. apiVersion: kfdef.apps.kubeflow.org/v1 kind: KfDef metadata: creationTimestamp: '2022-06-24T18:55:12Z' finalizers: - kfdef-finalizer.kfdef.apps.kubeflow.org generation: 2 managedFields: - apiVersion: kfdef.apps.kubeflow.org/v1 fieldsType: FieldsV1 fieldsV1: 'f:spec': .: {} 'f:applications': {} 'f:repos': {} manager: Mozilla operation: Update time: '2022-06-24T18:55:12Z' - apiVersion: kfdef.apps.kubeflow.org/v1 fieldsType: FieldsV1 fieldsV1: 'f:metadata': 'f:finalizers': .: {} 'v:\"kfdef-finalizer.kfdef.apps.kubeflow.org\"': {} 'f:status': {} manager: opendatahub-operator operation: Update time: '2022-06-24T18:55:12Z' name: opendatahub namespace: jph-demo resourceVersion: '27393048' uid: f54399a6-faa7-4724-bf3d-be04a63d3120 spec: applications: - kustomizeConfig: repoRef: name: manifests path: odh-common name: odh-common - kustomizeConfig: parameters: - name: s3_endpoint_url value: s3.odh.com repoRef: name: manifests path: jupyterhub/jupyterhub name: jupyterhub - kustomizeConfig: overlays: - additional repoRef: name: manifests path: jupyterhub/notebook-images name: notebook-images repos: - name: kf-manifests uri: >- https://github.com/opendatahub-io/manifests/tarball/v1.4.0-rc.2-openshift - name: manifests uri: 'https://github.com/opendatahub-io/odh-manifests/tarball/v1.2' status: {} Update the spec of the resource to match the above and click Create. If you accepted the default name, this will trigger the creation of an Open Data Hub deployment named opendatahub with JupyterHub. Verify the installation by viewing the project workload. JupyterHub and traefik-proxy should be running. Click Routes under Networking and url to launch Jupyterhub is created Open JupyterHub on web browser Configure GPU and start server Check for GPU in notebook Reference: Check the blog on Using the NVIDIA GPU Operator to Run Distributed TensorFlow 2.4 GPU Benchmarks in OpenShift 4","title":"Installing the Open Data Hub Operator"},{"location":"jup/concepts/","text":"Basic Concepts Source-To-Image (S2I) Source-to-Image (S2I) is a toolkit and workflow for building reproducible container images from source code. S2I produces ready-to-run images by injecting source code into a container image and letting the container prepare that source code for execution. By creating self-assembling builder images, you can version and control your build environments exactly like you use container images to version your runtime environments. S2I Builds Creating Images How it works Start a container from the builder image with the application source injected into a known directory The container process transforms that source code into the appropriate runnable setup - in this case, it will create an image stream with tag corresponding to the Python version being used, with the underlying image reference referring to a specific version of the image on quay.io, rather than the latest build. This ensures that the version of the image doesn't change to a newer version of the image which you haven't tested. Goals and benefits 1. Reproducibility Allow build environments to be tightly versioned by encapsulating them within a container image and defining a simple interface (injected source code) for callers. Reproducible builds are a key requirement to enabling security updates and continuous integration in containerized infrastructure, and builder images help ensure repeatability as well as the ability to swap runtimes. 2. Flexibility Any existing build system that can run on Linux can be run inside of a container, and each individual builder can also be part of a larger pipeline. In addition, the scripts that process the application source code can be injected into the builder image, allowing authors to adapt existing images to enable source handling. 3. Speed Instead of building multiple layers in a single Dockerfile, S2I encourages authors to represent an application in a single image layer. This saves time during creation and deployment, and allows for better control over the output of the final image. 4. Security Dockerfiles are run without many of the normal operational controls of containers, usually running as root and having access to the container network. S2I can be used to control what permissions and privileges are available to the builder image since the build is launched in a single container. In concert with platforms like OpenShift, source-to-image can enable admins to tightly control what privileges developers have at build time. Routes An OpenShift Route exposes a service at a host name, like www.example.com, so that external clients can reach it by name. When a Route object is created on OpenShift, it gets picked up by the built-in HAProxy load balancer in order to expose the requested service and make it externally available with the given configuration. You might be familiar with the Kubernetes Ingress object and might already be asking \"what's the difference?\". Red Hat created the concept of Route in order to fill this need and then contributed the design principles behind this to the community; which heavily influenced the Ingress design. Though a Route does have some additional features as can be seen in the chart below. NOTE: DNS resolution for a host name is handled separately from routing; your administrator may have configured a cloud domain that will always correctly resolve to the router, or if using an unrelated host name you may need to modify its DNS records independently to resolve to the router. Also of note is that an individual route can override some defaults by providing specific configurations in its annotations. See route specific annotations for more details. ImageStreams An ImageStream stores a mapping of tags to images, metadata overrides that are applied when images are tagged in a stream, and an optional reference to a Docker image repository on a registry. What are the benefits? Using an ImageStream makes it easy to change a tag for a container image. Otherwise to change a tag you need to download the whole image, change it locally, then push it all back. Also promoting applications by having to do that to change the tag and then update the deployment object entails many steps. With ImageStreams you upload a container image once and then you manage it\u2019s virtual tags internally in OpenShift. In one project you may use the dev tag and only change reference to it internally, in prod you may use a prod tag and also manage it internally. You don't really have to deal with the registry! You can also use ImageStreams in conjunction with DeploymentConfigs to set a trigger that will start a deployment as soon as a new image appears or a tag changes its reference. See below for more details: Image Streams Blog post OpenShift Docs - Understanding containers, images, and image streams Builds A build is the process of transforming input parameters into a resulting object. Most often, the process is used to transform input parameters or source code into a runnable image. A BuildConfig object is the definition of the entire build process. OpenShift Container Platform leverages Kubernetes by creating Docker-formatted containers from build images and pushing them to a container image registry. Build objects share common characteristics: inputs for a build, the need to complete a build process, logging the build process, publishing resources from successful builds, and publishing the final status of the build. Builds take advantage of resource restrictions, specifying limitations on resources such as CPU usage, memory usage, and build or pod execution time. See Understanding image builds for more details.","title":"Concepts"},{"location":"jup/concepts/#basic-concepts","text":"","title":"Basic Concepts"},{"location":"jup/concepts/#source-to-image-s2i","text":"Source-to-Image (S2I) is a toolkit and workflow for building reproducible container images from source code. S2I produces ready-to-run images by injecting source code into a container image and letting the container prepare that source code for execution. By creating self-assembling builder images, you can version and control your build environments exactly like you use container images to version your runtime environments. S2I Builds Creating Images","title":"Source-To-Image (S2I)"},{"location":"jup/concepts/#how-it-works","text":"Start a container from the builder image with the application source injected into a known directory The container process transforms that source code into the appropriate runnable setup - in this case, it will create an image stream with tag corresponding to the Python version being used, with the underlying image reference referring to a specific version of the image on quay.io, rather than the latest build. This ensures that the version of the image doesn't change to a newer version of the image which you haven't tested.","title":"How it works"},{"location":"jup/concepts/#goals-and-benefits","text":"","title":"Goals and benefits"},{"location":"jup/concepts/#1-reproducibility","text":"Allow build environments to be tightly versioned by encapsulating them within a container image and defining a simple interface (injected source code) for callers. Reproducible builds are a key requirement to enabling security updates and continuous integration in containerized infrastructure, and builder images help ensure repeatability as well as the ability to swap runtimes.","title":"1. Reproducibility"},{"location":"jup/concepts/#2-flexibility","text":"Any existing build system that can run on Linux can be run inside of a container, and each individual builder can also be part of a larger pipeline. In addition, the scripts that process the application source code can be injected into the builder image, allowing authors to adapt existing images to enable source handling.","title":"2. Flexibility"},{"location":"jup/concepts/#3-speed","text":"Instead of building multiple layers in a single Dockerfile, S2I encourages authors to represent an application in a single image layer. This saves time during creation and deployment, and allows for better control over the output of the final image.","title":"3. Speed"},{"location":"jup/concepts/#4-security","text":"Dockerfiles are run without many of the normal operational controls of containers, usually running as root and having access to the container network. S2I can be used to control what permissions and privileges are available to the builder image since the build is launched in a single container. In concert with platforms like OpenShift, source-to-image can enable admins to tightly control what privileges developers have at build time.","title":"4. Security"},{"location":"jup/concepts/#routes","text":"An OpenShift Route exposes a service at a host name, like www.example.com, so that external clients can reach it by name. When a Route object is created on OpenShift, it gets picked up by the built-in HAProxy load balancer in order to expose the requested service and make it externally available with the given configuration. You might be familiar with the Kubernetes Ingress object and might already be asking \"what's the difference?\". Red Hat created the concept of Route in order to fill this need and then contributed the design principles behind this to the community; which heavily influenced the Ingress design. Though a Route does have some additional features as can be seen in the chart below. NOTE: DNS resolution for a host name is handled separately from routing; your administrator may have configured a cloud domain that will always correctly resolve to the router, or if using an unrelated host name you may need to modify its DNS records independently to resolve to the router. Also of note is that an individual route can override some defaults by providing specific configurations in its annotations. See route specific annotations for more details.","title":"Routes"},{"location":"jup/concepts/#imagestreams","text":"An ImageStream stores a mapping of tags to images, metadata overrides that are applied when images are tagged in a stream, and an optional reference to a Docker image repository on a registry.","title":"ImageStreams"},{"location":"jup/concepts/#what-are-the-benefits","text":"Using an ImageStream makes it easy to change a tag for a container image. Otherwise to change a tag you need to download the whole image, change it locally, then push it all back. Also promoting applications by having to do that to change the tag and then update the deployment object entails many steps. With ImageStreams you upload a container image once and then you manage it\u2019s virtual tags internally in OpenShift. In one project you may use the dev tag and only change reference to it internally, in prod you may use a prod tag and also manage it internally. You don't really have to deal with the registry! You can also use ImageStreams in conjunction with DeploymentConfigs to set a trigger that will start a deployment as soon as a new image appears or a tag changes its reference. See below for more details: Image Streams Blog post OpenShift Docs - Understanding containers, images, and image streams","title":"What are the benefits?"},{"location":"jup/concepts/#builds","text":"A build is the process of transforming input parameters into a resulting object. Most often, the process is used to transform input parameters or source code into a runnable image. A BuildConfig object is the definition of the entire build process. OpenShift Container Platform leverages Kubernetes by creating Docker-formatted containers from build images and pushing them to a container image registry. Build objects share common characteristics: inputs for a build, the need to complete a build process, logging the build process, publishing resources from successful builds, and publishing the final status of the build. Builds take advantage of resource restrictions, specifying limitations on resources such as CPU usage, memory usage, and build or pod execution time. See Understanding image builds for more details.","title":"Builds"},{"location":"misc/references/","text":"Common Managed OpenShift References / Tasks Managed OpenShift Overviews Red Hat OpenShift Managed services Microsoft Azure Red Hat OpenShift - ARO Red Hat OpenShift on AWS - ROSA Red Hat OpenShift on IBM Cloud Red Hat OpenShift Dedicated - OSD Managed OpenShift Documentation OpenShift Container Platform v4.7 Azure Red Hat OpenShift v4.x - ARO Red Hat OpenShift on AWS v4.x - ROSA Red Hat OpenShift on IBM Cloud v4.x OpenShift Dedicated v4.x - OSD Common Customer Topics Red Hat OpenShift on AWS - ROSA Creating a ROSA cluster with Private Link enabled ROSA Installation Prerequisites ROSA STS Workflow Shared Responsiblity Matrix (who does what) Red Hat Process and Security for ROSA ROSA Support Azure on Red Hat OpenShift ARO ARO Installation Process ARO Support Azure Compliance Monitoring Authentication Education ARO Getting Started ROSA Getting Started","title":"Common Managed OpenShift References / Tasks"},{"location":"misc/references/#common-managed-openshift-references-tasks","text":"","title":"Common Managed OpenShift References / Tasks"},{"location":"misc/references/#managed-openshift-overviews","text":"Red Hat OpenShift Managed services Microsoft Azure Red Hat OpenShift - ARO Red Hat OpenShift on AWS - ROSA Red Hat OpenShift on IBM Cloud Red Hat OpenShift Dedicated - OSD","title":"Managed OpenShift Overviews"},{"location":"misc/references/#managed-openshift-documentation","text":"OpenShift Container Platform v4.7 Azure Red Hat OpenShift v4.x - ARO Red Hat OpenShift on AWS v4.x - ROSA Red Hat OpenShift on IBM Cloud v4.x OpenShift Dedicated v4.x - OSD","title":"Managed OpenShift Documentation"},{"location":"misc/references/#common-customer-topics","text":"","title":"Common Customer Topics"},{"location":"misc/references/#red-hat-openshift-on-aws-rosa","text":"Creating a ROSA cluster with Private Link enabled ROSA Installation Prerequisites ROSA STS Workflow Shared Responsiblity Matrix (who does what) Red Hat Process and Security for ROSA ROSA Support","title":"Red Hat OpenShift on AWS - ROSA"},{"location":"misc/references/#azure-on-red-hat-openshift-aro","text":"ARO Installation Process ARO Support Azure Compliance Monitoring Authentication","title":"Azure on Red Hat OpenShift ARO"},{"location":"misc/references/#education","text":"ARO Getting Started ROSA Getting Started","title":"Education"},{"location":"misc/cost-management/","text":"Red Hat Cost Management for Cloud Services Author: Charlotte Fung Last edited: 09/05/2022 Adopted from Official Documentation for Cost Management Service Red Hat Cost Management is a software as a service (SaaS) offering available free of charge as part of your Red Hat subscriptions. Cost management helps you monitor and analyze your OpenShift Container Platform and Public cloud costs in order to improve the management of your business. Some capabilities of cost management are : Visualize costs across hybrid cloud infrastructure Track cost trends Map charges to projects and organizations Normalize data and add markups with cost models Generate showback and chargeback information In this document, I will show you how to connect your OpenShift and Cloud provider sources to Cost Management in order to collect cost and usage. Prerequisites A Public Cloud subscritption (Azure Subscription) An OpenShift Cluster (to create an Azure Red Hat OpenShift (ARO) cluster, click here ) Adding your OpenShift source to Cost Management Installing the Cost Management Metric Operator Log into the Openshift cluster web console with cluster-admin credentials On the left navigation pane under Administator perspective, select Operators --> OperatorHub Search for and locate cost management metrics operator . Click on the displayed Cost Management Metrics Operator When the Install Operator window appears, you must select the costmanagement-metrics-operator namespace for installation. If it does not exist, it will be created for you. Click on install button. After a short wait, Cost Management Metrics Operator appears in the Installed Operators tab under Project: all Projects or Project: costmanagement-metrics-operator Configuring the Operator instance for a new installation Once installed, click on the Cost Management In the detail window, click + Create Instance A Cost Management Metrics Operator > Create CostManagementMetricsConfig window appears Click the YAML view radio button to view and modify the contents of the YAML configuration file Modify the following two lines in the YAML file to look like the following Change SOURCE-NAME to the new name of your source (ex. my-openshift-cost-source ) yaml create_source: true name: <SOURCE-NAME> Click the Create button. This creates a new source for cost management that will appear in the console.redhat.com Cost Management applications Adding your Microsoft Azure source to Cost Management 1. Creating a Microsoft Azure Source in your Red Hat account In the console.redhat.com click on All apps and services tab in the left top corner of the screen to navigate to this window. Click on Sources under Settings On Sources page, click on Cloud sources tab and then click Add a source . This opens up the Sources Wizard Enter a name for your source and click next Select cost management as the application and Microsoft Azure as the source type. Click Next . We will create the storage account and resource group in Azure account before proceeding. Keep this window open. 2. Configuring your Microsoft Azure The following steps are required to configure your Azure account to be a cost management source Creating a storage account and resource group Configuring a storage account contributor and reader roles for access Scheduling daily exports 2.1 Creating an Azure resource group and storage account using Azure CLI First create a new resource group bash az group create \\ --name storage-resource-group \\ --location eastus If you're not sure which region to specify for the --location parameter, you can retrieve a list of supported regions for your subscription with the az account list-locations command. bash az account list-locations \\ --query \"[].{Region:name}\" \\ --out table Next, create a standard general-purpose v2 storage account with read-access geo-redundant storage. Ensure the name of your storage account is unique across Azure bash az storage account create \\ --name <account-name> \\ --resource-group storage-resource-group \\ --location eastus \\ --sku Standard_RAGRS \\ --kind StorageV2 Make note of the resource group and storage account. We will need them in the subsequent steps Return to Sources wizard in console.redhat.com , enter the Resource group name and Storage account name and click Next . Leave this window for now and proceed to next step below. 2.2 Configuring Azure roles using Azure CLI We need to grant cost management read-only access to Azure cost data by configuring a Storage Account Contributor and Reader role in Azure Run the following command to obtain your Subscription ID : bash SUBSCRIPTION=$(az account show --query \"{subscription_id: id}\" -o tsv) Return to the console.redhat.com Sources wizard, enter your Subscription ID . Click Next to move to the next screen In Azure CLI, create a cost management Storage Account Contributor role, an obtain your tenant ID , client (application) ID , and client secret bash az ad sp create-for-rbac -n \"CostManagement\" \\ --role \"Storage Account Contributor\" \\ --query '{\"tenant\": tenant, \"client_id\": appId, \"secret\": password}' Return to Sources wizard in console.redhat.com , enter your Azure Tenant ID , Client ID , and Client Secret . Run the following command to create cost management Reader role with your subscription ID. Copy the full command from the Sources wizard, which will automatically substitute your Azure subscription ID obtained earlier. bash az role assignment create --role \"Cost Management Reader\" \\ --assignee http://CostManagement --subscription ${SUBSCRIPTION} 1. Click Next in Sources wizard. 2.3 Configuring a Daily Azure data export schedule using Azure Portal Cost management requires a data export from a Subscription level scope In the Azure Portal home page, click on Subscriptions Select the Subscription you want to track from the list, and then select Cost Analysis in the menu. At the top of the Cost analysis page, select configure subscription Click on the Export tab, and then Schedule export In the Exports wizard, fill out the Export details For Export Type , select Daily export of billing-period-to-date costs For Storage account , select the account you created earlier Enter any value for the Container name and Directory path for the export. These values provide the tree structure in the storage account where report files are stored. Click Create to start exporting data to the Azure storage container. Return to Sources wizard after creating the export schedule and click Next . Review the source details Click Finish to complete adding the Azure source to cost management Cost management will begin polling Azure for cost data, which will appear on the cost management dashboard (console.redhat.com/openshift/cost-management/). Managing your Costs After adding your Openshift Container Platform and Cloud Provider sources, Cost management will show cost data by Source Cloud provider cost and usage related to running your OpenShift Container Platform clusters on their platform See the following video for a quick overview of Cost Management for OpenShift followed by a demo of the product Next steps for managing your costs Limiting access to cost management resources - Use role-based access control to limit visibility of resources in cost management reports. Managing cost data using tagging - Tags allow you to organize your resources by cost and allocate the costs to different parts of your cloud infrastructure Using cost models - Configure cost models to associate prices to metrics and usage. Visualizing your costs using Cost Explorer - Allows you to see your costs through time.","title":"Red Hat Cost Management for Cloud Services"},{"location":"misc/cost-management/#red-hat-cost-management-for-cloud-services","text":"Author: Charlotte Fung Last edited: 09/05/2022 Adopted from Official Documentation for Cost Management Service Red Hat Cost Management is a software as a service (SaaS) offering available free of charge as part of your Red Hat subscriptions. Cost management helps you monitor and analyze your OpenShift Container Platform and Public cloud costs in order to improve the management of your business. Some capabilities of cost management are : Visualize costs across hybrid cloud infrastructure Track cost trends Map charges to projects and organizations Normalize data and add markups with cost models Generate showback and chargeback information In this document, I will show you how to connect your OpenShift and Cloud provider sources to Cost Management in order to collect cost and usage.","title":"Red Hat Cost Management for Cloud Services"},{"location":"misc/cost-management/#prerequisites","text":"A Public Cloud subscritption (Azure Subscription) An OpenShift Cluster (to create an Azure Red Hat OpenShift (ARO) cluster, click here )","title":"Prerequisites"},{"location":"misc/cost-management/#adding-your-openshift-source-to-cost-management","text":"","title":"Adding your OpenShift source to Cost Management"},{"location":"misc/cost-management/#installing-the-cost-management-metric-operator","text":"Log into the Openshift cluster web console with cluster-admin credentials On the left navigation pane under Administator perspective, select Operators --> OperatorHub Search for and locate cost management metrics operator . Click on the displayed Cost Management Metrics Operator When the Install Operator window appears, you must select the costmanagement-metrics-operator namespace for installation. If it does not exist, it will be created for you. Click on install button. After a short wait, Cost Management Metrics Operator appears in the Installed Operators tab under Project: all Projects or Project: costmanagement-metrics-operator","title":"Installing the Cost Management Metric Operator"},{"location":"misc/cost-management/#configuring-the-operator-instance-for-a-new-installation","text":"Once installed, click on the Cost Management In the detail window, click + Create Instance A Cost Management Metrics Operator > Create CostManagementMetricsConfig window appears Click the YAML view radio button to view and modify the contents of the YAML configuration file Modify the following two lines in the YAML file to look like the following Change SOURCE-NAME to the new name of your source (ex. my-openshift-cost-source ) yaml create_source: true name: <SOURCE-NAME> Click the Create button. This creates a new source for cost management that will appear in the console.redhat.com Cost Management applications","title":"Configuring the Operator instance for a new installation"},{"location":"misc/cost-management/#adding-your-microsoft-azure-source-to-cost-management","text":"","title":"Adding your Microsoft Azure source to Cost Management"},{"location":"misc/cost-management/#1-creating-a-microsoft-azure-source-in-your-red-hat-account","text":"In the console.redhat.com click on All apps and services tab in the left top corner of the screen to navigate to this window. Click on Sources under Settings On Sources page, click on Cloud sources tab and then click Add a source . This opens up the Sources Wizard Enter a name for your source and click next Select cost management as the application and Microsoft Azure as the source type. Click Next . We will create the storage account and resource group in Azure account before proceeding. Keep this window open.","title":"1. Creating a Microsoft Azure Source in your Red Hat account"},{"location":"misc/cost-management/#2-configuring-your-microsoft-azure","text":"The following steps are required to configure your Azure account to be a cost management source Creating a storage account and resource group Configuring a storage account contributor and reader roles for access Scheduling daily exports","title":"2. Configuring your Microsoft Azure"},{"location":"misc/cost-management/#21-creating-an-azure-resource-group-and-storage-account-using-azure-cli","text":"First create a new resource group bash az group create \\ --name storage-resource-group \\ --location eastus If you're not sure which region to specify for the --location parameter, you can retrieve a list of supported regions for your subscription with the az account list-locations command. bash az account list-locations \\ --query \"[].{Region:name}\" \\ --out table Next, create a standard general-purpose v2 storage account with read-access geo-redundant storage. Ensure the name of your storage account is unique across Azure bash az storage account create \\ --name <account-name> \\ --resource-group storage-resource-group \\ --location eastus \\ --sku Standard_RAGRS \\ --kind StorageV2 Make note of the resource group and storage account. We will need them in the subsequent steps Return to Sources wizard in console.redhat.com , enter the Resource group name and Storage account name and click Next . Leave this window for now and proceed to next step below.","title":"2.1 Creating an Azure resource group and storage account using Azure CLI"},{"location":"misc/cost-management/#22-configuring-azure-roles-using-azure-cli","text":"We need to grant cost management read-only access to Azure cost data by configuring a Storage Account Contributor and Reader role in Azure Run the following command to obtain your Subscription ID : bash SUBSCRIPTION=$(az account show --query \"{subscription_id: id}\" -o tsv) Return to the console.redhat.com Sources wizard, enter your Subscription ID . Click Next to move to the next screen In Azure CLI, create a cost management Storage Account Contributor role, an obtain your tenant ID , client (application) ID , and client secret bash az ad sp create-for-rbac -n \"CostManagement\" \\ --role \"Storage Account Contributor\" \\ --query '{\"tenant\": tenant, \"client_id\": appId, \"secret\": password}' Return to Sources wizard in console.redhat.com , enter your Azure Tenant ID , Client ID , and Client Secret . Run the following command to create cost management Reader role with your subscription ID. Copy the full command from the Sources wizard, which will automatically substitute your Azure subscription ID obtained earlier. bash az role assignment create --role \"Cost Management Reader\" \\ --assignee http://CostManagement --subscription ${SUBSCRIPTION} 1. Click Next in Sources wizard.","title":"2.2 Configuring Azure roles using Azure CLI"},{"location":"misc/cost-management/#23-configuring-a-daily-azure-data-export-schedule-using-azure-portal","text":"Cost management requires a data export from a Subscription level scope In the Azure Portal home page, click on Subscriptions Select the Subscription you want to track from the list, and then select Cost Analysis in the menu. At the top of the Cost analysis page, select configure subscription Click on the Export tab, and then Schedule export In the Exports wizard, fill out the Export details For Export Type , select Daily export of billing-period-to-date costs For Storage account , select the account you created earlier Enter any value for the Container name and Directory path for the export. These values provide the tree structure in the storage account where report files are stored. Click Create to start exporting data to the Azure storage container. Return to Sources wizard after creating the export schedule and click Next . Review the source details Click Finish to complete adding the Azure source to cost management Cost management will begin polling Azure for cost data, which will appear on the cost management dashboard (console.redhat.com/openshift/cost-management/).","title":"2.3 Configuring a Daily Azure data export schedule using Azure Portal"},{"location":"misc/cost-management/#managing-your-costs","text":"After adding your Openshift Container Platform and Cloud Provider sources, Cost management will show cost data by Source Cloud provider cost and usage related to running your OpenShift Container Platform clusters on their platform See the following video for a quick overview of Cost Management for OpenShift followed by a demo of the product","title":"Managing your Costs"},{"location":"misc/cost-management/#next-steps-for-managing-your-costs","text":"Limiting access to cost management resources - Use role-based access control to limit visibility of resources in cost management reports. Managing cost data using tagging - Tags allow you to organize your resources by cost and allocate the costs to different parts of your cloud infrastructure Using cost models - Configure cost models to associate prices to metrics and usage. Visualizing your costs using Cost Explorer - Allows you to see your costs through time.","title":"Next steps for managing your costs"},{"location":"o11y/az-log-analytics/","text":"Shipping logs to Azure Log Analytics This document follows the steps outlined by Microsoft in their documentation Follow docs. Step 4, needs additional command of: az resource list --resource-type Microsoft.RedHatOpenShift/OpenShiftClusters -o json to capture resource ID of ARO cluster as well, needed for export in step 6 bash enable-monitoring.sh --resource-id $azureAroV4ClusterResourceId --workspace-id $logAnalyticsWorkspaceResourceId works successfully can verify pods starting Verify logs flowing with container solutions showing in log analytics workbook? Configure Prometheus metric scraping following steps outlined here: https://docs.microsoft.com/en-us/azure/azure-monitor/containers/container-insights-prometheus-integration It looks like config maps are not set in the previous step despite what the article says. This may actually be an OpenShift v3 thing and not a v4 thing. I had to do the apply process after downloading the config. Afterward pods did not restart on their own and had to be manually deleted. Automatic recreation pulls in new config and should begins shipping metrics Verify metrics with a query: (from https://docs.microsoft.com/en-us/azure/azure-monitor/containers/container-insights-log-search#query-prometheus-metrics-data) InsightsMetrics | where TimeGenerated > ago(1h) | where Name == 'reads' | extend Tags = todynamic(Tags) | extend HostName = tostring(Tags.hostName), Device = Tags.name | extend NodeDisk = strcat(Device, \"/\", HostName) | order by NodeDisk asc, TimeGenerated asc | serialize | extend PrevVal = iif(prev(NodeDisk) != NodeDisk, 0.0, prev(Val)), PrevTimeGenerated = iif(prev(NodeDisk) != NodeDisk, datetime(null), prev(TimeGenerated)) | where isnotnull(PrevTimeGenerated) and PrevTimeGenerated != TimeGenerated | extend Rate = iif(PrevVal > Val, Val / (datetime_diff('Second', TimeGenerated, PrevTimeGenerated) * 1), iif(PrevVal == Val, 0.0, (Val - PrevVal) / (datetime_diff('Second', TimeGenerated, PrevTimeGenerated) * 1))) | where isnotnull(Rate) | project TimeGenerated, NodeDisk, Rate | render timechart","title":"Shipping logs to Azure Log Analytics"},{"location":"o11y/az-log-analytics/#shipping-logs-to-azure-log-analytics","text":"This document follows the steps outlined by Microsoft in their documentation Follow docs. Step 4, needs additional command of: az resource list --resource-type Microsoft.RedHatOpenShift/OpenShiftClusters -o json to capture resource ID of ARO cluster as well, needed for export in step 6 bash enable-monitoring.sh --resource-id $azureAroV4ClusterResourceId --workspace-id $logAnalyticsWorkspaceResourceId works successfully can verify pods starting Verify logs flowing with container solutions showing in log analytics workbook?","title":"Shipping logs to Azure Log Analytics"},{"location":"o11y/az-log-analytics/#configure-prometheus-metric-scraping","text":"following steps outlined here: https://docs.microsoft.com/en-us/azure/azure-monitor/containers/container-insights-prometheus-integration It looks like config maps are not set in the previous step despite what the article says. This may actually be an OpenShift v3 thing and not a v4 thing. I had to do the apply process after downloading the config. Afterward pods did not restart on their own and had to be manually deleted. Automatic recreation pulls in new config and should begins shipping metrics Verify metrics with a query: (from https://docs.microsoft.com/en-us/azure/azure-monitor/containers/container-insights-log-search#query-prometheus-metrics-data) InsightsMetrics | where TimeGenerated > ago(1h) | where Name == 'reads' | extend Tags = todynamic(Tags) | extend HostName = tostring(Tags.hostName), Device = Tags.name | extend NodeDisk = strcat(Device, \"/\", HostName) | order by NodeDisk asc, TimeGenerated asc | serialize | extend PrevVal = iif(prev(NodeDisk) != NodeDisk, 0.0, prev(Val)), PrevTimeGenerated = iif(prev(NodeDisk) != NodeDisk, datetime(null), prev(TimeGenerated)) | where isnotnull(PrevTimeGenerated) and PrevTimeGenerated != TimeGenerated | extend Rate = iif(PrevVal > Val, Val / (datetime_diff('Second', TimeGenerated, PrevTimeGenerated) * 1), iif(PrevVal == Val, 0.0, (Val - PrevVal) / (datetime_diff('Second', TimeGenerated, PrevTimeGenerated) * 1))) | where isnotnull(Rate) | project TimeGenerated, NodeDisk, Rate | render timechart","title":"Configure Prometheus metric scraping"},{"location":"o11y/openshift-logging/","text":"OpenShift Logging A guid to shipping logs and metrics on OpenShift Author: Aaron Aldrich Prerequisites OpenShift CLI (oc) Rights to install operators on the cluster Setup OpenShift Logging This is for setup of centralized logging on OpenShift making use of Elasticsearch OSS edition. This largely follows the processes outlined in the OpenShift documentation here . Retention and storage considerations are reviewed in Red Hat's primary source documentation. This setup is primarily concerned with simplicity and basic log searching. Consequently it is insufficient for long-lived retention or for advanced visualization of logs. For more advanced observability setups, you'll want to look at Forwarding Logs to Third Party Systems Create a namespace for the OpenShift Elasticsearch Operator. This is necessary to avoid potential conflicts with community operators that could send similarly named metrics/logs into the stack. bash oc create -f - <<EOF apiVersion: v1 kind: Namespace metadata: name: openshift-operators-redhat annotations: openshift.io/node-selector: \"\" labels: openshift.io/cluster-monitoring: \"true\" EOF Create a namespace for the OpenShift Logging Operator bash oc create -f - <<EOF apiVersion: v1 kind: Namespace metadata: name: openshift-logging annotations: openshift.io/node-selector: \"\" labels: openshift.io/cluster-monitoring: \"true\" EOF Install the OpenShift Elasticsearch Operator by creating the following objects: Operator Group for OpenShift Elasticsearch Operator bash oc create -f - <<EOF apiVersion: operators.coreos.com/v1 kind: OperatorGroup metadata: name: openshift-operators-redhat namespace: openshift-operators-redhat spec: {} EOF Subscription object to subscribe a Namespace to the OpenShift Elasticsearch Operator bash oc create -f - <<EOF apiVersion: operators.coreos.com/v1alpha1 kind: Subscription metadata: name: \"elasticsearch-operator\" namespace: \"openshift-operators-redhat\" spec: channel: \"stable\" installPlanApproval: \"Automatic\" source: \"redhat-operators\" sourceNamespace: \"openshift-marketplace\" name: \"elasticsearch-operator\" EOF Verify Operator Installation bash oc get csv --all-namespaces Example Output NAMESPACE NAME DISPLAY VERSION REPLACES PHASE default elasticsearch-operator.5.0.0-202007012112.p0 OpenShift Elasticsearch Operator 5.0.0-202007012112.p0 Succeeded kube-node-lease elasticsearch-operator.5.0.0-202007012112.p0 OpenShift Elasticsearch Operator 5.0.0-202007012112.p0 Succeeded kube-public elasticsearch-operator.5.0.0-202007012112.p0 OpenShift Elasticsearch Operator 5.0.0-202007012112.p0 Succeeded kube-system elasticsearch-operator.5.0.0-202007012112.p0 OpenShift Elasticsearch Operator 5.0.0-202007012112.p0 Succeeded openshift-apiserver-operator elasticsearch-operator.5.0.0-202007012112.p0 OpenShift Elasticsearch Operator 5.0.0-202007012112.p0 Succeeded openshift-apiserver elasticsearch-operator.5.0.0-202007012112.p0 OpenShift Elasticsearch Operator 5.0.0-202007012112.p0 Succeeded openshift-authentication-operator elasticsearch-operator.5.0.0-202007012112.p0 OpenShift Elasticsearch Operator 5.0.0-202007012112.p0 Succeeded openshift-authentication elasticsearch-operator.5.0. 0-202007012112.p0 OpenShift Elasticsearch Operator 5.0.0-202007012112.p0 Succeeded ... Install the Red Hat OpenShift Logging Operator by creating the following objects: The Cluster Logging OperatorGroup bash oc create -f - <<EOF apiVersion: operators.coreos.com/v1 kind: OperatorGroup metadata: name: cluster-logging namespace: openshift-logging spec: targetNamespaces: - openshift-logging EOF Subscription Object to subscribe a Namespace to the Red Hat OpenShift Logging Operator bash oc create -f - <<EOF apiVersion: operators.coreos.com/v1alpha1 kind: Subscription metadata: name: cluster-logging namespace: openshift-logging spec: channel: \"stable\" name: cluster-logging source: redhat-operators sourceNamespace: openshift-marketplace EOF Verify the Operator installation, the PHASE should be Succeeded bash oc get csv -n openshift-logging Example Output NAME DISPLAY VERSION REPLACES PHASE cluster-logging.5.0.5-11 Red Hat OpenShift Logging 5.0.5-11 Succeeded elasticsearch-operator.5.0.5-11 OpenShift Elasticsearch Operator 5.0.5-11 Succeeded Create an OpenShift Logging instance: NOTE : For the storageClassName below, you will need to adjust for the platform on which you're running OpenShift. managed-premium as listed below is for Azure Red Hat OpenShift (ARO). You can verify your available storage classes with oc get storageClasses bash oc create -f - <<EOF apiVersion: \"logging.openshift.io/v1\" kind: \"ClusterLogging\" metadata: name: \"instance\" namespace: \"openshift-logging\" spec: managementState: \"Managed\" logStore: type: \"elasticsearch\" retentionPolicy: application: maxAge: 1d infra: maxAge: 7d audit: maxAge: 7d elasticsearch: nodeCount: 3 storage: storageClassName: \"managed-premium\" size: 200G resources: requests: memory: \"8Gi\" proxy: resources: limits: memory: 256Mi requests: memory: 256Mi redundancyPolicy: \"SingleRedundancy\" visualization: type: \"kibana\" kibana: replicas: 1 curation: type: \"curator\" curator: schedule: \"30 3 * * *\" collection: logs: type: \"fluentd\" fluentd: {} EOF It will take a few minutes for everything to start up. You can monitor this progress by watching the pods. bash watch oc get pods -n openshift-logging Your logging instances are now configured and recieving logs. To view them, you will need to log into your Kibana instance and create the appropriate index patterns. For more information on index patterns, see the Kibana documentation. NOTE : The following restrictions and notes apply to index patterns: - All users can view the app- logs for namespaces they have access to - Only cluster-admins can view the infra- and audit- logs - For best accuracy, use the @timestamp field for determining chronology","title":"OpenShift Logging"},{"location":"o11y/openshift-logging/#openshift-logging","text":"A guid to shipping logs and metrics on OpenShift Author: Aaron Aldrich","title":"OpenShift Logging"},{"location":"o11y/openshift-logging/#prerequisites","text":"OpenShift CLI (oc) Rights to install operators on the cluster","title":"Prerequisites"},{"location":"o11y/openshift-logging/#setup-openshift-logging","text":"This is for setup of centralized logging on OpenShift making use of Elasticsearch OSS edition. This largely follows the processes outlined in the OpenShift documentation here . Retention and storage considerations are reviewed in Red Hat's primary source documentation. This setup is primarily concerned with simplicity and basic log searching. Consequently it is insufficient for long-lived retention or for advanced visualization of logs. For more advanced observability setups, you'll want to look at Forwarding Logs to Third Party Systems Create a namespace for the OpenShift Elasticsearch Operator. This is necessary to avoid potential conflicts with community operators that could send similarly named metrics/logs into the stack. bash oc create -f - <<EOF apiVersion: v1 kind: Namespace metadata: name: openshift-operators-redhat annotations: openshift.io/node-selector: \"\" labels: openshift.io/cluster-monitoring: \"true\" EOF Create a namespace for the OpenShift Logging Operator bash oc create -f - <<EOF apiVersion: v1 kind: Namespace metadata: name: openshift-logging annotations: openshift.io/node-selector: \"\" labels: openshift.io/cluster-monitoring: \"true\" EOF Install the OpenShift Elasticsearch Operator by creating the following objects: Operator Group for OpenShift Elasticsearch Operator bash oc create -f - <<EOF apiVersion: operators.coreos.com/v1 kind: OperatorGroup metadata: name: openshift-operators-redhat namespace: openshift-operators-redhat spec: {} EOF Subscription object to subscribe a Namespace to the OpenShift Elasticsearch Operator bash oc create -f - <<EOF apiVersion: operators.coreos.com/v1alpha1 kind: Subscription metadata: name: \"elasticsearch-operator\" namespace: \"openshift-operators-redhat\" spec: channel: \"stable\" installPlanApproval: \"Automatic\" source: \"redhat-operators\" sourceNamespace: \"openshift-marketplace\" name: \"elasticsearch-operator\" EOF Verify Operator Installation bash oc get csv --all-namespaces Example Output NAMESPACE NAME DISPLAY VERSION REPLACES PHASE default elasticsearch-operator.5.0.0-202007012112.p0 OpenShift Elasticsearch Operator 5.0.0-202007012112.p0 Succeeded kube-node-lease elasticsearch-operator.5.0.0-202007012112.p0 OpenShift Elasticsearch Operator 5.0.0-202007012112.p0 Succeeded kube-public elasticsearch-operator.5.0.0-202007012112.p0 OpenShift Elasticsearch Operator 5.0.0-202007012112.p0 Succeeded kube-system elasticsearch-operator.5.0.0-202007012112.p0 OpenShift Elasticsearch Operator 5.0.0-202007012112.p0 Succeeded openshift-apiserver-operator elasticsearch-operator.5.0.0-202007012112.p0 OpenShift Elasticsearch Operator 5.0.0-202007012112.p0 Succeeded openshift-apiserver elasticsearch-operator.5.0.0-202007012112.p0 OpenShift Elasticsearch Operator 5.0.0-202007012112.p0 Succeeded openshift-authentication-operator elasticsearch-operator.5.0.0-202007012112.p0 OpenShift Elasticsearch Operator 5.0.0-202007012112.p0 Succeeded openshift-authentication elasticsearch-operator.5.0. 0-202007012112.p0 OpenShift Elasticsearch Operator 5.0.0-202007012112.p0 Succeeded ... Install the Red Hat OpenShift Logging Operator by creating the following objects: The Cluster Logging OperatorGroup bash oc create -f - <<EOF apiVersion: operators.coreos.com/v1 kind: OperatorGroup metadata: name: cluster-logging namespace: openshift-logging spec: targetNamespaces: - openshift-logging EOF Subscription Object to subscribe a Namespace to the Red Hat OpenShift Logging Operator bash oc create -f - <<EOF apiVersion: operators.coreos.com/v1alpha1 kind: Subscription metadata: name: cluster-logging namespace: openshift-logging spec: channel: \"stable\" name: cluster-logging source: redhat-operators sourceNamespace: openshift-marketplace EOF Verify the Operator installation, the PHASE should be Succeeded bash oc get csv -n openshift-logging Example Output NAME DISPLAY VERSION REPLACES PHASE cluster-logging.5.0.5-11 Red Hat OpenShift Logging 5.0.5-11 Succeeded elasticsearch-operator.5.0.5-11 OpenShift Elasticsearch Operator 5.0.5-11 Succeeded Create an OpenShift Logging instance: NOTE : For the storageClassName below, you will need to adjust for the platform on which you're running OpenShift. managed-premium as listed below is for Azure Red Hat OpenShift (ARO). You can verify your available storage classes with oc get storageClasses bash oc create -f - <<EOF apiVersion: \"logging.openshift.io/v1\" kind: \"ClusterLogging\" metadata: name: \"instance\" namespace: \"openshift-logging\" spec: managementState: \"Managed\" logStore: type: \"elasticsearch\" retentionPolicy: application: maxAge: 1d infra: maxAge: 7d audit: maxAge: 7d elasticsearch: nodeCount: 3 storage: storageClassName: \"managed-premium\" size: 200G resources: requests: memory: \"8Gi\" proxy: resources: limits: memory: 256Mi requests: memory: 256Mi redundancyPolicy: \"SingleRedundancy\" visualization: type: \"kibana\" kibana: replicas: 1 curation: type: \"curator\" curator: schedule: \"30 3 * * *\" collection: logs: type: \"fluentd\" fluentd: {} EOF It will take a few minutes for everything to start up. You can monitor this progress by watching the pods. bash watch oc get pods -n openshift-logging Your logging instances are now configured and recieving logs. To view them, you will need to log into your Kibana instance and create the appropriate index patterns. For more information on index patterns, see the Kibana documentation. NOTE : The following restrictions and notes apply to index patterns: - All users can view the app- logs for namespaces they have access to - Only cluster-admins can view the infra- and audit- logs - For best accuracy, use the @timestamp field for determining chronology","title":"Setup OpenShift Logging"},{"location":"ocp/common-images-namespace/","text":"OpenShift - Sharing Common images Paul Czarkowski 21 June 2021 In OpenShift images (stored in the in-cluster registry) are protected by Kubernetes RBAC and by default only the namespace in which the image was built can access it. For example if you build an image in project-a only project-a can use that image, or build from it. If you wanted the default service account in project-b to have access to the images in project-a you would run the following. oc policy add-role-to-user \\ system:image-puller system:serviceaccount:project-b:default \\ --namespace=project-a However if you had to do this for every namespace it could become quite combersome. Instead if you choose to have a set of common images in a common-images namespace you could make them available to all authenticated users like so. oc adm policy add-cluster-role-to-group system:image-puller \\ system:authenticated --namespace=common-images oc adm policy add-role-to-group view system:authenticated \\ -n common-images Note: It's important to understand and accept the security implications that come with this. If any Pod in the cluster is compromised it will have access to pull any images in this namespace. See Global Image Puller for an example Kubernetes Controller that may allow for a more surgical (but still automated) way to grant access to images.","title":"OpenShift - Sharing Common images"},{"location":"ocp/common-images-namespace/#openshift-sharing-common-images","text":"Paul Czarkowski 21 June 2021 In OpenShift images (stored in the in-cluster registry) are protected by Kubernetes RBAC and by default only the namespace in which the image was built can access it. For example if you build an image in project-a only project-a can use that image, or build from it. If you wanted the default service account in project-b to have access to the images in project-a you would run the following. oc policy add-role-to-user \\ system:image-puller system:serviceaccount:project-b:default \\ --namespace=project-a However if you had to do this for every namespace it could become quite combersome. Instead if you choose to have a set of common images in a common-images namespace you could make them available to all authenticated users like so. oc adm policy add-cluster-role-to-group system:image-puller \\ system:authenticated --namespace=common-images oc adm policy add-role-to-group view system:authenticated \\ -n common-images Note: It's important to understand and accept the security implications that come with this. If any Pod in the cluster is compromised it will have access to pull any images in this namespace. See Global Image Puller for an example Kubernetes Controller that may allow for a more surgical (but still automated) way to grant access to images.","title":"OpenShift - Sharing Common images"},{"location":"rosa/ack/","text":"Using AWS Controllers for Kubernetes (ACK) on ROSA Updated: 06/02/2022 by Paul Czarkowski AWS Controllers for Kubernetes (ACK) lets you define and use AWS service resources directly from Kubernetes. With ACK, you can take advantage of AWS-managed services for your Kubernetes applications without needing to define resources outside of the cluster or run services that provide supporting capabilities like databases or message queues within the cluster. ROSA clusters have a set of the ACK controllers in Operator Hub which makes it relatively easy to get started and use it. Caution should be taken as it is a tech preview product from AWS. This tutorial shows how to use the ACK S3 controller as an example, but can be adapted for any other ACK controller that has an operator in the OperatorHub of your cluster. Prerequisites A ROSA cluster AWS CLI Helm 3 CLI Pre-install instructions Set some useful environment variables bash export CLUSTER=ansible-rosa export NAMESPACE=ack-system export IAM_USER=${CLUSTER}-ack-controller export S3_POLICY_ARN=arn:aws:iam::aws:policy/AmazonS3FullAccess export SCRATCH_DIR=/tmp/ack export ACK_SERVICE=s3 export AWS_PAGER=\"\" mkdir -p $SCRATCH_DIR Create and bind an IAM service account for ACK to use bash aws iam create-user --user-name $IAM_USER Create an access key for the user bash read -r ACCESS_KEY_ID ACCESS_KEY < <(aws iam create-access-key \\ --user-name $IAM_USER \\ --query 'AccessKey.[AccessKeyId,SecretAccessKey]' --output text) Find the ARN of the recommended IAM policy Note: you can find the recommended policy in each projects github repo, example https://github.com/aws-controllers-k8s/s3-controller/blob/main/config/iam/recommended-policy-arn bash aws iam attach-user-policy \\ --user-name $IAM_USER \\ --policy-arn \"$S3_POLICY_ARN\" Install the ACK S3 Controller Log into your OpenShift console, click to OperatorHub and search for \"ack\" Select the S3 controller and install it. Create a config map for ACK to use bash cat <<EOF > $SCRATCH_DIR/config.txt ACK_ENABLE_DEVELOPMENT_LOGGING=true ACK_LOG_LEVEL=debug ACK_WATCH_NAMESPACE= AWS_REGION=us-west-2 AWS_ENDPOINT_URL= ACK_RESOURCE_TAGS=$CLUSTER_NAME EOF Apply the config map bash oc create configmap --namespace ack-system \\ --from-env-file=$SCRATCH_DIR/config.txt ack-user-config Create a secret for ACK to use bash cat <<EOF > $SCRATCH_DIR/secrets.txt AWS_ACCESS_KEY_ID=$ACCESS_KEY_ID AWS_SECRET_ACCESS_KEY=$ACCESS_KEY EOF Apply the secret bash oc create secret generic --namespace ack-system \\ --from-env-file=$SCRATCH_DIR/secrets.txt ack-user-secrets Check the ack-s3-controller is running bash kubectl -n ack-system get pods bash NAME READY STATUS RESTARTS AGE ack-s3-controller-6dc4b4c-zgs2m 1/1 Running 0 145m If its not, restart it so that it can read the new configmap/secret. bash kubectl rollout restart deployment ack-s3-controller Deploy an S3 Bucket Resource bash cat << EOF | oc apply -f - apiVersion: s3.services.k8s.aws/v1alpha1 kind: Bucket metadata: name: $CLUSTER-bucket spec: name: $CLUSTER-bucket EOF Verify the S3 Bucket Resource bash aws s3 ls | grep $CLUSTER-bucket 2022-06-02 12:20:25 ansible-rosa-bucket","title":"Using AWS Controllers for Kubernetes (ACK) on ROSA"},{"location":"rosa/ack/#using-aws-controllers-for-kubernetes-ack-on-rosa","text":"Updated: 06/02/2022 by Paul Czarkowski AWS Controllers for Kubernetes (ACK) lets you define and use AWS service resources directly from Kubernetes. With ACK, you can take advantage of AWS-managed services for your Kubernetes applications without needing to define resources outside of the cluster or run services that provide supporting capabilities like databases or message queues within the cluster. ROSA clusters have a set of the ACK controllers in Operator Hub which makes it relatively easy to get started and use it. Caution should be taken as it is a tech preview product from AWS. This tutorial shows how to use the ACK S3 controller as an example, but can be adapted for any other ACK controller that has an operator in the OperatorHub of your cluster.","title":"Using AWS Controllers for Kubernetes (ACK) on ROSA"},{"location":"rosa/ack/#prerequisites","text":"A ROSA cluster AWS CLI Helm 3 CLI","title":"Prerequisites"},{"location":"rosa/ack/#pre-install-instructions","text":"Set some useful environment variables bash export CLUSTER=ansible-rosa export NAMESPACE=ack-system export IAM_USER=${CLUSTER}-ack-controller export S3_POLICY_ARN=arn:aws:iam::aws:policy/AmazonS3FullAccess export SCRATCH_DIR=/tmp/ack export ACK_SERVICE=s3 export AWS_PAGER=\"\" mkdir -p $SCRATCH_DIR Create and bind an IAM service account for ACK to use bash aws iam create-user --user-name $IAM_USER Create an access key for the user bash read -r ACCESS_KEY_ID ACCESS_KEY < <(aws iam create-access-key \\ --user-name $IAM_USER \\ --query 'AccessKey.[AccessKeyId,SecretAccessKey]' --output text) Find the ARN of the recommended IAM policy Note: you can find the recommended policy in each projects github repo, example https://github.com/aws-controllers-k8s/s3-controller/blob/main/config/iam/recommended-policy-arn bash aws iam attach-user-policy \\ --user-name $IAM_USER \\ --policy-arn \"$S3_POLICY_ARN\"","title":"Pre-install instructions"},{"location":"rosa/ack/#install-the-ack-s3-controller","text":"Log into your OpenShift console, click to OperatorHub and search for \"ack\" Select the S3 controller and install it. Create a config map for ACK to use bash cat <<EOF > $SCRATCH_DIR/config.txt ACK_ENABLE_DEVELOPMENT_LOGGING=true ACK_LOG_LEVEL=debug ACK_WATCH_NAMESPACE= AWS_REGION=us-west-2 AWS_ENDPOINT_URL= ACK_RESOURCE_TAGS=$CLUSTER_NAME EOF Apply the config map bash oc create configmap --namespace ack-system \\ --from-env-file=$SCRATCH_DIR/config.txt ack-user-config Create a secret for ACK to use bash cat <<EOF > $SCRATCH_DIR/secrets.txt AWS_ACCESS_KEY_ID=$ACCESS_KEY_ID AWS_SECRET_ACCESS_KEY=$ACCESS_KEY EOF Apply the secret bash oc create secret generic --namespace ack-system \\ --from-env-file=$SCRATCH_DIR/secrets.txt ack-user-secrets Check the ack-s3-controller is running bash kubectl -n ack-system get pods bash NAME READY STATUS RESTARTS AGE ack-s3-controller-6dc4b4c-zgs2m 1/1 Running 0 145m If its not, restart it so that it can read the new configmap/secret. bash kubectl rollout restart deployment ack-s3-controller Deploy an S3 Bucket Resource bash cat << EOF | oc apply -f - apiVersion: s3.services.k8s.aws/v1alpha1 kind: Bucket metadata: name: $CLUSTER-bucket spec: name: $CLUSTER-bucket EOF Verify the S3 Bucket Resource bash aws s3 ls | grep $CLUSTER-bucket 2022-06-02 12:20:25 ansible-rosa-bucket","title":"Install the ACK S3 Controller"},{"location":"rosa/alb-sts/","text":"Installing the AWS Load Balancer Controller (ALB) on ROSA Updated: 02/22/2022 In most situations you will want to stick with the OpenShift native Ingress Controller in order to use the native Ingress and Route resources to provide access to your applications. However if you absolutely require an ALB or NLB based Load Balancer then running the AWS Load Balancer Controller (ALB) may be worth looking at. Prerequisites A multi-region ROSA cluster with STS enabled AWS CLI Helm 3 CLI Getting Started Set some environment variables Disable AWS cli output paging bash export AWS_PAGER=\"\" export ALB_VERSION=\"v2.4.0\" export CLUSTER_NAME=\"cz-demo\" export SCRATCH_DIR=\"/tmp/alb-sts\" export OIDC_PROVIDER=$(oc get authentication.config.openshift.io cluster -o json \\ | jq -r .spec.serviceAccountIssuer| sed -e \"s/^https:\\/\\///\") export AWS_ACCOUNT_ID=$(aws sts get-caller-identity --query Account --output text) export REGION=$(rosa describe cluster -c $CLUSTER_NAME -o json | jq -r .region.id) export NAMESPACE=\"alb-controller\" export SA=\"alb-controller\" rm -rf $SCRATCH_DIR mkdir -p $SCRATCH_DIR Configure IAM credentials Create AWS Policy and Service Account ```bash wget -O $SCRATCH_DIR/iam-policy.json \\ https://raw.githubusercontent.com/kubernetes-sigs/aws-load-balancer-controller/$ALB_VERSION/docs/install/iam_policy.json POLICY_ARN=$(aws iam create-policy --policy-name \\ \"AWSLoadBalancerControllerIAMPolicy-$ALB_VERSION\" \\ --policy-document file://$SCRATCH_DIR/iam-policy.json \\ --query Policy.Arn --output text) echo $POLICY_ARN ``` If the Policy already exists you can use this instead ``bash POLICY_ARN=$(aws iam list-policies \\ --query 'Policies[?PolicyName== AWSLoadBalancerControllerIAMPolicy-'$ALB_VERSION'`].Arn' \\ --output text) echo $POLICY_ARN ``` Create a Trust Policy bash cat <<EOF > $SCRATCH_DIR/TrustPolicy.json { \"Version\": \"2012-10-17\", \"Statement\": [ { \"Effect\": \"Allow\", \"Principal\": { \"Federated\": \"arn:aws:iam::${AWS_ACCOUNT_ID}:oidc-provider/${OIDC_PROVIDER}\" }, \"Action\": \"sts:AssumeRoleWithWebIdentity\", \"Condition\": { \"StringEquals\": { \"${OIDC_PROVIDER}:sub\": [ \"system:serviceaccount:${NAMESPACE}:${SA}\" ] } } } ] } EOF Create Role for ALB Controller bash ALB_ROLE=$(aws iam create-role \\ --role-name \"$CLUSTER_NAME-alb-controller\" \\ --assume-role-policy-document file://$SCRATCH_DIR/TrustPolicy.json \\ --query \"Role.Arn\" --output text) echo $ALB_ROLE Attach the Policies to the Role bash aws iam attach-role-policy \\ --role-name \"$CLUSTER_NAME-alb-controller\" \\ --policy-arn $POLICY_ARN Configure Cluster subnets Get the Instance Name of one of your worker nodes bash NODE=$(oc get nodes --selector=node-role.kubernetes.io/worker \\ -o jsonpath='{.items[0].metadata.name}') echo $NODE Get the VPC ID of your worker nodes bash VPC=$(aws ec2 describe-instances \\ --filters \"Name=private-dns-name,Values=$NODE\" \\ --query 'Reservations[*].Instances[*].{VpcId:VpcId}' \\ | jq -r '.[0][0].VpcId') echo $VPC Get list of Subnets bash SUBNET_IDS=$(aws ec2 describe-subnets --output json \\ --filters Name=tag-value,Values=\"${CLUSTER_NAME}-*public*\" \\ --query \"Subnets[].SubnetId\" --output text | sed 's/\\t/ /g') echo ${SUBNET_IDS} Add tags to those subnets (change the subnet ids in the resources line) bash aws ec2 create-tags \\ --resources $(echo ${SUBNET_IDS}) \\ --tags Key=kubernetes.io/role/elb,Value='' Get cluster name (according to AWS Tags) ``bash AWS_CLUSTER=$(basename $(aws ec2 describe-subnets \\ --filters Name=tag-value,Values=\"${CLUSTER_NAME}-*public*\" \\ --query 'Subnets[0].Tags[?Value== shared`].Key[]' | jq -r '.[0]')) echo $AWS_CLUSTER ``` Create a namespace for the controller bash oc new-project $NAMESPACE Apply CRDs bash kubectl apply -k \\ \"github.com/aws/eks-charts/stable/aws-load-balancer-controller//crds?ref=master\" Add the helm repo and install the controller (install helm3 if not already) bash helm repo add eks https://aws.github.io/eks-charts helm repo update helm upgrade alb-controller eks/aws-load-balancer-controller -i \\ -n $NAMESPACE --set clusterName=$CLUSTER_NAME \\ --set serviceAccount.name=$SA \\ --set \"vpcId=$VPC\" \\ --set \"region=$REGION\" \\ --set serviceAccount.annotations.'eks\\.amazonaws\\.com/role-arn'=$ALB_ROLE \\ --set \"clusterName=$AWS_CLUSTER\" \\ --set \"image.repository=amazon/aws-alb-ingress-controller\" \\ --set \"image.tag=$ALB_VERSION\" --version 1.4.0 Update SCC to allow setting fsgroup in Deployment bash oc adm policy add-scc-to-user anyuid -z $SA -n $NAMESPACE Deploy Sample Application Create a new application in OpenShift bash oc new-project demo oc new-app https://github.com/sclorg/django-ex.git kubectl -n demo patch service django-ex -p '{\"spec\":{\"type\":\"NodePort\"}}' Create an Ingress to trigger an ALB Note: Setting the alb.ingress.kubernetes.io/group.name allows you to create multiple ALB Ingresses using the same ALB which can help reduce your AWS costs. bash cat << EOF | kubectl apply -f - apiVersion: networking.k8s.io/v1 kind: Ingress metadata: name: django-ex namespace: demo annotations: kubernetes.io/ingress.class: alb alb.ingress.kubernetes.io/scheme: internet-facing alb.ingress.kubernetes.io/target-type: instance alb.ingress.kubernetes.io/group.name: \"demo\" labels: app: django-ex spec: rules: - host: foo.bar http: paths: - pathType: Prefix path: / backend: service: name: django-ex port: number: 8080 EOF Check the logs of the ALB controller bash kubectl -n $NAMESPACE logs -f \\ deployment/alb-controller-aws-load-balancer-controller Save the ingress address bash URL=$(kubectl -n demo get ingress django-ex \\ -o jsonpath='{.status.loadBalancer.ingress[0].hostname}') bash curl -s --header \"Host: foo.bar\" $URL | head html <!doctype html> <html lang=\"en\"> <head> <meta charset=\"utf-8\"> <meta http-equiv=\"X-UA-Compatible\" content=\"IE=edge,chrome=1\"> <title>Welcome to OpenShift</title> Cleanup Delete the demo app bash kubectl delete ns demo Uninstall the ALB Controller bash helm delete -n $NAMESPACE alb-controller Get PolicyARN bash POLICY_ARN=$(aws iam list-policies \\ --query 'Policies[?PolicyName==`AWSLoadBalancerControllerIAMPolicy-'$ALB_VERSION'`].Arn' \\ --output text) Dettach the Policy from the Role bash aws iam detach-role-policy \\ --role-name \"$CLUSTER_NAME-alb-controller\" \\ --policy-arn $POLICY_ARN Delete Role for ALB Controller bash aws iam delete-role \\ --role-name \"$CLUSTER_NAME-alb-controller\"","title":"Installing the AWS Load Balancer Controller (ALB) on ROSA"},{"location":"rosa/alb-sts/#installing-the-aws-load-balancer-controller-alb-on-rosa","text":"Updated: 02/22/2022 In most situations you will want to stick with the OpenShift native Ingress Controller in order to use the native Ingress and Route resources to provide access to your applications. However if you absolutely require an ALB or NLB based Load Balancer then running the AWS Load Balancer Controller (ALB) may be worth looking at.","title":"Installing the AWS Load Balancer Controller (ALB) on ROSA"},{"location":"rosa/alb-sts/#prerequisites","text":"A multi-region ROSA cluster with STS enabled AWS CLI Helm 3 CLI","title":"Prerequisites"},{"location":"rosa/alb-sts/#getting-started","text":"Set some environment variables Disable AWS cli output paging bash export AWS_PAGER=\"\" export ALB_VERSION=\"v2.4.0\" export CLUSTER_NAME=\"cz-demo\" export SCRATCH_DIR=\"/tmp/alb-sts\" export OIDC_PROVIDER=$(oc get authentication.config.openshift.io cluster -o json \\ | jq -r .spec.serviceAccountIssuer| sed -e \"s/^https:\\/\\///\") export AWS_ACCOUNT_ID=$(aws sts get-caller-identity --query Account --output text) export REGION=$(rosa describe cluster -c $CLUSTER_NAME -o json | jq -r .region.id) export NAMESPACE=\"alb-controller\" export SA=\"alb-controller\" rm -rf $SCRATCH_DIR mkdir -p $SCRATCH_DIR","title":"Getting Started"},{"location":"rosa/alb-sts/#configure-iam-credentials","text":"Create AWS Policy and Service Account ```bash wget -O $SCRATCH_DIR/iam-policy.json \\ https://raw.githubusercontent.com/kubernetes-sigs/aws-load-balancer-controller/$ALB_VERSION/docs/install/iam_policy.json POLICY_ARN=$(aws iam create-policy --policy-name \\ \"AWSLoadBalancerControllerIAMPolicy-$ALB_VERSION\" \\ --policy-document file://$SCRATCH_DIR/iam-policy.json \\ --query Policy.Arn --output text) echo $POLICY_ARN ``` If the Policy already exists you can use this instead ``bash POLICY_ARN=$(aws iam list-policies \\ --query 'Policies[?PolicyName== AWSLoadBalancerControllerIAMPolicy-'$ALB_VERSION'`].Arn' \\ --output text) echo $POLICY_ARN ``` Create a Trust Policy bash cat <<EOF > $SCRATCH_DIR/TrustPolicy.json { \"Version\": \"2012-10-17\", \"Statement\": [ { \"Effect\": \"Allow\", \"Principal\": { \"Federated\": \"arn:aws:iam::${AWS_ACCOUNT_ID}:oidc-provider/${OIDC_PROVIDER}\" }, \"Action\": \"sts:AssumeRoleWithWebIdentity\", \"Condition\": { \"StringEquals\": { \"${OIDC_PROVIDER}:sub\": [ \"system:serviceaccount:${NAMESPACE}:${SA}\" ] } } } ] } EOF Create Role for ALB Controller bash ALB_ROLE=$(aws iam create-role \\ --role-name \"$CLUSTER_NAME-alb-controller\" \\ --assume-role-policy-document file://$SCRATCH_DIR/TrustPolicy.json \\ --query \"Role.Arn\" --output text) echo $ALB_ROLE Attach the Policies to the Role bash aws iam attach-role-policy \\ --role-name \"$CLUSTER_NAME-alb-controller\" \\ --policy-arn $POLICY_ARN","title":"Configure IAM credentials"},{"location":"rosa/alb-sts/#configure-cluster-subnets","text":"Get the Instance Name of one of your worker nodes bash NODE=$(oc get nodes --selector=node-role.kubernetes.io/worker \\ -o jsonpath='{.items[0].metadata.name}') echo $NODE Get the VPC ID of your worker nodes bash VPC=$(aws ec2 describe-instances \\ --filters \"Name=private-dns-name,Values=$NODE\" \\ --query 'Reservations[*].Instances[*].{VpcId:VpcId}' \\ | jq -r '.[0][0].VpcId') echo $VPC Get list of Subnets bash SUBNET_IDS=$(aws ec2 describe-subnets --output json \\ --filters Name=tag-value,Values=\"${CLUSTER_NAME}-*public*\" \\ --query \"Subnets[].SubnetId\" --output text | sed 's/\\t/ /g') echo ${SUBNET_IDS} Add tags to those subnets (change the subnet ids in the resources line) bash aws ec2 create-tags \\ --resources $(echo ${SUBNET_IDS}) \\ --tags Key=kubernetes.io/role/elb,Value='' Get cluster name (according to AWS Tags) ``bash AWS_CLUSTER=$(basename $(aws ec2 describe-subnets \\ --filters Name=tag-value,Values=\"${CLUSTER_NAME}-*public*\" \\ --query 'Subnets[0].Tags[?Value== shared`].Key[]' | jq -r '.[0]')) echo $AWS_CLUSTER ``` Create a namespace for the controller bash oc new-project $NAMESPACE Apply CRDs bash kubectl apply -k \\ \"github.com/aws/eks-charts/stable/aws-load-balancer-controller//crds?ref=master\" Add the helm repo and install the controller (install helm3 if not already) bash helm repo add eks https://aws.github.io/eks-charts helm repo update helm upgrade alb-controller eks/aws-load-balancer-controller -i \\ -n $NAMESPACE --set clusterName=$CLUSTER_NAME \\ --set serviceAccount.name=$SA \\ --set \"vpcId=$VPC\" \\ --set \"region=$REGION\" \\ --set serviceAccount.annotations.'eks\\.amazonaws\\.com/role-arn'=$ALB_ROLE \\ --set \"clusterName=$AWS_CLUSTER\" \\ --set \"image.repository=amazon/aws-alb-ingress-controller\" \\ --set \"image.tag=$ALB_VERSION\" --version 1.4.0 Update SCC to allow setting fsgroup in Deployment bash oc adm policy add-scc-to-user anyuid -z $SA -n $NAMESPACE","title":"Configure Cluster subnets"},{"location":"rosa/alb-sts/#deploy-sample-application","text":"Create a new application in OpenShift bash oc new-project demo oc new-app https://github.com/sclorg/django-ex.git kubectl -n demo patch service django-ex -p '{\"spec\":{\"type\":\"NodePort\"}}' Create an Ingress to trigger an ALB Note: Setting the alb.ingress.kubernetes.io/group.name allows you to create multiple ALB Ingresses using the same ALB which can help reduce your AWS costs. bash cat << EOF | kubectl apply -f - apiVersion: networking.k8s.io/v1 kind: Ingress metadata: name: django-ex namespace: demo annotations: kubernetes.io/ingress.class: alb alb.ingress.kubernetes.io/scheme: internet-facing alb.ingress.kubernetes.io/target-type: instance alb.ingress.kubernetes.io/group.name: \"demo\" labels: app: django-ex spec: rules: - host: foo.bar http: paths: - pathType: Prefix path: / backend: service: name: django-ex port: number: 8080 EOF Check the logs of the ALB controller bash kubectl -n $NAMESPACE logs -f \\ deployment/alb-controller-aws-load-balancer-controller Save the ingress address bash URL=$(kubectl -n demo get ingress django-ex \\ -o jsonpath='{.status.loadBalancer.ingress[0].hostname}') bash curl -s --header \"Host: foo.bar\" $URL | head html <!doctype html> <html lang=\"en\"> <head> <meta charset=\"utf-8\"> <meta http-equiv=\"X-UA-Compatible\" content=\"IE=edge,chrome=1\"> <title>Welcome to OpenShift</title>","title":"Deploy Sample Application"},{"location":"rosa/alb-sts/#cleanup","text":"Delete the demo app bash kubectl delete ns demo Uninstall the ALB Controller bash helm delete -n $NAMESPACE alb-controller Get PolicyARN bash POLICY_ARN=$(aws iam list-policies \\ --query 'Policies[?PolicyName==`AWSLoadBalancerControllerIAMPolicy-'$ALB_VERSION'`].Arn' \\ --output text) Dettach the Policy from the Role bash aws iam detach-role-policy \\ --role-name \"$CLUSTER_NAME-alb-controller\" \\ --policy-arn $POLICY_ARN Delete Role for ALB Controller bash aws iam delete-role \\ --role-name \"$CLUSTER_NAME-alb-controller\"","title":"Cleanup"},{"location":"rosa/aws-efs-csi-operator-sts/","text":"Enabling the AWS EFS CSI Driver Operator on ROSA Author: Paul Czarkowski Modified: 07/11/2022 The Amazon Web Services Elastic File System (AWS EFS) is a Network File System (NFS) that can be provisioned on Red Hat OpenShift Service on AWS clusters. With the release of OpenShift 4.10 the EFS CSI Driver is now GA and available. This is a guide to quickly enable the EFS Operator on ROSA to a Red Hat OpenShift on AWS (ROSA) cluster with STS enabled. Note: The official supported installation instructions for the EFS CSI Driver on ROSA are available here . Prerequisites A Red Hat OpenShift on AWS (ROSA) 4.10 cluster The OC CLI The AWS CLI JQ Set up environment export some environment variables bash export CLUSTER_NAME=\"sts-cluster\" export OIDC_PROVIDER=$(oc get authentication.config.openshift.io cluster -o json \\ | jq -r .spec.serviceAccountIssuer| sed -e \"s/^https:\\/\\///\") export AWS_ACCOUNT_ID=$(aws sts get-caller-identity --query Account --output text) export SCRATCH_DIR=/tmp/scratch export AWS_PAGER=\"\" mkdir -p $SCRATCH_DIR Prepare AWS Account In order to use the AWS EFS CSI Driver we need to create IAM roles and policies that can be attached to the Operator. Create an IAM Policy bash cat << EOF > $SCRATCH_DIR/efs-policy.json { \"Version\": \"2012-10-17\", \"Statement\": [ { \"Effect\": \"Allow\", \"Action\": [ \"elasticfilesystem:DescribeAccessPoints\", \"elasticfilesystem:DescribeFileSystems\", \"elasticfilesystem:DescribeMountTargets\", \"ec2:DescribeAvailabilityZones\" ], \"Resource\": \"*\" }, { \"Effect\": \"Allow\", \"Action\": [ \"elasticfilesystem:CreateAccessPoint\" ], \"Resource\": \"*\", \"Condition\": { \"StringLike\": { \"aws:RequestTag/efs.csi.aws.com/cluster\": \"true\" } } }, { \"Effect\": \"Allow\", \"Action\": \"elasticfilesystem:DeleteAccessPoint\", \"Resource\": \"*\", \"Condition\": { \"StringEquals\": { \"aws:ResourceTag/efs.csi.aws.com/cluster\": \"true\" } } } ] } EOF Create the Policy This creates a named policy for the cluster, you could use a generic policy for multiple clusters to keep things simpler. bash POLICY=$(aws iam create-policy --policy-name \"${CLUSTER_NAME}-rosa-efs-csi\" \\ --policy-document file://$SCRATCH_DIR/efs-policy.json \\ --query 'Policy.Arn' --output text) || \\ POLICY=$(aws iam list-policies \\ --query 'Policies[?PolicyName==`rosa-efs-csi`].Arn' \\ --output text) echo $POLICY Create a Trust Policy bash cat <<EOF > $SCRATCH_DIR/TrustPolicy.json { \"Version\": \"2012-10-17\", \"Statement\": [ { \"Effect\": \"Allow\", \"Principal\": { \"Federated\": \"arn:aws:iam::${AWS_ACCOUNT_ID}:oidc-provider/${OIDC_PROVIDER}\" }, \"Action\": \"sts:AssumeRoleWithWebIdentity\", \"Condition\": { \"StringEquals\": { \"${OIDC_PROVIDER}:sub\": [ \"system:serviceaccount:openshift-cluster-csi-drivers:aws-efs-csi-driver-operator\", \"system:serviceaccount:openshift-cluster-csi-drivers:aws-efs-csi-driver-controller-sa\" ] } } } ] } EOF Create Role for the EFS CSI Driver Operator ```bash bash ROLE=$(aws iam create-role \\ --role-name \"${CLUSTER_NAME}-aws-efs-csi-operator\" \\ --assume-role-policy-document file://$SCRATCH_DIR/TrustPolicy.json \\ --query \"Role.Arn\" --output text) echo $ROLE Attach the Policies to the Role bash aws iam attach-role-policy \\ --role-name \"${CLUSTER_NAME}-aws-efs-csi-operator\" \\ --policy-arn $POLICY Deploy and test the AWS EFS Operator Create a Secret to tell the AWS EFS Operator which IAM role to request. bash cat << EOF | oc apply -f - apiVersion: v1 kind: Secret metadata: name: aws-efs-cloud-credentials namespace: openshift-cluster-csi-drivers stringData: credentials: |- [default] role_arn = $ROLE web_identity_token_file = /var/run/secrets/openshift/serviceaccount/token EOF Install the EFS Operator ```bash cat <<EOF | oc create -f - apiVersion: operators.coreos.com/v1 kind: OperatorGroup metadata: generateName: openshift-cluster-csi-drivers- namespace: openshift-cluster-csi-drivers apiVersion: operators.coreos.com/v1alpha1 kind: Subscription metadata: labels: operators.coreos.com/aws-efs-csi-driver-operator.openshift-cluster-csi-drivers: \"\" name: aws-efs-csi-driver-operator namespace: openshift-cluster-csi-drivers spec: channel: stable installPlanApproval: Automatic name: aws-efs-csi-driver-operator source: redhat-operators sourceNamespace: openshift-marketplace EOF ``` Wait until the Operator is running bash watch oc get deployment aws-efs-csi-driver-operator -n openshift-cluster-csi-drivers Install the AWS EFS CSI Driver bash cat <<EOF | oc apply -f - apiVersion: operator.openshift.io/v1 kind: ClusterCSIDriver metadata: name: efs.csi.aws.com spec: managementState: Managed EOF Wait until the CSI driver is running bash watch oc get daemonset aws-efs-csi-driver-node -n openshift-cluster-csi-drivers Create a storage class bash cat <<EOF | oc apply -f - allowVolumeExpansion: true apiVersion: storage.k8s.io/v1 kind: StorageClass metadata: name: efs-csi provisioner: efs.csi.aws.com parameters: reclaimPolicy: Delete volumeBindingMode: WaitForFirstConsumer EOF Prepare an AWS EFS Volume Run this set of commands to update the VPC to allow EFS access bash NODE=$(oc get nodes --selector=node-role.kubernetes.io/worker \\ -o jsonpath='{.items[0].metadata.name}') VPC=$(aws ec2 describe-instances \\ --filters \"Name=private-dns-name,Values=$NODE\" \\ --query 'Reservations[*].Instances[*].{VpcId:VpcId}' \\ | jq -r '.[0][0].VpcId') SUBNET=$(aws ec2 describe-subnets \\ --filters Name=vpc-id,Values=$VPC Name=tag:Name,Values='*-private*' \\ --query 'Subnets[*].{SubnetId:SubnetId}' \\ | jq -r '.[0].SubnetId') CIDR=$(aws ec2 describe-vpcs \\ --filters \"Name=vpc-id,Values=$VPC\" \\ --query 'Vpcs[*].CidrBlock' \\ | jq -r '.[0]') SG=$(aws ec2 describe-instances --filters \\ \"Name=private-dns-name,Values=$NODE\" \\ --query 'Reservations[*].Instances[*].{SecurityGroups:SecurityGroups}' \\ | jq -r '.[0][0].SecurityGroups[0].GroupId') echo \"CIDR - $CIDR, SG - $SG\" Assuming the CIDR and SG are correct, update the security group bash aws ec2 authorize-security-group-ingress \\ --group-id $SG \\ --protocol tcp \\ --port 2049 \\ --cidr $CIDR | jq . Create EFS File System bash EFS=$(aws efs create-file-system --creation-token efs-token-1 \\ --encrypted | jq -r '.FileSystemId') echo $EFS Configure Mount Target for EFS bash MOUNT_TARGET=$(aws efs create-mount-target --file-system-id $EFS \\ --subnet-id $SUBNET --security-groups $SG \\ | jq -r '.MountTargetId') echo $MOUNT_TARGET Create a Storage Class for the EFS volume bash cat <<EOF | oc apply -f - kind: StorageClass apiVersion: storage.k8s.io/v1 metadata: name: efs-sc provisioner: efs.csi.aws.com parameters: provisioningMode: efs-ap fileSystemId: $EFS directoryPerms: \"700\" gidRangeStart: \"1000\" gidRangeEnd: \"2000\" basePath: \"/dynamic_provisioning\" EOF Test Create a namespace bash oc new-project efs-demo Create a PVC bash cat <<EOF | oc apply -f - apiVersion: v1 kind: PersistentVolumeClaim metadata: name: pvc-efs-volume spec: storageClassName: efs-sc accessModes: - ReadWriteMany resources: requests: storage: 5Gi EOF Create a Pod to write to the EFS Volume bash cat <<EOF | oc apply -f - apiVersion: v1 kind: Pod metadata: name: test-efs spec: volumes: - name: efs-storage-vol persistentVolumeClaim: claimName: pvc-efs-volume containers: - name: test-efs image: centos:latest command: [ \"/bin/bash\", \"-c\", \"--\" ] args: [ \"while true; do echo 'hello efs' | tee -a /mnt/efs-data/verify-efs && sleep 5; done;\" ] volumeMounts: - mountPath: \"/mnt/efs-data\" name: efs-storage-vol EOF It may take a few minutes for the pod to be ready. If you see errors such as Output: Failed to resolve \"fs-XXXX.efs.us-east-2.amazonaws.com\" it likely means its still setting up the EFS volume, just wait longer. Wait for the Pod to be ready bash watch oc get pod test-efs Create a Pod to read from the EFS Volume bash cat <<EOF | oc apply -f - apiVersion: v1 kind: Pod metadata: name: test-efs-read spec: volumes: - name: efs-storage-vol persistentVolumeClaim: claimName: pvc-efs-volume containers: - name: test-efs-read image: centos:latest command: [ \"/bin/bash\", \"-c\", \"--\" ] args: [ \"tail -f /mnt/efs-data/verify-efs\" ] volumeMounts: - mountPath: \"/mnt/efs-data\" name: efs-storage-vol EOF Verify the second POD can read the EFS Volume bash oc logs test-efs-read You should see a stream of \"hello efs\" hello efs hello efs hello efs hello efs hello efs hello efs hello efs hello efs hello efs hello efs Cleanup Delete the Pods bash oc delete pod -n efs-demo test-efs test-efs-read Delete the Volume bash oc delete -n efs-demo pvc pvc-efs-volume Delete the Namespace bash oc delete project efs-demo Delete the storage class bash oc delete storageclass efs-sc Delete the EFS Shared Volume via AWS bash aws efs delete-mount-target --mount-target-id $MOUNT_TARGET aws efs delete-file-system --file-system-id $EFS > Note if you receive the error `An error occurred (FileSystemInUse)` wait a few minutes and try again. Detach the Policies to the Role bash aws iam detach-role-policy \\ --role-name \"${CLUSTER_NAME}-aws-efs-csi-operator\" \\ --policy-arn $POLICY Delete the Role bash aws iam delete-role --role-name \\ ${CLUSTER_NAME}-aws-efs-csi-operator Delete the Policy bash aws iam delete-policy --policy-arn \\ $POLICY","title":"Enabling the AWS EFS CSI Driver Operator on ROSA"},{"location":"rosa/aws-efs-csi-operator-sts/#enabling-the-aws-efs-csi-driver-operator-on-rosa","text":"Author: Paul Czarkowski Modified: 07/11/2022 The Amazon Web Services Elastic File System (AWS EFS) is a Network File System (NFS) that can be provisioned on Red Hat OpenShift Service on AWS clusters. With the release of OpenShift 4.10 the EFS CSI Driver is now GA and available. This is a guide to quickly enable the EFS Operator on ROSA to a Red Hat OpenShift on AWS (ROSA) cluster with STS enabled. Note: The official supported installation instructions for the EFS CSI Driver on ROSA are available here .","title":"Enabling the AWS EFS CSI Driver Operator on ROSA"},{"location":"rosa/aws-efs-csi-operator-sts/#prerequisites","text":"A Red Hat OpenShift on AWS (ROSA) 4.10 cluster The OC CLI The AWS CLI JQ","title":"Prerequisites"},{"location":"rosa/aws-efs-csi-operator-sts/#set-up-environment","text":"export some environment variables bash export CLUSTER_NAME=\"sts-cluster\" export OIDC_PROVIDER=$(oc get authentication.config.openshift.io cluster -o json \\ | jq -r .spec.serviceAccountIssuer| sed -e \"s/^https:\\/\\///\") export AWS_ACCOUNT_ID=$(aws sts get-caller-identity --query Account --output text) export SCRATCH_DIR=/tmp/scratch export AWS_PAGER=\"\" mkdir -p $SCRATCH_DIR","title":"Set up environment"},{"location":"rosa/aws-efs-csi-operator-sts/#prepare-aws-account","text":"In order to use the AWS EFS CSI Driver we need to create IAM roles and policies that can be attached to the Operator. Create an IAM Policy bash cat << EOF > $SCRATCH_DIR/efs-policy.json { \"Version\": \"2012-10-17\", \"Statement\": [ { \"Effect\": \"Allow\", \"Action\": [ \"elasticfilesystem:DescribeAccessPoints\", \"elasticfilesystem:DescribeFileSystems\", \"elasticfilesystem:DescribeMountTargets\", \"ec2:DescribeAvailabilityZones\" ], \"Resource\": \"*\" }, { \"Effect\": \"Allow\", \"Action\": [ \"elasticfilesystem:CreateAccessPoint\" ], \"Resource\": \"*\", \"Condition\": { \"StringLike\": { \"aws:RequestTag/efs.csi.aws.com/cluster\": \"true\" } } }, { \"Effect\": \"Allow\", \"Action\": \"elasticfilesystem:DeleteAccessPoint\", \"Resource\": \"*\", \"Condition\": { \"StringEquals\": { \"aws:ResourceTag/efs.csi.aws.com/cluster\": \"true\" } } } ] } EOF Create the Policy This creates a named policy for the cluster, you could use a generic policy for multiple clusters to keep things simpler. bash POLICY=$(aws iam create-policy --policy-name \"${CLUSTER_NAME}-rosa-efs-csi\" \\ --policy-document file://$SCRATCH_DIR/efs-policy.json \\ --query 'Policy.Arn' --output text) || \\ POLICY=$(aws iam list-policies \\ --query 'Policies[?PolicyName==`rosa-efs-csi`].Arn' \\ --output text) echo $POLICY Create a Trust Policy bash cat <<EOF > $SCRATCH_DIR/TrustPolicy.json { \"Version\": \"2012-10-17\", \"Statement\": [ { \"Effect\": \"Allow\", \"Principal\": { \"Federated\": \"arn:aws:iam::${AWS_ACCOUNT_ID}:oidc-provider/${OIDC_PROVIDER}\" }, \"Action\": \"sts:AssumeRoleWithWebIdentity\", \"Condition\": { \"StringEquals\": { \"${OIDC_PROVIDER}:sub\": [ \"system:serviceaccount:openshift-cluster-csi-drivers:aws-efs-csi-driver-operator\", \"system:serviceaccount:openshift-cluster-csi-drivers:aws-efs-csi-driver-controller-sa\" ] } } } ] } EOF Create Role for the EFS CSI Driver Operator ```bash bash ROLE=$(aws iam create-role \\ --role-name \"${CLUSTER_NAME}-aws-efs-csi-operator\" \\ --assume-role-policy-document file://$SCRATCH_DIR/TrustPolicy.json \\ --query \"Role.Arn\" --output text) echo $ROLE Attach the Policies to the Role bash aws iam attach-role-policy \\ --role-name \"${CLUSTER_NAME}-aws-efs-csi-operator\" \\ --policy-arn $POLICY","title":"Prepare AWS Account"},{"location":"rosa/aws-efs-csi-operator-sts/#deploy-and-test-the-aws-efs-operator","text":"Create a Secret to tell the AWS EFS Operator which IAM role to request. bash cat << EOF | oc apply -f - apiVersion: v1 kind: Secret metadata: name: aws-efs-cloud-credentials namespace: openshift-cluster-csi-drivers stringData: credentials: |- [default] role_arn = $ROLE web_identity_token_file = /var/run/secrets/openshift/serviceaccount/token EOF Install the EFS Operator ```bash cat <<EOF | oc create -f - apiVersion: operators.coreos.com/v1 kind: OperatorGroup metadata: generateName: openshift-cluster-csi-drivers- namespace: openshift-cluster-csi-drivers apiVersion: operators.coreos.com/v1alpha1 kind: Subscription metadata: labels: operators.coreos.com/aws-efs-csi-driver-operator.openshift-cluster-csi-drivers: \"\" name: aws-efs-csi-driver-operator namespace: openshift-cluster-csi-drivers spec: channel: stable installPlanApproval: Automatic name: aws-efs-csi-driver-operator source: redhat-operators sourceNamespace: openshift-marketplace EOF ``` Wait until the Operator is running bash watch oc get deployment aws-efs-csi-driver-operator -n openshift-cluster-csi-drivers Install the AWS EFS CSI Driver bash cat <<EOF | oc apply -f - apiVersion: operator.openshift.io/v1 kind: ClusterCSIDriver metadata: name: efs.csi.aws.com spec: managementState: Managed EOF Wait until the CSI driver is running bash watch oc get daemonset aws-efs-csi-driver-node -n openshift-cluster-csi-drivers Create a storage class bash cat <<EOF | oc apply -f - allowVolumeExpansion: true apiVersion: storage.k8s.io/v1 kind: StorageClass metadata: name: efs-csi provisioner: efs.csi.aws.com parameters: reclaimPolicy: Delete volumeBindingMode: WaitForFirstConsumer EOF","title":"Deploy and test the AWS EFS Operator"},{"location":"rosa/aws-efs-csi-operator-sts/#prepare-an-aws-efs-volume","text":"Run this set of commands to update the VPC to allow EFS access bash NODE=$(oc get nodes --selector=node-role.kubernetes.io/worker \\ -o jsonpath='{.items[0].metadata.name}') VPC=$(aws ec2 describe-instances \\ --filters \"Name=private-dns-name,Values=$NODE\" \\ --query 'Reservations[*].Instances[*].{VpcId:VpcId}' \\ | jq -r '.[0][0].VpcId') SUBNET=$(aws ec2 describe-subnets \\ --filters Name=vpc-id,Values=$VPC Name=tag:Name,Values='*-private*' \\ --query 'Subnets[*].{SubnetId:SubnetId}' \\ | jq -r '.[0].SubnetId') CIDR=$(aws ec2 describe-vpcs \\ --filters \"Name=vpc-id,Values=$VPC\" \\ --query 'Vpcs[*].CidrBlock' \\ | jq -r '.[0]') SG=$(aws ec2 describe-instances --filters \\ \"Name=private-dns-name,Values=$NODE\" \\ --query 'Reservations[*].Instances[*].{SecurityGroups:SecurityGroups}' \\ | jq -r '.[0][0].SecurityGroups[0].GroupId') echo \"CIDR - $CIDR, SG - $SG\" Assuming the CIDR and SG are correct, update the security group bash aws ec2 authorize-security-group-ingress \\ --group-id $SG \\ --protocol tcp \\ --port 2049 \\ --cidr $CIDR | jq . Create EFS File System bash EFS=$(aws efs create-file-system --creation-token efs-token-1 \\ --encrypted | jq -r '.FileSystemId') echo $EFS Configure Mount Target for EFS bash MOUNT_TARGET=$(aws efs create-mount-target --file-system-id $EFS \\ --subnet-id $SUBNET --security-groups $SG \\ | jq -r '.MountTargetId') echo $MOUNT_TARGET Create a Storage Class for the EFS volume bash cat <<EOF | oc apply -f - kind: StorageClass apiVersion: storage.k8s.io/v1 metadata: name: efs-sc provisioner: efs.csi.aws.com parameters: provisioningMode: efs-ap fileSystemId: $EFS directoryPerms: \"700\" gidRangeStart: \"1000\" gidRangeEnd: \"2000\" basePath: \"/dynamic_provisioning\" EOF","title":"Prepare an AWS EFS Volume"},{"location":"rosa/aws-efs-csi-operator-sts/#test","text":"Create a namespace bash oc new-project efs-demo Create a PVC bash cat <<EOF | oc apply -f - apiVersion: v1 kind: PersistentVolumeClaim metadata: name: pvc-efs-volume spec: storageClassName: efs-sc accessModes: - ReadWriteMany resources: requests: storage: 5Gi EOF Create a Pod to write to the EFS Volume bash cat <<EOF | oc apply -f - apiVersion: v1 kind: Pod metadata: name: test-efs spec: volumes: - name: efs-storage-vol persistentVolumeClaim: claimName: pvc-efs-volume containers: - name: test-efs image: centos:latest command: [ \"/bin/bash\", \"-c\", \"--\" ] args: [ \"while true; do echo 'hello efs' | tee -a /mnt/efs-data/verify-efs && sleep 5; done;\" ] volumeMounts: - mountPath: \"/mnt/efs-data\" name: efs-storage-vol EOF It may take a few minutes for the pod to be ready. If you see errors such as Output: Failed to resolve \"fs-XXXX.efs.us-east-2.amazonaws.com\" it likely means its still setting up the EFS volume, just wait longer. Wait for the Pod to be ready bash watch oc get pod test-efs Create a Pod to read from the EFS Volume bash cat <<EOF | oc apply -f - apiVersion: v1 kind: Pod metadata: name: test-efs-read spec: volumes: - name: efs-storage-vol persistentVolumeClaim: claimName: pvc-efs-volume containers: - name: test-efs-read image: centos:latest command: [ \"/bin/bash\", \"-c\", \"--\" ] args: [ \"tail -f /mnt/efs-data/verify-efs\" ] volumeMounts: - mountPath: \"/mnt/efs-data\" name: efs-storage-vol EOF Verify the second POD can read the EFS Volume bash oc logs test-efs-read You should see a stream of \"hello efs\" hello efs hello efs hello efs hello efs hello efs hello efs hello efs hello efs hello efs hello efs","title":"Test"},{"location":"rosa/aws-efs-csi-operator-sts/#cleanup","text":"Delete the Pods bash oc delete pod -n efs-demo test-efs test-efs-read Delete the Volume bash oc delete -n efs-demo pvc pvc-efs-volume Delete the Namespace bash oc delete project efs-demo Delete the storage class bash oc delete storageclass efs-sc Delete the EFS Shared Volume via AWS bash aws efs delete-mount-target --mount-target-id $MOUNT_TARGET aws efs delete-file-system --file-system-id $EFS > Note if you receive the error `An error occurred (FileSystemInUse)` wait a few minutes and try again. Detach the Policies to the Role bash aws iam detach-role-policy \\ --role-name \"${CLUSTER_NAME}-aws-efs-csi-operator\" \\ --policy-arn $POLICY Delete the Role bash aws iam delete-role --role-name \\ ${CLUSTER_NAME}-aws-efs-csi-operator Delete the Policy bash aws iam delete-policy --policy-arn \\ $POLICY","title":"Cleanup"},{"location":"rosa/aws-efs-operator-on-rosa/","text":"Enabling the AWS EFS Operator on ROSA The Amazon Web Services Elastic File System (AWS EFS) is a Network File System (NFS) that can be provisioned on Red Hat OpenShift Service on AWS clusters. AWS also provides and supports a CSI EFS Driver to be used with Kubernetes that allows Kubernetes workloads to leverage this shared file storage. This is a guide to quickly enable the EFS Operator on ROSA to See here for the official ROSA documentation. Prerequisites A Red Hat OpenShift on AWS (ROSA) cluster The OC CLI The AWS CLI JQ Prepare AWS Account Get the Instance Name of one of your worker nodes bash NODE=$(oc get nodes --selector=node-role.kubernetes.io/worker \\ -o jsonpath='{.items[0].metadata.name}') Get the VPC ID of your worker nodes bash VPC=$(aws ec2 describe-instances \\ --filters \"Name=private-dns-name,Values=$NODE\" \\ --query 'Reservations[*].Instances[*].{VpcId:VpcId}' \\ | jq -r '.[0][0].VpcId') Get subnets in your VPC bash SUBNET=$(aws ec2 describe-subnets \\ --filters Name=vpc-id,Values=$VPC Name=tag:kubernetes.io/role/internal-elb,Values='' \\ --query 'Subnets[*].{SubnetId:SubnetId}' \\ | jq -r '.[0].SubnetId') Get the CIDR block of your worker nodes bash CIDR=$(aws ec2 describe-vpcs \\ --filters \"Name=vpc-id,Values=$VPC\" \\ --query 'Vpcs[*].CidrBlock' \\ | jq -r '.[0]') Get the Security Group of your worker nodes bash SG=$(aws ec2 describe-instances --filters \\ \"Name=private-dns-name,Values=$NODE\" \\ --query 'Reservations[*].Instances[*].{SecurityGroups:SecurityGroups}' \\ | jq -r '.[0][0].SecurityGroups[0].GroupId') Add EFS to security group bash aws ec2 authorize-security-group-ingress \\ --group-id $SG \\ --protocol tcp \\ --port 2049 \\ --cidr $CIDR | jq . Create EFS File System Note: You may want to create separate/additional access-points for each application/shared vol. bash EFS=$(aws efs create-file-system --creation-token efs-token-1 \\ --encrypted | jq -r '.FileSystemId') Configure Mount Target for EFS bash MOUNT_TARGET=$(aws efs create-mount-target --file-system-id $EFS \\ --subnet-id $SUBNET --security-groups $SG \\ | jq -r '.MountTargetId') 1. Create Access Point for EFS bash ACCESS_POINT=$(aws efs create-access-point --file-system-id $EFS \\ --client-token efs-token-1 \\ | jq -r '.AccessPointId') Deploy and test the AWS EFS Operator Install the EFS Operator bash cat <<EOF | oc apply -f - apiVersion: operators.coreos.com/v1alpha1 kind: Subscription metadata: labels: operators.coreos.com/aws-efs-operator.openshift-operators: \"\" name: aws-efs-operator namespace: openshift-operators spec: channel: stable installPlanApproval: Automatic name: aws-efs-operator source: community-operators sourceNamespace: openshift-marketplace startingCSV: aws-efs-operator.v0.0.8 EOF Create a namespace bash oc new-project efs-demo Create a EFS Shared Volume bash cat <<EOF | oc apply -f - apiVersion: aws-efs.managed.openshift.io/v1alpha1 kind: SharedVolume metadata: name: efs-volume namespace: efs-demo spec: accessPointID: ${ACCESS_POINT} fileSystemID: ${EFS} EOF Create a POD to write to the EFS Volume bash cat <<EOF | oc apply -f - apiVersion: v1 kind: Pod metadata: name: test-efs spec: volumes: - name: efs-storage-vol persistentVolumeClaim: claimName: pvc-efs-volume containers: - name: test-efs image: centos:latest command: [ \"/bin/bash\", \"-c\", \"--\" ] args: [ \"while true; do echo 'hello efs' | tee -a /mnt/efs-data/verify-efs && sleep 5; done;\" ] volumeMounts: - mountPath: \"/mnt/efs-data\" name: efs-storage-vol EOF Create a POD to read from the EFS Volume bash cat <<EOF | oc apply -f - apiVersion: v1 kind: Pod metadata: name: test-efs-read spec: volumes: - name: efs-storage-vol persistentVolumeClaim: claimName: pvc-efs-volume containers: - name: test-efs-read image: centos:latest command: [ \"/bin/bash\", \"-c\", \"--\" ] args: [ \"tail -f /mnt/efs-data/verify-efs\" ] volumeMounts: - mountPath: \"/mnt/efs-data\" name: efs-storage-vol EOF Verify the second POD can read the EFS Volume bash oc logs test-efs-read You should see a stream of \"hello efs\" hello efs hello efs hello efs hello efs hello efs hello efs hello efs hello efs hello efs hello efs Cleanup Delete the Pods bash oc delete pod -n efs-demo test-efs test-efs-read Delete the Volume bash oc delete -n efs-demo SharedVolume efs-volume Delete the Namespace bash oc delete project efs-demo Delete the EFS Shared Volume via AWS bash aws efs delete-mount-target --mount-target-id $MOUNT_TARGET | jq . aws efs delete-access-point --access-point-id $ACCESS_POINT | jq . aws efs delete-file-system --file-system-id $EFS | jq .","title":"Enabling the AWS EFS Operator on ROSA"},{"location":"rosa/aws-efs-operator-on-rosa/#enabling-the-aws-efs-operator-on-rosa","text":"The Amazon Web Services Elastic File System (AWS EFS) is a Network File System (NFS) that can be provisioned on Red Hat OpenShift Service on AWS clusters. AWS also provides and supports a CSI EFS Driver to be used with Kubernetes that allows Kubernetes workloads to leverage this shared file storage. This is a guide to quickly enable the EFS Operator on ROSA to See here for the official ROSA documentation.","title":"Enabling the AWS EFS Operator on ROSA"},{"location":"rosa/aws-efs-operator-on-rosa/#prerequisites","text":"A Red Hat OpenShift on AWS (ROSA) cluster The OC CLI The AWS CLI JQ","title":"Prerequisites"},{"location":"rosa/aws-efs-operator-on-rosa/#prepare-aws-account","text":"Get the Instance Name of one of your worker nodes bash NODE=$(oc get nodes --selector=node-role.kubernetes.io/worker \\ -o jsonpath='{.items[0].metadata.name}') Get the VPC ID of your worker nodes bash VPC=$(aws ec2 describe-instances \\ --filters \"Name=private-dns-name,Values=$NODE\" \\ --query 'Reservations[*].Instances[*].{VpcId:VpcId}' \\ | jq -r '.[0][0].VpcId') Get subnets in your VPC bash SUBNET=$(aws ec2 describe-subnets \\ --filters Name=vpc-id,Values=$VPC Name=tag:kubernetes.io/role/internal-elb,Values='' \\ --query 'Subnets[*].{SubnetId:SubnetId}' \\ | jq -r '.[0].SubnetId') Get the CIDR block of your worker nodes bash CIDR=$(aws ec2 describe-vpcs \\ --filters \"Name=vpc-id,Values=$VPC\" \\ --query 'Vpcs[*].CidrBlock' \\ | jq -r '.[0]') Get the Security Group of your worker nodes bash SG=$(aws ec2 describe-instances --filters \\ \"Name=private-dns-name,Values=$NODE\" \\ --query 'Reservations[*].Instances[*].{SecurityGroups:SecurityGroups}' \\ | jq -r '.[0][0].SecurityGroups[0].GroupId') Add EFS to security group bash aws ec2 authorize-security-group-ingress \\ --group-id $SG \\ --protocol tcp \\ --port 2049 \\ --cidr $CIDR | jq . Create EFS File System Note: You may want to create separate/additional access-points for each application/shared vol. bash EFS=$(aws efs create-file-system --creation-token efs-token-1 \\ --encrypted | jq -r '.FileSystemId') Configure Mount Target for EFS bash MOUNT_TARGET=$(aws efs create-mount-target --file-system-id $EFS \\ --subnet-id $SUBNET --security-groups $SG \\ | jq -r '.MountTargetId') 1. Create Access Point for EFS bash ACCESS_POINT=$(aws efs create-access-point --file-system-id $EFS \\ --client-token efs-token-1 \\ | jq -r '.AccessPointId')","title":"Prepare AWS Account"},{"location":"rosa/aws-efs-operator-on-rosa/#deploy-and-test-the-aws-efs-operator","text":"Install the EFS Operator bash cat <<EOF | oc apply -f - apiVersion: operators.coreos.com/v1alpha1 kind: Subscription metadata: labels: operators.coreos.com/aws-efs-operator.openshift-operators: \"\" name: aws-efs-operator namespace: openshift-operators spec: channel: stable installPlanApproval: Automatic name: aws-efs-operator source: community-operators sourceNamespace: openshift-marketplace startingCSV: aws-efs-operator.v0.0.8 EOF Create a namespace bash oc new-project efs-demo Create a EFS Shared Volume bash cat <<EOF | oc apply -f - apiVersion: aws-efs.managed.openshift.io/v1alpha1 kind: SharedVolume metadata: name: efs-volume namespace: efs-demo spec: accessPointID: ${ACCESS_POINT} fileSystemID: ${EFS} EOF Create a POD to write to the EFS Volume bash cat <<EOF | oc apply -f - apiVersion: v1 kind: Pod metadata: name: test-efs spec: volumes: - name: efs-storage-vol persistentVolumeClaim: claimName: pvc-efs-volume containers: - name: test-efs image: centos:latest command: [ \"/bin/bash\", \"-c\", \"--\" ] args: [ \"while true; do echo 'hello efs' | tee -a /mnt/efs-data/verify-efs && sleep 5; done;\" ] volumeMounts: - mountPath: \"/mnt/efs-data\" name: efs-storage-vol EOF Create a POD to read from the EFS Volume bash cat <<EOF | oc apply -f - apiVersion: v1 kind: Pod metadata: name: test-efs-read spec: volumes: - name: efs-storage-vol persistentVolumeClaim: claimName: pvc-efs-volume containers: - name: test-efs-read image: centos:latest command: [ \"/bin/bash\", \"-c\", \"--\" ] args: [ \"tail -f /mnt/efs-data/verify-efs\" ] volumeMounts: - mountPath: \"/mnt/efs-data\" name: efs-storage-vol EOF Verify the second POD can read the EFS Volume bash oc logs test-efs-read You should see a stream of \"hello efs\" hello efs hello efs hello efs hello efs hello efs hello efs hello efs hello efs hello efs hello efs","title":"Deploy and test the AWS EFS Operator"},{"location":"rosa/aws-efs-operator-on-rosa/#cleanup","text":"Delete the Pods bash oc delete pod -n efs-demo test-efs test-efs-read Delete the Volume bash oc delete -n efs-demo SharedVolume efs-volume Delete the Namespace bash oc delete project efs-demo Delete the EFS Shared Volume via AWS bash aws efs delete-mount-target --mount-target-id $MOUNT_TARGET | jq . aws efs delete-access-point --access-point-id $ACCESS_POINT | jq . aws efs delete-file-system --file-system-id $EFS | jq .","title":"Cleanup"},{"location":"rosa/aws-secrets-manager-csi/","text":"Using AWS Secrets Manager CSI on Red Hat OpenShift on AWS with STS Author Paul Czarkowski last modified 2021-08-17 The AWS Secrets and Configuration Provider (ASCP) provides a way to expose AWS Secrets as Kubernetes storage volumes. With the ASCP, you can store and manage your secrets in Secrets Manager and then retrieve them through your workloads running on ROSA or OSD. This is made even easier / more secure through the use of AWS STS and Kubernetes PodIdentity. Prerequisites A ROSA cluster deployed with STS Helm 3 aws CLI jq Preparing Environment Validate that your cluster has STS bash oc get authentication.config.openshift.io cluster -o json \\ | jq .spec.serviceAccountIssuer You should see something like the following, if not you should not proceed, instead look to the Red Hat documentation on creating an STS cluster . \"https://rh-oidc.s3.us-east-1.amazonaws.com/xxxxxx\" Set SecurityContextConstraints to allow the CSI driver to run bash oc new-project csi-secrets-store oc adm policy add-scc-to-user privileged \\ system:serviceaccount:csi-secrets-store:secrets-store-csi-driver oc adm policy add-scc-to-user privileged \\ system:serviceaccount:csi-secrets-store:csi-secrets-store-provider-aws Create some environment variables to refer to later bash export ROSA_CLUSTER_NAME=my-cluster export ROSA_CLUSTER_ID=$(rosa describe cluster -c $ROSA_CLUSTER_NAME --output json | jq -r .id) export REGION=us-east-2 export OIDC_ENDPOINT=$(oc get authentication.config.openshift.io cluster -o json | jq .spec.serviceAccountIssuer) export AWS_ACCOUNT_ID=`aws sts get-caller-identity --query Account --output text` export AWS_PAGER=\"\" Deploy the AWS Secrets and Configuration Provider Use Helm to register the secrets store csi driver bash helm repo add secrets-store-csi-driver https://kubernetes-sigs.github.io/secrets-store-csi-driver/charts Update your Helm Repositories bash helm repo update Install the secrets store csi driver bash helm upgrade --install -n csi-secrets-store csi-secrets-store-driver secrets-store-csi-driver/secrets-store-csi-driver Deploy the AWS provider bash kubectl -n csi-secrets-store apply -f \\ https://raw.githubusercontent.com/rh-mobb/documentation/main/docs/security/secrets-store-csi/aws-provider-installer.yaml Check that both Daemonsets are running bash kubectl -n csi-secrets-store get ds \\ csi-secrets-store-provider-aws \\ csi-secrets-store-driver-secrets-store-csi-driver Creating a Secret and IAM Access Policies Create a secret in Secrets Manager ```bash SECRET_ARN=$(aws --region \"$REGION\" secretsmanager create-secret \\ --name MySecret --secret-string \\ '{\"username\":\"shadowman\", \"password\":\"hunter2\"}' \\ --query ARN --output text) echo $SECRET_ARN ``` Create IAM Access Policy document bash cat << EOF > policy.json { \"Version\": \"2012-10-17\", \"Statement\": [{ \"Effect\": \"Allow\", \"Action\": [ \"secretsmanager:GetSecretValue\", \"secretsmanager:DescribeSecret\" ], \"Resource\": [\"$SECRET_ARN\"] }] } EOF Create an IAM Access Policy bash POLICY_ARN=$(aws --region \"$REGION\" --query Policy.Arn \\ --output text iam create-policy \\ --policy-name openshift-access-to-mysecret-policy \\ --policy-document file://policy.json) echo $POLICY_ARN Create IAM Role trust policy document Note you can use Conditions to lock down to a specific namespace or service account here. But for simplicity we're keeping it open. bash cat <<EOF > trust-policy.json { \"Version\": \"2012-10-17\", \"Statement\": [ { \"Effect\": \"Allow\", \"Principal\": { \"Federated\": \"arn:aws:iam::$AWS_ACCOUNT_ID:oidc-provider/rh-oidc.s3.us-east-1.amazonaws.com/$ROSA_CLUSTER_ID\" }, \"Action\": \"sts:AssumeRoleWithWebIdentity\" } ] } EOF Create IAM Role bash ROLE_ARN=$(aws iam create-role --role-name openshift-access-to-mysecret \\ --assume-role-policy-document file://trust-policy.json \\ --query Role.Arn --output text) echo $ROLE_ARN Attach Role to the Policy bash aws iam attach-role-policy --role-name openshift-access-to-mysecret --policy-arn $POLICY_ARN Create an Application to use this secret Create an OpenShift project bash oc new-project my-application Annotate the default service account to use the STS Role bash oc annotate -n my-application serviceaccount default \\ eks.amazonaws.com/role-arn=$ROLE_ARN Create a secret provider class to access our secret bash cat << EOF | kubectl apply -f - apiVersion: secrets-store.csi.x-k8s.io/v1 kind: SecretProviderClass metadata: name: my-application-aws-secrets spec: provider: aws parameters: objects: | - objectName: \"MySecret\" objectType: \"secretsmanager\" EOF Create a Deployment using our secret bash cat << EOF | kubectl apply -f - apiVersion: v1 kind: Pod metadata: name: my-application labels: app: my-application spec: volumes: - name: secrets-store-inline csi: driver: secrets-store.csi.k8s.io readOnly: true volumeAttributes: secretProviderClass: \"my-application-aws-secrets\" containers: - name: my-application-deployment image: k8s.gcr.io/e2e-test-images/busybox:1.29 command: - \"/bin/sleep\" - \"10000\" volumeMounts: - name: secrets-store-inline mountPath: \"/mnt/secrets-store\" readOnly: true EOF Verify the Pod has the secret mounted bash kubectl exec -it my-application -- cat /mnt/secrets-store/MySecret Cleanup Delete application bash oc delete project my-application Delete the secrets store csi driver bash helm delete -n kube-system csi-secrets-store Delete the AWS provider bash kubectl -n kube-system delete -f \\ https://raw.githubusercontent.com/aws/secrets-store-csi-driver-provider-aws/main/deployment/aws-provider-installer.yaml Delete Security Context Constraints bash oc adm policy remove-scc-from-user privileged \\ system:serviceaccount:kube-system:secrets-store-csi-driver oc adm policy remove-scc-from-user privileged \\ system:serviceaccount:kube-system:csi-secrets-store-provider-aws Delete AWS Roles and Policies bash aws iam detach-role-policy --role-name openshift-access-to-mysecret --policy-arn $POLICY_ARN aws iam delete-role --role-name openshift-access-to-mysecret aws iam delete-policy --policy-arn $POLICY_ARN","title":"AWS Secrets CSI w/ ROSA STS"},{"location":"rosa/aws-secrets-manager-csi/#using-aws-secrets-manager-csi-on-red-hat-openshift-on-aws-with-sts","text":"Author Paul Czarkowski last modified 2021-08-17 The AWS Secrets and Configuration Provider (ASCP) provides a way to expose AWS Secrets as Kubernetes storage volumes. With the ASCP, you can store and manage your secrets in Secrets Manager and then retrieve them through your workloads running on ROSA or OSD. This is made even easier / more secure through the use of AWS STS and Kubernetes PodIdentity.","title":"Using AWS Secrets Manager CSI on Red Hat OpenShift on AWS with STS"},{"location":"rosa/aws-secrets-manager-csi/#prerequisites","text":"A ROSA cluster deployed with STS Helm 3 aws CLI jq","title":"Prerequisites"},{"location":"rosa/aws-secrets-manager-csi/#preparing-environment","text":"Validate that your cluster has STS bash oc get authentication.config.openshift.io cluster -o json \\ | jq .spec.serviceAccountIssuer You should see something like the following, if not you should not proceed, instead look to the Red Hat documentation on creating an STS cluster . \"https://rh-oidc.s3.us-east-1.amazonaws.com/xxxxxx\" Set SecurityContextConstraints to allow the CSI driver to run bash oc new-project csi-secrets-store oc adm policy add-scc-to-user privileged \\ system:serviceaccount:csi-secrets-store:secrets-store-csi-driver oc adm policy add-scc-to-user privileged \\ system:serviceaccount:csi-secrets-store:csi-secrets-store-provider-aws Create some environment variables to refer to later bash export ROSA_CLUSTER_NAME=my-cluster export ROSA_CLUSTER_ID=$(rosa describe cluster -c $ROSA_CLUSTER_NAME --output json | jq -r .id) export REGION=us-east-2 export OIDC_ENDPOINT=$(oc get authentication.config.openshift.io cluster -o json | jq .spec.serviceAccountIssuer) export AWS_ACCOUNT_ID=`aws sts get-caller-identity --query Account --output text` export AWS_PAGER=\"\"","title":"Preparing Environment"},{"location":"rosa/aws-secrets-manager-csi/#deploy-the-aws-secrets-and-configuration-provider","text":"Use Helm to register the secrets store csi driver bash helm repo add secrets-store-csi-driver https://kubernetes-sigs.github.io/secrets-store-csi-driver/charts Update your Helm Repositories bash helm repo update Install the secrets store csi driver bash helm upgrade --install -n csi-secrets-store csi-secrets-store-driver secrets-store-csi-driver/secrets-store-csi-driver Deploy the AWS provider bash kubectl -n csi-secrets-store apply -f \\ https://raw.githubusercontent.com/rh-mobb/documentation/main/docs/security/secrets-store-csi/aws-provider-installer.yaml Check that both Daemonsets are running bash kubectl -n csi-secrets-store get ds \\ csi-secrets-store-provider-aws \\ csi-secrets-store-driver-secrets-store-csi-driver","title":"Deploy the AWS Secrets and Configuration Provider"},{"location":"rosa/aws-secrets-manager-csi/#creating-a-secret-and-iam-access-policies","text":"Create a secret in Secrets Manager ```bash SECRET_ARN=$(aws --region \"$REGION\" secretsmanager create-secret \\ --name MySecret --secret-string \\ '{\"username\":\"shadowman\", \"password\":\"hunter2\"}' \\ --query ARN --output text) echo $SECRET_ARN ``` Create IAM Access Policy document bash cat << EOF > policy.json { \"Version\": \"2012-10-17\", \"Statement\": [{ \"Effect\": \"Allow\", \"Action\": [ \"secretsmanager:GetSecretValue\", \"secretsmanager:DescribeSecret\" ], \"Resource\": [\"$SECRET_ARN\"] }] } EOF Create an IAM Access Policy bash POLICY_ARN=$(aws --region \"$REGION\" --query Policy.Arn \\ --output text iam create-policy \\ --policy-name openshift-access-to-mysecret-policy \\ --policy-document file://policy.json) echo $POLICY_ARN Create IAM Role trust policy document Note you can use Conditions to lock down to a specific namespace or service account here. But for simplicity we're keeping it open. bash cat <<EOF > trust-policy.json { \"Version\": \"2012-10-17\", \"Statement\": [ { \"Effect\": \"Allow\", \"Principal\": { \"Federated\": \"arn:aws:iam::$AWS_ACCOUNT_ID:oidc-provider/rh-oidc.s3.us-east-1.amazonaws.com/$ROSA_CLUSTER_ID\" }, \"Action\": \"sts:AssumeRoleWithWebIdentity\" } ] } EOF Create IAM Role bash ROLE_ARN=$(aws iam create-role --role-name openshift-access-to-mysecret \\ --assume-role-policy-document file://trust-policy.json \\ --query Role.Arn --output text) echo $ROLE_ARN Attach Role to the Policy bash aws iam attach-role-policy --role-name openshift-access-to-mysecret --policy-arn $POLICY_ARN","title":"Creating a Secret and IAM Access Policies"},{"location":"rosa/aws-secrets-manager-csi/#create-an-application-to-use-this-secret","text":"Create an OpenShift project bash oc new-project my-application Annotate the default service account to use the STS Role bash oc annotate -n my-application serviceaccount default \\ eks.amazonaws.com/role-arn=$ROLE_ARN Create a secret provider class to access our secret bash cat << EOF | kubectl apply -f - apiVersion: secrets-store.csi.x-k8s.io/v1 kind: SecretProviderClass metadata: name: my-application-aws-secrets spec: provider: aws parameters: objects: | - objectName: \"MySecret\" objectType: \"secretsmanager\" EOF Create a Deployment using our secret bash cat << EOF | kubectl apply -f - apiVersion: v1 kind: Pod metadata: name: my-application labels: app: my-application spec: volumes: - name: secrets-store-inline csi: driver: secrets-store.csi.k8s.io readOnly: true volumeAttributes: secretProviderClass: \"my-application-aws-secrets\" containers: - name: my-application-deployment image: k8s.gcr.io/e2e-test-images/busybox:1.29 command: - \"/bin/sleep\" - \"10000\" volumeMounts: - name: secrets-store-inline mountPath: \"/mnt/secrets-store\" readOnly: true EOF Verify the Pod has the secret mounted bash kubectl exec -it my-application -- cat /mnt/secrets-store/MySecret","title":"Create an Application to use this secret"},{"location":"rosa/aws-secrets-manager-csi/#cleanup","text":"Delete application bash oc delete project my-application Delete the secrets store csi driver bash helm delete -n kube-system csi-secrets-store Delete the AWS provider bash kubectl -n kube-system delete -f \\ https://raw.githubusercontent.com/aws/secrets-store-csi-driver-provider-aws/main/deployment/aws-provider-installer.yaml Delete Security Context Constraints bash oc adm policy remove-scc-from-user privileged \\ system:serviceaccount:kube-system:secrets-store-csi-driver oc adm policy remove-scc-from-user privileged \\ system:serviceaccount:kube-system:csi-secrets-store-provider-aws Delete AWS Roles and Policies bash aws iam detach-role-policy --role-name openshift-access-to-mysecret --policy-arn $POLICY_ARN aws iam delete-role --role-name openshift-access-to-mysecret aws iam delete-policy --policy-arn $POLICY_ARN","title":"Cleanup"},{"location":"rosa/byo-vpc/","text":"Creating a Public/Private BYO VPC for ROSA This is example Terraform to create a single AZ VPC in which to deploy a single AZ ROSA cluster. This is intended to be used as a guide to get started quickly, not to be used in production. Pre-Requisites Terraform Deploy Download this repo bash git clone https://github.com/rh-mobb/documentation.git cd documentation/docs/rosa/byo-vpc Modify main.tf as needed, then run bash terraform init terraform plan terraform apply Cleanup To destroy resources bash terraform destroy","title":"Index"},{"location":"rosa/byo-vpc/#creating-a-publicprivate-byo-vpc-for-rosa","text":"This is example Terraform to create a single AZ VPC in which to deploy a single AZ ROSA cluster. This is intended to be used as a guide to get started quickly, not to be used in production.","title":"Creating a Public/Private BYO VPC for ROSA"},{"location":"rosa/byo-vpc/#pre-requisites","text":"Terraform","title":"Pre-Requisites"},{"location":"rosa/byo-vpc/#deploy","text":"Download this repo bash git clone https://github.com/rh-mobb/documentation.git cd documentation/docs/rosa/byo-vpc Modify main.tf as needed, then run bash terraform init terraform plan terraform apply","title":"Deploy"},{"location":"rosa/byo-vpc/#cleanup","text":"To destroy resources bash terraform destroy","title":"Cleanup"},{"location":"rosa/clf-cloudwatch-sts/","text":"Configuring the Cluster Log Forwarder for CloudWatch Logs and STS DRAFT Author: Paul Czarkowski last edited: 2022-08-31 This guide shows how to deploy the Cluster Log Forwarder operator and configure it to use STS authentication to forward logs to CloudWatch. Prerequisites A ROSA cluster (configured with STS) The jq cli command The aws cli command Environment Setup Configure the following environment variables Change the cluster name to match your ROSA cluster and ensure you're logged into the cluster as an Administrator. Ensure all fields are outputted correctly before moving on. bash export ROSA_CLUSTER_NAME=my-cluster export ROSA_CLUSTER_ID=$(rosa describe cluster -c ${ROSA_CLUSTER_NAME} --output json | jq -r .id) export REGION=$(rosa describe cluster -c ${ROSA_CLUSTER_NAME} --output json | jq -r .region.id) export OIDC_ENDPOINT=$(oc get authentication.config.openshift.io cluster -o json | jq -r .spec.serviceAccountIssuer) export AWS_ACCOUNT_ID=`aws sts get-caller-identity --query Account --output text` export AWS_PAGER=\"\" export SCRATCH=\"/tmp/${ROSA_CLUSTER_NAME}/clf-cloudwatch-sts\" mkdir -p ${SCRATCH} echo \"Cluster ID: ${ROSA_CLUSTER_ID}, Region: ${REGION}, OIDC Endpoint: ${OIDC_ENDPOINT}, AWS Account ID: ${AWS_ACCOUNT_ID}\" Prepare AWS Account Create an IAM Policy for OpenShift Log Forwarding bash POLICY_ARN=$(aws iam list-policies --query \"Policies[?PolicyName=='RosaCloudWatch'].{ARN:Arn}\" --output text) if [[ -z \"${POLICY_ARN}\" ]]; then cat << EOF > ${SCRATCH}/policy.json { \"Version\": \"2012-10-17\", \"Statement\": [ { \"Effect\": \"Allow\", \"Action\": [ \"logs:CreateLogGroup\", \"logs:CreateLogStream\", \"logs:DescribeLogGroups\", \"logs:DescribeLogStreams\", \"logs:PutLogEvents\", \"logs:PutRetentionPolicy\" ], \"Resource\": \"arn:aws:logs:*:*:*\" } ] } EOF POLICY_ARN=$(aws iam create-policy --policy-name \"RosaCloudWatch\" \\ --policy-document file:///${SCRATCH}/policy.json --query Policy.Arn --output text) fi echo ${POLICY_ARN} Create an IAM Role trust policy for the cluster bash cat <<EOF > ${SCRATCH}/trust-policy.json { \"Version\": \"2012-10-17\", \"Statement\": [{ \"Effect\": \"Allow\", \"Principal\": { \"Federated\": \"arn:aws:iam::${AWS_ACCOUNT_ID}:oidc-provider/rh-oidc.s3.us-east-1.amazonaws.com/${ROSA_CLUSTER_ID}\" }, \"Action\": \"sts:AssumeRoleWithWebIdentity\", \"Condition\": { \"StringEquals\": { \"rh-oidc.s3.us-east-1.amazonaws.com/${ROSA_CLUSTER_ID}:sub\": \"system:serviceaccount:openshift-logging:logcollector\" } } }] } EOF ROLE_ARN=$(aws iam create-role --role-name \"${ROSA_CLUSTER_NAME}-RosaCloudWatch\" \\ --assume-role-policy-document file://${SCRATCH}/trust-policy.json \\ --query Role.Arn --output text) echo ${ROLE_ARN} Attach the IAM Policy to the IAM Role bash aws iam attach-role-policy --role-name \"${ROSA_CLUSTER_NAME}-RosaCloudWatch\" \\ --policy-arn ${POLICY_ARN} Deploy Operators Deploy the Cluster Logging operator bash cat << EOF | oc apply -f - apiVersion: operators.coreos.com/v1alpha1 kind: Subscription metadata: labels: operators.coreos.com/cluster-logging.openshift-logging: \"\" name: cluster-logging namespace: openshift-logging spec: channel: stable installPlanApproval: Automatic name: cluster-logging source: redhat-operators sourceNamespace: openshift-marketplace startingCSV: cluster-logging.5.5.0 EOF Deploy the Elasticsearch operator Note: This is only needed for CRDs and won't actually deploy a Elasticsearch cluster. Create a secret bash cat << EOF | oc apply -f - apiVersion: v1 kind: Secret metadata: name: cloudwatch-credentials namespace: openshift-logging stringData: role_arn: $ROLE_ARN EOF Configure Cluster Logging Create a cluster log forwarding resource bash cat << EOF | oc apply -f - apiVersion: \"logging.openshift.io/v1\" kind: ClusterLogForwarder metadata: name: instance namespace: openshift-logging spec: outputs: - name: cw type: cloudwatch cloudwatch: groupBy: namespaceName groupPrefix: rosa-${ROSA_CLUSTER_NAME} region: ${REGION} secret: name: cloudwatch-credentials pipelines: - name: to-cloudwatch inputRefs: - infrastructure - audit - application outputRefs: - cw EOF Create a cluster logging resource bash cat << EOF | oc apply -f - apiVersion: logging.openshift.io/v1 kind: ClusterLogging metadata: name: instance namespace: openshift-logging spec: collection: type: fluentd forwarder: managementState: Managed EOF Check AWS CloudWatch for logs Use the AWS console or CLI to validate that there are log streams from the cluster Note: If this is a fresh cluster you may not see a log group for application logs as there are no applications running yet. bash aws logs describe-log-groups --log-group-name-prefix rosa-${ROSA_CLUSTER_NAME} { \"logGroups\": [ { \"logGroupName\": \"rosa-xxxx.audit\", \"creationTime\": 1661286368369, \"metricFilterCount\": 0, \"arn\": \"arn:aws:logs:us-east-2:xxxx:log-group:rosa-xxxx.audit:*\", \"storedBytes\": 0 }, { \"logGroupName\": \"rosa-xxxx.infrastructure\", \"creationTime\": 1661286369821, \"metricFilterCount\": 0, \"arn\": \"arn:aws:logs:us-east-2:xxxx:log-group:rosa-xxxx.infrastructure:*\", \"storedBytes\": 0 } ] } Cleanup Delete the Cluster Log Forwarding resource bash oc delete -n openshift-logging clusterlogforwarder instance Delete the Cluster Logging resource bash oc delete -n openshift-logging clusterlogging instance Detach the IAM Policy to the IAM Role bash aws iam detach-role-policy --role-name \"${ROSA_CLUSTER_NAME}-RosaCloudWatch\" \\ --policy-arn \"${POLICY_ARN}\" Delete the IAM Role bash aws iam delete-role --role-name \"${ROSA_CLUSTER_NAME}-RosaCloudWatch\" Delete the IAM Policy Only run this command if there are no other resources using the Policy bash aws iam delete-policy --policy-arn \"${POLICY_ARN}\" Delete the CloudWatch Log Groups bash aws logs delete-log-group --log-group-name \"rosa-${ROSA_CLUSTER_NAME}.audit\" aws logs delete-log-group --log-group-name \"rosa-${ROSA_CLUSTER_NAME}.infrastructure\"","title":"Configuring the Cluster Log Forwarder for CloudWatch Logs and STS"},{"location":"rosa/clf-cloudwatch-sts/#configuring-the-cluster-log-forwarder-for-cloudwatch-logs-and-sts","text":"DRAFT Author: Paul Czarkowski last edited: 2022-08-31 This guide shows how to deploy the Cluster Log Forwarder operator and configure it to use STS authentication to forward logs to CloudWatch.","title":"Configuring the Cluster Log Forwarder for CloudWatch Logs and STS"},{"location":"rosa/clf-cloudwatch-sts/#prerequisites","text":"A ROSA cluster (configured with STS) The jq cli command The aws cli command","title":"Prerequisites"},{"location":"rosa/clf-cloudwatch-sts/#environment-setup","text":"Configure the following environment variables Change the cluster name to match your ROSA cluster and ensure you're logged into the cluster as an Administrator. Ensure all fields are outputted correctly before moving on. bash export ROSA_CLUSTER_NAME=my-cluster export ROSA_CLUSTER_ID=$(rosa describe cluster -c ${ROSA_CLUSTER_NAME} --output json | jq -r .id) export REGION=$(rosa describe cluster -c ${ROSA_CLUSTER_NAME} --output json | jq -r .region.id) export OIDC_ENDPOINT=$(oc get authentication.config.openshift.io cluster -o json | jq -r .spec.serviceAccountIssuer) export AWS_ACCOUNT_ID=`aws sts get-caller-identity --query Account --output text` export AWS_PAGER=\"\" export SCRATCH=\"/tmp/${ROSA_CLUSTER_NAME}/clf-cloudwatch-sts\" mkdir -p ${SCRATCH} echo \"Cluster ID: ${ROSA_CLUSTER_ID}, Region: ${REGION}, OIDC Endpoint: ${OIDC_ENDPOINT}, AWS Account ID: ${AWS_ACCOUNT_ID}\"","title":"Environment Setup"},{"location":"rosa/clf-cloudwatch-sts/#prepare-aws-account","text":"Create an IAM Policy for OpenShift Log Forwarding bash POLICY_ARN=$(aws iam list-policies --query \"Policies[?PolicyName=='RosaCloudWatch'].{ARN:Arn}\" --output text) if [[ -z \"${POLICY_ARN}\" ]]; then cat << EOF > ${SCRATCH}/policy.json { \"Version\": \"2012-10-17\", \"Statement\": [ { \"Effect\": \"Allow\", \"Action\": [ \"logs:CreateLogGroup\", \"logs:CreateLogStream\", \"logs:DescribeLogGroups\", \"logs:DescribeLogStreams\", \"logs:PutLogEvents\", \"logs:PutRetentionPolicy\" ], \"Resource\": \"arn:aws:logs:*:*:*\" } ] } EOF POLICY_ARN=$(aws iam create-policy --policy-name \"RosaCloudWatch\" \\ --policy-document file:///${SCRATCH}/policy.json --query Policy.Arn --output text) fi echo ${POLICY_ARN} Create an IAM Role trust policy for the cluster bash cat <<EOF > ${SCRATCH}/trust-policy.json { \"Version\": \"2012-10-17\", \"Statement\": [{ \"Effect\": \"Allow\", \"Principal\": { \"Federated\": \"arn:aws:iam::${AWS_ACCOUNT_ID}:oidc-provider/rh-oidc.s3.us-east-1.amazonaws.com/${ROSA_CLUSTER_ID}\" }, \"Action\": \"sts:AssumeRoleWithWebIdentity\", \"Condition\": { \"StringEquals\": { \"rh-oidc.s3.us-east-1.amazonaws.com/${ROSA_CLUSTER_ID}:sub\": \"system:serviceaccount:openshift-logging:logcollector\" } } }] } EOF ROLE_ARN=$(aws iam create-role --role-name \"${ROSA_CLUSTER_NAME}-RosaCloudWatch\" \\ --assume-role-policy-document file://${SCRATCH}/trust-policy.json \\ --query Role.Arn --output text) echo ${ROLE_ARN} Attach the IAM Policy to the IAM Role bash aws iam attach-role-policy --role-name \"${ROSA_CLUSTER_NAME}-RosaCloudWatch\" \\ --policy-arn ${POLICY_ARN}","title":"Prepare AWS Account"},{"location":"rosa/clf-cloudwatch-sts/#deploy-operators","text":"Deploy the Cluster Logging operator bash cat << EOF | oc apply -f - apiVersion: operators.coreos.com/v1alpha1 kind: Subscription metadata: labels: operators.coreos.com/cluster-logging.openshift-logging: \"\" name: cluster-logging namespace: openshift-logging spec: channel: stable installPlanApproval: Automatic name: cluster-logging source: redhat-operators sourceNamespace: openshift-marketplace startingCSV: cluster-logging.5.5.0 EOF Deploy the Elasticsearch operator Note: This is only needed for CRDs and won't actually deploy a Elasticsearch cluster. Create a secret bash cat << EOF | oc apply -f - apiVersion: v1 kind: Secret metadata: name: cloudwatch-credentials namespace: openshift-logging stringData: role_arn: $ROLE_ARN EOF","title":"Deploy Operators"},{"location":"rosa/clf-cloudwatch-sts/#configure-cluster-logging","text":"Create a cluster log forwarding resource bash cat << EOF | oc apply -f - apiVersion: \"logging.openshift.io/v1\" kind: ClusterLogForwarder metadata: name: instance namespace: openshift-logging spec: outputs: - name: cw type: cloudwatch cloudwatch: groupBy: namespaceName groupPrefix: rosa-${ROSA_CLUSTER_NAME} region: ${REGION} secret: name: cloudwatch-credentials pipelines: - name: to-cloudwatch inputRefs: - infrastructure - audit - application outputRefs: - cw EOF Create a cluster logging resource bash cat << EOF | oc apply -f - apiVersion: logging.openshift.io/v1 kind: ClusterLogging metadata: name: instance namespace: openshift-logging spec: collection: type: fluentd forwarder: managementState: Managed EOF","title":"Configure Cluster Logging"},{"location":"rosa/clf-cloudwatch-sts/#check-aws-cloudwatch-for-logs","text":"Use the AWS console or CLI to validate that there are log streams from the cluster Note: If this is a fresh cluster you may not see a log group for application logs as there are no applications running yet. bash aws logs describe-log-groups --log-group-name-prefix rosa-${ROSA_CLUSTER_NAME} { \"logGroups\": [ { \"logGroupName\": \"rosa-xxxx.audit\", \"creationTime\": 1661286368369, \"metricFilterCount\": 0, \"arn\": \"arn:aws:logs:us-east-2:xxxx:log-group:rosa-xxxx.audit:*\", \"storedBytes\": 0 }, { \"logGroupName\": \"rosa-xxxx.infrastructure\", \"creationTime\": 1661286369821, \"metricFilterCount\": 0, \"arn\": \"arn:aws:logs:us-east-2:xxxx:log-group:rosa-xxxx.infrastructure:*\", \"storedBytes\": 0 } ] }","title":"Check AWS CloudWatch for logs"},{"location":"rosa/clf-cloudwatch-sts/#cleanup","text":"Delete the Cluster Log Forwarding resource bash oc delete -n openshift-logging clusterlogforwarder instance Delete the Cluster Logging resource bash oc delete -n openshift-logging clusterlogging instance Detach the IAM Policy to the IAM Role bash aws iam detach-role-policy --role-name \"${ROSA_CLUSTER_NAME}-RosaCloudWatch\" \\ --policy-arn \"${POLICY_ARN}\" Delete the IAM Role bash aws iam delete-role --role-name \"${ROSA_CLUSTER_NAME}-RosaCloudWatch\" Delete the IAM Policy Only run this command if there are no other resources using the Policy bash aws iam delete-policy --policy-arn \"${POLICY_ARN}\" Delete the CloudWatch Log Groups bash aws logs delete-log-group --log-group-name \"rosa-${ROSA_CLUSTER_NAME}.audit\" aws logs delete-log-group --log-group-name \"rosa-${ROSA_CLUSTER_NAME}.infrastructure\"","title":"Cleanup"},{"location":"rosa/cluster-metrics-to-aws-prometheus/","text":"ROSA - Federating Metrics to AWS Prometheus Federating Metrics from ROSA/OSD is a bit tricky as the cluster metrics require pulling from its /federated endpoint while the user workload metrics require using the prometheus remoteWrite configuration. This guide will walk you through using the MOBB Helm Chart to deploy the necessary agents to federate the metrics into AWS Prometheus and then use Grafana to visualize those metrics. As a bonus it will set up a CloudWatch datasource to view any metrics or logs you have in Cloud Watch. Make sure to use a region where Amazon Prometheus service is supported Prerequisites A ROSA cluster deployed with STS aws CLI jq Set up environment Create environment variables bash export CLUSTER=my-cluster export REGION=us-east-2 export PROM_NAMESPACE=custom-metrics export PROM_SA=aws-prometheus-proxy export SCRATCH_DIR=/tmp/scratch export OIDC_PROVIDER=$(oc get authentication.config.openshift.io cluster -o json | jq -r .spec.serviceAccountIssuer| sed -e \"s/^https:\\/\\///\") export AWS_ACCOUNT_ID=$(aws sts get-caller-identity --query Account --output text) export AWS_PAGER=\"\" mkdir -p $SCRATCH_DIR Create namespace bash oc new-project $PROM_NAMESPACE Deploy Operators Add the MOBB chart repository to your Helm bash helm repo add mobb https://rh-mobb.github.io/helm-charts/ Update your repositories bash helm repo update Use the mobb/operatorhub chart to deploy the needed operators bash helm upgrade -n $PROM_NAMESPACE custom-metrics-operators \\ mobb/operatorhub --version 0.1.1 --install \\ --values https://raw.githubusercontent.com/rh-mobb/helm-charts/main/charts/rosa-aws-prometheus/files/operatorhub.yaml Deploy and Configure the AWS Sigv4 Proxy and the Grafana Agent Create a Policy for access to AWS Prometheus bash cat <<EOF > $SCRATCH_DIR/PermissionPolicyIngest.json { \"Version\": \"2012-10-17\", \"Statement\": [ {\"Effect\": \"Allow\", \"Action\": [ \"aps:RemoteWrite\", \"aps:GetSeries\", \"aps:GetLabels\", \"aps:GetMetricMetadata\" ], \"Resource\": \"*\" } ] } EOF Apply the Policy bash PROM_POLICY=$(aws iam create-policy --policy-name $PROM_SA-prom \\ --policy-document file://$SCRATCH_DIR/PermissionPolicyIngest.json \\ --query 'Policy.Arn' --output text) echo $PROM_POLICY 1. Create a Policy for access to AWS CloudWatch bash cat <<EOF > $SCRATCH_DIR/PermissionPolicyCloudWatch.json { \"Version\": \"2012-10-17\", \"Statement\": [ { \"Sid\": \"AllowReadingMetricsFromCloudWatch\", \"Effect\": \"Allow\", \"Action\": [ \"cloudwatch:DescribeAlarmsForMetric\", \"cloudwatch:DescribeAlarmHistory\", \"cloudwatch:DescribeAlarms\", \"cloudwatch:ListMetrics\", \"cloudwatch:GetMetricStatistics\", \"cloudwatch:GetMetricData\" ], \"Resource\": \"*\" }, { \"Sid\": \"AllowReadingLogsFromCloudWatch\", \"Effect\": \"Allow\", \"Action\": [ \"logs:DescribeLogGroups\", \"logs:GetLogGroupFields\", \"logs:StartQuery\", \"logs:StopQuery\", \"logs:GetQueryResults\", \"logs:GetLogEvents\" ], \"Resource\": \"*\" }, { \"Sid\": \"AllowReadingTagsInstancesRegionsFromEC2\", \"Effect\": \"Allow\", \"Action\": [ \"ec2:DescribeTags\", \"ec2:DescribeInstances\", \"ec2:DescribeRegions\" ], \"Resource\": \"*\" }, { \"Sid\": \"AllowReadingResourcesForTags\", \"Effect\": \"Allow\", \"Action\": \"tag:GetResources\", \"Resource\": \"*\" } ] } EOF Apply the Policy bash CW_POLICY=$(aws iam create-policy --policy-name $PROM_SA-cw \\ --policy-document file://$SCRATCH_DIR/PermissionPolicyCloudWatch.json \\ --query 'Policy.Arn' --output text) echo $CW_POLICY Create a Trust Policy bash cat <<EOF > $SCRATCH_DIR/TrustPolicy.json { \"Version\": \"2012-10-17\", \"Statement\": [ { \"Effect\": \"Allow\", \"Principal\": { \"Federated\": \"arn:aws:iam::${AWS_ACCOUNT_ID}:oidc-provider/${OIDC_PROVIDER}\" }, \"Action\": \"sts:AssumeRoleWithWebIdentity\", \"Condition\": { \"StringEquals\": { \"${OIDC_PROVIDER}:sub\": [ \"system:serviceaccount:${PROM_NAMESPACE}:${PROM_SA}\", \"system:serviceaccount:${PROM_NAMESPACE}:grafana-serviceaccount\" ] } } } ] } EOF Create Role for AWS Prometheus and CloudWatch bash PROM_ROLE=$(aws iam create-role \\ --role-name \"prometheus-$CLUSTER\" \\ --assume-role-policy-document file://$SCRATCH_DIR/TrustPolicy.json \\ --query \"Role.Arn\" --output text) echo $PROM_ROLE Attach the Policies to the Role ```bash aws iam attach-role-policy \\ --role-name \"prometheus-$CLUSTER\" \\ --policy-arn arn:aws:iam::aws:policy/AmazonPrometheusQueryAccess aws iam attach-role-policy \\ --role-name \"prometheus-$CLUSTER\" \\ --policy-arn $CW_POLICY ``` Create an AWS Prometheus Workspace bash PROM_WS=$(aws amp create-workspace --alias $CLUSTER \\ --query \"workspaceId\" --output text) echo $PROM_WS Deploy AWS Prometheus Proxy Helm Chart bash helm upgrade --install -n $PROM_NAMESPACE --set \"aws.region=$REGION\" \\ --set \"aws.roleArn=$PROM_ROLE\" --set \"fullnameOverride=$PROM_SA\" \\ --set \"aws.workspaceId=$PROM_WS\" \\ --set \"grafana-cr.serviceAccountAnnotations.eks\\.amazonaws\\.com/role-arn=$PROM_ROLE\" \\ aws-prometheus-proxy mobb/rosa-aws-prometheus Configure remoteWrite for user workloads bash cat << EOF | kubectl apply -f - apiVersion: v1 kind: ConfigMap metadata: name: user-workload-monitoring-config namespace: openshift-user-workload-monitoring data: config.yaml: | prometheus: remoteWrite: - url: \"http://aws-prometheus-proxy.$PROM_NAMESPACE.svc.cluster.local:8005/workspaces/$PROM_WS/api/v1/remote_write\" EOF Verify Metrics are being collected Access Grafana and check for metrics bash oc get route -n custom-metrics grafana-route -o jsonpath='{.status.ingress[0].host}' Browse to the URL provided in the above command and log in with your OpenShift Credentials Enable Admin by hitting sign in and user admin and password Browse to /datasources and verify that cloudwatch and prometheus are present If not, you may have hit a race condition that can be fixed by running the following then trying again bash kubectl delete grafanadatasources.integreatly.org aws-prometheus-proxy-prometheus helm upgrade --install -n $PROM_NAMESPACE --set \"aws.region=$REGION\" \\ --set \"aws.roleArn=$PROM_ROLE\" --set \"fullnameOverride=$PROM_SA\" \\ --set \"aws.workspaceId=$PROM_WS\" \\ --set \"grafana-cr.serviceAccountAnnotations.eks\\.amazonaws\\.com/role-arn=$PROM_ROLE\" \\ aws-prometheus-proxy mobb/rosa-aws-prometheus Browse to /dashboards and select the custom-metrics -> NodeExporter / Use Method / Cluster dashboard Cleanup Delete the aws-prometheus-proxy Helm Release bash helm delete -n custom-metrics aws-prometheus-proxy Delete the custom-metrics-operators Helm Release bash helm delete -n custom-metrics custom-metrics-operators Delete the custom-metrics namespace bash kubectl delete namespace custom-metrics Detach AWS Role Policies ```bash aws iam detach-role-policy \\ --role-name \"prometheus-$CLUSTER\" \\ --policy-arn arn:aws:iam::aws:policy/AmazonPrometheusQueryAccess aws iam detach-role-policy \\ --role-name \"prometheus-$CLUSTER\" \\ --policy-arn $CW_POLICY ``` Delete the custom Cloud Watch Policy bash aws iam delete-policy --policy-arn $CW_POLICY Delete the AWS Prometheus Role bash aws iam delete-role --role-name \"prometheus-$CLUSTER\" Delete AWS Prometheus Workspace bash aws amp delete-workspace --workspace-id $PROM_WS","title":"ROSA - Federating Metrics to AWS Prometheus"},{"location":"rosa/cluster-metrics-to-aws-prometheus/#rosa-federating-metrics-to-aws-prometheus","text":"Federating Metrics from ROSA/OSD is a bit tricky as the cluster metrics require pulling from its /federated endpoint while the user workload metrics require using the prometheus remoteWrite configuration. This guide will walk you through using the MOBB Helm Chart to deploy the necessary agents to federate the metrics into AWS Prometheus and then use Grafana to visualize those metrics. As a bonus it will set up a CloudWatch datasource to view any metrics or logs you have in Cloud Watch. Make sure to use a region where Amazon Prometheus service is supported","title":"ROSA - Federating Metrics to AWS Prometheus"},{"location":"rosa/cluster-metrics-to-aws-prometheus/#prerequisites","text":"A ROSA cluster deployed with STS aws CLI jq","title":"Prerequisites"},{"location":"rosa/cluster-metrics-to-aws-prometheus/#set-up-environment","text":"Create environment variables bash export CLUSTER=my-cluster export REGION=us-east-2 export PROM_NAMESPACE=custom-metrics export PROM_SA=aws-prometheus-proxy export SCRATCH_DIR=/tmp/scratch export OIDC_PROVIDER=$(oc get authentication.config.openshift.io cluster -o json | jq -r .spec.serviceAccountIssuer| sed -e \"s/^https:\\/\\///\") export AWS_ACCOUNT_ID=$(aws sts get-caller-identity --query Account --output text) export AWS_PAGER=\"\" mkdir -p $SCRATCH_DIR Create namespace bash oc new-project $PROM_NAMESPACE","title":"Set up environment"},{"location":"rosa/cluster-metrics-to-aws-prometheus/#deploy-operators","text":"Add the MOBB chart repository to your Helm bash helm repo add mobb https://rh-mobb.github.io/helm-charts/ Update your repositories bash helm repo update Use the mobb/operatorhub chart to deploy the needed operators bash helm upgrade -n $PROM_NAMESPACE custom-metrics-operators \\ mobb/operatorhub --version 0.1.1 --install \\ --values https://raw.githubusercontent.com/rh-mobb/helm-charts/main/charts/rosa-aws-prometheus/files/operatorhub.yaml","title":"Deploy Operators"},{"location":"rosa/cluster-metrics-to-aws-prometheus/#deploy-and-configure-the-aws-sigv4-proxy-and-the-grafana-agent","text":"Create a Policy for access to AWS Prometheus bash cat <<EOF > $SCRATCH_DIR/PermissionPolicyIngest.json { \"Version\": \"2012-10-17\", \"Statement\": [ {\"Effect\": \"Allow\", \"Action\": [ \"aps:RemoteWrite\", \"aps:GetSeries\", \"aps:GetLabels\", \"aps:GetMetricMetadata\" ], \"Resource\": \"*\" } ] } EOF Apply the Policy bash PROM_POLICY=$(aws iam create-policy --policy-name $PROM_SA-prom \\ --policy-document file://$SCRATCH_DIR/PermissionPolicyIngest.json \\ --query 'Policy.Arn' --output text) echo $PROM_POLICY 1. Create a Policy for access to AWS CloudWatch bash cat <<EOF > $SCRATCH_DIR/PermissionPolicyCloudWatch.json { \"Version\": \"2012-10-17\", \"Statement\": [ { \"Sid\": \"AllowReadingMetricsFromCloudWatch\", \"Effect\": \"Allow\", \"Action\": [ \"cloudwatch:DescribeAlarmsForMetric\", \"cloudwatch:DescribeAlarmHistory\", \"cloudwatch:DescribeAlarms\", \"cloudwatch:ListMetrics\", \"cloudwatch:GetMetricStatistics\", \"cloudwatch:GetMetricData\" ], \"Resource\": \"*\" }, { \"Sid\": \"AllowReadingLogsFromCloudWatch\", \"Effect\": \"Allow\", \"Action\": [ \"logs:DescribeLogGroups\", \"logs:GetLogGroupFields\", \"logs:StartQuery\", \"logs:StopQuery\", \"logs:GetQueryResults\", \"logs:GetLogEvents\" ], \"Resource\": \"*\" }, { \"Sid\": \"AllowReadingTagsInstancesRegionsFromEC2\", \"Effect\": \"Allow\", \"Action\": [ \"ec2:DescribeTags\", \"ec2:DescribeInstances\", \"ec2:DescribeRegions\" ], \"Resource\": \"*\" }, { \"Sid\": \"AllowReadingResourcesForTags\", \"Effect\": \"Allow\", \"Action\": \"tag:GetResources\", \"Resource\": \"*\" } ] } EOF Apply the Policy bash CW_POLICY=$(aws iam create-policy --policy-name $PROM_SA-cw \\ --policy-document file://$SCRATCH_DIR/PermissionPolicyCloudWatch.json \\ --query 'Policy.Arn' --output text) echo $CW_POLICY Create a Trust Policy bash cat <<EOF > $SCRATCH_DIR/TrustPolicy.json { \"Version\": \"2012-10-17\", \"Statement\": [ { \"Effect\": \"Allow\", \"Principal\": { \"Federated\": \"arn:aws:iam::${AWS_ACCOUNT_ID}:oidc-provider/${OIDC_PROVIDER}\" }, \"Action\": \"sts:AssumeRoleWithWebIdentity\", \"Condition\": { \"StringEquals\": { \"${OIDC_PROVIDER}:sub\": [ \"system:serviceaccount:${PROM_NAMESPACE}:${PROM_SA}\", \"system:serviceaccount:${PROM_NAMESPACE}:grafana-serviceaccount\" ] } } } ] } EOF Create Role for AWS Prometheus and CloudWatch bash PROM_ROLE=$(aws iam create-role \\ --role-name \"prometheus-$CLUSTER\" \\ --assume-role-policy-document file://$SCRATCH_DIR/TrustPolicy.json \\ --query \"Role.Arn\" --output text) echo $PROM_ROLE Attach the Policies to the Role ```bash aws iam attach-role-policy \\ --role-name \"prometheus-$CLUSTER\" \\ --policy-arn arn:aws:iam::aws:policy/AmazonPrometheusQueryAccess aws iam attach-role-policy \\ --role-name \"prometheus-$CLUSTER\" \\ --policy-arn $CW_POLICY ``` Create an AWS Prometheus Workspace bash PROM_WS=$(aws amp create-workspace --alias $CLUSTER \\ --query \"workspaceId\" --output text) echo $PROM_WS Deploy AWS Prometheus Proxy Helm Chart bash helm upgrade --install -n $PROM_NAMESPACE --set \"aws.region=$REGION\" \\ --set \"aws.roleArn=$PROM_ROLE\" --set \"fullnameOverride=$PROM_SA\" \\ --set \"aws.workspaceId=$PROM_WS\" \\ --set \"grafana-cr.serviceAccountAnnotations.eks\\.amazonaws\\.com/role-arn=$PROM_ROLE\" \\ aws-prometheus-proxy mobb/rosa-aws-prometheus Configure remoteWrite for user workloads bash cat << EOF | kubectl apply -f - apiVersion: v1 kind: ConfigMap metadata: name: user-workload-monitoring-config namespace: openshift-user-workload-monitoring data: config.yaml: | prometheus: remoteWrite: - url: \"http://aws-prometheus-proxy.$PROM_NAMESPACE.svc.cluster.local:8005/workspaces/$PROM_WS/api/v1/remote_write\" EOF","title":"Deploy and Configure the AWS Sigv4 Proxy and the Grafana Agent"},{"location":"rosa/cluster-metrics-to-aws-prometheus/#verify-metrics-are-being-collected","text":"Access Grafana and check for metrics bash oc get route -n custom-metrics grafana-route -o jsonpath='{.status.ingress[0].host}' Browse to the URL provided in the above command and log in with your OpenShift Credentials Enable Admin by hitting sign in and user admin and password Browse to /datasources and verify that cloudwatch and prometheus are present If not, you may have hit a race condition that can be fixed by running the following then trying again bash kubectl delete grafanadatasources.integreatly.org aws-prometheus-proxy-prometheus helm upgrade --install -n $PROM_NAMESPACE --set \"aws.region=$REGION\" \\ --set \"aws.roleArn=$PROM_ROLE\" --set \"fullnameOverride=$PROM_SA\" \\ --set \"aws.workspaceId=$PROM_WS\" \\ --set \"grafana-cr.serviceAccountAnnotations.eks\\.amazonaws\\.com/role-arn=$PROM_ROLE\" \\ aws-prometheus-proxy mobb/rosa-aws-prometheus Browse to /dashboards and select the custom-metrics -> NodeExporter / Use Method / Cluster dashboard","title":"Verify Metrics are being collected"},{"location":"rosa/cluster-metrics-to-aws-prometheus/#cleanup","text":"Delete the aws-prometheus-proxy Helm Release bash helm delete -n custom-metrics aws-prometheus-proxy Delete the custom-metrics-operators Helm Release bash helm delete -n custom-metrics custom-metrics-operators Delete the custom-metrics namespace bash kubectl delete namespace custom-metrics Detach AWS Role Policies ```bash aws iam detach-role-policy \\ --role-name \"prometheus-$CLUSTER\" \\ --policy-arn arn:aws:iam::aws:policy/AmazonPrometheusQueryAccess aws iam detach-role-policy \\ --role-name \"prometheus-$CLUSTER\" \\ --policy-arn $CW_POLICY ``` Delete the custom Cloud Watch Policy bash aws iam delete-policy --policy-arn $CW_POLICY Delete the AWS Prometheus Role bash aws iam delete-role --role-name \"prometheus-$CLUSTER\" Delete AWS Prometheus Workspace bash aws amp delete-workspace --workspace-id $PROM_WS","title":"Cleanup"},{"location":"rosa/custom-alertmanager/","text":"Custom Alerts in ROSA 4.11.x Starting with ROSA 4.11 clusters the OpenShift Administrator can enable a second AlertManager instance in user workload metrics which can be used to create custom alerts. Prerequisites AWS CLI A Red Hat OpenShift for AWS (ROSA) cluster 4.11.0 or higher Create Environment Variables Configure User Workload Monitoring to include AlertManager Edit the user workload config to include AlertManager Note: If you have other modifications to this config, you will need to hand edit the resource rather than brute forcing it like below. bash cat << EOF | oc apply -f - apiVersion: v1 kind: ConfigMap metadata: name: user-workload-monitoring-config namespace: openshift-user-workload-monitoring data: config.yaml: | alertmanager: enabled: true enableAlertmanagerConfig: true EOF Verify that a new Alert Manager instance is defined bash oc -n openshift-user-workload-monitoring get alertmanager NAME VERSION REPLICAS AGE user-workload 0.24.0 2 2m If you want non-admin users to be able to define alerts in their own namespaces you can run the following. bash oc -n <namespace> adm policy add-role-to-user alert-routing-edit <user> Update the Alert Manager Configuration file This will create a basic AlertManager configuration to send alerts to a slack channel. Configuring slack is outside the scope of this document. Update the variables to suit your slack integration. bash SLACK_API_URL=https://hooks.slack.com/services/XXX/XXX/XXX SLACK_CHANNEL='#paultest' cat << EOF | kubectl apply -f - apiVersion: v1 kind: Secret metadata: name: alertmanager-user-workload namespace: openshift-user-workload-monitoring stringData: alertmanager.yaml: | global: slack_api_url: \"${SLACK_API_URL}\" route: receiver: Default group_by: [alertname] receivers: - name: Default slack_configs: - channel: ${SLACK_CHANNEL} send_resolved: true EOF Create an Example Alert Create a Namespace for your custom alert bash oc create namespace custom-alert Verify it works by creating a Prometheus Rule that will fire off an alert bash cat << EOF | kubectl apply -n custom-alert -f - apiVersion: monitoring.coreos.com/v1 kind: PrometheusRule metadata: name: prometheus-example-rules spec: groups: - name: example.rules rules: - alert: ExampleAlert expr: vector(1) EOF Forward a port to the alert manager service bash kubectl port-forward -n openshift-user-workload-monitoring \\ svc/alertmanager-operated 9093:9093 Browse to http://localhost:9093/#/alerts to see the alert \"ExampleAlert\" Check the Alert was sent to Slack","title":"ROSA 4.11+"},{"location":"rosa/custom-alertmanager/#custom-alerts-in-rosa-411x","text":"Starting with ROSA 4.11 clusters the OpenShift Administrator can enable a second AlertManager instance in user workload metrics which can be used to create custom alerts.","title":"Custom Alerts in ROSA 4.11.x"},{"location":"rosa/custom-alertmanager/#prerequisites","text":"AWS CLI A Red Hat OpenShift for AWS (ROSA) cluster 4.11.0 or higher","title":"Prerequisites"},{"location":"rosa/custom-alertmanager/#create-environment-variables","text":"","title":"Create Environment Variables"},{"location":"rosa/custom-alertmanager/#configure-user-workload-monitoring-to-include-alertmanager","text":"Edit the user workload config to include AlertManager Note: If you have other modifications to this config, you will need to hand edit the resource rather than brute forcing it like below. bash cat << EOF | oc apply -f - apiVersion: v1 kind: ConfigMap metadata: name: user-workload-monitoring-config namespace: openshift-user-workload-monitoring data: config.yaml: | alertmanager: enabled: true enableAlertmanagerConfig: true EOF Verify that a new Alert Manager instance is defined bash oc -n openshift-user-workload-monitoring get alertmanager NAME VERSION REPLICAS AGE user-workload 0.24.0 2 2m If you want non-admin users to be able to define alerts in their own namespaces you can run the following. bash oc -n <namespace> adm policy add-role-to-user alert-routing-edit <user> Update the Alert Manager Configuration file This will create a basic AlertManager configuration to send alerts to a slack channel. Configuring slack is outside the scope of this document. Update the variables to suit your slack integration. bash SLACK_API_URL=https://hooks.slack.com/services/XXX/XXX/XXX SLACK_CHANNEL='#paultest' cat << EOF | kubectl apply -f - apiVersion: v1 kind: Secret metadata: name: alertmanager-user-workload namespace: openshift-user-workload-monitoring stringData: alertmanager.yaml: | global: slack_api_url: \"${SLACK_API_URL}\" route: receiver: Default group_by: [alertname] receivers: - name: Default slack_configs: - channel: ${SLACK_CHANNEL} send_resolved: true EOF","title":"Configure User Workload Monitoring to include AlertManager"},{"location":"rosa/custom-alertmanager/#create-an-example-alert","text":"Create a Namespace for your custom alert bash oc create namespace custom-alert Verify it works by creating a Prometheus Rule that will fire off an alert bash cat << EOF | kubectl apply -n custom-alert -f - apiVersion: monitoring.coreos.com/v1 kind: PrometheusRule metadata: name: prometheus-example-rules spec: groups: - name: example.rules rules: - alert: ExampleAlert expr: vector(1) EOF Forward a port to the alert manager service bash kubectl port-forward -n openshift-user-workload-monitoring \\ svc/alertmanager-operated 9093:9093 Browse to http://localhost:9093/#/alerts to see the alert \"ExampleAlert\" Check the Alert was sent to Slack","title":"Create an Example Alert"},{"location":"rosa/custom-alertmanager-4.9/","text":"Custom AlertManager in ROSA 4.9.x ROSA 4.9.x introduces a new way to provide custom AlertManager configuration to receive alerts from User Workload Management. The OpenShift Administrator can use the Prometheus Operator to create a custom AlertManager resource and then use the AlertManagerConfig resource to configure User Workload Monitoring to use the custom AlertManager. Prerequisites AWS CLI A Red Hat OpenShift for AWS (ROSA) cluster 4.9.0 or higher Create Environment Variables Before we get started we need to set some environment variables to be used throughout the guide. bash export PROM_NAMESPACE=custom-alert-manager Install Prometheus Operator If you prefer you can do this from the Operator Hub in the cluster console itself. Create a OperatorGroup and Subscription for the Prometheus Operator ```bash cat << EOF | kubectl apply -f - apiVersion: v1 kind: Namespace metadata: name: ${PROM_NAMESPACE} apiVersion: operators.coreos.com/v1 kind: OperatorGroup metadata: name: federated-metrics namespace: ${PROM_NAMESPACE} spec: targetNamespaces: - ${PROM_NAMESPACE} apiVersion: operators.coreos.com/v1alpha1 kind: Subscription metadata: name: prometheus namespace: ${PROM_NAMESPACE} spec: channel: beta installPlanApproval: Automatic name: prometheus source: community-operators sourceNamespace: openshift-marketplace EOF ``` Deploy AlertManager Create an Alert Manager Configuration file This will create a basic AlertManager configuration to send alerts to a slack channel. Configuring slack is outside the scope of this document. Update the variables to suit your slack integration. ```bash SLACK_API_URL=https://hooks.slack.com/services/XXX/XXX/XXX SLACK_CHANNEL='#paultest' cat << EOF | kubectl apply -n ${PROM_NAMESPACE} -f - apiVersion: v1 kind: Secret metadata: name: custom-alertmanager namespace: ${PROM_NAMESPACE} stringData: alertmanager.yaml: | global: slack_api_url: \"${SLACK_API_URL}\" route: receiver: slack-notifications group_by: [alertname] receivers: - name: slack-notifications slack_configs: - channel: ${SLACK_CHANNEL} send_resolved: true apiVersion: monitoring.coreos.com/v1 kind: Alertmanager metadata: name: custom-alertmanager namespace: ${PROM_NAMESPACE} spec: securityContext: {} replicas: 3 configSecret: custom-alertmanager apiVersion: v1 kind: Service metadata: name: custom-alertmanager namespace: ${PROM_NAMESPACE} spec: type: ClusterIP ports: - name: web port: 9093 protocol: TCP targetPort: web selector: alertmanager: custom-alertmanager EOF ``` Configure User Workload Monitoring to use the custom AlertManager Create an AlertManagerConfig for User Workload Monitoring Note: This next command assumes the existing config.yaml in the user-workload-monitoring-config config map is empty. You should verify it with kubectl get -n openshift-user-workload-monitoring cm user-workload-monitoring-config -o yaml and simply edit in the differences if its not. bash cat << EOF | kubectl apply -f - apiVersion: v1 kind: ConfigMap metadata: name: user-workload-monitoring-config namespace: openshift-user-workload-monitoring data: config.yaml: | thanosRuler: additionalAlertmanagerConfigs: - scheme: http pathPrefix: / timeout: \"30s\" apiVersion: v1 staticConfigs: [\"custom-alertmanager.$PROM_NAMESPACE.svc.cluster.local:9093\"] EOF Create an Example Alert Verify it works by creating a Prometheus Rule that will fire off an alert bash cat << EOF | kubectl apply -n $PROM_NAMESPACE -f - apiVersion: monitoring.coreos.com/v1 kind: PrometheusRule metadata: name: prometheus-example-rules namespace: ${PROM_NAMESPACE} spec: groups: - name: example.rules rules: - alert: ExampleAlert expr: vector(1) EOF Forward a port to the alert manager service bash kubectl port-forward -n ${PROM_NAMESPACE} svc/custom-alertmanager 9093:9093 Browse to http://localhost:9093/#/alerts to see the alert \"ExampleAlert\" Check the Alert was sent to Slack","title":"ROSA 4.9.X"},{"location":"rosa/custom-alertmanager-4.9/#custom-alertmanager-in-rosa-49x","text":"ROSA 4.9.x introduces a new way to provide custom AlertManager configuration to receive alerts from User Workload Management. The OpenShift Administrator can use the Prometheus Operator to create a custom AlertManager resource and then use the AlertManagerConfig resource to configure User Workload Monitoring to use the custom AlertManager.","title":"Custom AlertManager in ROSA 4.9.x"},{"location":"rosa/custom-alertmanager-4.9/#prerequisites","text":"AWS CLI A Red Hat OpenShift for AWS (ROSA) cluster 4.9.0 or higher","title":"Prerequisites"},{"location":"rosa/custom-alertmanager-4.9/#create-environment-variables","text":"Before we get started we need to set some environment variables to be used throughout the guide. bash export PROM_NAMESPACE=custom-alert-manager","title":"Create Environment Variables"},{"location":"rosa/custom-alertmanager-4.9/#install-prometheus-operator","text":"If you prefer you can do this from the Operator Hub in the cluster console itself. Create a OperatorGroup and Subscription for the Prometheus Operator ```bash cat << EOF | kubectl apply -f - apiVersion: v1 kind: Namespace metadata: name: ${PROM_NAMESPACE} apiVersion: operators.coreos.com/v1 kind: OperatorGroup metadata: name: federated-metrics namespace: ${PROM_NAMESPACE} spec: targetNamespaces: - ${PROM_NAMESPACE} apiVersion: operators.coreos.com/v1alpha1 kind: Subscription metadata: name: prometheus namespace: ${PROM_NAMESPACE} spec: channel: beta installPlanApproval: Automatic name: prometheus source: community-operators sourceNamespace: openshift-marketplace EOF ```","title":"Install Prometheus Operator"},{"location":"rosa/custom-alertmanager-4.9/#deploy-alertmanager","text":"Create an Alert Manager Configuration file This will create a basic AlertManager configuration to send alerts to a slack channel. Configuring slack is outside the scope of this document. Update the variables to suit your slack integration. ```bash SLACK_API_URL=https://hooks.slack.com/services/XXX/XXX/XXX SLACK_CHANNEL='#paultest' cat << EOF | kubectl apply -n ${PROM_NAMESPACE} -f - apiVersion: v1 kind: Secret metadata: name: custom-alertmanager namespace: ${PROM_NAMESPACE} stringData: alertmanager.yaml: | global: slack_api_url: \"${SLACK_API_URL}\" route: receiver: slack-notifications group_by: [alertname] receivers: - name: slack-notifications slack_configs: - channel: ${SLACK_CHANNEL} send_resolved: true apiVersion: monitoring.coreos.com/v1 kind: Alertmanager metadata: name: custom-alertmanager namespace: ${PROM_NAMESPACE} spec: securityContext: {} replicas: 3 configSecret: custom-alertmanager apiVersion: v1 kind: Service metadata: name: custom-alertmanager namespace: ${PROM_NAMESPACE} spec: type: ClusterIP ports: - name: web port: 9093 protocol: TCP targetPort: web selector: alertmanager: custom-alertmanager EOF ```","title":"Deploy AlertManager"},{"location":"rosa/custom-alertmanager-4.9/#configure-user-workload-monitoring-to-use-the-custom-alertmanager","text":"Create an AlertManagerConfig for User Workload Monitoring Note: This next command assumes the existing config.yaml in the user-workload-monitoring-config config map is empty. You should verify it with kubectl get -n openshift-user-workload-monitoring cm user-workload-monitoring-config -o yaml and simply edit in the differences if its not. bash cat << EOF | kubectl apply -f - apiVersion: v1 kind: ConfigMap metadata: name: user-workload-monitoring-config namespace: openshift-user-workload-monitoring data: config.yaml: | thanosRuler: additionalAlertmanagerConfigs: - scheme: http pathPrefix: / timeout: \"30s\" apiVersion: v1 staticConfigs: [\"custom-alertmanager.$PROM_NAMESPACE.svc.cluster.local:9093\"] EOF","title":"Configure User Workload Monitoring to use the custom AlertManager"},{"location":"rosa/custom-alertmanager-4.9/#create-an-example-alert","text":"Verify it works by creating a Prometheus Rule that will fire off an alert bash cat << EOF | kubectl apply -n $PROM_NAMESPACE -f - apiVersion: monitoring.coreos.com/v1 kind: PrometheusRule metadata: name: prometheus-example-rules namespace: ${PROM_NAMESPACE} spec: groups: - name: example.rules rules: - alert: ExampleAlert expr: vector(1) EOF Forward a port to the alert manager service bash kubectl port-forward -n ${PROM_NAMESPACE} svc/custom-alertmanager 9093:9093 Browse to http://localhost:9093/#/alerts to see the alert \"ExampleAlert\" Check the Alert was sent to Slack","title":"Create an Example Alert"},{"location":"rosa/ecr/","text":"Configuring a ROSA cluster to pull images from AWS Elastic Container Registry (ECR) Prerequisites AWS CLI Openshift CLI 4.8+ Docker Background There are two options to use to authenticate wth Amazon ECR to pull images. The traditional method is to create a pull secret for ecr. Example: oc create secret docker-registry ecr-pull-secret --docker-server=<registry id>.dkr.ecr.<region>.amazonaws.com \\ --docker-username=AWS --docker-password=$(aws ecr get-login-password) --namespace=hello-world However Amazon ECR tokens expire every 12 hours which will mean you will need to re-authenticate every 12 hours either through scripting or do so manually. A second, and preferred method, is to attach an ECR Policy to your cluster's worker machine profiles which this guide will walk you through. Attach ECR Policy Role You can attach an ECR policy to your cluster giving the cluster permissions to pull images from your registries. ROSA worker machine instances comes with pre-defined IAM roles, named differently depending on whether its a STS cluster or a non-STS cluster. STS Cluster Role ManagedOpenShift-Worker-Role is the IAM role attached to ROSA STS compute instances. non-STS Cluster Role <cluster name>-<identifier>-worker-role is the IAM role attached to ROSA non-STS compute instances. Tip: To find the non-STS cluster role run the following command with your cluster name: aws iam list-roles | grep <cluster_name> ECR Policies ECR has several pre-defined policies that give permissions to interact with the service. In the case of ROSA, we will be pulling images from ECR and will only need to add the AmazonEC2ContainerRegistryReadOnly policy. Add the AmazonEC2ContainerRegistryReadOnly policy to the ManagedOpenShift-Worker-Role for STS clusters or the <cluster name>-<identifier>-worker-role for non-STS clusters. STS Example: aws iam attach-role-policy \\ --role-name ManagedOpenShift-Worker-Role \\ --policy-arn \"arn:aws:iam::aws:policy/AmazonEC2ContainerRegistryReadOnly\" Test it Out Log into ECR aws ecr get-login-password --region region | docker login --username AWS \\ --password-stdin aws_account_id.dkr.ecr.region.amazonaws.com Create a repository aws ecr create-repository \\ --repository-name hello-world \\ --image-scanning-configuration scanOnPush=true \\ --region region Pull an image docker pull openshift/hello-openshift Tag the image for ecr docker tag openshift/hello-openshift:latest <registry id>.dkr.ecr.<region>.amazonaws.com/hello-world:latest note: you can find the registry id and URI with the following command aws ecr describe-repositories Push the image to ECR docker push <registry id>.dkr.ecr.<region>.amazonaws.com/hello-world:latest Create a new project oc new project hello-world Create a new app using the image on ECR oc new-app --name hello-world --image <registry id>.dkr.ecr.<region>.amazonaws.com/hello-world:latest View a list of pods in the namespace you created: oc get pods Expected output: If you see the hello-world pod running ... congratulations! You can now pull images from your ECR repository. Clean up Simply delete the project you created to test pulling images: oc delete project hello-world You may also want to remove the arn:aws:iam::aws:policy/AmazonEC2ContainerRegistryReadOnly policy from the worker nodes if you do no want them to continue to have access to the ECR.","title":"Configuring a ROSA cluster to pull images from AWS Elastic Container Registry (ECR)"},{"location":"rosa/ecr/#configuring-a-rosa-cluster-to-pull-images-from-aws-elastic-container-registry-ecr","text":"","title":"Configuring a ROSA cluster to pull images from AWS Elastic Container Registry (ECR)"},{"location":"rosa/ecr/#prerequisites","text":"AWS CLI Openshift CLI 4.8+ Docker","title":"Prerequisites"},{"location":"rosa/ecr/#background","text":"There are two options to use to authenticate wth Amazon ECR to pull images. The traditional method is to create a pull secret for ecr. Example: oc create secret docker-registry ecr-pull-secret --docker-server=<registry id>.dkr.ecr.<region>.amazonaws.com \\ --docker-username=AWS --docker-password=$(aws ecr get-login-password) --namespace=hello-world However Amazon ECR tokens expire every 12 hours which will mean you will need to re-authenticate every 12 hours either through scripting or do so manually. A second, and preferred method, is to attach an ECR Policy to your cluster's worker machine profiles which this guide will walk you through.","title":"Background"},{"location":"rosa/ecr/#attach-ecr-policy-role","text":"You can attach an ECR policy to your cluster giving the cluster permissions to pull images from your registries. ROSA worker machine instances comes with pre-defined IAM roles, named differently depending on whether its a STS cluster or a non-STS cluster.","title":"Attach ECR Policy Role"},{"location":"rosa/ecr/#sts-cluster-role","text":"ManagedOpenShift-Worker-Role is the IAM role attached to ROSA STS compute instances.","title":"STS Cluster Role"},{"location":"rosa/ecr/#non-sts-cluster-role","text":"<cluster name>-<identifier>-worker-role is the IAM role attached to ROSA non-STS compute instances. Tip: To find the non-STS cluster role run the following command with your cluster name: aws iam list-roles | grep <cluster_name>","title":"non-STS Cluster Role"},{"location":"rosa/ecr/#ecr-policies","text":"ECR has several pre-defined policies that give permissions to interact with the service. In the case of ROSA, we will be pulling images from ECR and will only need to add the AmazonEC2ContainerRegistryReadOnly policy. Add the AmazonEC2ContainerRegistryReadOnly policy to the ManagedOpenShift-Worker-Role for STS clusters or the <cluster name>-<identifier>-worker-role for non-STS clusters. STS Example: aws iam attach-role-policy \\ --role-name ManagedOpenShift-Worker-Role \\ --policy-arn \"arn:aws:iam::aws:policy/AmazonEC2ContainerRegistryReadOnly\"","title":"ECR Policies"},{"location":"rosa/ecr/#test-it-out","text":"Log into ECR aws ecr get-login-password --region region | docker login --username AWS \\ --password-stdin aws_account_id.dkr.ecr.region.amazonaws.com Create a repository aws ecr create-repository \\ --repository-name hello-world \\ --image-scanning-configuration scanOnPush=true \\ --region region Pull an image docker pull openshift/hello-openshift Tag the image for ecr docker tag openshift/hello-openshift:latest <registry id>.dkr.ecr.<region>.amazonaws.com/hello-world:latest note: you can find the registry id and URI with the following command aws ecr describe-repositories Push the image to ECR docker push <registry id>.dkr.ecr.<region>.amazonaws.com/hello-world:latest Create a new project oc new project hello-world Create a new app using the image on ECR oc new-app --name hello-world --image <registry id>.dkr.ecr.<region>.amazonaws.com/hello-world:latest View a list of pods in the namespace you created: oc get pods Expected output: If you see the hello-world pod running ... congratulations! You can now pull images from your ECR repository.","title":"Test it Out"},{"location":"rosa/ecr/#clean-up","text":"Simply delete the project you created to test pulling images: oc delete project hello-world You may also want to remove the arn:aws:iam::aws:policy/AmazonEC2ContainerRegistryReadOnly policy from the worker nodes if you do no want them to continue to have access to the ECR.","title":"Clean up"},{"location":"rosa/ecr-secret-operator/","text":"ECR Secret Operator Amazon Elastic Container Registry Private Registry Authentication provides a temporary token that is valid only for 12 hours. It is a challenge for automating container image build process to refresh the token or secret in a timely manner. This operators frequently talks with AWS ECR GetAuthroization Token and create/update the secret, so that the service account can perform docker image build. How to use this operator Prerequisites Create an ECR private repository Provide AWS Authentication to the operator. Two Options: IAM User STS Assume Role Install the operator Install the operator from operator hub community Create the ECR Secret CRD echo << EOF | oc apply -f - apiVersion: ecr.mobb.redhat.com/v1alpha1 kind: Secret metadata: name: ecr-secret namespace: test-ecr-secret-operator spec: generated_secret_name: ecr-docker-secret ecr_registry: [ACCOUNT_ID].dkr.ecr.us-east-2.amazonaws.com frequency: 10h region: us-east-2 EOF A docker registry secret is created by the operator momentally and the token is patched every 10 hours oc get secret ecr-docker-secret NAME TYPE DATA AGE ecr-docker-secret kubernetes.io/dockerconfigjson 1 16h A sample build process with generated secret Link the secret to builder oc secrets link builder ecr-docker-secret Configure build config to point to your ECR Container repository oc create imagestream ruby oc tag openshift/ruby:2.5-ubi8 ruby:2.5 echo << EOF | oc apply -f - kind: BuildConfig apiVersion: build.openshift.io/v1 metadata: name: ruby-sample-build namespace: test-ecr-secret-operator spec: runPolicy: Serial source: git: uri: \"https://github.com/openshift/ruby-hello-world\" strategy: sourceStrategy: from: kind: \"ImageStreamTag\" name: \"ruby:2.5\" incremental: true output: to: kind: \"DockerImage\" name: \"[ACCOUNT_ID].dkr.ecr.us-east-2.amazonaws.com/test:latest\" postCommit: script: \"bundle exec rake test\" EOF oc start-build ruby-sample-build --wait Build should succeed and push the image to the the private ECR Container repository","title":"ECR Secret Operator"},{"location":"rosa/ecr-secret-operator/#ecr-secret-operator","text":"Amazon Elastic Container Registry Private Registry Authentication provides a temporary token that is valid only for 12 hours. It is a challenge for automating container image build process to refresh the token or secret in a timely manner. This operators frequently talks with AWS ECR GetAuthroization Token and create/update the secret, so that the service account can perform docker image build.","title":"ECR Secret Operator"},{"location":"rosa/ecr-secret-operator/#how-to-use-this-operator","text":"","title":"How to use this operator"},{"location":"rosa/ecr-secret-operator/#prerequisites","text":"Create an ECR private repository Provide AWS Authentication to the operator. Two Options: IAM User STS Assume Role","title":"Prerequisites"},{"location":"rosa/ecr-secret-operator/#install-the-operator","text":"Install the operator from operator hub community","title":"Install the operator"},{"location":"rosa/ecr-secret-operator/#create-the-ecr-secret-crd","text":"echo << EOF | oc apply -f - apiVersion: ecr.mobb.redhat.com/v1alpha1 kind: Secret metadata: name: ecr-secret namespace: test-ecr-secret-operator spec: generated_secret_name: ecr-docker-secret ecr_registry: [ACCOUNT_ID].dkr.ecr.us-east-2.amazonaws.com frequency: 10h region: us-east-2 EOF A docker registry secret is created by the operator momentally and the token is patched every 10 hours oc get secret ecr-docker-secret NAME TYPE DATA AGE ecr-docker-secret kubernetes.io/dockerconfigjson 1 16h","title":"Create the ECR Secret CRD"},{"location":"rosa/ecr-secret-operator/#a-sample-build-process-with-generated-secret","text":"Link the secret to builder oc secrets link builder ecr-docker-secret Configure build config to point to your ECR Container repository oc create imagestream ruby oc tag openshift/ruby:2.5-ubi8 ruby:2.5 echo << EOF | oc apply -f - kind: BuildConfig apiVersion: build.openshift.io/v1 metadata: name: ruby-sample-build namespace: test-ecr-secret-operator spec: runPolicy: Serial source: git: uri: \"https://github.com/openshift/ruby-hello-world\" strategy: sourceStrategy: from: kind: \"ImageStreamTag\" name: \"ruby:2.5\" incremental: true output: to: kind: \"DockerImage\" name: \"[ACCOUNT_ID].dkr.ecr.us-east-2.amazonaws.com/test:latest\" postCommit: script: \"bundle exec rake test\" EOF oc start-build ruby-sample-build --wait Build should succeed and push the image to the the private ECR Container repository","title":"A sample build process with generated secret"},{"location":"rosa/ecr-secret-operator/iam_assume_role/","text":"Create STS Assume Role About AWS STS and Assume Role Notes: These are sample commands. Please fill in your own resource parameters E.g. ARN Prequisites An STS Openshift Cluster Create the policy cat <<EOF > /tmp/iam_policy.json { \"Version\": \"2012-10-17\", \"Statement\": [ { \"Effect\": \"Allow\", \"Action\": [ \"ecr:GetAuthorizationToken\" ], \"Resource\": \"*\" } ] } EOF aws iam create-policy \\ --policy-name ECRLoginPolicy \\ --policy-document file:///tmp/iam_policy.json Create the role and attach the policy cat <<EOF > /tmp/trust_policy.json { \"Version\": \"2012-10-17\", \"Statement\": [ { \"Effect\": \"Allow\", \"Principal\": { \"Federated\": \"arn:aws:iam::[ACCOUNT_ID]:oidc-provider/rh-oidc.s3.us-east-1.amazonaws.com/1ou2pbj9v68ghlc63bo0mad059cj1elf\" }, \"Action\": \"sts:AssumeRoleWithWebIdentity\", \"Condition\": { \"StringEquals\": { \"rh-oidc.s3.us-east-1.amazonaws.com/1ou2pbj9v68ghlc63bo0mad059cj1elf:sub\": \"system:serviceaccount:ecr-secret-operator:ecr-secret-operator-controller-manager\" } } } ] } EOF aws iam create-role --role-name ECRLogin --assume-role-policy-document file:///tmp/trust_policy.json aws iam attach-role-policy --role-name ECRLogin --policy-arn arn:aws:iam::[ACCOUNT_ID]:policy/ECRLoginPolicy Create the repository policy cat <<EOF > /tmp/repo_policy.json { \"Version\": \"2012-10-17\", \"Statement\": [ { \"Sid\": \"AllowPushPull\", \"Effect\": \"Allow\", \"Principal\": { \"AWS\": [ \"arn:aws:iam::[ACCOUNT_ID]:role/ECRLogin\" ] }, \"Action\": [ \"ecr:BatchGetImage\", \"ecr:BatchCheckLayerAvailability\", \"ecr:CompleteLayerUpload\", \"ecr:GetDownloadUrlForLayer\", \"ecr:InitiateLayerUpload\", \"ecr:PutImage\", \"ecr:UploadLayerPart\" ] } ] } EOF aws ecr set-repository-policy --repository-name test --policy-text file:///tmp/repo_policy.json Create STS kubernetes Secret cat <<EOF > /tmp/credentials [default] role_arn = arn:aws:iam::[ACCOUNT_ID]:role/ECRLogin web_identity_token_file = /var/run/secrets/openshift/serviceaccount/token EOF oc create secret generic aws-ecr-cloud-credentials --from-file=credentials=/tmp/credentials","title":"Iam assume role"},{"location":"rosa/ecr-secret-operator/iam_assume_role/#create-sts-assume-role","text":"About AWS STS and Assume Role Notes: These are sample commands. Please fill in your own resource parameters E.g. ARN Prequisites An STS Openshift Cluster Create the policy cat <<EOF > /tmp/iam_policy.json { \"Version\": \"2012-10-17\", \"Statement\": [ { \"Effect\": \"Allow\", \"Action\": [ \"ecr:GetAuthorizationToken\" ], \"Resource\": \"*\" } ] } EOF aws iam create-policy \\ --policy-name ECRLoginPolicy \\ --policy-document file:///tmp/iam_policy.json Create the role and attach the policy cat <<EOF > /tmp/trust_policy.json { \"Version\": \"2012-10-17\", \"Statement\": [ { \"Effect\": \"Allow\", \"Principal\": { \"Federated\": \"arn:aws:iam::[ACCOUNT_ID]:oidc-provider/rh-oidc.s3.us-east-1.amazonaws.com/1ou2pbj9v68ghlc63bo0mad059cj1elf\" }, \"Action\": \"sts:AssumeRoleWithWebIdentity\", \"Condition\": { \"StringEquals\": { \"rh-oidc.s3.us-east-1.amazonaws.com/1ou2pbj9v68ghlc63bo0mad059cj1elf:sub\": \"system:serviceaccount:ecr-secret-operator:ecr-secret-operator-controller-manager\" } } } ] } EOF aws iam create-role --role-name ECRLogin --assume-role-policy-document file:///tmp/trust_policy.json aws iam attach-role-policy --role-name ECRLogin --policy-arn arn:aws:iam::[ACCOUNT_ID]:policy/ECRLoginPolicy Create the repository policy cat <<EOF > /tmp/repo_policy.json { \"Version\": \"2012-10-17\", \"Statement\": [ { \"Sid\": \"AllowPushPull\", \"Effect\": \"Allow\", \"Principal\": { \"AWS\": [ \"arn:aws:iam::[ACCOUNT_ID]:role/ECRLogin\" ] }, \"Action\": [ \"ecr:BatchGetImage\", \"ecr:BatchCheckLayerAvailability\", \"ecr:CompleteLayerUpload\", \"ecr:GetDownloadUrlForLayer\", \"ecr:InitiateLayerUpload\", \"ecr:PutImage\", \"ecr:UploadLayerPart\" ] } ] } EOF aws ecr set-repository-policy --repository-name test --policy-text file:///tmp/repo_policy.json Create STS kubernetes Secret cat <<EOF > /tmp/credentials [default] role_arn = arn:aws:iam::[ACCOUNT_ID]:role/ECRLogin web_identity_token_file = /var/run/secrets/openshift/serviceaccount/token EOF oc create secret generic aws-ecr-cloud-credentials --from-file=credentials=/tmp/credentials","title":"Create STS Assume Role"},{"location":"rosa/ecr-secret-operator/iam_user/","text":"Create IAM user and Policy Notes: These are sample commands. Please fill in your own resource parameters E.g. ARN Create the policy cat <<EOF > /tmp/iam_policy.json { \"Version\": \"2012-10-17\", \"Statement\": [ { \"Effect\": \"Allow\", \"Action\": [ \"ecr:GetAuthorizationToken\" ], \"Resource\": \"*\" } ] } EOF aws iam create-policy \\ --policy-name ECRLoginPolicy \\ --policy-document file:///tmp/iam_policy.json Create a user and access key and attach the policy aws iam create-user --user-name ecr-bot aws create-access-key --user-name ecr-bot aws iam attach-user-policy --policy-arn arn:aws:iam::[ACCOUNT_ID]:policy/ECRLoginPolicy --user-name ecr-bot Notes: Save access key id and key for later usage Set up a specific ECR repository access cat <<EOF > /tmp/repo_policy.json { \"Version\": \"2012-10-17\", \"Statement\": [ { \"Sid\": \"AllowPushPull\", \"Effect\": \"Allow\", \"Principal\": { \"AWS\": [ \"arn:aws:iam::[ACCOUNT_ID]:user/ecr-bot\" ] }, \"Action\": [ \"ecr:BatchGetImage\", \"ecr:BatchCheckLayerAvailability\", \"ecr:CompleteLayerUpload\", \"ecr:GetDownloadUrlForLayer\", \"ecr:InitiateLayerUpload\", \"ecr:PutImage\", \"ecr:UploadLayerPart\" ] } ] } EOF aws ecr set-repository-policy --repository-name test --policy-text file:///tmp/repo_policy.json Create kubernetes Secret with iam user cat <<EOF > /tmp/credentials [default] aws_access_key_id=\"\" aws_secret_access_key=\"\" EOF oc create secret generic aws-ecr-cloud-credentials --from-file=credentials=/tmp/credentials","title":"Iam user"},{"location":"rosa/ecr-secret-operator/iam_user/#create-iam-user-and-policy","text":"Notes: These are sample commands. Please fill in your own resource parameters E.g. ARN Create the policy cat <<EOF > /tmp/iam_policy.json { \"Version\": \"2012-10-17\", \"Statement\": [ { \"Effect\": \"Allow\", \"Action\": [ \"ecr:GetAuthorizationToken\" ], \"Resource\": \"*\" } ] } EOF aws iam create-policy \\ --policy-name ECRLoginPolicy \\ --policy-document file:///tmp/iam_policy.json Create a user and access key and attach the policy aws iam create-user --user-name ecr-bot aws create-access-key --user-name ecr-bot aws iam attach-user-policy --policy-arn arn:aws:iam::[ACCOUNT_ID]:policy/ECRLoginPolicy --user-name ecr-bot Notes: Save access key id and key for later usage Set up a specific ECR repository access cat <<EOF > /tmp/repo_policy.json { \"Version\": \"2012-10-17\", \"Statement\": [ { \"Sid\": \"AllowPushPull\", \"Effect\": \"Allow\", \"Principal\": { \"AWS\": [ \"arn:aws:iam::[ACCOUNT_ID]:user/ecr-bot\" ] }, \"Action\": [ \"ecr:BatchGetImage\", \"ecr:BatchCheckLayerAvailability\", \"ecr:CompleteLayerUpload\", \"ecr:GetDownloadUrlForLayer\", \"ecr:InitiateLayerUpload\", \"ecr:PutImage\", \"ecr:UploadLayerPart\" ] } ] } EOF aws ecr set-repository-policy --repository-name test --policy-text file:///tmp/repo_policy.json Create kubernetes Secret with iam user cat <<EOF > /tmp/credentials [default] aws_access_key_id=\"\" aws_secret_access_key=\"\" EOF oc create secret generic aws-ecr-cloud-credentials --from-file=credentials=/tmp/credentials","title":"Create IAM user and Policy"},{"location":"rosa/federated-metrics/","text":"Federating System and User metrics to S3 in Red Hat OpenShift for AWS Paul Czarkowski 06/07/2021 This guide walks through setting up federating Prometheus metrics to S3 storage. ToDo - Add Authorization in front of Thanos APIs Prerequisites A ROSA cluster deployed with STS aws CLI Set up environment Create environment variables bash export CLUSTER_NAME=my-cluster export S3_BUCKET=my-thanos-bucket export REGION=us-east-2 export NAMESPACE=federated-metrics export SA=aws-prometheus-proxy export SCRATCH_DIR=/tmp/scratch export OIDC_PROVIDER=$(oc get authentication.config.openshift.io cluster -o json | jq -r .spec.serviceAccountIssuer| sed -e \"s/^https:\\/\\///\") export AWS_ACCOUNT_ID=$(aws sts get-caller-identity --query Account --output text) export AWS_PAGER=\"\" rm -rf $SCRATCH_DIR mkdir -p $SCRATCH_DIR Create namespace bash oc new-project $NAMESPACE AWS Preperation Create an S3 bucket bash aws s3 mb s3://$S3_BUCKET Create a Policy for access to S3 bash cat <<EOF > $SCRATCH_DIR/s3-policy.json { \"Version\": \"2012-10-17\", \"Statement\": [ { \"Sid\": \"Statement\", \"Effect\": \"Allow\", \"Action\": [ \"s3:ListBucket\", \"s3:GetObject\", \"s3:DeleteObject\", \"s3:PutObject\", \"s3:PutObjectAcl\" ], \"Resource\": [ \"arn:aws:s3:::$S3_BUCKET/*\", \"arn:aws:s3:::$S3_BUCKET\" ] } ] } EOF Apply the Policy bash S3_POLICY=$(aws iam create-policy --policy-name $CLUSTER_NAME-thanos \\ --policy-document file://$SCRATCH_DIR/s3-policy.json \\ --query 'Policy.Arn' --output text) echo $S3_POLICY Create a Trust Policy bash cat <<EOF > $SCRATCH_DIR/TrustPolicy.json { \"Version\": \"2012-10-17\", \"Statement\": [ { \"Effect\": \"Allow\", \"Principal\": { \"Federated\": \"arn:aws:iam::${AWS_ACCOUNT_ID}:oidc-provider/${OIDC_PROVIDER}\" }, \"Action\": \"sts:AssumeRoleWithWebIdentity\", \"Condition\": { \"StringEquals\": { \"${OIDC_PROVIDER}:sub\": [ \"system:serviceaccount:${NAMESPACE}:${SA}\" ] } } } ] } EOF Create Role for AWS Prometheus and CloudWatch bash S3_ROLE=$(aws iam create-role \\ --role-name \"$CLUSTER-thanos-s3\" \\ --assume-role-policy-document file://$SCRATCH_DIR/TrustPolicy.json \\ --query \"Role.Arn\" --output text) echo $S3_ROLE Attach the Policies to the Role bash aws iam attach-role-policy \\ --role-name \"$CLUSTER-thanos-s3\" \\ --policy-arn $S3_POLICY Deploy Operators Add the MOBB chart repository to your Helm bash helm repo add mobb https://rh-mobb.github.io/helm-charts/ Update your repositories bash helm repo update Use the mobb/operatorhub chart to deploy the needed operators bash helm upgrade -n $echNAMESPACE custom-metrics-operators \\ mobb/operatorhub --version 0.1.1 --install \\ --values https://raw.githubusercontent.com/rh-mobb/helm-charts/main/charts/rosa-thanos-s3/files/operatorhub.yaml Deploy Thanos Store Gateway Deploy ROSA Thanos S3 Helm Chart helm upgrade -n $NAMESPACE rosa-thanos-s3 --install mobb/rosa-thanos-s3 \\ --set \"aws.roleArn=$ROLE_ARN\" \\ --set \"rosa.clusterName=$CLUSTER_NAME\" Append remoteWrite settings to the user-workload-monitoring config to forward user workload metrics to Thanos. Check if the User Workload Config Map exists: bash oc -n openshift-user-workload-monitoring get \\ configmaps user-workload-monitoring-config If the config doesn't exist run: bash cat << EOF | kubectl apply -f - apiVersion: v1 kind: ConfigMap metadata: name: user-workload-monitoring-config namespace: openshift-user-workload-monitoring data: config.yaml: | prometheus: remoteWrite: - url: \"http://thanos-receive.${NAMESPACE}.svc.cluster.local:9091/api/v1/receive\" EOF Otherwise update it with the following: bash oc -n openshift-user-workload-monitoring edit \\ configmaps user-workload-monitoring-config yaml data: config.yaml: | ... prometheus: ... remoteWrite: - url: \"http://thanos-receive.thanos-receiver.svc.cluster.local:9091/api/v1/receive\" Check metrics are flowing by logging into Grafana get the Route URL for Grafana (remember its https) and login using username root and the password you updated to (or the default of secret ). bash oc -n thanos-receiver get route grafana-route Once logged in go to Dashboards->Manage and expand the federated-metrics group and you should see the cluster metrics dashboards. Click on the Use Method / Cluster Dashboard and you should see metrics. \\o/.","title":"Federating ROSA Metrics to S3"},{"location":"rosa/federated-metrics/#federating-system-and-user-metrics-to-s3-in-red-hat-openshift-for-aws","text":"Paul Czarkowski 06/07/2021 This guide walks through setting up federating Prometheus metrics to S3 storage. ToDo - Add Authorization in front of Thanos APIs","title":"Federating System and User metrics to S3 in Red Hat OpenShift for AWS"},{"location":"rosa/federated-metrics/#prerequisites","text":"A ROSA cluster deployed with STS aws CLI","title":"Prerequisites"},{"location":"rosa/federated-metrics/#set-up-environment","text":"Create environment variables bash export CLUSTER_NAME=my-cluster export S3_BUCKET=my-thanos-bucket export REGION=us-east-2 export NAMESPACE=federated-metrics export SA=aws-prometheus-proxy export SCRATCH_DIR=/tmp/scratch export OIDC_PROVIDER=$(oc get authentication.config.openshift.io cluster -o json | jq -r .spec.serviceAccountIssuer| sed -e \"s/^https:\\/\\///\") export AWS_ACCOUNT_ID=$(aws sts get-caller-identity --query Account --output text) export AWS_PAGER=\"\" rm -rf $SCRATCH_DIR mkdir -p $SCRATCH_DIR Create namespace bash oc new-project $NAMESPACE","title":"Set up environment"},{"location":"rosa/federated-metrics/#aws-preperation","text":"Create an S3 bucket bash aws s3 mb s3://$S3_BUCKET Create a Policy for access to S3 bash cat <<EOF > $SCRATCH_DIR/s3-policy.json { \"Version\": \"2012-10-17\", \"Statement\": [ { \"Sid\": \"Statement\", \"Effect\": \"Allow\", \"Action\": [ \"s3:ListBucket\", \"s3:GetObject\", \"s3:DeleteObject\", \"s3:PutObject\", \"s3:PutObjectAcl\" ], \"Resource\": [ \"arn:aws:s3:::$S3_BUCKET/*\", \"arn:aws:s3:::$S3_BUCKET\" ] } ] } EOF Apply the Policy bash S3_POLICY=$(aws iam create-policy --policy-name $CLUSTER_NAME-thanos \\ --policy-document file://$SCRATCH_DIR/s3-policy.json \\ --query 'Policy.Arn' --output text) echo $S3_POLICY Create a Trust Policy bash cat <<EOF > $SCRATCH_DIR/TrustPolicy.json { \"Version\": \"2012-10-17\", \"Statement\": [ { \"Effect\": \"Allow\", \"Principal\": { \"Federated\": \"arn:aws:iam::${AWS_ACCOUNT_ID}:oidc-provider/${OIDC_PROVIDER}\" }, \"Action\": \"sts:AssumeRoleWithWebIdentity\", \"Condition\": { \"StringEquals\": { \"${OIDC_PROVIDER}:sub\": [ \"system:serviceaccount:${NAMESPACE}:${SA}\" ] } } } ] } EOF Create Role for AWS Prometheus and CloudWatch bash S3_ROLE=$(aws iam create-role \\ --role-name \"$CLUSTER-thanos-s3\" \\ --assume-role-policy-document file://$SCRATCH_DIR/TrustPolicy.json \\ --query \"Role.Arn\" --output text) echo $S3_ROLE Attach the Policies to the Role bash aws iam attach-role-policy \\ --role-name \"$CLUSTER-thanos-s3\" \\ --policy-arn $S3_POLICY","title":"AWS Preperation"},{"location":"rosa/federated-metrics/#deploy-operators","text":"Add the MOBB chart repository to your Helm bash helm repo add mobb https://rh-mobb.github.io/helm-charts/ Update your repositories bash helm repo update Use the mobb/operatorhub chart to deploy the needed operators bash helm upgrade -n $echNAMESPACE custom-metrics-operators \\ mobb/operatorhub --version 0.1.1 --install \\ --values https://raw.githubusercontent.com/rh-mobb/helm-charts/main/charts/rosa-thanos-s3/files/operatorhub.yaml","title":"Deploy Operators"},{"location":"rosa/federated-metrics/#deploy-thanos-store-gateway","text":"Deploy ROSA Thanos S3 Helm Chart helm upgrade -n $NAMESPACE rosa-thanos-s3 --install mobb/rosa-thanos-s3 \\ --set \"aws.roleArn=$ROLE_ARN\" \\ --set \"rosa.clusterName=$CLUSTER_NAME\" Append remoteWrite settings to the user-workload-monitoring config to forward user workload metrics to Thanos. Check if the User Workload Config Map exists: bash oc -n openshift-user-workload-monitoring get \\ configmaps user-workload-monitoring-config If the config doesn't exist run: bash cat << EOF | kubectl apply -f - apiVersion: v1 kind: ConfigMap metadata: name: user-workload-monitoring-config namespace: openshift-user-workload-monitoring data: config.yaml: | prometheus: remoteWrite: - url: \"http://thanos-receive.${NAMESPACE}.svc.cluster.local:9091/api/v1/receive\" EOF Otherwise update it with the following: bash oc -n openshift-user-workload-monitoring edit \\ configmaps user-workload-monitoring-config yaml data: config.yaml: | ... prometheus: ... remoteWrite: - url: \"http://thanos-receive.thanos-receiver.svc.cluster.local:9091/api/v1/receive\"","title":"Deploy Thanos Store Gateway"},{"location":"rosa/federated-metrics/#check-metrics-are-flowing-by-logging-into-grafana","text":"get the Route URL for Grafana (remember its https) and login using username root and the password you updated to (or the default of secret ). bash oc -n thanos-receiver get route grafana-route Once logged in go to Dashboards->Manage and expand the federated-metrics group and you should see the cluster metrics dashboards. Click on the Use Method / Cluster Dashboard and you should see metrics. \\o/.","title":"Check metrics are flowing by logging into Grafana"},{"location":"rosa/federated-metrics/user-defined/","text":"User Workload Monitoring on Azure Red Hat OpenShift In Azure Red Hat OpenShift (ARO) Monitoring for User Defined Projects is disabled by default. Follow these instructions to enable it. Enabling See docs for more indepth details. Check the cluster-monitoring-config ConfigMap object bash oc -n openshift-monitoring get configmap cluster-monitoring-config -o yaml Enable User Workload Monitoring by doing one of the following If the data.config.yaml is not {} you should edit it and add the enableUserWorkload: true line manually. bash oc -n openshift-monitoring edit configmap cluster-monitoring-config Otherwise if its {} then you can run the following command safely. bash cat << EOF | oc apply -f - apiVersion: v1 kind: ConfigMap metadata: name: cluster-monitoring-config namespace: openshift-monitoring data: config.yaml: | enableUserWorkload: true EOF Create a config for User Workload Monitoring to set retention and This will configure the user workload instance to have PVC storage and will set basic data retention values. Feel free to edit it to suit your needs. Remember if you're going to have PVCs enabled they are tied to an AZ, to for a multi-AZ cluster you should ensure you have at least 2 workers per AZ so that they can failover. bash cat << EOF | oc apply -f - apiVersion: v1 kind: ConfigMap metadata: name: user-workload-monitoring-config namespace: openshift-user-workload-monitoring data: config.yaml: | prometheus: volumeClaimTemplate: spec: storageClassName: managed-premium volumeMode: Filesystem resources: requests: storage: 40Gi retention: 24h resources: requests: cpu: 200m memory: 2Gi EOF Deploy an example application with a service monitor resource bash oc apply -f example-app.yaml Wait a few minutes and then check your cluster metrics. Switch to Developer mode Change the Project to ns1 Click the Monitoring button Grafana Create a Project for the Grafana Operator + Application bash oc new-project custom-grafana Install the Grafana Operator (or via the OperatorHub in the GUI) bash cat << EOF | oc apply -f - apiVersion: operators.coreos.com/v1alpha1 kind: Subscription metadata: name: grafana-operator namespace: custom-grafana labels: operators.coreos.com/grafana-operator.custom-grafana: '' spec: channel: alpha installPlanApproval: Automatic name: grafana-operator source: community-operators sourceNamespace: openshift-marketplace startingCSV: grafana-operator.v3.10.1 EOF Once the Grafana Operator is running create a Grafana Instance cat << EOF | oc apply -f - apiVersion: integreatly.org/v1alpha1 kind: Grafana metadata: name: custom-grafana namespace: custom-grafana spec: adminPassword: bad-password adminUser: admin basicAuth: true config: auth: disable_signout_menu: false auth.anonymous: enabled: false log: level: warn mode: console security: admin_password: secret admin_user: root dashboardLabelSelector: - matchExpressions: - key: app operator: In values: - grafana ingress: enabled: true EOF Once the instance has been created you should be able to log in by getting the route and using the admin user/pass from above. bash oc -n custom-grafana get routes The output should look like NAME HOST/PORT PATH SERVICES PORT TERMINATION WILDCARD grafana-route grafana-route-custom-grafana.apps.w4l8w924.eastus.aroapp.io grafana-service 3000 edge None Copy and paste the host into your browser and log in to verify its working. Grant the grafana instance access to cluster-metrics bash oc adm policy add-cluster-role-to-user \\ cluster-monitoring-view -z grafana-serviceaccount Save the service accounts bearer token as a variable bash BEARER_TOKEN=`oc serviceaccounts get-token grafana-serviceaccount -n custom-grafana` Create a datasource to access the Thanos Querier bash cat << EOF | oc apply -f - apiVersion: integreatly.org/v1alpha1 kind: GrafanaDataSource metadata: name: prometheus-grafanadatasource namespace: custom-grafana spec: datasources: - access: proxy editable: true isDefault: true jsonData: httpHeaderName1: 'Authorization' timeInterval: 5s tlsSkipVerify: true name: Prometheus secureJsonData: httpHeaderValue1: 'Bearer ${BEARER_TOKEN}' type: prometheus url: 'https://thanos-querier.openshift-monitoring.svc.cluster.local:9091' name: prometheus-grafanadatasource.yaml EOF Add system dashboards to Grafana The dashboards.yaml file was created by running the script generate-dashboards.sh which fetches the dashboard json files from the openshift-monitoring namespace. oc apply -f dashboards.yaml","title":"User Workload Monitoring on Azure Red Hat OpenShift"},{"location":"rosa/federated-metrics/user-defined/#user-workload-monitoring-on-azure-red-hat-openshift","text":"In Azure Red Hat OpenShift (ARO) Monitoring for User Defined Projects is disabled by default. Follow these instructions to enable it.","title":"User Workload Monitoring on Azure Red Hat OpenShift"},{"location":"rosa/federated-metrics/user-defined/#enabling","text":"See docs for more indepth details. Check the cluster-monitoring-config ConfigMap object bash oc -n openshift-monitoring get configmap cluster-monitoring-config -o yaml Enable User Workload Monitoring by doing one of the following If the data.config.yaml is not {} you should edit it and add the enableUserWorkload: true line manually. bash oc -n openshift-monitoring edit configmap cluster-monitoring-config Otherwise if its {} then you can run the following command safely. bash cat << EOF | oc apply -f - apiVersion: v1 kind: ConfigMap metadata: name: cluster-monitoring-config namespace: openshift-monitoring data: config.yaml: | enableUserWorkload: true EOF Create a config for User Workload Monitoring to set retention and This will configure the user workload instance to have PVC storage and will set basic data retention values. Feel free to edit it to suit your needs. Remember if you're going to have PVCs enabled they are tied to an AZ, to for a multi-AZ cluster you should ensure you have at least 2 workers per AZ so that they can failover. bash cat << EOF | oc apply -f - apiVersion: v1 kind: ConfigMap metadata: name: user-workload-monitoring-config namespace: openshift-user-workload-monitoring data: config.yaml: | prometheus: volumeClaimTemplate: spec: storageClassName: managed-premium volumeMode: Filesystem resources: requests: storage: 40Gi retention: 24h resources: requests: cpu: 200m memory: 2Gi EOF Deploy an example application with a service monitor resource bash oc apply -f example-app.yaml Wait a few minutes and then check your cluster metrics. Switch to Developer mode Change the Project to ns1 Click the Monitoring button","title":"Enabling"},{"location":"rosa/federated-metrics/user-defined/#grafana","text":"Create a Project for the Grafana Operator + Application bash oc new-project custom-grafana Install the Grafana Operator (or via the OperatorHub in the GUI) bash cat << EOF | oc apply -f - apiVersion: operators.coreos.com/v1alpha1 kind: Subscription metadata: name: grafana-operator namespace: custom-grafana labels: operators.coreos.com/grafana-operator.custom-grafana: '' spec: channel: alpha installPlanApproval: Automatic name: grafana-operator source: community-operators sourceNamespace: openshift-marketplace startingCSV: grafana-operator.v3.10.1 EOF Once the Grafana Operator is running create a Grafana Instance cat << EOF | oc apply -f - apiVersion: integreatly.org/v1alpha1 kind: Grafana metadata: name: custom-grafana namespace: custom-grafana spec: adminPassword: bad-password adminUser: admin basicAuth: true config: auth: disable_signout_menu: false auth.anonymous: enabled: false log: level: warn mode: console security: admin_password: secret admin_user: root dashboardLabelSelector: - matchExpressions: - key: app operator: In values: - grafana ingress: enabled: true EOF Once the instance has been created you should be able to log in by getting the route and using the admin user/pass from above. bash oc -n custom-grafana get routes The output should look like NAME HOST/PORT PATH SERVICES PORT TERMINATION WILDCARD grafana-route grafana-route-custom-grafana.apps.w4l8w924.eastus.aroapp.io grafana-service 3000 edge None Copy and paste the host into your browser and log in to verify its working. Grant the grafana instance access to cluster-metrics bash oc adm policy add-cluster-role-to-user \\ cluster-monitoring-view -z grafana-serviceaccount Save the service accounts bearer token as a variable bash BEARER_TOKEN=`oc serviceaccounts get-token grafana-serviceaccount -n custom-grafana` Create a datasource to access the Thanos Querier bash cat << EOF | oc apply -f - apiVersion: integreatly.org/v1alpha1 kind: GrafanaDataSource metadata: name: prometheus-grafanadatasource namespace: custom-grafana spec: datasources: - access: proxy editable: true isDefault: true jsonData: httpHeaderName1: 'Authorization' timeInterval: 5s tlsSkipVerify: true name: Prometheus secureJsonData: httpHeaderValue1: 'Bearer ${BEARER_TOKEN}' type: prometheus url: 'https://thanos-querier.openshift-monitoring.svc.cluster.local:9091' name: prometheus-grafanadatasource.yaml EOF Add system dashboards to Grafana The dashboards.yaml file was created by running the script generate-dashboards.sh which fetches the dashboard json files from the openshift-monitoring namespace. oc apply -f dashboards.yaml","title":"Grafana"},{"location":"rosa/federated-metrics-prometheus/","text":"Federating Metrics to a centralized Prometheus Cluster Red Hat Openshift for AWS (ROSA) comes with two built-in monitoring stacks. ClusterMonitoring and User Workload Monitoring . They are both based on Prometheus, the first targets the Cluster Operator (Red Hat SRE) and the latter targets the Cluster user (you!). Both provide amazing metrics insights inside the Cluster's web console, showing overall cluster metrics as well as namespace specific workload metrics, all integrated with your configured IDP. However the Alert Manager instance is locked down and used to send alerts to the Red Hat SRE team. This means that the customer cannot create alerts for either the cluster resources, or their own workloads. This is being worked on and future versions of ROSA will provide a way for the end user to create alerts for their own workloads. Until that work is done, the ROSA cluster administrator can deploy a Prometheus instance and configure it to send alerts to themselves. Thankfully with Prometheus' federated metrics feature and the Prometheus Operator, this can be done in a few simple steps. This guide is heavily influenced by Tommer Amber's guide for OCP 4.x. Pre-requisites Make sure the following pre-requisites are met: Helm 3 A Red Hat OpenShift for AWS (ROSA) cluster 4.8 or higher Prepare Environment Set the following environment variables bash export NAMESPACE=federated-metrics Create the namespace bash oc new-project $NAMESPACE Add the MOBB chart repository to your Helm bash helm repo add mobb https://rh-mobb.github.io/helm-charts/ Update your repositories bash helm repo update Use the mobb/operatorhub chart to deploy the needed operators bash helm upgrade -n $NAMESPACE federated-metrics-operators \\ mobb/operatorhub --version 0.1.1 --install \\ --values https://raw.githubusercontent.com/rh-mobb/helm-charts/main/charts/rosa-federated-prometheus/files/operatorhub.yaml Wait until the two operators are running bash watch kubectl get pods -n $NAMESPACE NAME READY STATUS RESTARTS AGE grafana-operator-controller-manager-775f8d98c9-822h7 2/2 Running 0 7m33s operatorhubio-dtb2v 1/1 Running 0 8m32s prometheus-operator-5cb6844699-t7wfd 1/1 Running 0 7m29s Deploy the monitoring stack Install the mobb/rosa-federated-prometheus Helm Chart bash helm upgrade --install -n $NAMESPACE monitoring \\ --set grafana-cr.basicAuthPassword='mypassword' \\ --set fullnameOverride='monitoring' \\ --version 0.5.3 \\ mobb/rosa-federated-prometheus Validate Prometheus Ensure the new Prometheus instance's Pods are running bash kubectl get pods -n ${NAMESPACE} -l app=prometheus -o wide You should see the following: bash NAME READY STATUS RESTARTS AGE IP NODE NOMINATED NODE READINESS GATES prometheus-federation-prometheus-0 3/3 Running 1 7m58s 10.131.0.104 ip-10-0-215-84.us-east-2.compute.internal <none> <none> prometheus-federation-prometheus-1 3/3 Running 1 7m58s 10.128.2.21 ip-10-0-146-85.us-east-2.compute.internal <none> <none> Log into the new Prometheus instance Fetch the Route: bash kubectl -n ${NAMESPACE} get route prometheus-route You should see the following: bash NAME HOST/PORT PATH SERVICES PORT TERMINATION WILDCARD prometheus-route prometheus-route-custom-prometheus.apps.mycluster.jnmf.p1.openshiftapps.com monitoring-prometheus-cr web-proxy reencrypt None Open the Prometheus Route in your browser (the HOST/PATH field from above) It should take you through authorization and then you should see the Prometheus UI. add /targets to the end of the URL to see the list of available targets Switch out the trailing path to be graph?g0.range_input=1h&g0.expr=kubelet_running_containers&g0.tab=0 to see the graph of the number of running containers fetched from cluster monitoring. click on Alerts in the menu to see our example Alert Validate Alert Manager forward a port to Alert Manager bash kubectl -n ${NAMESPACE} port-forward svc/monitoring-alertmanager-cr 9093:9093 Browse to http://localhost:9093/#/alerts to see the alert \"ExampleAlert\" Validate Grafana and Dashboards Find the Grafana Route bash kubectl get route grafana-route bash NAME HOST/PORT PATH SERVICES PORT TERMINATION WILDCARD grafana-route grafana-route-federated-metrics.apps.metrics.9l1z.p1.openshiftapps.com / grafana-service grafana-proxy reencrypt None Log into grafana using your cluster's idp Click login and login to Grafana as admin with the password you set when doing helm install . Click on Configuration -> Datasources and check that the prometheus data source is loaded. Sometimes due to Kubernetes resource ordering the Data Source may not be loaded. We can force the Operator to reload it by running kubectl annotate -n $NAMESPACE grafanadatasources.integreatly.org federated reroll=true Click on Dashboards -> Manage and click on the \"Use Method / Cluster\" dashboard. Cleanup Delete the helm release bash helm -n $NAMESPACE delete monitoring Delete the namespace bash kubectl delete namespace $NAMESPACE","title":"Federating Metrics to a centralized Prometheus Cluster"},{"location":"rosa/federated-metrics-prometheus/#federating-metrics-to-a-centralized-prometheus-cluster","text":"Red Hat Openshift for AWS (ROSA) comes with two built-in monitoring stacks. ClusterMonitoring and User Workload Monitoring . They are both based on Prometheus, the first targets the Cluster Operator (Red Hat SRE) and the latter targets the Cluster user (you!). Both provide amazing metrics insights inside the Cluster's web console, showing overall cluster metrics as well as namespace specific workload metrics, all integrated with your configured IDP. However the Alert Manager instance is locked down and used to send alerts to the Red Hat SRE team. This means that the customer cannot create alerts for either the cluster resources, or their own workloads. This is being worked on and future versions of ROSA will provide a way for the end user to create alerts for their own workloads. Until that work is done, the ROSA cluster administrator can deploy a Prometheus instance and configure it to send alerts to themselves. Thankfully with Prometheus' federated metrics feature and the Prometheus Operator, this can be done in a few simple steps. This guide is heavily influenced by Tommer Amber's guide for OCP 4.x.","title":"Federating Metrics to a centralized Prometheus Cluster"},{"location":"rosa/federated-metrics-prometheus/#pre-requisites","text":"Make sure the following pre-requisites are met: Helm 3 A Red Hat OpenShift for AWS (ROSA) cluster 4.8 or higher","title":"Pre-requisites"},{"location":"rosa/federated-metrics-prometheus/#prepare-environment","text":"Set the following environment variables bash export NAMESPACE=federated-metrics Create the namespace bash oc new-project $NAMESPACE Add the MOBB chart repository to your Helm bash helm repo add mobb https://rh-mobb.github.io/helm-charts/ Update your repositories bash helm repo update Use the mobb/operatorhub chart to deploy the needed operators bash helm upgrade -n $NAMESPACE federated-metrics-operators \\ mobb/operatorhub --version 0.1.1 --install \\ --values https://raw.githubusercontent.com/rh-mobb/helm-charts/main/charts/rosa-federated-prometheus/files/operatorhub.yaml Wait until the two operators are running bash watch kubectl get pods -n $NAMESPACE NAME READY STATUS RESTARTS AGE grafana-operator-controller-manager-775f8d98c9-822h7 2/2 Running 0 7m33s operatorhubio-dtb2v 1/1 Running 0 8m32s prometheus-operator-5cb6844699-t7wfd 1/1 Running 0 7m29s","title":"Prepare Environment"},{"location":"rosa/federated-metrics-prometheus/#deploy-the-monitoring-stack","text":"Install the mobb/rosa-federated-prometheus Helm Chart bash helm upgrade --install -n $NAMESPACE monitoring \\ --set grafana-cr.basicAuthPassword='mypassword' \\ --set fullnameOverride='monitoring' \\ --version 0.5.3 \\ mobb/rosa-federated-prometheus","title":"Deploy the monitoring stack"},{"location":"rosa/federated-metrics-prometheus/#validate-prometheus","text":"Ensure the new Prometheus instance's Pods are running bash kubectl get pods -n ${NAMESPACE} -l app=prometheus -o wide You should see the following: bash NAME READY STATUS RESTARTS AGE IP NODE NOMINATED NODE READINESS GATES prometheus-federation-prometheus-0 3/3 Running 1 7m58s 10.131.0.104 ip-10-0-215-84.us-east-2.compute.internal <none> <none> prometheus-federation-prometheus-1 3/3 Running 1 7m58s 10.128.2.21 ip-10-0-146-85.us-east-2.compute.internal <none> <none> Log into the new Prometheus instance Fetch the Route: bash kubectl -n ${NAMESPACE} get route prometheus-route You should see the following: bash NAME HOST/PORT PATH SERVICES PORT TERMINATION WILDCARD prometheus-route prometheus-route-custom-prometheus.apps.mycluster.jnmf.p1.openshiftapps.com monitoring-prometheus-cr web-proxy reencrypt None Open the Prometheus Route in your browser (the HOST/PATH field from above) It should take you through authorization and then you should see the Prometheus UI. add /targets to the end of the URL to see the list of available targets Switch out the trailing path to be graph?g0.range_input=1h&g0.expr=kubelet_running_containers&g0.tab=0 to see the graph of the number of running containers fetched from cluster monitoring. click on Alerts in the menu to see our example Alert","title":"Validate Prometheus"},{"location":"rosa/federated-metrics-prometheus/#validate-alert-manager","text":"forward a port to Alert Manager bash kubectl -n ${NAMESPACE} port-forward svc/monitoring-alertmanager-cr 9093:9093 Browse to http://localhost:9093/#/alerts to see the alert \"ExampleAlert\"","title":"Validate Alert Manager"},{"location":"rosa/federated-metrics-prometheus/#validate-grafana-and-dashboards","text":"Find the Grafana Route bash kubectl get route grafana-route bash NAME HOST/PORT PATH SERVICES PORT TERMINATION WILDCARD grafana-route grafana-route-federated-metrics.apps.metrics.9l1z.p1.openshiftapps.com / grafana-service grafana-proxy reencrypt None Log into grafana using your cluster's idp Click login and login to Grafana as admin with the password you set when doing helm install . Click on Configuration -> Datasources and check that the prometheus data source is loaded. Sometimes due to Kubernetes resource ordering the Data Source may not be loaded. We can force the Operator to reload it by running kubectl annotate -n $NAMESPACE grafanadatasources.integreatly.org federated reroll=true Click on Dashboards -> Manage and click on the \"Use Method / Cluster\" dashboard.","title":"Validate Grafana and Dashboards"},{"location":"rosa/federated-metrics-prometheus/#cleanup","text":"Delete the helm release bash helm -n $NAMESPACE delete monitoring Delete the namespace bash kubectl delete namespace $NAMESPACE","title":"Cleanup"},{"location":"rosa/kms/","text":"Creating a ROSA cluster in STS mode with custom KMS key Byron Miller Last updated 4/21/2022 Tip Official Documentation ROSA STS with custom KMS key This guide will walk you through installing ROSA (Red Hat OpenShift Service on AWS) with a customer-provided KMS key that will be used to encrypt both the root volumes of nodes as well as persistent volumes for mounted EBS claims. Prerequisites AWS CLI ROSA CLI v1.1.11 or higher OpenShift CLI - rosa download openshift-client Prepare AWS Account for ROSA Configure the AWS CLI by running the following command bash aws configure You will be required to enter an AWS Access Key ID and an AWS Secret Access Key along with a default region name and output format bash % aws configure AWS Access Key ID []: AWS Secret Access Key []: Default region name [us-east-2]: Default output format [json]: The AWS Access Key ID and AWS Secret Access Key values can be obtained by logging in to the AWS console and creating an Access Key in the Security Credentials section of the IAM dashboard for your user Validate your credentials bash aws sts get-caller-identity You should receive output similar to the following json { \"UserId\": <your ID>, \"Account\": <your account>, \"Arn\": <your arn> } You will need to save the account ID for adding it to your KMS key to define installer role, so take note. If this is a brand new AWS account that has never had a AWS Load Balancer installed in it, you should run the following bash aws iam create-service-linked-role --aws-service-name \\ \"elasticloadbalancing.amazonaws.com\" Set the AWS region you plan to deploy your cluser into. For this example, we will deploy into us-east-2 . bash export AWS_REGION=\"us-east-2\" Create KMS Key For this example, we will create a custom KMS key using the AWS CLI. If you would prefer, you could use an existing key instead. Create a customer-managed KMS key bash KMS_ARN=$(aws kms create-key --region $AWS_REGION --description 'Custom ROSA Encryption Key' --query KeyMetadata.Arn --output text) This command will save the ARN output of this custom key for further steps. Generate the necessary key policy to allow the ROSA STS roles to access the key. Use the below command to populate a sample policy, or create your own. Important note, if you specify a custom STS role prefix, you will need to update that in the command below. bash AWS_ACCOUNT=$(aws sts get-caller-identity --query Account --output text); cat << EOF > rosa-key-policy.json { \"Version\": \"2012-10-17\", \"Id\": \"key-rosa-policy-1\", \"Statement\": [ { \"Sid\": \"Enable IAM User Permissions\", \"Effect\": \"Allow\", \"Principal\": { \"AWS\": \"arn:aws:iam::${AWS_ACCOUNT}:root\" }, \"Action\": \"kms:*\", \"Resource\": \"*\" }, { \"Sid\": \"Allow ROSA use of the key\", \"Effect\": \"Allow\", \"Principal\": { \"AWS\": [ \"arn:aws:iam::${AWS_ACCOUNT}:role/ManagedOpenShift-Support-Role\", \"arn:aws:iam::${AWS_ACCOUNT}:role/ManagedOpenShift-Installer-Role\", \"arn:aws:iam::${AWS_ACCOUNT}:role/ManagedOpenShift-Worker-Role\", \"arn:aws:iam::${AWS_ACCOUNT}:role/ManagedOpenShift-ControlPlane-Role\" ] }, \"Action\": [ \"kms:Encrypt\", \"kms:Decrypt\", \"kms:ReEncrypt*\", \"kms:GenerateDataKey*\", \"kms:DescribeKey\" ], \"Resource\": \"*\" }, { \"Sid\": \"Allow attachment of persistent resources\", \"Effect\": \"Allow\", \"Principal\": { \"AWS\": [ \"arn:aws:iam::${AWS_ACCOUNT}:role/ManagedOpenShift-Support-Role\", \"arn:aws:iam::${AWS_ACCOUNT}:role/ManagedOpenShift-Installer-Role\", \"arn:aws:iam::${AWS_ACCOUNT}:role/ManagedOpenShift-Worker-Role\", \"arn:aws:iam::${AWS_ACCOUNT}:role/ManagedOpenShift-ControlPlane-Role\" ] }, \"Action\": [ \"kms:CreateGrant\", \"kms:ListGrants\", \"kms:RevokeGrant\" ], \"Resource\": \"*\", \"Condition\": { \"Bool\": { \"kms:GrantIsForAWSResource\": \"true\" } } } ] } EOF Apply the newly generated key policy to the custom KMS key. bash aws kms put-key-policy --key-id $KMS_ARN \\ --policy file://rosa-key-policy.json \\ --policy-name default Create ROSA Cluster Make sure your ROSA CLI version is at minimum v1.1.11 or higher. bash rosa version Create the ROSA STS Account Roles If you have already installed account-roles into your aws account, you can skip this step. bash rosa create account-roles --mode auto -y Set Environment Variables bash ROSA_CLUSTER_NAME=poc-kmskey Using the ROSA CLI, create your cluster. While this is an example, feel free to customize this command to best suit your needs. bash rosa create cluster --cluster-name $ROSA_CLUSTER_NAME --sts \\ --region $AWS_REGION --compute-nodes 2 --machine-cidr 10.0.0.0/16 \\ --service-cidr 172.30.0.0/16 --pod-cidr 10.128.0.0/14 --host-prefix 23 \\ --kms-key-arn $KMS_ARN Create the operator roles necessary for the cluster to function. bash rosa create operator-roles -c $ROSA_CLUSTER_NAME --mode auto --yes Create the OIDC provider necessary for the cluster to authenticate. bash rosa create oidc-provider -c $ROSA_CLUSTER_NAME --mode auto --yes Validate that the cluster is now installing. Within 5 minutes, the cluster state should move beyond pending and show installing . bash watch \"rosa describe cluster -c $ROSA_CLUSTER_NAME\" Watch the install logs as the cluster installs. bash rosa logs install -c $ROSA_CLUSTER_NAME --watch --tail 10 Validate the cluster Once the cluster has finished installing we can validate our access to the cluster. Create an Admin user bash rosa create admin -c $ROSA_CLUSTER_NAME Run the resulting login statement from output. May take 2-3 minutes before authentication is fully synced Verify the default persistent volumes in the cluster. bash oc get pv Output: bash NAME CAPACITY ACCESS MODES RECLAIM POLICY STATUS CLAIM STORAGECLASS REASON AGE pvc-00dac374-a45e-43fa-a313-ae0491e8edf1 10Gi RWO Delete Bound openshift-monitoring/alertmanager-data-alertmanager-main-1 gp2-customer-kms 26m pvc-7d211496-4ddf-4200-921c-1404b754afa5 10Gi RWO Delete Bound openshift-monitoring/alertmanager-data-alertmanager-main-0 gp2-customer-kms 26m pvc-b5243cef-ec30-4e5c-a348-aeb8136a908c 100Gi RWO Delete Bound openshift-monitoring/prometheus-data-prometheus-k8s-0 gp2-customer-kms 26m pvc-ec60c1cf-72cf-4ac6-ab12-8e9e5afdc15f 100Gi RWO Delete Bound openshift-monitoring/prometheus-data-prometheus-k8s-1 gp2-customer-kms 26m You should see the StroageClass set to gp2-customer-kms . This is the default StorageClass which is encrypted using the customer-provided key. Cleanup Delete the ROSA cluster bash rosa delete cluster -c $ROSA_CLUSTER_NAME Once the cluster is deleted, delete the cluster's STS roles. bash rosa delete operator-roles -c $ROSA_CLUSTER_NAME --yes --mode auto rosa delete oidc-provider -c $ROSA_CLUSTER_NAME --yes --mode auto","title":"Creating a ROSA cluster in STS mode with custom KMS key"},{"location":"rosa/kms/#creating-a-rosa-cluster-in-sts-mode-with-custom-kms-key","text":"Byron Miller Last updated 4/21/2022 Tip Official Documentation ROSA STS with custom KMS key This guide will walk you through installing ROSA (Red Hat OpenShift Service on AWS) with a customer-provided KMS key that will be used to encrypt both the root volumes of nodes as well as persistent volumes for mounted EBS claims.","title":"Creating a ROSA cluster in STS mode with custom KMS key"},{"location":"rosa/kms/#prerequisites","text":"AWS CLI ROSA CLI v1.1.11 or higher OpenShift CLI - rosa download openshift-client","title":"Prerequisites"},{"location":"rosa/kms/#prepare-aws-account-for-rosa","text":"Configure the AWS CLI by running the following command bash aws configure You will be required to enter an AWS Access Key ID and an AWS Secret Access Key along with a default region name and output format bash % aws configure AWS Access Key ID []: AWS Secret Access Key []: Default region name [us-east-2]: Default output format [json]: The AWS Access Key ID and AWS Secret Access Key values can be obtained by logging in to the AWS console and creating an Access Key in the Security Credentials section of the IAM dashboard for your user Validate your credentials bash aws sts get-caller-identity You should receive output similar to the following json { \"UserId\": <your ID>, \"Account\": <your account>, \"Arn\": <your arn> } You will need to save the account ID for adding it to your KMS key to define installer role, so take note. If this is a brand new AWS account that has never had a AWS Load Balancer installed in it, you should run the following bash aws iam create-service-linked-role --aws-service-name \\ \"elasticloadbalancing.amazonaws.com\" Set the AWS region you plan to deploy your cluser into. For this example, we will deploy into us-east-2 . bash export AWS_REGION=\"us-east-2\"","title":"Prepare AWS Account for ROSA"},{"location":"rosa/kms/#create-kms-key","text":"For this example, we will create a custom KMS key using the AWS CLI. If you would prefer, you could use an existing key instead. Create a customer-managed KMS key bash KMS_ARN=$(aws kms create-key --region $AWS_REGION --description 'Custom ROSA Encryption Key' --query KeyMetadata.Arn --output text) This command will save the ARN output of this custom key for further steps. Generate the necessary key policy to allow the ROSA STS roles to access the key. Use the below command to populate a sample policy, or create your own. Important note, if you specify a custom STS role prefix, you will need to update that in the command below. bash AWS_ACCOUNT=$(aws sts get-caller-identity --query Account --output text); cat << EOF > rosa-key-policy.json { \"Version\": \"2012-10-17\", \"Id\": \"key-rosa-policy-1\", \"Statement\": [ { \"Sid\": \"Enable IAM User Permissions\", \"Effect\": \"Allow\", \"Principal\": { \"AWS\": \"arn:aws:iam::${AWS_ACCOUNT}:root\" }, \"Action\": \"kms:*\", \"Resource\": \"*\" }, { \"Sid\": \"Allow ROSA use of the key\", \"Effect\": \"Allow\", \"Principal\": { \"AWS\": [ \"arn:aws:iam::${AWS_ACCOUNT}:role/ManagedOpenShift-Support-Role\", \"arn:aws:iam::${AWS_ACCOUNT}:role/ManagedOpenShift-Installer-Role\", \"arn:aws:iam::${AWS_ACCOUNT}:role/ManagedOpenShift-Worker-Role\", \"arn:aws:iam::${AWS_ACCOUNT}:role/ManagedOpenShift-ControlPlane-Role\" ] }, \"Action\": [ \"kms:Encrypt\", \"kms:Decrypt\", \"kms:ReEncrypt*\", \"kms:GenerateDataKey*\", \"kms:DescribeKey\" ], \"Resource\": \"*\" }, { \"Sid\": \"Allow attachment of persistent resources\", \"Effect\": \"Allow\", \"Principal\": { \"AWS\": [ \"arn:aws:iam::${AWS_ACCOUNT}:role/ManagedOpenShift-Support-Role\", \"arn:aws:iam::${AWS_ACCOUNT}:role/ManagedOpenShift-Installer-Role\", \"arn:aws:iam::${AWS_ACCOUNT}:role/ManagedOpenShift-Worker-Role\", \"arn:aws:iam::${AWS_ACCOUNT}:role/ManagedOpenShift-ControlPlane-Role\" ] }, \"Action\": [ \"kms:CreateGrant\", \"kms:ListGrants\", \"kms:RevokeGrant\" ], \"Resource\": \"*\", \"Condition\": { \"Bool\": { \"kms:GrantIsForAWSResource\": \"true\" } } } ] } EOF Apply the newly generated key policy to the custom KMS key. bash aws kms put-key-policy --key-id $KMS_ARN \\ --policy file://rosa-key-policy.json \\ --policy-name default","title":"Create KMS Key"},{"location":"rosa/kms/#create-rosa-cluster","text":"Make sure your ROSA CLI version is at minimum v1.1.11 or higher. bash rosa version Create the ROSA STS Account Roles If you have already installed account-roles into your aws account, you can skip this step. bash rosa create account-roles --mode auto -y Set Environment Variables bash ROSA_CLUSTER_NAME=poc-kmskey Using the ROSA CLI, create your cluster. While this is an example, feel free to customize this command to best suit your needs. bash rosa create cluster --cluster-name $ROSA_CLUSTER_NAME --sts \\ --region $AWS_REGION --compute-nodes 2 --machine-cidr 10.0.0.0/16 \\ --service-cidr 172.30.0.0/16 --pod-cidr 10.128.0.0/14 --host-prefix 23 \\ --kms-key-arn $KMS_ARN Create the operator roles necessary for the cluster to function. bash rosa create operator-roles -c $ROSA_CLUSTER_NAME --mode auto --yes Create the OIDC provider necessary for the cluster to authenticate. bash rosa create oidc-provider -c $ROSA_CLUSTER_NAME --mode auto --yes Validate that the cluster is now installing. Within 5 minutes, the cluster state should move beyond pending and show installing . bash watch \"rosa describe cluster -c $ROSA_CLUSTER_NAME\" Watch the install logs as the cluster installs. bash rosa logs install -c $ROSA_CLUSTER_NAME --watch --tail 10","title":"Create ROSA Cluster"},{"location":"rosa/kms/#validate-the-cluster","text":"Once the cluster has finished installing we can validate our access to the cluster. Create an Admin user bash rosa create admin -c $ROSA_CLUSTER_NAME Run the resulting login statement from output. May take 2-3 minutes before authentication is fully synced Verify the default persistent volumes in the cluster. bash oc get pv Output: bash NAME CAPACITY ACCESS MODES RECLAIM POLICY STATUS CLAIM STORAGECLASS REASON AGE pvc-00dac374-a45e-43fa-a313-ae0491e8edf1 10Gi RWO Delete Bound openshift-monitoring/alertmanager-data-alertmanager-main-1 gp2-customer-kms 26m pvc-7d211496-4ddf-4200-921c-1404b754afa5 10Gi RWO Delete Bound openshift-monitoring/alertmanager-data-alertmanager-main-0 gp2-customer-kms 26m pvc-b5243cef-ec30-4e5c-a348-aeb8136a908c 100Gi RWO Delete Bound openshift-monitoring/prometheus-data-prometheus-k8s-0 gp2-customer-kms 26m pvc-ec60c1cf-72cf-4ac6-ab12-8e9e5afdc15f 100Gi RWO Delete Bound openshift-monitoring/prometheus-data-prometheus-k8s-1 gp2-customer-kms 26m You should see the StroageClass set to gp2-customer-kms . This is the default StorageClass which is encrypted using the customer-provided key.","title":"Validate the cluster"},{"location":"rosa/kms/#cleanup","text":"Delete the ROSA cluster bash rosa delete cluster -c $ROSA_CLUSTER_NAME Once the cluster is deleted, delete the cluster's STS roles. bash rosa delete operator-roles -c $ROSA_CLUSTER_NAME --yes --mode auto rosa delete oidc-provider -c $ROSA_CLUSTER_NAME --yes --mode auto","title":"Cleanup"},{"location":"rosa/metrics-to-cloudwatch-agent/","text":"Using the AWS Cloud Watch agent to publish metrics to CloudWatch in ROSA This document shows how you can use the AWS Cloud Watch agent to scrape Prometheus endpoints and publish metrics to CloudWatch in a Red Hat OpenShift Container Platform (ROSA) cluster. It pulls from The AWS documentation for installing the CloudWatch agent to Kubernetes and collections and publishes metrics for the Kubernetes API Server and provides a simple Dashboard to view the results. Currently the AWS Cloud Watch Agent does not support pulling all metrics from the Prometheus federated endpoint, but the hope is that when it does we can ship all Cluster and User Workload metrics to CloudWatch. Prerequisites AWS CLI jq A ROSA Cluster Prepare AWS Account Turn off AWS CLI Paging bash export AWS_PAGER=\"\" Set some environment variables Change these to suit your environment. bash export CLUSTER_NAME=metrics export CLUSTER_REGION=us-east-2 export SCRATCH_DIR=/tmp/scratch mkdir -p $SCRATCH_DIR Create an AWS IAM User for Cloud Watch ```bash aws iam create-user \\ --user-name $CLUSTER_NAME-cloud-watch \\ $SCRATCH_DIR/aws-user.json ``` Fetch Access and Secret Keys for IAM User ```bash aws iam create-access-key \\ --user-name $CLUSTER_NAME-cloud-watch \\ $SCRATCH_DIR/aws-access-key.json ``` Attach Policy to AWS IAM User bash aws iam attach-user-policy \\ --user-name $CLUSTER_NAME-cloud-watch \\ --policy-arn \"arn:aws:iam::aws:policy/CloudWatchAgentServerPolicy\" Deploy Cloud Watch Prometheus Agent Create a namespace for Cloud Watch bash kubectl create namespace amazon-cloudwatch Download the Cloud Watch Agent Kubernetes manifests bash wget -O $SCRATCH_DIR/cloud-watch.yaml https://github.com/rh-mobb/documentation/tree/main/docs/rosa/metrics-to-cloudwatch-agent/cloud-watch.yaml?raw=true Update the Cloud Watch Agent Kubernetes manifests bash sed -i \"s/{{cluster_name}}/$CLUSTER_NAME/g\" $SCRATCH_DIR/cloud-watch.yaml sed -i \"s/{{region_name}}/$CLUSTER_REGION/g\" $SCRATCH_DIR/cloud-watch.yaml Provide AWS Creds to the Cloud Watch Agent ``bash AWS_ID= cat $SCRATCH_DIR/aws-access-key.json | jq -r '.AccessKey.AccessKeyId' AWS_KEY= cat $SCRATCH_DIR/aws-access-key.json | jq -r '.AccessKey.SecretAccessKey'` echo \"[AmazonCloudWatchAgent]\\naws_access_key_id = $AWS_ID\\naws_secret_access_key = $AWS_KEY\" \\ $SCRATCH_DIR/credentials oc --namespace amazon-cloudwatch \\ create secret generic aws-credentials \\ --from-file=credentials=$SCRATCH_DIR/credentials ``` Allow Cloud Watch Agent to run as Root user (inside the container) bash oc -n amazon-cloudwatch adm policy \\ add-scc-to-user anyuid -z cwagent-prometheus Apply the Cloud Watch Agent Kubernetes manifests bash kubectl apply -f $SCRATCH_DIR/cloud-watch.yaml Check the Pod is running bash kubectl get pods -n amazon-cloudwatch You should see: NAME READY STATUS RESTARTS AGE cwagent-prometheus-54cd498c9c-btmjm 1/1 Running 0 60m Create Sample Dashboard Download the Sample Dashboard bash wget -O $SCRATCH_DIR/dashboard.json https://github.com/rh-mobb/documentation/tree/main/docs/rosa/metrics-to-cloudwatch-agent/dashboard.json?raw=true Update the Sample Dashboard bash sed -i \"s/{{YOUR_CLUSTER_NAME}}/$CLUSTER_NAME/g\" $SCRATCH_DIR/dashboard.json sed -i \"s/{{YOUR_AWS_REGION}}/$CLUSTER_REGION/g\" $SCRATCH_DIR/dashboard.json Browse to https://us-east-2.console.aws.amazon.com/cloudwatch Create a Dashboard, call it \"Kubernetes API Server\" Click Actions -> View/edit source Paste the JSON contents from $SCRATCH_DIR/dashboard.json into the text area View the dashboard","title":"Using the AWS Cloud Watch agent to publish metrics to CloudWatch in ROSA"},{"location":"rosa/metrics-to-cloudwatch-agent/#using-the-aws-cloud-watch-agent-to-publish-metrics-to-cloudwatch-in-rosa","text":"This document shows how you can use the AWS Cloud Watch agent to scrape Prometheus endpoints and publish metrics to CloudWatch in a Red Hat OpenShift Container Platform (ROSA) cluster. It pulls from The AWS documentation for installing the CloudWatch agent to Kubernetes and collections and publishes metrics for the Kubernetes API Server and provides a simple Dashboard to view the results. Currently the AWS Cloud Watch Agent does not support pulling all metrics from the Prometheus federated endpoint, but the hope is that when it does we can ship all Cluster and User Workload metrics to CloudWatch.","title":"Using the AWS Cloud Watch agent to publish metrics to CloudWatch in ROSA"},{"location":"rosa/metrics-to-cloudwatch-agent/#prerequisites","text":"AWS CLI jq A ROSA Cluster","title":"Prerequisites"},{"location":"rosa/metrics-to-cloudwatch-agent/#prepare-aws-account","text":"Turn off AWS CLI Paging bash export AWS_PAGER=\"\" Set some environment variables Change these to suit your environment. bash export CLUSTER_NAME=metrics export CLUSTER_REGION=us-east-2 export SCRATCH_DIR=/tmp/scratch mkdir -p $SCRATCH_DIR Create an AWS IAM User for Cloud Watch ```bash aws iam create-user \\ --user-name $CLUSTER_NAME-cloud-watch \\ $SCRATCH_DIR/aws-user.json ``` Fetch Access and Secret Keys for IAM User ```bash aws iam create-access-key \\ --user-name $CLUSTER_NAME-cloud-watch \\ $SCRATCH_DIR/aws-access-key.json ``` Attach Policy to AWS IAM User bash aws iam attach-user-policy \\ --user-name $CLUSTER_NAME-cloud-watch \\ --policy-arn \"arn:aws:iam::aws:policy/CloudWatchAgentServerPolicy\"","title":"Prepare AWS Account"},{"location":"rosa/metrics-to-cloudwatch-agent/#deploy-cloud-watch-prometheus-agent","text":"Create a namespace for Cloud Watch bash kubectl create namespace amazon-cloudwatch Download the Cloud Watch Agent Kubernetes manifests bash wget -O $SCRATCH_DIR/cloud-watch.yaml https://github.com/rh-mobb/documentation/tree/main/docs/rosa/metrics-to-cloudwatch-agent/cloud-watch.yaml?raw=true Update the Cloud Watch Agent Kubernetes manifests bash sed -i \"s/{{cluster_name}}/$CLUSTER_NAME/g\" $SCRATCH_DIR/cloud-watch.yaml sed -i \"s/{{region_name}}/$CLUSTER_REGION/g\" $SCRATCH_DIR/cloud-watch.yaml Provide AWS Creds to the Cloud Watch Agent ``bash AWS_ID= cat $SCRATCH_DIR/aws-access-key.json | jq -r '.AccessKey.AccessKeyId' AWS_KEY= cat $SCRATCH_DIR/aws-access-key.json | jq -r '.AccessKey.SecretAccessKey'` echo \"[AmazonCloudWatchAgent]\\naws_access_key_id = $AWS_ID\\naws_secret_access_key = $AWS_KEY\" \\ $SCRATCH_DIR/credentials oc --namespace amazon-cloudwatch \\ create secret generic aws-credentials \\ --from-file=credentials=$SCRATCH_DIR/credentials ``` Allow Cloud Watch Agent to run as Root user (inside the container) bash oc -n amazon-cloudwatch adm policy \\ add-scc-to-user anyuid -z cwagent-prometheus Apply the Cloud Watch Agent Kubernetes manifests bash kubectl apply -f $SCRATCH_DIR/cloud-watch.yaml Check the Pod is running bash kubectl get pods -n amazon-cloudwatch You should see: NAME READY STATUS RESTARTS AGE cwagent-prometheus-54cd498c9c-btmjm 1/1 Running 0 60m","title":"Deploy Cloud Watch Prometheus Agent"},{"location":"rosa/metrics-to-cloudwatch-agent/#create-sample-dashboard","text":"Download the Sample Dashboard bash wget -O $SCRATCH_DIR/dashboard.json https://github.com/rh-mobb/documentation/tree/main/docs/rosa/metrics-to-cloudwatch-agent/dashboard.json?raw=true Update the Sample Dashboard bash sed -i \"s/{{YOUR_CLUSTER_NAME}}/$CLUSTER_NAME/g\" $SCRATCH_DIR/dashboard.json sed -i \"s/{{YOUR_AWS_REGION}}/$CLUSTER_REGION/g\" $SCRATCH_DIR/dashboard.json Browse to https://us-east-2.console.aws.amazon.com/cloudwatch Create a Dashboard, call it \"Kubernetes API Server\" Click Actions -> View/edit source Paste the JSON contents from $SCRATCH_DIR/dashboard.json into the text area View the dashboard","title":"Create Sample Dashboard"},{"location":"rosa/private-link/","text":"Creating a ROSA cluster with Private Link enabled Prerequisites AWS CLI Rosa CLI v1.0.8 jq Create VPC and Subnets The following instructions use the AWS CLI to create the necessary networking to deploy a Private Link ROSA cluster into a Single AZ and are intended to be a guide. Ideally you would use an Automation tool like Ansible or Terraform to manage your VPCs. When creating subnets, make sure that subnet(s) are created to availability zone that has ROSA instances types available. If AZ is not \"forced\", subnet is created to random AZ in the region. Force AZ using --availability-zone argument in create-subnet command. Use rosa list instance-types to list ROSA instance types and check available types availability in AZ with aws ec2 describe-instance-type-offerings --location-type availability-zone --filters Name=location,Values=AZ_NAME_HERE --region REGION_HERE --output text | egrep \"YOU_PREFERRED_INSTANCE_TYPE\" . As an example, you cannot install ROSA to us-east-1e AZ, but us-east-1b works fine. Option 1 - VPC with a private subnet and AWS Site-to-Site VPN access. Todo Option 2 - VPC with public and private subnets and AWS Site-to-Site VPN access Todo Option 3 - VPC with public and private subnets (NAT) This will create both a Private and Public subnet. All cluster resources will live in the private subnet, the public subnet only exists to NAT the egress traffic to the Internet. As an alternative use the Terraform instructions provided here then skip down to the rosa create command. Set a Cluster name ROSA_CLUSTER_NAME=private-link Create a VPC to install a ROSA cluster into `` VPC_ID= aws ec2 create-vpc --cidr-block 10.0.0.0/16 | jq -r .Vpc.VpcId` aws ec2 create-tags --resources $VPC_ID \\ --tags Key=Name,Value=$ROSA_CLUSTER_NAME | jq . aws ec2 modify-vpc-attribute --vpc-id $VPC_ID --enable-dns-hostnames | jq . ``` Create a Public Subnet for the cluster to NAT egress traffic out of ``bash PUBLIC_SUBNET= aws ec2 create-subnet --vpc-id $VPC_ID --cidr-block 10.0.128.0/17 | jq -r .Subnet.SubnetId` aws ec2 create-tags --resources $PUBLIC_SUBNET \\ --tags Key=Name,Value=$ROSA_CLUSTER_NAME-public | jq . ``` Create a Private Subnet for the cluster machines to live in ``bash PRIVATE_SUBNET= aws ec2 create-subnet --vpc-id $VPC_ID --cidr-block 10.0.0.0/17 | jq -r .Subnet.SubnetId` aws ec2 create-tags --resources $PRIVATE_SUBNET \\ --tags Key=Name,Value=$ROSA_CLUSTER_NAME-private | jq . ``` Create an Internet Gateway for NAT egress traffic ``bash I_GW= aws ec2 create-internet-gateway | jq -r .InternetGateway.InternetGatewayId` aws ec2 attach-internet-gateway --vpc-id $VPC_ID --internet-gateway-id $I_GW | jq . aws ec2 create-tags --resources $I_GW \\ --tags Key=Name,Value=$ROSA_CLUSTER_NAME | jq . ``` Create a Route Table for NAT egress traffic ``bash R_TABLE= aws ec2 create-route-table --vpc-id $VPC_ID | jq -r .RouteTable.RouteTableId` aws ec2 create-route --route-table-id $R_TABLE --destination-cidr-block 0.0.0.0/0 --gateway-id $I_GW | jq . aws ec2 describe-route-tables --route-table-id $R_TABLE | jq . aws ec2 associate-route-table --subnet-id $PUBLIC_SUBNET --route-table-id $R_TABLE | jq . aws ec2 create-tags --resources $R_TABLE \\ --tags Key=Name,Value=$ROSA_CLUSTER_NAME | jq . ``` Create a NAT Gateway for the Private network ``bash EIP= aws ec2 allocate-address --domain vpc | jq -r .AllocationId NAT_GW= aws ec2 create-nat-gateway --subnet-id $PUBLIC_SUBNET \\ --allocation-id $EIP | jq -r .NatGateway.NatGatewayId` aws ec2 create-tags --resources $EIP --resources $NAT_GW \\ --tags Key=Name,Value=$ROSA_CLUSTER_NAME | jq . ``` Create a Route Table for the Private subnet to the NAT ``bash R_TABLE_NAT= aws ec2 create-route-table --vpc-id $VPC_ID | jq -r .RouteTable.RouteTableId` while ! aws ec2 describe-route-tables --route-table-id $R_TABLE_NAT \\ | jq .; do sleep 1; done aws ec2 create-route --route-table-id $R_TABLE_NAT --destination-cidr-block 0.0.0.0/0 --gateway-id $NAT_GW | jq . aws ec2 associate-route-table --subnet-id $PRIVATE_SUBNET --route-table-id $R_TABLE_NAT | jq . aws ec2 create-tags --resources $R_TABLE_NAT $EIP \\ --tags Key=Name,Value=$ROSA_CLUSTER_NAME-private | jq . ``` Deploy ROSA Create ROSA cluster in the private subnet bash rosa create cluster --private-link \\ --cluster-name=$ROSA_CLUSTER_NAME \\ --machine-cidr=10.0.0.0/16 \\ --subnet-ids=$PRIVATE_SUBNET Test Connectivity Create an Instance to use as a jump host TODO: CLI instructions Through the GUI: Navigate to the EC2 console and launch a new instance Select the AMI for your instance, if you don't have a standard, the Amazon Linux 2 AMI works just fine Choose your instance type, the t2.micro/free tier is sufficient for our needs, and click Next: Configure Instance Details Change the Network settings to setup this host inside your private-link VPC Change the Subnet setting to use the private-link-public subnet Change Auto-assign Public IP to Enable Default settings for Storage and Tags are OK, if you do not need to change them for your own reasons, select 6. Configure Security Group from the top navigation or click through using the Next buttons If you already have a security group created to allow access from your computer to AWS, choose Select an existing security group and choose that group from the list and skip to Review and Launch . Otherwise, select Create a new security group and continue. To allow access only from your current public IP, change the Source heading to use My IP Click Review and Launch , verify all settings are correct and follow the standard AWS instructions for finalizing the setup and selecting/creating the security keys. Once launched, open the instance summary for the jump host instance and note the public IP address. Create a ROSA admin user and save the login command for use later rosa create admin -c $ROSA_CLUSTER_NAME Note the DNS name of your private cluster, use the rosa describe command if needed rosa describe cluster -c private-link update /etc/hosts to point the openshift domains to localhost. Use the DNS of your openshift cluster as described in the previous step in place of $YOUR_OPENSHIFT_DNS below 127.0.0.1 api.$YOUR_OPENSHIFT_DNS 127.0.0.1 console-openshift-console.apps.$YOUR_OPENSHIFT_DNS 127.0.0.1 oauth-openshift.apps.$YOUR_OPENSHIFT_DNS SSH to that instance, tunneling traffic for the appropriate hostnames. Be sure to use your new/existing private key, the OpenShift DNS for $YOUR_OPENSHIFT_DNS and your jump host IP for $YOUR_EC2_IP bash sudo ssh -i PATH/TO/YOUR_KEY.pem \\ -L 6443:api.$YOUR_OPENSHIFT_DNS:6443 \\ -L 443:console-openshift-console.apps.$YOUR_OPENSHIFT_DNS:443 \\ -L 80:console-openshift-console.apps.$YOUR_OPENSHIFT_DNS:80 \\ ec2-user@$YOUR_EC2_IP Log into the cluster using oc login command from the create admin command above. ex. bash oc login https://api.private-test.3d1n.p1.openshiftapps.com:6443 --username cluster-admin --password GQSGJ-daqfN-8QNY3-tS9gU Check that you can access the Console by opening the console url in your browser. Cleanup Delete ROSA bash rosa delete cluster -c $ROSA_CLUSTER_NAME -y Delete AWS resources bash aws ec2 delete-nat-gateway --nat-gateway-id $NAT_GW | jq . aws ec2 release-address --allocation-id=$EIP | jq . aws ec2 detach-internet-gateway --vpc-id $VPC_ID \\ --internet-gateway-id $I_GW | jq . aws ec2 delete-subnet --subnet-id=$PRIVATE_SUBNET | jq . aws ec2 delete-subnet --subnet-id=$PUBLIC_SUBNET | jq . aws ec2 delete-route-table --route-table-id=$R_TABLE | jq . aws ec2 delete-route-table --route-table-id=$R_TABLE_NAT | jq . aws ec2 delete-vpc --vpc-id=$VPC_ID | jq .","title":"Private Link"},{"location":"rosa/private-link/#creating-a-rosa-cluster-with-private-link-enabled","text":"","title":"Creating a ROSA cluster with Private Link enabled"},{"location":"rosa/private-link/#prerequisites","text":"AWS CLI Rosa CLI v1.0.8 jq","title":"Prerequisites"},{"location":"rosa/private-link/#create-vpc-and-subnets","text":"The following instructions use the AWS CLI to create the necessary networking to deploy a Private Link ROSA cluster into a Single AZ and are intended to be a guide. Ideally you would use an Automation tool like Ansible or Terraform to manage your VPCs. When creating subnets, make sure that subnet(s) are created to availability zone that has ROSA instances types available. If AZ is not \"forced\", subnet is created to random AZ in the region. Force AZ using --availability-zone argument in create-subnet command. Use rosa list instance-types to list ROSA instance types and check available types availability in AZ with aws ec2 describe-instance-type-offerings --location-type availability-zone --filters Name=location,Values=AZ_NAME_HERE --region REGION_HERE --output text | egrep \"YOU_PREFERRED_INSTANCE_TYPE\" . As an example, you cannot install ROSA to us-east-1e AZ, but us-east-1b works fine.","title":"Create VPC and Subnets"},{"location":"rosa/private-link/#option-1-vpc-with-a-private-subnet-and-aws-site-to-site-vpn-access","text":"Todo","title":"Option 1 - VPC with a private subnet and AWS Site-to-Site VPN access."},{"location":"rosa/private-link/#option-2-vpc-with-public-and-private-subnets-and-aws-site-to-site-vpn-access","text":"Todo","title":"Option 2 - VPC with public and private subnets and AWS Site-to-Site VPN access"},{"location":"rosa/private-link/#option-3-vpc-with-public-and-private-subnets-nat","text":"This will create both a Private and Public subnet. All cluster resources will live in the private subnet, the public subnet only exists to NAT the egress traffic to the Internet. As an alternative use the Terraform instructions provided here then skip down to the rosa create command. Set a Cluster name ROSA_CLUSTER_NAME=private-link Create a VPC to install a ROSA cluster into `` VPC_ID= aws ec2 create-vpc --cidr-block 10.0.0.0/16 | jq -r .Vpc.VpcId` aws ec2 create-tags --resources $VPC_ID \\ --tags Key=Name,Value=$ROSA_CLUSTER_NAME | jq . aws ec2 modify-vpc-attribute --vpc-id $VPC_ID --enable-dns-hostnames | jq . ``` Create a Public Subnet for the cluster to NAT egress traffic out of ``bash PUBLIC_SUBNET= aws ec2 create-subnet --vpc-id $VPC_ID --cidr-block 10.0.128.0/17 | jq -r .Subnet.SubnetId` aws ec2 create-tags --resources $PUBLIC_SUBNET \\ --tags Key=Name,Value=$ROSA_CLUSTER_NAME-public | jq . ``` Create a Private Subnet for the cluster machines to live in ``bash PRIVATE_SUBNET= aws ec2 create-subnet --vpc-id $VPC_ID --cidr-block 10.0.0.0/17 | jq -r .Subnet.SubnetId` aws ec2 create-tags --resources $PRIVATE_SUBNET \\ --tags Key=Name,Value=$ROSA_CLUSTER_NAME-private | jq . ``` Create an Internet Gateway for NAT egress traffic ``bash I_GW= aws ec2 create-internet-gateway | jq -r .InternetGateway.InternetGatewayId` aws ec2 attach-internet-gateway --vpc-id $VPC_ID --internet-gateway-id $I_GW | jq . aws ec2 create-tags --resources $I_GW \\ --tags Key=Name,Value=$ROSA_CLUSTER_NAME | jq . ``` Create a Route Table for NAT egress traffic ``bash R_TABLE= aws ec2 create-route-table --vpc-id $VPC_ID | jq -r .RouteTable.RouteTableId` aws ec2 create-route --route-table-id $R_TABLE --destination-cidr-block 0.0.0.0/0 --gateway-id $I_GW | jq . aws ec2 describe-route-tables --route-table-id $R_TABLE | jq . aws ec2 associate-route-table --subnet-id $PUBLIC_SUBNET --route-table-id $R_TABLE | jq . aws ec2 create-tags --resources $R_TABLE \\ --tags Key=Name,Value=$ROSA_CLUSTER_NAME | jq . ``` Create a NAT Gateway for the Private network ``bash EIP= aws ec2 allocate-address --domain vpc | jq -r .AllocationId NAT_GW= aws ec2 create-nat-gateway --subnet-id $PUBLIC_SUBNET \\ --allocation-id $EIP | jq -r .NatGateway.NatGatewayId` aws ec2 create-tags --resources $EIP --resources $NAT_GW \\ --tags Key=Name,Value=$ROSA_CLUSTER_NAME | jq . ``` Create a Route Table for the Private subnet to the NAT ``bash R_TABLE_NAT= aws ec2 create-route-table --vpc-id $VPC_ID | jq -r .RouteTable.RouteTableId` while ! aws ec2 describe-route-tables --route-table-id $R_TABLE_NAT \\ | jq .; do sleep 1; done aws ec2 create-route --route-table-id $R_TABLE_NAT --destination-cidr-block 0.0.0.0/0 --gateway-id $NAT_GW | jq . aws ec2 associate-route-table --subnet-id $PRIVATE_SUBNET --route-table-id $R_TABLE_NAT | jq . aws ec2 create-tags --resources $R_TABLE_NAT $EIP \\ --tags Key=Name,Value=$ROSA_CLUSTER_NAME-private | jq . ```","title":"Option 3 - VPC with public and private subnets (NAT)"},{"location":"rosa/private-link/#deploy-rosa","text":"Create ROSA cluster in the private subnet bash rosa create cluster --private-link \\ --cluster-name=$ROSA_CLUSTER_NAME \\ --machine-cidr=10.0.0.0/16 \\ --subnet-ids=$PRIVATE_SUBNET","title":"Deploy ROSA"},{"location":"rosa/private-link/#test-connectivity","text":"Create an Instance to use as a jump host TODO: CLI instructions Through the GUI: Navigate to the EC2 console and launch a new instance Select the AMI for your instance, if you don't have a standard, the Amazon Linux 2 AMI works just fine Choose your instance type, the t2.micro/free tier is sufficient for our needs, and click Next: Configure Instance Details Change the Network settings to setup this host inside your private-link VPC Change the Subnet setting to use the private-link-public subnet Change Auto-assign Public IP to Enable Default settings for Storage and Tags are OK, if you do not need to change them for your own reasons, select 6. Configure Security Group from the top navigation or click through using the Next buttons If you already have a security group created to allow access from your computer to AWS, choose Select an existing security group and choose that group from the list and skip to Review and Launch . Otherwise, select Create a new security group and continue. To allow access only from your current public IP, change the Source heading to use My IP Click Review and Launch , verify all settings are correct and follow the standard AWS instructions for finalizing the setup and selecting/creating the security keys. Once launched, open the instance summary for the jump host instance and note the public IP address. Create a ROSA admin user and save the login command for use later rosa create admin -c $ROSA_CLUSTER_NAME Note the DNS name of your private cluster, use the rosa describe command if needed rosa describe cluster -c private-link update /etc/hosts to point the openshift domains to localhost. Use the DNS of your openshift cluster as described in the previous step in place of $YOUR_OPENSHIFT_DNS below 127.0.0.1 api.$YOUR_OPENSHIFT_DNS 127.0.0.1 console-openshift-console.apps.$YOUR_OPENSHIFT_DNS 127.0.0.1 oauth-openshift.apps.$YOUR_OPENSHIFT_DNS SSH to that instance, tunneling traffic for the appropriate hostnames. Be sure to use your new/existing private key, the OpenShift DNS for $YOUR_OPENSHIFT_DNS and your jump host IP for $YOUR_EC2_IP bash sudo ssh -i PATH/TO/YOUR_KEY.pem \\ -L 6443:api.$YOUR_OPENSHIFT_DNS:6443 \\ -L 443:console-openshift-console.apps.$YOUR_OPENSHIFT_DNS:443 \\ -L 80:console-openshift-console.apps.$YOUR_OPENSHIFT_DNS:80 \\ ec2-user@$YOUR_EC2_IP Log into the cluster using oc login command from the create admin command above. ex. bash oc login https://api.private-test.3d1n.p1.openshiftapps.com:6443 --username cluster-admin --password GQSGJ-daqfN-8QNY3-tS9gU Check that you can access the Console by opening the console url in your browser.","title":"Test Connectivity"},{"location":"rosa/private-link/#cleanup","text":"Delete ROSA bash rosa delete cluster -c $ROSA_CLUSTER_NAME -y Delete AWS resources bash aws ec2 delete-nat-gateway --nat-gateway-id $NAT_GW | jq . aws ec2 release-address --allocation-id=$EIP | jq . aws ec2 detach-internet-gateway --vpc-id $VPC_ID \\ --internet-gateway-id $I_GW | jq . aws ec2 delete-subnet --subnet-id=$PRIVATE_SUBNET | jq . aws ec2 delete-subnet --subnet-id=$PUBLIC_SUBNET | jq . aws ec2 delete-route-table --route-table-id=$R_TABLE | jq . aws ec2 delete-route-table --route-table-id=$R_TABLE_NAT | jq . aws ec2 delete-vpc --vpc-id=$VPC_ID | jq .","title":"Cleanup"},{"location":"rosa/private-link/public-ingress/","text":"Adding a Public Ingress endpoint to a ROSA Private-Link Cluster The is an example guide for creating a public ingress endpoint for a ROSA Private-Link cluster. Be aware of the security implications of creating a public subnet in your ROSA VPC this way. Prerequisites AWS CLI Rosa CLI v1.0.8 jq A ROSA PL cluster Getting Started Set some environment variables Set the following environment variables, changing them to suit your cluster. bash export ROSA_CLUSTER_NAME=private-link # this should be a free CIDR inside your VPC export PUBLIC_CIDR=10.0.2.0/24 export AWS_PAGER=\"\" export EMAIL=username.taken@gmail.com export DOMAIN=public.aws.mobb.ninja export SCRATCH_DIR=/tmp/scratch mkdir -p $SCRATCH_DIR Create a public subnet If you followed the above instructions to create the ROSA Private-Link cluster, you should already have a public subnet in your VPC. You can skip this step. Get a Private Subnet ID from the cluster. bash PRIVATE_SUBNET_ID=$(rosa describe cluster -c $ROSA_CLUSTER_NAME -o json \\ | jq -r '.aws.subnet_ids[0]') echo $PRIVATE_SUBNET_ID Get the VPC ID from the subnet ID. bash VPC_ID=$(aws ec2 describe-subnets --subnet-ids $PRIVATE_SUBNET_ID \\ --query 'Subnets[0].VpcId' --output text) echo $VPC_ID Get the Cluster Tag from the subnet bash TAG=$(aws ec2 describe-subnets --subnet-ids $PRIVATE_SUBNET_ID \\ --query 'Subnets[0].Tags[?Value == `shared`]' | jq -r '.[0].Key') echo $TAG Create a public subnet bash PUBLIC_SUBNET=`aws ec2 create-subnet --vpc-id $VPC_ID --cidr-block $PUBLIC_CIDR \\ --query 'Subnet.SubnetId' --output text` echo $PUBLIC_SUBNET Tag the public subnet for the cluster bash aws ec2 create-tags --resources $PUBLIC_SUBNET \\ --tags Key=Name,Value=$ROSA_CLUSTER_NAME-public \\ Key=$TAG,Value=\"shared\" Key=kubernetes.io/role/elb,Value=\"true\" Create a Custom Domain Create TLS Key Pair for custom domain using certbot: Skip this if you already have a key pair. bash certbot certonly --manual \\ --preferred-challenges=dns \\ --email $EMAIL \\ --server https://acme-v02.api.letsencrypt.org/directory \\ --agree-tos \\ --config-dir \"$SCRATCH_DIR/config\" \\ --work-dir \"$SCRATCH_DIR/work\" \\ --logs-dir \"$SCRATCH_DIR/logs\" \\ -d \"*.$DOMAIN\" Create TLS secret for custom domain: Note use your own keypair paths if not using certbot. bash CERTS=/tmp/scratch/config/live/$DOMAIN oc new-project my-custom-route oc create secret tls acme-tls --cert=$CERTS/fullchain.pem --key=$CERTS/privkey.pem Create Custom Domain resource: bash cat << EOF | oc apply -f - apiVersion: managed.openshift.io/v1alpha1 kind: CustomDomain metadata: name: acme spec: domain: $DOMAIN certificate: name: acme-tls namespace: my-custom-route EOF Wait for the domain to be ready: bash watch oc get customdomains Once its ready grab the endpoint: bash ENDPOINT=$(oc get customdomains acme -o jsonpath='{.status.endpoint}') echo $ENDPOINT Create a CNAME in your DNS provider for *.<$DOMAIN> that points at the endpoint from the above command. Deploy a public application Create a new project bash oc new-project my-public-app Create a new application bash oc new-app --docker-image=docker.io/openshift/hello-openshift Create a route for the application bash oc create route edge --service=hello-openshift hello-openshift-tls \\ --hostname hello.$DOMAIN Check that you can access the application: bash curl https://hello.$DOMAIN You should see the output Hello OpenShift!","title":"Adding a Public Ingress endpoint to a ROSA Private-Link Cluster"},{"location":"rosa/private-link/public-ingress/#adding-a-public-ingress-endpoint-to-a-rosa-private-link-cluster","text":"The is an example guide for creating a public ingress endpoint for a ROSA Private-Link cluster. Be aware of the security implications of creating a public subnet in your ROSA VPC this way.","title":"Adding a Public Ingress endpoint to a ROSA Private-Link Cluster"},{"location":"rosa/private-link/public-ingress/#prerequisites","text":"AWS CLI Rosa CLI v1.0.8 jq A ROSA PL cluster","title":"Prerequisites"},{"location":"rosa/private-link/public-ingress/#getting-started","text":"","title":"Getting Started"},{"location":"rosa/private-link/public-ingress/#set-some-environment-variables","text":"Set the following environment variables, changing them to suit your cluster. bash export ROSA_CLUSTER_NAME=private-link # this should be a free CIDR inside your VPC export PUBLIC_CIDR=10.0.2.0/24 export AWS_PAGER=\"\" export EMAIL=username.taken@gmail.com export DOMAIN=public.aws.mobb.ninja export SCRATCH_DIR=/tmp/scratch mkdir -p $SCRATCH_DIR","title":"Set some environment variables"},{"location":"rosa/private-link/public-ingress/#create-a-public-subnet","text":"If you followed the above instructions to create the ROSA Private-Link cluster, you should already have a public subnet in your VPC. You can skip this step. Get a Private Subnet ID from the cluster. bash PRIVATE_SUBNET_ID=$(rosa describe cluster -c $ROSA_CLUSTER_NAME -o json \\ | jq -r '.aws.subnet_ids[0]') echo $PRIVATE_SUBNET_ID Get the VPC ID from the subnet ID. bash VPC_ID=$(aws ec2 describe-subnets --subnet-ids $PRIVATE_SUBNET_ID \\ --query 'Subnets[0].VpcId' --output text) echo $VPC_ID Get the Cluster Tag from the subnet bash TAG=$(aws ec2 describe-subnets --subnet-ids $PRIVATE_SUBNET_ID \\ --query 'Subnets[0].Tags[?Value == `shared`]' | jq -r '.[0].Key') echo $TAG Create a public subnet bash PUBLIC_SUBNET=`aws ec2 create-subnet --vpc-id $VPC_ID --cidr-block $PUBLIC_CIDR \\ --query 'Subnet.SubnetId' --output text` echo $PUBLIC_SUBNET Tag the public subnet for the cluster bash aws ec2 create-tags --resources $PUBLIC_SUBNET \\ --tags Key=Name,Value=$ROSA_CLUSTER_NAME-public \\ Key=$TAG,Value=\"shared\" Key=kubernetes.io/role/elb,Value=\"true\"","title":"Create a public subnet"},{"location":"rosa/private-link/public-ingress/#create-a-custom-domain","text":"Create TLS Key Pair for custom domain using certbot: Skip this if you already have a key pair. bash certbot certonly --manual \\ --preferred-challenges=dns \\ --email $EMAIL \\ --server https://acme-v02.api.letsencrypt.org/directory \\ --agree-tos \\ --config-dir \"$SCRATCH_DIR/config\" \\ --work-dir \"$SCRATCH_DIR/work\" \\ --logs-dir \"$SCRATCH_DIR/logs\" \\ -d \"*.$DOMAIN\" Create TLS secret for custom domain: Note use your own keypair paths if not using certbot. bash CERTS=/tmp/scratch/config/live/$DOMAIN oc new-project my-custom-route oc create secret tls acme-tls --cert=$CERTS/fullchain.pem --key=$CERTS/privkey.pem Create Custom Domain resource: bash cat << EOF | oc apply -f - apiVersion: managed.openshift.io/v1alpha1 kind: CustomDomain metadata: name: acme spec: domain: $DOMAIN certificate: name: acme-tls namespace: my-custom-route EOF Wait for the domain to be ready: bash watch oc get customdomains Once its ready grab the endpoint: bash ENDPOINT=$(oc get customdomains acme -o jsonpath='{.status.endpoint}') echo $ENDPOINT Create a CNAME in your DNS provider for *.<$DOMAIN> that points at the endpoint from the above command.","title":"Create a Custom Domain"},{"location":"rosa/private-link/public-ingress/#deploy-a-public-application","text":"Create a new project bash oc new-project my-public-app Create a new application bash oc new-app --docker-image=docker.io/openshift/hello-openshift Create a route for the application bash oc create route edge --service=hello-openshift hello-openshift-tls \\ --hostname hello.$DOMAIN Check that you can access the application: bash curl https://hello.$DOMAIN You should see the output Hello OpenShift!","title":"Deploy a public application"},{"location":"rosa/sts/","text":"Creating a ROSA cluster in STS mode Paul Czarkowski Last updated 05/31/2022 Tip The official documentation for installing a ROSA cluster in STS mode can be found here . STS allows us to deploy ROSA without needing a ROSA admin account, instead it uses roles and policies with Amazon STS (secure token service) to gain access to the AWS resources needed to install and operate the cluster. This is a summary of the official docs that can be used as a line by line install guide and later used as a basis for automation in your favorite automation tool . Note that some commands (OIDC for STS) will be hard coded to US-EAST-1, do not be tempted to change these to use $REGION instead or you will fail installation. Prerequisites AWS CLI Rosa CLI v1.2.2 OpenShift CLI - rosa download openshift-client jq Prepare local environment set some environment variables bash export VERSION=4.10.15 \\ ROSA_CLUSTER_NAME=mycluster \\ AWS_ACCOUNT_ID=`aws sts get-caller-identity --query Account --output text` \\ REGION=us-east-2 \\ AWS_PAGER=\"\" Prepare AWS and Red Hat accounts If this is your first time deploying ROSA you need to do some preparation as described here . Stop just before running rosa init we don't need to do that for STS mode. If this is a brand new AWS account that has never had a AWS Load Balancer installed in it, you should run the following bash aws iam create-service-linked-role --aws-service-name \\ \"elasticloadbalancing.amazonaws.com\" Associate your AWS account To perform ROSA cluster provisioning tasks, you must create ocm-role and user-role IAM resources in your AWS account and link them to your Red Hat organization. OCM Role The first role you will create is the ocm-role which the OpenShift Cluster Manager will use to be able to administer and Create ROSA clusters. If you haven't already created the ocm-role, you can create and link the role with one command. bash rosa create ocm-role Tip If you have multiple AWS accounts that you want to associate with your Red Hat Organization, you can use the --profile option to specify the AWS profile you would like to associate. If you have already created the ocm-role, you can just link the ocm-role to your Red Hat organization. bash rosa link ocm-user --role-arm <arn> Tip You can get your OCM role arn from AWS IAM: bash aws iam list-roles | grep OCM User Role The second is the user-role that allows OCM to verify that users creating a cluster have access to the current AWS account. If you haven't already created the user-role, you can create and link the role with one command. bash rosa create user-role Tip If you have multiple AWS accounts that you want to associate with your Red Hat Organization, you can use the --profile option to specify the AWS profile you would like to associate. If you have already created the user-role, you can just link the user-role to your Red Hat organization. bash rosa link user-role --role-arn <arn> Tip You can get your User role arn from the ROSA cli: rosa whoami look for the AWS ARN: field Deploy ROSA cluster Make you your ROSA CLI version is correct (v1.2.2 or higher) bash rosa version 1. Run the rosa cli to create your cluster You can run the command as provided in the ouput of the previous step to deploy in interactive mode. Add any other arguments to this command to suit your cluster. for example --private-link and --subnet-ids=subnet-12345678,subnet-87654321 . bash rosa create cluster --sts --cluster-name ${ROSA_CLUSTER_NAME} \\ --region ${REGION} --version ${VERSION} --mode auto -y Validate The cluster is now installing The State should have moved beyond pending and show installing or ready . bash watch \"rosa describe cluster -c $ROSA_CLUSTER_NAME\" Watch the install logs bash rosa logs install -c $ROSA_CLUSTER_NAME --watch --tail 10 Validate the cluster Once the cluster has finished installing we can validate we can access it Create an Admin user bash rosa create admin -c $ROSA_CLUSTER_NAME Wait a few moments and run the oc login command it provides. Cleanup Delete the ROSA cluster bash rosa delete cluster -c $ROSA_CLUSTER_NAME 1. Clean up the STS roles Once the cluster is deleted we can delete the STS roles. > Note you can get the correct commands with the ID filled in from the output of the previous step. ```bash rosa delete operator-roles -c <id> --yes --mode auto rosa delete oidc-provider -c <id> --yes --mode auto ```","title":"Create ROSA Cluster w/STS"},{"location":"rosa/sts/#creating-a-rosa-cluster-in-sts-mode","text":"Paul Czarkowski Last updated 05/31/2022 Tip The official documentation for installing a ROSA cluster in STS mode can be found here . STS allows us to deploy ROSA without needing a ROSA admin account, instead it uses roles and policies with Amazon STS (secure token service) to gain access to the AWS resources needed to install and operate the cluster. This is a summary of the official docs that can be used as a line by line install guide and later used as a basis for automation in your favorite automation tool . Note that some commands (OIDC for STS) will be hard coded to US-EAST-1, do not be tempted to change these to use $REGION instead or you will fail installation.","title":"Creating a ROSA cluster in STS mode"},{"location":"rosa/sts/#prerequisites","text":"AWS CLI Rosa CLI v1.2.2 OpenShift CLI - rosa download openshift-client jq","title":"Prerequisites"},{"location":"rosa/sts/#prepare-local-environment","text":"set some environment variables bash export VERSION=4.10.15 \\ ROSA_CLUSTER_NAME=mycluster \\ AWS_ACCOUNT_ID=`aws sts get-caller-identity --query Account --output text` \\ REGION=us-east-2 \\ AWS_PAGER=\"\"","title":"Prepare local environment"},{"location":"rosa/sts/#prepare-aws-and-red-hat-accounts","text":"If this is your first time deploying ROSA you need to do some preparation as described here . Stop just before running rosa init we don't need to do that for STS mode. If this is a brand new AWS account that has never had a AWS Load Balancer installed in it, you should run the following bash aws iam create-service-linked-role --aws-service-name \\ \"elasticloadbalancing.amazonaws.com\" Associate your AWS account To perform ROSA cluster provisioning tasks, you must create ocm-role and user-role IAM resources in your AWS account and link them to your Red Hat organization. OCM Role The first role you will create is the ocm-role which the OpenShift Cluster Manager will use to be able to administer and Create ROSA clusters. If you haven't already created the ocm-role, you can create and link the role with one command. bash rosa create ocm-role Tip If you have multiple AWS accounts that you want to associate with your Red Hat Organization, you can use the --profile option to specify the AWS profile you would like to associate. If you have already created the ocm-role, you can just link the ocm-role to your Red Hat organization. bash rosa link ocm-user --role-arm <arn> Tip You can get your OCM role arn from AWS IAM: bash aws iam list-roles | grep OCM User Role The second is the user-role that allows OCM to verify that users creating a cluster have access to the current AWS account. If you haven't already created the user-role, you can create and link the role with one command. bash rosa create user-role Tip If you have multiple AWS accounts that you want to associate with your Red Hat Organization, you can use the --profile option to specify the AWS profile you would like to associate. If you have already created the user-role, you can just link the user-role to your Red Hat organization. bash rosa link user-role --role-arn <arn> Tip You can get your User role arn from the ROSA cli: rosa whoami look for the AWS ARN: field","title":"Prepare AWS and Red Hat accounts"},{"location":"rosa/sts/#deploy-rosa-cluster","text":"Make you your ROSA CLI version is correct (v1.2.2 or higher) bash rosa version 1. Run the rosa cli to create your cluster You can run the command as provided in the ouput of the previous step to deploy in interactive mode. Add any other arguments to this command to suit your cluster. for example --private-link and --subnet-ids=subnet-12345678,subnet-87654321 . bash rosa create cluster --sts --cluster-name ${ROSA_CLUSTER_NAME} \\ --region ${REGION} --version ${VERSION} --mode auto -y Validate The cluster is now installing The State should have moved beyond pending and show installing or ready . bash watch \"rosa describe cluster -c $ROSA_CLUSTER_NAME\" Watch the install logs bash rosa logs install -c $ROSA_CLUSTER_NAME --watch --tail 10","title":"Deploy ROSA cluster"},{"location":"rosa/sts/#validate-the-cluster","text":"Once the cluster has finished installing we can validate we can access it Create an Admin user bash rosa create admin -c $ROSA_CLUSTER_NAME Wait a few moments and run the oc login command it provides.","title":"Validate the cluster"},{"location":"rosa/sts/#cleanup","text":"Delete the ROSA cluster bash rosa delete cluster -c $ROSA_CLUSTER_NAME 1. Clean up the STS roles Once the cluster is deleted we can delete the STS roles. > Note you can get the correct commands with the ID filled in from the output of the previous step. ```bash rosa delete operator-roles -c <id> --yes --mode auto rosa delete oidc-provider -c <id> --yes --mode auto ```","title":"Cleanup"},{"location":"rosa/sts-and-pod-identity/","text":"Integrating with AWS resources using Pod Identity Prerequisites ROSA CLI AWS CLI ROSA Cluster with STS","title":"Integrating with AWS resources using Pod Identity"},{"location":"rosa/sts-and-pod-identity/#integrating-with-aws-resources-using-pod-identity","text":"","title":"Integrating with AWS resources using Pod Identity"},{"location":"rosa/sts-and-pod-identity/#prerequisites","text":"ROSA CLI AWS CLI ROSA Cluster with STS","title":"Prerequisites"},{"location":"rosa/sts-cluster-logging-addon/","text":"Work Around to fix the issue with the logging-addon on ROSA STS Clusters Currently, the logging-addon is not working on ROSA STS clusters. This is due to permissions missing from the Operator itself. This is a work around to provide credentials to the addon. Note: Please see the official Red Hat KCS for more information. Prerequisites An STS based ROSA Cluster Workaround Uninstall the logging-addon from the cluster bash rosa uninstall addon -c <mycluster> cluster-logging-operator -y Create a IAM Trust Policy document bash cat << EOF > /tmp/trust-policy.json { \"Version\": \"2012-10-17\", \"Statement\": [ { \"Effect\": \"Allow\", \"Action\": [ \"logs:CreateLogGroup\", \"logs:CreateLogStream\", \"logs:DescribeLogGroups\", \"logs:DescribeLogStreams\", \"logs:PutLogEvents\", \"logs:GetLogEvents\", \"logs:PutRetentionPolicy\", \"logs:GetLogRecord\" ], \"Resource\": \"arn:aws:logs:*:*:*\" } ] } EOF Create IAM Policy bash POLICY_ARN=$(aws iam create-policy --policy-name \"RosaCloudWatchAddon\" --policy-document file:///tmp/trust-policy.json --query Policy.Arn --output text) echo $POLICY_ARN Create service account bash aws iam create-user --user-name RosaCloudWatchAddon \\ --query User.Arn --output text Attach policy to user bash aws iam attach-user-policy --user-name RosaCloudWatchAddon \\ --policy-arn ${POLICY_ARN} Create access key and save the output (Paste the AccessKeyId and SecretAccessKey into values.yaml ) bash aws iam create-access-key --user-name RosaCloudWatchAddon bash export AWS_ID=<from above> export AWS_KEY=<from above> Create a secret for the addon to use bash cat << EOF | kubectl apply -f - apiVersion: v1 kind: Secret metadata: name: instance namespace: openshift-logging stringData: aws_access_key_id: ${AWS_ID} aws_secret_access_key: ${AWS_KEY} EOF Install the logging-addon from the cluster bash rosa install addon -c <mycluster> cluster-logging-operator -y Accept the defaults (or change them as appropriate) ? Use AWS CloudWatch: Yes ? Collect Applications logs: Yes ? Collect Infrastructure logs: Yes ? Collect Audit logs (optional): No ? CloudWatch region (optional): I: Add-on 'cluster-logging-operator' is now installing. To check the status run 'rosa list addons -c mycluster'","title":"Work Around to fix the issue with the logging-addon on ROSA STS Clusters"},{"location":"rosa/sts-cluster-logging-addon/#work-around-to-fix-the-issue-with-the-logging-addon-on-rosa-sts-clusters","text":"Currently, the logging-addon is not working on ROSA STS clusters. This is due to permissions missing from the Operator itself. This is a work around to provide credentials to the addon. Note: Please see the official Red Hat KCS for more information.","title":"Work Around to fix the issue with the logging-addon on ROSA STS Clusters"},{"location":"rosa/sts-cluster-logging-addon/#prerequisites","text":"An STS based ROSA Cluster","title":"Prerequisites"},{"location":"rosa/sts-cluster-logging-addon/#workaround","text":"Uninstall the logging-addon from the cluster bash rosa uninstall addon -c <mycluster> cluster-logging-operator -y Create a IAM Trust Policy document bash cat << EOF > /tmp/trust-policy.json { \"Version\": \"2012-10-17\", \"Statement\": [ { \"Effect\": \"Allow\", \"Action\": [ \"logs:CreateLogGroup\", \"logs:CreateLogStream\", \"logs:DescribeLogGroups\", \"logs:DescribeLogStreams\", \"logs:PutLogEvents\", \"logs:GetLogEvents\", \"logs:PutRetentionPolicy\", \"logs:GetLogRecord\" ], \"Resource\": \"arn:aws:logs:*:*:*\" } ] } EOF Create IAM Policy bash POLICY_ARN=$(aws iam create-policy --policy-name \"RosaCloudWatchAddon\" --policy-document file:///tmp/trust-policy.json --query Policy.Arn --output text) echo $POLICY_ARN Create service account bash aws iam create-user --user-name RosaCloudWatchAddon \\ --query User.Arn --output text Attach policy to user bash aws iam attach-user-policy --user-name RosaCloudWatchAddon \\ --policy-arn ${POLICY_ARN} Create access key and save the output (Paste the AccessKeyId and SecretAccessKey into values.yaml ) bash aws iam create-access-key --user-name RosaCloudWatchAddon bash export AWS_ID=<from above> export AWS_KEY=<from above> Create a secret for the addon to use bash cat << EOF | kubectl apply -f - apiVersion: v1 kind: Secret metadata: name: instance namespace: openshift-logging stringData: aws_access_key_id: ${AWS_ID} aws_secret_access_key: ${AWS_KEY} EOF Install the logging-addon from the cluster bash rosa install addon -c <mycluster> cluster-logging-operator -y Accept the defaults (or change them as appropriate) ? Use AWS CloudWatch: Yes ? Collect Applications logs: Yes ? Collect Infrastructure logs: Yes ? Collect Audit logs (optional): No ? CloudWatch region (optional): I: Add-on 'cluster-logging-operator' is now installing. To check the status run 'rosa list addons -c mycluster'","title":"Workaround"},{"location":"rosa/sts-with-private-link/","text":"Creating a ROSA cluster with Private Link enabled (custom VPC) and STS Steve Mirman, Paul Czarkowski Last updated 1/28/2022 This is a combination of the private-link and sts setup documents to show the full picture Prerequisites AWS CLI Rosa CLI v1.1.7 jq AWS Preparation If this is a brand new AWS account that has never had a AWS Load Balancer installed in it, you should run the following bash aws iam create-service-linked-role --aws-service-name \\ \"elasticloadbalancing.amazonaws.com\" Create the AWS Virtual Private Cloud (VPC) and Subnets For this scenario, we will be using a newly created VPC with both public and private subnets. All of the cluster resources will reside in the private subnet. The public subnet will be used for traffic to the Internet (egress) Note : If you already have a Transit Gateway (TGW) or similar, you can skip the public subnet configuration Note : When creating subnets, make sure that subnet(s) are created in availability zones that have ROSA instances types available. If AZ is not \"forced\", the subnet is created in a random AZ in the region. Force AZ using the --availability-zone argument in the create-subnet command. Use rosa list instance-types to list the ROSA instance types Use aws ec2 describe-instance-type-offerings to check that your desired AZ supports your desired instance type Example using us-east-1 , us-east-1b , and m5.xlarge : aws ec2 describe-instance-type-offerings --location-type availability-zone --filters Name=location,Values=us-east-1b --region us-east-1 --output text | egrep m5.xlarge Result should display INSTANCETYPEOFFERINGS [instance-type] [az] availability-zone if your selected region supports your desired instance type Configure the following environment variables, adjusting for ROSA_CLUSTER_NAME , VERSION and REGION as necessary bash export VERSION=4.9.15 \\ ROSA_CLUSTER_NAME=pl-sts-cluster \\ AWS_ACCOUNT_ID=`aws sts get-caller-identity --query Account --output text` \\ REGION=us-east-1 \\ AWS_PAGER=\"\" Create a VPC for use by ROSA Create the VPC and return the ID as VPC_ID VPC_ID=`aws ec2 create-vpc --cidr-block 10.0.0.0/16 | jq -r .Vpc.VpcId` echo $VPC_ID Tag the newly created VPC with the cluster name bash aws ec2 create-tags --resources $VPC_ID \\ --tags Key=Name,Value=$ROSA_CLUSTER_NAME Configure the VPC to allow DNS hostnames for their public IP addresses bash aws ec2 modify-vpc-attribute --vpc-id $VPC_ID --enable-dns-hostnames The new VPC should be visible in the AWS console Create a Public Subnet to allow egress traffic to the Internet Create the public subnet in the VPC CIDR block range and return the ID as PUBLIC_SUBNET bash PUBLIC_SUBNET=`aws ec2 create-subnet --vpc-id $VPC_ID --cidr-block 10.0.128.0/17 | jq -r .Subnet.SubnetId` echo $PUBLIC_SUBNET Tag the public subnet with the cluster name bash aws ec2 create-tags --resources $PUBLIC_SUBNET \\ --tags Key=Name,Value=$ROSA_CLUSTER_NAME-public Create a Private Subnet for the cluster Create the private subnet in the VPC CIDR block range and return the ID as PRIVATE_SUBNET bash PRIVATE_SUBNET=`aws ec2 create-subnet --vpc-id $VPC_ID \\ --cidr-block 10.0.0.0/17 | jq -r .Subnet.SubnetId` echo $PRIVATE_SUBNET Tag the private subnet with the cluster name bash aws ec2 create-tags --resources $PRIVATE_SUBNET \\ --tags Key=Name,Value=$ROSA_CLUSTER_NAME-private Both subnets should now be visible in the AWS console Create an Internet Gateway for NAT egress traffic Create the Internet Gateway and return the ID as I_GW I_GW=`aws ec2 create-internet-gateway | jq -r .InternetGateway.InternetGatewayId` echo $I_GW Attach the new Internet Gateway to the VPC bash aws ec2 attach-internet-gateway --vpc-id $VPC_ID --internet-gateway-id $I_GW Tag the Internet Gateway with the cluster name bash aws ec2 create-tags --resources $I_GW \\ --tags Key=Name,Value=$ROSA_CLUSTER_NAME The new Internet Gateway should be created and attached to your VPC Create a Route Table for NAT egress traffic Create the Route Table and return the ID as R_TABLE bash R_TABLE=`aws ec2 create-route-table --vpc-id $VPC_ID \\ | jq -r .RouteTable.RouteTableId` echo $R_TABLE Create a route with no IP limitations (0.0.0.0/0) to the Internet Gateway bash aws ec2 create-route --route-table-id $R_TABLE \\ --destination-cidr-block 0.0.0.0/0 --gateway-id $I_GW Verify the route table settings bash aws ec2 describe-route-tables --route-table-id $R_TABLE Example output Associate the Route Table with the Public subnet bash aws ec2 associate-route-table --subnet-id $PUBLIC_SUBNET \\ --route-table-id $R_TABLE Example output Tag the Route Table with the cluster name bash aws ec2 create-tags --resources $R_TABLE \\ --tags Key=Name,Value=$ROSA_CLUSTER_NAME Create a NAT Gateway for the Private network Allocate and elastic IP address and return the ID as EIP bash EIP=`aws ec2 allocate-address --domain vpc | jq -r .AllocationId` echo $EIP Create a new NAT Gateway in the Public subnet with the new Elastic IP address and return the ID as NAT_GW bash NAT_GW=`aws ec2 create-nat-gateway --subnet-id $PUBLIC_SUBNET \\ --allocation-id $EIP | jq -r .NatGateway.NatGatewayId` echo $NAT_GW Tag the Elastic IP with the cluster name bash aws ec2 create-tags --resources $EIP --resources $NAT_GW \\ --tags Key=Name,Value=$ROSA_CLUSTER_NAME The new NAT Gateway should be created and associated with your VPC Create a Route Table for the Private subnet to the NAT Gateway Create a Route Table in the VPC and return the ID as R_TABLE_NAT bash R_TABLE_NAT=`aws ec2 create-route-table --vpc-id $VPC_ID \\ | jq -r .RouteTable.RouteTableId` echo $R_TABLE_NAT Loop through a Route Table check until it is created bash while ! aws ec2 describe-route-tables \\ --route-table-id $R_TABLE_NAT \\ | jq .; do sleep 1; done Example output! Create a route in the new Route Table for all addresses to the NAT Gateway bash aws ec2 create-route --route-table-id $R_TABLE_NAT \\ --destination-cidr-block 0.0.0.0/0 \\ --gateway-id $NAT_GW Associate the Route Table with the Private subnet bash aws ec2 associate-route-table --subnet-id $PRIVATE_SUBNET \\ --route-table-id $R_TABLE_NAT Tag the Route Table with the cluster name bash aws ec2 create-tags --resources $R_TABLE_NAT $EIP \\ --tags Key=Name,Value=$ROSA_CLUSTER_NAME-private Configure the AWS Security Token Service (STS) for use with ROSA The AWS Security Token Service (STS) allows us to deploy ROSA without needing a ROSA admin account, instead it uses roles and policies to gain access to the AWS resources needed to install and operate the cluster. This is a summary of the official OpenShift docs that can be used as a line by line install guide. Note that some commands (OIDC for STS) will be hard coded to US-EAST-1, do not be tempted to change these to use $region instead or you will fail installation. Make you your ROSA CLI version is correct (v1.1.0 or higher) bash rosa version Create the IAM Account Roles rosa create account-roles --mode auto --yes Deploy ROSA cluster Run the rosa cli to create your cluster bash rosa create cluster -y --cluster-name ${ROSA_CLUSTER_NAME} \\ --region ${REGION} --version ${VERSION} \\ --subnet-ids=$PRIVATE_SUBNET \\ --private-link --machine-cidr=10.0.0.0/16 \\ --sts Confirm the Private Link set up Create the Operator Roles bash rosa create operator-roles -c $ROSA_CLUSTER_NAME --mode auto --yes Create the OIDC provider. bash rosa create oidc-provider -c $ROSA_CLUSTER_NAME --mode auto --yes Validate The cluster is now installing The State should have moved beyond pending and show installing or ready . bash watch \"rosa describe cluster -c $ROSA_CLUSTER_NAME\" Watch the install logs bash rosa logs install -c $ROSA_CLUSTER_NAME --watch --tail 10 Validate the cluster Once the cluster has finished installing it is time to validate. Validation when using Private Link requires the use of a jump host . You can create them using the AWS Console or the AWS CLI as depicted below: Option 1 : Create a jump host instance through the AWS Console Navigate to the EC2 console and launch a new instance Select the AMI for your instance, if you don't have a standard, the Amazon Linux 2 AMI works just fine Choose your instance type, the t2.micro/free tier is sufficient for our needs, and click Next: Configure Instance Details Change the Network settings to setup this host inside your private-link VPC Change the Subnet setting to use the private-link-public subnet Change Auto-assign Public IP to Enable Default settings for Storage and Tags are fine. Make the following changes in the 6. Configure Security Group tab (either by clicking through the screens or selecting from the top bar) If you already have a security group created to allow access from your computer to AWS, choose Select an existing security group and choose that group from the list, otherwise, select Create a new security group and continue. To allow access only from your current public IP, change the Source heading to use My IP Click Review and Launch , verify all settings are correct, and follow the standard AWS instructions for finalizing the setup and selecting/creating the security keys. Once launched, open the instance summary for the jump host instance and note the public IP address. Option 2 : Create a jumphost instance using the AWS CLI Create an additional Security Group for the jumphost ```bash TAG_SG=\"$ROSA_CLUSTER_NAME-jumphost-sg\" aws ec2 create-security-group --group-name ${ROSA_CLUSTER_NAME}-jumphost-sg --description ${ROSA_CLUSTER_NAME}-jumphost-sg --vpc-id ${VPC_ID} --tag-specifications \"ResourceType=security-group,Tags=[{Key=Name,Value=$TAG_SG}] ``` Grab the Security Group Id generated in the previous step bash PublicSecurityGroupId=$(aws ec2 describe-security-groups --filters \"Name=tag:Name,Values=${ROSA_CLUSTER_NAME}-jumphost-sg\" | jq -r '.SecurityGroups[0].GroupId') echo $PublicSecurityGroupId Add a rule to Allow the ssh into the Public Security Group bash aws ec2 authorize-security-group-ingress --group-id $PublicSecurityGroupId --protocol tcp --port 22 --cidr 0.0.0.0/0 (Optional) Create a Key Pair for your jumphost if your have not a previous one bash aws ec2 create-key-pair --key-name $ROSA_CLUSTER_NAME-key --query 'KeyMaterial' --output text > PATH/TO/YOUR_KEY.pem chmod 400 PATH/TO/YOUR_KEY.pem Define an AMI_ID to be used for your jump host bash AMI_ID=\"ami-0022f774911c1d690\" This AMI_ID corresponds an Amazon Linux within the us-east-1 region and could be not available in your region. Find your AMI ID and use the proper ID. Launch an ec2 instance for your jumphost using the parameters defined in early steps: ```bash TAG_VM=\"$ROSA_CLUSTER_NAME-jumphost-vm\" aws ec2 run-instances --image-id $AMI_ID --count 1 --instance-type t2.micro --key-name $ROSA_CLUSTER_NAME-key --security-group-ids $PublicSecurityGroupId --subnet-id $PUBLIC_SUBNET --associate-public-ip-address --tag-specifications \"ResourceType=instance,Tags=[{Key=Name,Value=$TAG_VM}]\" ``` This instance will be associated with a Public IP directly. Wait until the ec2 instance is in Running state, grab the Public IP associated to the instance and check the if the ssh port and: ```bash IpPublicBastion=$(aws ec2 describe-instances --filters \"Name=tag:Name,Values=$TAG_VM\" | jq -r '.Reservations[0].Instances[0].PublicIpAddress') echo $IpPublicBastion nc -vz $IpPublicBastion 22 ``` Create a ROSA admin user and save the login command for use later rosa create admin -c $ROSA_CLUSTER_NAME Note the DNS name of your private cluster, use the rosa describe command if needed rosa describe cluster -c $ROSA_CLUSTER_NAME update /etc/hosts to point the openshift domains to localhost. Use the DNS of your openshift cluster as described in the previous step in place of $YOUR_OPENSHIFT_DNS below 127.0.0.1 api.$YOUR_OPENSHIFT_DNS 127.0.0.1 console-openshift-console.apps.$YOUR_OPENSHIFT_DNS 127.0.0.1 oauth-openshift.apps.$YOUR_OPENSHIFT_DNS SSH to that instance, tunneling traffic for the appropriate hostnames. Be sure to use your new/existing private key, the OpenShift DNS for $YOUR_OPENSHIFT_DNS and your jump host IP for $YOUR_EC2_IP bash sudo ssh -i PATH/TO/YOUR_KEY.pem \\ -L 6443:api.$YOUR_OPENSHIFT_DNS:6443 \\ -L 443:console-openshift-console.apps.$YOUR_OPENSHIFT_DNS:443 \\ -L 80:console-openshift-console.apps.$YOUR_OPENSHIFT_DNS:80 \\ ec2-user@$YOUR_EC2_IP From your EC2 jump instances, download the OC CLI and install it locally Download the OC CLI for Linux wget https://mirror.openshift.com/pub/openshift-v4/clients/ocp/stable/openshift-client-linux.tar.gz Unzip and untar the binary gunzip openshift-client-linux.tar.gz tar -xvf openshift-client-linux.tar log into the cluster using oc login command from the create admin command above. ex. bash ./oc login https://api.$YOUR_OPENSHIFT_DNS.p1.openshiftapps.com:6443 --username cluster-admin --password $YOUR_OPENSHIFT_PWD Check that you can access the Console by opening the console url in your browser. Cleanup Delete ROSA bash rosa delete cluster -c $ROSA_CLUSTER_NAME -y Watch the logs and wait until the cluster is deleted bash rosa logs uninstall -c $ROSA_CLUSTER_NAME --watch Clean up the STS roles Note you can get the correct commands with the ID filled in from the output of the previous step. bash rosa delete operator-roles -c <id> --mode auto --yes rosa delete oidc-provider -c <id> --mode auto --yes Delete AWS resources bash aws ec2 delete-nat-gateway --nat-gateway-id $NAT_GW | jq . aws ec2 release-address --allocation-id=$EIP | jq . aws ec2 detach-internet-gateway --vpc-id $VPC_ID --internet-gateway-id $I_GW | jq . aws ec2 delete-subnet --subnet-id=$PRIVATE_SUBNET | jq . aws ec2 delete-subnet --subnet-id=$PUBLIC_SUBNET | jq . aws ec2 delete-route-table --route-table-id=$R_TABLE | jq . aws ec2 delete-route-table --route-table-id=$R_TABLE_NAT | jq . aws ec2 delete-vpc --vpc-id=$VPC_ID | jq .","title":"Creating a ROSA cluster with Private Link enabled (custom VPC) and STS"},{"location":"rosa/sts-with-private-link/#creating-a-rosa-cluster-with-private-link-enabled-custom-vpc-and-sts","text":"Steve Mirman, Paul Czarkowski Last updated 1/28/2022 This is a combination of the private-link and sts setup documents to show the full picture","title":"Creating a ROSA cluster with Private Link enabled (custom VPC) and STS"},{"location":"rosa/sts-with-private-link/#prerequisites","text":"AWS CLI Rosa CLI v1.1.7 jq","title":"Prerequisites"},{"location":"rosa/sts-with-private-link/#aws-preparation","text":"If this is a brand new AWS account that has never had a AWS Load Balancer installed in it, you should run the following bash aws iam create-service-linked-role --aws-service-name \\ \"elasticloadbalancing.amazonaws.com\"","title":"AWS Preparation"},{"location":"rosa/sts-with-private-link/#create-the-aws-virtual-private-cloud-vpc-and-subnets","text":"For this scenario, we will be using a newly created VPC with both public and private subnets. All of the cluster resources will reside in the private subnet. The public subnet will be used for traffic to the Internet (egress) Note : If you already have a Transit Gateway (TGW) or similar, you can skip the public subnet configuration Note : When creating subnets, make sure that subnet(s) are created in availability zones that have ROSA instances types available. If AZ is not \"forced\", the subnet is created in a random AZ in the region. Force AZ using the --availability-zone argument in the create-subnet command. Use rosa list instance-types to list the ROSA instance types Use aws ec2 describe-instance-type-offerings to check that your desired AZ supports your desired instance type Example using us-east-1 , us-east-1b , and m5.xlarge : aws ec2 describe-instance-type-offerings --location-type availability-zone --filters Name=location,Values=us-east-1b --region us-east-1 --output text | egrep m5.xlarge Result should display INSTANCETYPEOFFERINGS [instance-type] [az] availability-zone if your selected region supports your desired instance type Configure the following environment variables, adjusting for ROSA_CLUSTER_NAME , VERSION and REGION as necessary bash export VERSION=4.9.15 \\ ROSA_CLUSTER_NAME=pl-sts-cluster \\ AWS_ACCOUNT_ID=`aws sts get-caller-identity --query Account --output text` \\ REGION=us-east-1 \\ AWS_PAGER=\"\" Create a VPC for use by ROSA Create the VPC and return the ID as VPC_ID VPC_ID=`aws ec2 create-vpc --cidr-block 10.0.0.0/16 | jq -r .Vpc.VpcId` echo $VPC_ID Tag the newly created VPC with the cluster name bash aws ec2 create-tags --resources $VPC_ID \\ --tags Key=Name,Value=$ROSA_CLUSTER_NAME Configure the VPC to allow DNS hostnames for their public IP addresses bash aws ec2 modify-vpc-attribute --vpc-id $VPC_ID --enable-dns-hostnames The new VPC should be visible in the AWS console Create a Public Subnet to allow egress traffic to the Internet Create the public subnet in the VPC CIDR block range and return the ID as PUBLIC_SUBNET bash PUBLIC_SUBNET=`aws ec2 create-subnet --vpc-id $VPC_ID --cidr-block 10.0.128.0/17 | jq -r .Subnet.SubnetId` echo $PUBLIC_SUBNET Tag the public subnet with the cluster name bash aws ec2 create-tags --resources $PUBLIC_SUBNET \\ --tags Key=Name,Value=$ROSA_CLUSTER_NAME-public Create a Private Subnet for the cluster Create the private subnet in the VPC CIDR block range and return the ID as PRIVATE_SUBNET bash PRIVATE_SUBNET=`aws ec2 create-subnet --vpc-id $VPC_ID \\ --cidr-block 10.0.0.0/17 | jq -r .Subnet.SubnetId` echo $PRIVATE_SUBNET Tag the private subnet with the cluster name bash aws ec2 create-tags --resources $PRIVATE_SUBNET \\ --tags Key=Name,Value=$ROSA_CLUSTER_NAME-private Both subnets should now be visible in the AWS console Create an Internet Gateway for NAT egress traffic Create the Internet Gateway and return the ID as I_GW I_GW=`aws ec2 create-internet-gateway | jq -r .InternetGateway.InternetGatewayId` echo $I_GW Attach the new Internet Gateway to the VPC bash aws ec2 attach-internet-gateway --vpc-id $VPC_ID --internet-gateway-id $I_GW Tag the Internet Gateway with the cluster name bash aws ec2 create-tags --resources $I_GW \\ --tags Key=Name,Value=$ROSA_CLUSTER_NAME The new Internet Gateway should be created and attached to your VPC Create a Route Table for NAT egress traffic Create the Route Table and return the ID as R_TABLE bash R_TABLE=`aws ec2 create-route-table --vpc-id $VPC_ID \\ | jq -r .RouteTable.RouteTableId` echo $R_TABLE Create a route with no IP limitations (0.0.0.0/0) to the Internet Gateway bash aws ec2 create-route --route-table-id $R_TABLE \\ --destination-cidr-block 0.0.0.0/0 --gateway-id $I_GW Verify the route table settings bash aws ec2 describe-route-tables --route-table-id $R_TABLE Example output Associate the Route Table with the Public subnet bash aws ec2 associate-route-table --subnet-id $PUBLIC_SUBNET \\ --route-table-id $R_TABLE Example output Tag the Route Table with the cluster name bash aws ec2 create-tags --resources $R_TABLE \\ --tags Key=Name,Value=$ROSA_CLUSTER_NAME Create a NAT Gateway for the Private network Allocate and elastic IP address and return the ID as EIP bash EIP=`aws ec2 allocate-address --domain vpc | jq -r .AllocationId` echo $EIP Create a new NAT Gateway in the Public subnet with the new Elastic IP address and return the ID as NAT_GW bash NAT_GW=`aws ec2 create-nat-gateway --subnet-id $PUBLIC_SUBNET \\ --allocation-id $EIP | jq -r .NatGateway.NatGatewayId` echo $NAT_GW Tag the Elastic IP with the cluster name bash aws ec2 create-tags --resources $EIP --resources $NAT_GW \\ --tags Key=Name,Value=$ROSA_CLUSTER_NAME The new NAT Gateway should be created and associated with your VPC Create a Route Table for the Private subnet to the NAT Gateway Create a Route Table in the VPC and return the ID as R_TABLE_NAT bash R_TABLE_NAT=`aws ec2 create-route-table --vpc-id $VPC_ID \\ | jq -r .RouteTable.RouteTableId` echo $R_TABLE_NAT Loop through a Route Table check until it is created bash while ! aws ec2 describe-route-tables \\ --route-table-id $R_TABLE_NAT \\ | jq .; do sleep 1; done Example output! Create a route in the new Route Table for all addresses to the NAT Gateway bash aws ec2 create-route --route-table-id $R_TABLE_NAT \\ --destination-cidr-block 0.0.0.0/0 \\ --gateway-id $NAT_GW Associate the Route Table with the Private subnet bash aws ec2 associate-route-table --subnet-id $PRIVATE_SUBNET \\ --route-table-id $R_TABLE_NAT Tag the Route Table with the cluster name bash aws ec2 create-tags --resources $R_TABLE_NAT $EIP \\ --tags Key=Name,Value=$ROSA_CLUSTER_NAME-private","title":"Create the AWS Virtual Private Cloud (VPC) and Subnets"},{"location":"rosa/sts-with-private-link/#configure-the-aws-security-token-service-sts-for-use-with-rosa","text":"The AWS Security Token Service (STS) allows us to deploy ROSA without needing a ROSA admin account, instead it uses roles and policies to gain access to the AWS resources needed to install and operate the cluster. This is a summary of the official OpenShift docs that can be used as a line by line install guide. Note that some commands (OIDC for STS) will be hard coded to US-EAST-1, do not be tempted to change these to use $region instead or you will fail installation. Make you your ROSA CLI version is correct (v1.1.0 or higher) bash rosa version Create the IAM Account Roles rosa create account-roles --mode auto --yes","title":"Configure the AWS Security Token Service (STS) for use with ROSA"},{"location":"rosa/sts-with-private-link/#deploy-rosa-cluster","text":"Run the rosa cli to create your cluster bash rosa create cluster -y --cluster-name ${ROSA_CLUSTER_NAME} \\ --region ${REGION} --version ${VERSION} \\ --subnet-ids=$PRIVATE_SUBNET \\ --private-link --machine-cidr=10.0.0.0/16 \\ --sts Confirm the Private Link set up Create the Operator Roles bash rosa create operator-roles -c $ROSA_CLUSTER_NAME --mode auto --yes Create the OIDC provider. bash rosa create oidc-provider -c $ROSA_CLUSTER_NAME --mode auto --yes Validate The cluster is now installing The State should have moved beyond pending and show installing or ready . bash watch \"rosa describe cluster -c $ROSA_CLUSTER_NAME\" Watch the install logs bash rosa logs install -c $ROSA_CLUSTER_NAME --watch --tail 10","title":"Deploy ROSA cluster"},{"location":"rosa/sts-with-private-link/#validate-the-cluster","text":"Once the cluster has finished installing it is time to validate. Validation when using Private Link requires the use of a jump host . You can create them using the AWS Console or the AWS CLI as depicted below: Option 1 : Create a jump host instance through the AWS Console Navigate to the EC2 console and launch a new instance Select the AMI for your instance, if you don't have a standard, the Amazon Linux 2 AMI works just fine Choose your instance type, the t2.micro/free tier is sufficient for our needs, and click Next: Configure Instance Details Change the Network settings to setup this host inside your private-link VPC Change the Subnet setting to use the private-link-public subnet Change Auto-assign Public IP to Enable Default settings for Storage and Tags are fine. Make the following changes in the 6. Configure Security Group tab (either by clicking through the screens or selecting from the top bar) If you already have a security group created to allow access from your computer to AWS, choose Select an existing security group and choose that group from the list, otherwise, select Create a new security group and continue. To allow access only from your current public IP, change the Source heading to use My IP Click Review and Launch , verify all settings are correct, and follow the standard AWS instructions for finalizing the setup and selecting/creating the security keys. Once launched, open the instance summary for the jump host instance and note the public IP address. Option 2 : Create a jumphost instance using the AWS CLI Create an additional Security Group for the jumphost ```bash TAG_SG=\"$ROSA_CLUSTER_NAME-jumphost-sg\" aws ec2 create-security-group --group-name ${ROSA_CLUSTER_NAME}-jumphost-sg --description ${ROSA_CLUSTER_NAME}-jumphost-sg --vpc-id ${VPC_ID} --tag-specifications \"ResourceType=security-group,Tags=[{Key=Name,Value=$TAG_SG}] ``` Grab the Security Group Id generated in the previous step bash PublicSecurityGroupId=$(aws ec2 describe-security-groups --filters \"Name=tag:Name,Values=${ROSA_CLUSTER_NAME}-jumphost-sg\" | jq -r '.SecurityGroups[0].GroupId') echo $PublicSecurityGroupId Add a rule to Allow the ssh into the Public Security Group bash aws ec2 authorize-security-group-ingress --group-id $PublicSecurityGroupId --protocol tcp --port 22 --cidr 0.0.0.0/0 (Optional) Create a Key Pair for your jumphost if your have not a previous one bash aws ec2 create-key-pair --key-name $ROSA_CLUSTER_NAME-key --query 'KeyMaterial' --output text > PATH/TO/YOUR_KEY.pem chmod 400 PATH/TO/YOUR_KEY.pem Define an AMI_ID to be used for your jump host bash AMI_ID=\"ami-0022f774911c1d690\" This AMI_ID corresponds an Amazon Linux within the us-east-1 region and could be not available in your region. Find your AMI ID and use the proper ID. Launch an ec2 instance for your jumphost using the parameters defined in early steps: ```bash TAG_VM=\"$ROSA_CLUSTER_NAME-jumphost-vm\" aws ec2 run-instances --image-id $AMI_ID --count 1 --instance-type t2.micro --key-name $ROSA_CLUSTER_NAME-key --security-group-ids $PublicSecurityGroupId --subnet-id $PUBLIC_SUBNET --associate-public-ip-address --tag-specifications \"ResourceType=instance,Tags=[{Key=Name,Value=$TAG_VM}]\" ``` This instance will be associated with a Public IP directly. Wait until the ec2 instance is in Running state, grab the Public IP associated to the instance and check the if the ssh port and: ```bash IpPublicBastion=$(aws ec2 describe-instances --filters \"Name=tag:Name,Values=$TAG_VM\" | jq -r '.Reservations[0].Instances[0].PublicIpAddress') echo $IpPublicBastion nc -vz $IpPublicBastion 22 ``` Create a ROSA admin user and save the login command for use later rosa create admin -c $ROSA_CLUSTER_NAME Note the DNS name of your private cluster, use the rosa describe command if needed rosa describe cluster -c $ROSA_CLUSTER_NAME update /etc/hosts to point the openshift domains to localhost. Use the DNS of your openshift cluster as described in the previous step in place of $YOUR_OPENSHIFT_DNS below 127.0.0.1 api.$YOUR_OPENSHIFT_DNS 127.0.0.1 console-openshift-console.apps.$YOUR_OPENSHIFT_DNS 127.0.0.1 oauth-openshift.apps.$YOUR_OPENSHIFT_DNS SSH to that instance, tunneling traffic for the appropriate hostnames. Be sure to use your new/existing private key, the OpenShift DNS for $YOUR_OPENSHIFT_DNS and your jump host IP for $YOUR_EC2_IP bash sudo ssh -i PATH/TO/YOUR_KEY.pem \\ -L 6443:api.$YOUR_OPENSHIFT_DNS:6443 \\ -L 443:console-openshift-console.apps.$YOUR_OPENSHIFT_DNS:443 \\ -L 80:console-openshift-console.apps.$YOUR_OPENSHIFT_DNS:80 \\ ec2-user@$YOUR_EC2_IP From your EC2 jump instances, download the OC CLI and install it locally Download the OC CLI for Linux wget https://mirror.openshift.com/pub/openshift-v4/clients/ocp/stable/openshift-client-linux.tar.gz Unzip and untar the binary gunzip openshift-client-linux.tar.gz tar -xvf openshift-client-linux.tar log into the cluster using oc login command from the create admin command above. ex. bash ./oc login https://api.$YOUR_OPENSHIFT_DNS.p1.openshiftapps.com:6443 --username cluster-admin --password $YOUR_OPENSHIFT_PWD Check that you can access the Console by opening the console url in your browser.","title":"Validate the cluster"},{"location":"rosa/sts-with-private-link/#cleanup","text":"Delete ROSA bash rosa delete cluster -c $ROSA_CLUSTER_NAME -y Watch the logs and wait until the cluster is deleted bash rosa logs uninstall -c $ROSA_CLUSTER_NAME --watch Clean up the STS roles Note you can get the correct commands with the ID filled in from the output of the previous step. bash rosa delete operator-roles -c <id> --mode auto --yes rosa delete oidc-provider -c <id> --mode auto --yes Delete AWS resources bash aws ec2 delete-nat-gateway --nat-gateway-id $NAT_GW | jq . aws ec2 release-address --allocation-id=$EIP | jq . aws ec2 detach-internet-gateway --vpc-id $VPC_ID --internet-gateway-id $I_GW | jq . aws ec2 delete-subnet --subnet-id=$PRIVATE_SUBNET | jq . aws ec2 delete-subnet --subnet-id=$PUBLIC_SUBNET | jq . aws ec2 delete-route-table --route-table-id=$R_TABLE | jq . aws ec2 delete-route-table --route-table-id=$R_TABLE_NAT | jq . aws ec2 delete-vpc --vpc-id=$VPC_ID | jq .","title":"Cleanup"},{"location":"rosa/using-sts-with-aws-services/","text":"Extending ROSA STS to include authentication with AWS Services In this example we will deploy the Amazon Ingress Controller that uses ALBs, and configure it to use STS authentication. Deployment Configure STS Make sure your cluster has the pod identity webhook bash kubectl get mutatingwebhookconfigurations.admissionregistration.k8s.io pod-identity-webhook Download the IAM Policy for the AWS Load Balancer Hooks bash wget https://raw.githubusercontent.com/kubernetes-sigs/aws-load-balancer-controller/v2.2.0/docs/install/iam_policy.json Create AWS Role with inline policy bash aws iam create-role \\ --role-name AWSLoadBalancerController --query Policy.Arn --output text Create AWS Policy and Service Account bash POLICY_ARN=$(aws iam create-policy --policy-name \"AWSLoadBalancerControllerIAMPolicy\" --policy-document file://iam_policy.json --query Policy.Arn --output text) echo $POLICY_ARN Create service account Note I had issues with the policy, and for now just gave this user admin creds. Need to revisit and figure out. SA_ARN=$(aws iam create-user --user-name aws-lb-controller --permissions-boundary=$POLICY_ARN --query User.Arn --output text) Create access key ACCESS_KEY=$(aws iam create-access-key --user-name aws-lb-controller) Attach policy to user ```bash Paste the AccessKeyId and SecretAccessKey into values.yaml tag your public subnet with `` Create a namespace for the controller bash kubectl create ns aws-load-balancer-controller Apply CRDs bash kubectl apply -k \"github.com/aws/eks-charts/stable/aws-load-balancer-controller//crds?ref=master\" Add the helm repo and install the controller (install helm3 if not already) bash helm repo add eks https://aws.github.io/eks-charts helm install -n aws-load-balancer-controller \\ aws-load-balancer-controller eks/aws-load-balancer-controller \\ --values=./helm/values.yaml --create-namespace Deploy Sample Application oc new-project demo oc new-app https://github.com/sclorg/django-ex.git kubectl -n demo patch service django-ex -p '{\"spec\":{\"type\":\"NodePort\"}}' kubectl apply -f ingress.yaml","title":"Extending ROSA STS to include authentication with AWS Services"},{"location":"rosa/using-sts-with-aws-services/#extending-rosa-sts-to-include-authentication-with-aws-services","text":"In this example we will deploy the Amazon Ingress Controller that uses ALBs, and configure it to use STS authentication.","title":"Extending ROSA STS to include authentication with AWS Services"},{"location":"rosa/using-sts-with-aws-services/#deployment","text":"","title":"Deployment"},{"location":"rosa/using-sts-with-aws-services/#configure-sts","text":"Make sure your cluster has the pod identity webhook bash kubectl get mutatingwebhookconfigurations.admissionregistration.k8s.io pod-identity-webhook Download the IAM Policy for the AWS Load Balancer Hooks bash wget https://raw.githubusercontent.com/kubernetes-sigs/aws-load-balancer-controller/v2.2.0/docs/install/iam_policy.json Create AWS Role with inline policy bash aws iam create-role \\ --role-name AWSLoadBalancerController --query Policy.Arn --output text Create AWS Policy and Service Account bash POLICY_ARN=$(aws iam create-policy --policy-name \"AWSLoadBalancerControllerIAMPolicy\" --policy-document file://iam_policy.json --query Policy.Arn --output text) echo $POLICY_ARN Create service account Note I had issues with the policy, and for now just gave this user admin creds. Need to revisit and figure out. SA_ARN=$(aws iam create-user --user-name aws-lb-controller --permissions-boundary=$POLICY_ARN --query User.Arn --output text) Create access key ACCESS_KEY=$(aws iam create-access-key --user-name aws-lb-controller) Attach policy to user ```bash Paste the AccessKeyId and SecretAccessKey into values.yaml tag your public subnet with `` Create a namespace for the controller bash kubectl create ns aws-load-balancer-controller Apply CRDs bash kubectl apply -k \"github.com/aws/eks-charts/stable/aws-load-balancer-controller//crds?ref=master\" Add the helm repo and install the controller (install helm3 if not already) bash helm repo add eks https://aws.github.io/eks-charts helm install -n aws-load-balancer-controller \\ aws-load-balancer-controller eks/aws-load-balancer-controller \\ --values=./helm/values.yaml --create-namespace","title":"Configure STS"},{"location":"rosa/using-sts-with-aws-services/#deploy-sample-application","text":"oc new-project demo oc new-app https://github.com/sclorg/django-ex.git kubectl -n demo patch service django-ex -p '{\"spec\":{\"type\":\"NodePort\"}}' kubectl apply -f ingress.yaml","title":"Deploy Sample Application"},{"location":"security/rhacs/","text":"Deploying Red Hat Advanced Cluster Security in ARO/ROSA Author: Roberto Carratal\u00e1 Updated: 10/06/2022 This document is based in the RHACS workshop and in the RHACS official documentation . Prerequisites An ARO cluster or a ROSA cluster . Set up the OpenShift CLI (oc) Download the OS specific OpenShift CLI from Red Hat Unzip the downloaded file on your local machine Place the extracted oc executable in your OS path or local directory Login to ARO / ROSA Login to your ARO / ROSA clusters with user with cluster-admin privileges. Installing Red Hat Advanced Cluster Security in ARO/ROSA For install RHACS in ARO/ROSA you have two options: Option 1 - Manual Installation Option 2 - Automated Installation using Ansible Option 1 - Manual Installation For install RHACS using the Option 1 - Manual installation: Follow the steps within the RHACS Operator Installation Workshop to install the RHACS Operator. Follow the steps within the RHACS Central Cluster Installation Workshop to install the RHACS Central Cluster. Follow the steps within the RHACS Secured Cluster Configuration , to import the ARO/ROSA cluster into RHACS. Option 2 - Automated Installation using Ansible For install the RHACS in ROSA/ARO you can use the rhacs-demo repository that will install RH-ACS using Ansible playbooks: Clone the rhacm-demo repo and install the galaxy collection: ansible-galaxy collection install kubernetes.core pip3 install kubernetes jmespath git clone https://github.com/rh-mobb/rhacs-demo cd rhacs-demo Deploy the RHACS with the ansible-playbook command: ansible-playbook rhacs-install.yaml This will install RHACS and also a couple of example Apps to demo. If you want just the plain RHACS installation, use the rhacs-only-install.yaml playbook. Deploying Example Apps for demo RHACS Deploy some example apps for demo RHACS policies and violations: oc new-project test oc run shell --labels=app=shellshock,team=test-team \\ --image=vulnerables/cve-2014-6271 -n test oc run samba --labels=app=rce \\ --image=vulnerables/cve-2017-7494 -n test","title":"Deploying ACS in ROSA/ARO"},{"location":"security/rhacs/#deploying-red-hat-advanced-cluster-security-in-arorosa","text":"Author: Roberto Carratal\u00e1 Updated: 10/06/2022 This document is based in the RHACS workshop and in the RHACS official documentation .","title":"Deploying Red Hat Advanced Cluster Security in ARO/ROSA"},{"location":"security/rhacs/#prerequisites","text":"An ARO cluster or a ROSA cluster .","title":"Prerequisites"},{"location":"security/rhacs/#set-up-the-openshift-cli-oc","text":"Download the OS specific OpenShift CLI from Red Hat Unzip the downloaded file on your local machine Place the extracted oc executable in your OS path or local directory","title":"Set up the OpenShift CLI (oc)"},{"location":"security/rhacs/#login-to-aro-rosa","text":"Login to your ARO / ROSA clusters with user with cluster-admin privileges.","title":"Login to ARO / ROSA"},{"location":"security/rhacs/#installing-red-hat-advanced-cluster-security-in-arorosa","text":"For install RHACS in ARO/ROSA you have two options: Option 1 - Manual Installation Option 2 - Automated Installation using Ansible","title":"Installing Red Hat Advanced Cluster Security in ARO/ROSA"},{"location":"security/rhacs/#option-1-manual-installation","text":"For install RHACS using the Option 1 - Manual installation: Follow the steps within the RHACS Operator Installation Workshop to install the RHACS Operator. Follow the steps within the RHACS Central Cluster Installation Workshop to install the RHACS Central Cluster. Follow the steps within the RHACS Secured Cluster Configuration , to import the ARO/ROSA cluster into RHACS.","title":"Option 1 - Manual Installation"},{"location":"security/rhacs/#option-2-automated-installation-using-ansible","text":"For install the RHACS in ROSA/ARO you can use the rhacs-demo repository that will install RH-ACS using Ansible playbooks: Clone the rhacm-demo repo and install the galaxy collection: ansible-galaxy collection install kubernetes.core pip3 install kubernetes jmespath git clone https://github.com/rh-mobb/rhacs-demo cd rhacs-demo Deploy the RHACS with the ansible-playbook command: ansible-playbook rhacs-install.yaml This will install RHACS and also a couple of example Apps to demo. If you want just the plain RHACS installation, use the rhacs-only-install.yaml playbook.","title":"Option 2 - Automated Installation using Ansible"},{"location":"security/rhacs/#deploying-example-apps-for-demo-rhacs","text":"Deploy some example apps for demo RHACS policies and violations: oc new-project test oc run shell --labels=app=shellshock,team=test-team \\ --image=vulnerables/cve-2014-6271 -n test oc run samba --labels=app=rce \\ --image=vulnerables/cve-2017-7494 -n test","title":"Deploying Example Apps for demo RHACS"},{"location":"security/secrets-store-csi/","text":"Installing the Kubernetes Secret Store CSI on OpenShift The Kubernetes Secret Store CSI is a storage driver that allows you to mount secrets from external secret management systems like HashiCorp Vault and AWS Secrets. It comes in two parts, the Secret Store CSI, and a Secret provider driver. This document covers just the CSI itself. Prerequisites An OpenShift Cluster (ROSA, ARO, OSD, and OCP 4.x all work) kubectl helm v3 {% include_relative install-kubernetes-secret-store-driver.md %} {% include_relative uninstall-kubernetes-secret-store-driver.md %} Provider Specifics HashiCorp Vault","title":"Installing the Kubernetes Secret Store CSI on OpenShift"},{"location":"security/secrets-store-csi/#installing-the-kubernetes-secret-store-csi-on-openshift","text":"The Kubernetes Secret Store CSI is a storage driver that allows you to mount secrets from external secret management systems like HashiCorp Vault and AWS Secrets. It comes in two parts, the Secret Store CSI, and a Secret provider driver. This document covers just the CSI itself.","title":"Installing the Kubernetes Secret Store CSI on OpenShift"},{"location":"security/secrets-store-csi/#prerequisites","text":"An OpenShift Cluster (ROSA, ARO, OSD, and OCP 4.x all work) kubectl helm v3 {% include_relative install-kubernetes-secret-store-driver.md %} {% include_relative uninstall-kubernetes-secret-store-driver.md %}","title":"Prerequisites"},{"location":"security/secrets-store-csi/#provider-specifics","text":"HashiCorp Vault","title":"Provider Specifics"},{"location":"security/secrets-store-csi/azure-key-vault/","text":"Azure Key Vault CSI on Azure Red Hat OpenShift Author: Paul Czarkowski Modified: 08/16/2021 This document is adapted from the Azure Key Vault CSI Walkthrough specifically to run with Azure Red Hat OpenShift (ARO). Prerequisites An ARO cluster The AZ CLI (logged in) Helm 3.x CLI Environment Variables Run this command to set some environment variables to use throughout Note if you created the cluster from the instructions linked above these will re-use the same environment variables, or default them to openshift and eastus . bash export KEYVAULT_RESOURCE_GROUP=${AZR_RESOURCE_GROUP:-\"openshift\"} export KEYVAULT_LOCATION=${AZR_RESOURCE_LOCATION:-\"eastus\"} export KEYVAULT_NAME=secret-store-$(cat /dev/urandom | tr -dc 'a-zA-Z0-9' | fold -w 10 | head -n 1) export AZ_TENANT_ID=$(az account show -o tsv --query tenantId) {% include_relative install-kubernetes-secret-store-driver.md %} Deploy Azure Key Store CSI Add the Azure Helm Repository bash helm repo add csi-secrets-store-provider-azure \\ https://raw.githubusercontent.com/Azure/secrets-store-csi-driver-provider-azure/master/charts Update your local Helm Repositories bash helm repo update Install the Azure Key Vault CSI provider bash helm install -n k8s-secrets-store-csi azure-csi-provider \\ csi-secrets-store-provider-azure/csi-secrets-store-provider-azure \\ --set linux.privileged=true --set secrets-store-csi-driver.install=false \\ --set \"linux.providersDir=/var/run/secrets-store-csi-providers\" \\ --version=v1.0.1 Set SecurityContextConstraints to allow the CSI driver to run bash oc adm policy add-scc-to-user privileged \\ system:serviceaccount:k8s-secrets-store-csi:csi-secrets-store-provider-azure Create Keyvault and a Secret Create a namespace for your application bash oc new-project my-application Create an Azure Keyvault in your Resource Group that contains ARO bash az keyvault create -n ${KEYVAULT_NAME} \\ -g ${KEYVAULT_RESOURCE_GROUP} \\ --location ${KEYVAULT_LOCATION} Create a secret in the Keyvault bash az keyvault secret set \\ --vault-name ${KEYVAULT_NAME} \\ --name secret1 --value \"Hello\" Create a Service Principal for the keyvault Note: If this gives you an error, you may need upgrade your Azure CLI to the latest version. ```bash export SERVICE_PRINCIPAL_CLIENT_SECRET=\"$(az ad sp create-for-rbac --skip-assignment --name http://$KEYVAULT_NAME --query 'password' -otsv)\" export SERVICE_PRINCIPAL_CLIENT_ID=\"$(az ad sp list --display-name http://$KEYVAULT_NAME --query '[0].appId' -otsv)\" ``` Set an Access Policy for the Service Principal bash az keyvault set-policy -n ${KEYVAULT_NAME} \\ --secret-permissions get \\ --spn ${SERVICE_PRINCIPAL_CLIENT_ID} Create and label a secret for Kubernetes to use to access the Key Vault bash kubectl create secret generic secrets-store-creds \\ -n my-application \\ --from-literal clientid=${SERVICE_PRINCIPAL_CLIENT_ID} \\ --from-literal clientsecret=${SERVICE_PRINCIPAL_CLIENT_SECRET} kubectl -n my-application label secret \\ secrets-store-creds secrets-store.csi.k8s.io/used=true Deploy an Application that uses the CSI Create a Secret Provider Class to give access to this secret bash cat <<EOF | kubectl apply -f - apiVersion: secrets-store.csi.x-k8s.io/v1alpha1 kind: SecretProviderClass metadata: name: azure-kvname namespace: my-application spec: provider: azure parameters: usePodIdentity: \"false\" useVMManagedIdentity: \"false\" userAssignedIdentityID: \"\" keyvaultName: \"${KEYVAULT_NAME}\" objects: | array: - | objectName: secret1 objectType: secret objectVersion: \"\" tenantId: \"${AZ_TENANT_ID}\" EOF Create a Pod that uses the above Secret Provider Class bash cat <<EOF | kubectl apply -f - kind: Pod apiVersion: v1 metadata: name: busybox-secrets-store-inline namespace: my-application spec: containers: - name: busybox image: k8s.gcr.io/e2e-test-images/busybox:1.29 command: - \"/bin/sleep\" - \"10000\" volumeMounts: - name: secrets-store-inline mountPath: \"/mnt/secrets-store\" readOnly: true volumes: - name: secrets-store-inline csi: driver: secrets-store.csi.k8s.io readOnly: true volumeAttributes: secretProviderClass: \"azure-kvname\" nodePublishSecretRef: name: secrets-store-creds EOF Check the Secret is mounted bash kubectl exec busybox-secrets-store-inline -- ls /mnt/secrets-store/ Output should match: secret1 Print the Secret bash kubectl exec busybox-secrets-store-inline \\ -- cat /mnt/secrets-store/secret1 Output should match: Hello Cleanup Uninstall Helm bash helm uninstall -n k8s-secrets-store-csi azure-csi-provider Delete the app bash oc delete project my-application Delete the Azure Key Vault bash az keyvault delete -n ${KEYVAULT_NAME} Delete the Service Principal bash az ad sp delete --id ${SERVICE_PRINCIPAL_CLIENT_ID} {% include_relative uninstall-kubernetes-secret-store-driver.md %}","title":"Azure Key Vault CSI Driver"},{"location":"security/secrets-store-csi/azure-key-vault/#azure-key-vault-csi-on-azure-red-hat-openshift","text":"Author: Paul Czarkowski Modified: 08/16/2021 This document is adapted from the Azure Key Vault CSI Walkthrough specifically to run with Azure Red Hat OpenShift (ARO).","title":"Azure Key Vault CSI on Azure Red Hat OpenShift"},{"location":"security/secrets-store-csi/azure-key-vault/#prerequisites","text":"An ARO cluster The AZ CLI (logged in) Helm 3.x CLI","title":"Prerequisites"},{"location":"security/secrets-store-csi/azure-key-vault/#environment-variables","text":"Run this command to set some environment variables to use throughout Note if you created the cluster from the instructions linked above these will re-use the same environment variables, or default them to openshift and eastus . bash export KEYVAULT_RESOURCE_GROUP=${AZR_RESOURCE_GROUP:-\"openshift\"} export KEYVAULT_LOCATION=${AZR_RESOURCE_LOCATION:-\"eastus\"} export KEYVAULT_NAME=secret-store-$(cat /dev/urandom | tr -dc 'a-zA-Z0-9' | fold -w 10 | head -n 1) export AZ_TENANT_ID=$(az account show -o tsv --query tenantId) {% include_relative install-kubernetes-secret-store-driver.md %}","title":"Environment Variables"},{"location":"security/secrets-store-csi/azure-key-vault/#deploy-azure-key-store-csi","text":"Add the Azure Helm Repository bash helm repo add csi-secrets-store-provider-azure \\ https://raw.githubusercontent.com/Azure/secrets-store-csi-driver-provider-azure/master/charts Update your local Helm Repositories bash helm repo update Install the Azure Key Vault CSI provider bash helm install -n k8s-secrets-store-csi azure-csi-provider \\ csi-secrets-store-provider-azure/csi-secrets-store-provider-azure \\ --set linux.privileged=true --set secrets-store-csi-driver.install=false \\ --set \"linux.providersDir=/var/run/secrets-store-csi-providers\" \\ --version=v1.0.1 Set SecurityContextConstraints to allow the CSI driver to run bash oc adm policy add-scc-to-user privileged \\ system:serviceaccount:k8s-secrets-store-csi:csi-secrets-store-provider-azure","title":"Deploy Azure Key Store CSI"},{"location":"security/secrets-store-csi/azure-key-vault/#create-keyvault-and-a-secret","text":"Create a namespace for your application bash oc new-project my-application Create an Azure Keyvault in your Resource Group that contains ARO bash az keyvault create -n ${KEYVAULT_NAME} \\ -g ${KEYVAULT_RESOURCE_GROUP} \\ --location ${KEYVAULT_LOCATION} Create a secret in the Keyvault bash az keyvault secret set \\ --vault-name ${KEYVAULT_NAME} \\ --name secret1 --value \"Hello\" Create a Service Principal for the keyvault Note: If this gives you an error, you may need upgrade your Azure CLI to the latest version. ```bash export SERVICE_PRINCIPAL_CLIENT_SECRET=\"$(az ad sp create-for-rbac --skip-assignment --name http://$KEYVAULT_NAME --query 'password' -otsv)\" export SERVICE_PRINCIPAL_CLIENT_ID=\"$(az ad sp list --display-name http://$KEYVAULT_NAME --query '[0].appId' -otsv)\" ``` Set an Access Policy for the Service Principal bash az keyvault set-policy -n ${KEYVAULT_NAME} \\ --secret-permissions get \\ --spn ${SERVICE_PRINCIPAL_CLIENT_ID} Create and label a secret for Kubernetes to use to access the Key Vault bash kubectl create secret generic secrets-store-creds \\ -n my-application \\ --from-literal clientid=${SERVICE_PRINCIPAL_CLIENT_ID} \\ --from-literal clientsecret=${SERVICE_PRINCIPAL_CLIENT_SECRET} kubectl -n my-application label secret \\ secrets-store-creds secrets-store.csi.k8s.io/used=true","title":"Create Keyvault and a Secret"},{"location":"security/secrets-store-csi/azure-key-vault/#deploy-an-application-that-uses-the-csi","text":"Create a Secret Provider Class to give access to this secret bash cat <<EOF | kubectl apply -f - apiVersion: secrets-store.csi.x-k8s.io/v1alpha1 kind: SecretProviderClass metadata: name: azure-kvname namespace: my-application spec: provider: azure parameters: usePodIdentity: \"false\" useVMManagedIdentity: \"false\" userAssignedIdentityID: \"\" keyvaultName: \"${KEYVAULT_NAME}\" objects: | array: - | objectName: secret1 objectType: secret objectVersion: \"\" tenantId: \"${AZ_TENANT_ID}\" EOF Create a Pod that uses the above Secret Provider Class bash cat <<EOF | kubectl apply -f - kind: Pod apiVersion: v1 metadata: name: busybox-secrets-store-inline namespace: my-application spec: containers: - name: busybox image: k8s.gcr.io/e2e-test-images/busybox:1.29 command: - \"/bin/sleep\" - \"10000\" volumeMounts: - name: secrets-store-inline mountPath: \"/mnt/secrets-store\" readOnly: true volumes: - name: secrets-store-inline csi: driver: secrets-store.csi.k8s.io readOnly: true volumeAttributes: secretProviderClass: \"azure-kvname\" nodePublishSecretRef: name: secrets-store-creds EOF Check the Secret is mounted bash kubectl exec busybox-secrets-store-inline -- ls /mnt/secrets-store/ Output should match: secret1 Print the Secret bash kubectl exec busybox-secrets-store-inline \\ -- cat /mnt/secrets-store/secret1 Output should match: Hello","title":"Deploy an Application that uses the CSI"},{"location":"security/secrets-store-csi/azure-key-vault/#cleanup","text":"Uninstall Helm bash helm uninstall -n k8s-secrets-store-csi azure-csi-provider Delete the app bash oc delete project my-application Delete the Azure Key Vault bash az keyvault delete -n ${KEYVAULT_NAME} Delete the Service Principal bash az ad sp delete --id ${SERVICE_PRINCIPAL_CLIENT_ID} {% include_relative uninstall-kubernetes-secret-store-driver.md %}","title":"Cleanup"},{"location":"security/secrets-store-csi/hashicorp-vault/","text":"Installing the HashiCorp Vault Secret CSI Driver The HashiCorp Vault Secret CSI Driver allows you to access secrets stored in HashiCorp Vault as Kubernetes Volumes. Prerequisites An OpenShift Cluster (ROSA, ARO, OSD, and OCP 4.x all work) kubectl helm v3 {% include_relative install-kubernetes-secret-store-driver.md %} Install HashiCorp Vault with CSI driver enabled Add the HashiCorp Helm Repository bash helm repo add hashicorp https://helm.releases.hashicorp.com Update your Helm Repositories bash helm repo update Create a namespace for Vault bash oc new-project hashicorp-vault Create a SCC for the CSI driver bash oc adm policy add-scc-to-user privileged \\ system:serviceaccount:hashicorp-vault:vault-csi-provider Create a values file for Helm to use bash cat << EOF > values.yaml global: openshift: true csi: enabled: true daemonSet: providersDir: /var/run/secrets-store-csi-providers injector: enabled: false server: image: repository: \"registry.connect.redhat.com/hashicorp/vault\" tag: \"1.8.0-ubi\" dev: enabled: true EOF Install Hashicorp Vault with CSI enabled bash helm install -n hashicorp-vault vault \\ hashicorp/vault --values values.yaml Patch the CSI daemonset Currently the CSI has a bug in its manifest which we need to patch bash kubectl patch daemonset vault-csi-provider --type='json' \\ -p='[{\"op\": \"add\", \"path\": \"/spec/template/spec/containers/0/securityContext\", \"value\": {\"privileged\": true} }]' Configure Hashicorp Vault Get a bash prompt inside the Vault pod bash oc exec -it vault-0 -- bash Create a Secret in Vault bash vault kv put secret/db-pass password=\"hunter2\" Configure Vault to use Kubernetes Auth bash vault auth enable kubernetes Check your Cluster's token issuer bash oc get authentication.config cluster \\ -o json | jq -r .spec.serviceAccountIssuer Configure Kubernetes auth method If the issuer here does not match the above, update it. bash vault write auth/kubernetes/config \\ issuer=\"https://kubernetes.default.svc.cluster.local\" \\ token_reviewer_jwt=\"$(cat /var/run/secrets/kubernetes.io/serviceaccount/token)\" \\ kubernetes_host=\"https://$KUBERNETES_PORT_443_TCP_ADDR:443\" \\ kubernetes_ca_cert=@/var/run/secrets/kubernetes.io/serviceaccount/ca.crt Create a policy for our app bash vault policy write internal-app - <<EOF path \"secret/data/db-pass\" { capabilities = [\"read\"] } EOF Create an auth role to access it bash vault write auth/kubernetes/role/database \\ bound_service_account_names=webapp-sa \\ bound_service_account_namespaces=default \\ policies=internal-app \\ ttl=20m exit from the vault-0 pod bash exit Deploy a sample application Create a SecretProviderClass in the default namespace bash cat <<EOF | kubectl apply -f - apiVersion: secrets-store.csi.x-k8s.io/v1alpha1 kind: SecretProviderClass metadata: name: vault-database namespace: default spec: provider: vault parameters: vaultAddress: \"http://vault.hashicorp-vault:8200\" roleName: \"database\" objects: | - objectName: \"db-password\" secretPath: \"secret/data/db-pass\" secretKey: \"password\" EOF Create a service account webapp-sa bash kubectl create serviceaccount -n default webapp-sa Create a Pod to use the secret bash cat << EOF | kubectl apply -f - kind: Pod apiVersion: v1 metadata: name: webapp namespace: default spec: serviceAccountName: webapp-sa containers: - image: jweissig/app:0.0.1 name: webapp volumeMounts: - name: secrets-store-inline mountPath: \"/mnt/secrets-store\" readOnly: true volumes: - name: secrets-store-inline csi: driver: secrets-store.csi.k8s.io readOnly: true volumeAttributes: secretProviderClass: \"vault-database\" EOF Check the Pod has the secret bash kubectl -n default exec webapp \\ -- cat /mnt/secrets-store/db-password The output should match bash hunter2 Uninstall HashiCorp Vault with CSI driver enabled Delete the pod and bash kubectl delete -n default pod webapp kubectl delete -n default secretproviderclass vault-database kubectl delete -n default serviceaccount webapp-sa Delete the Hashicorp Vault Helm bash helm delete -n hashicorp-vault vault Delete the SCC for Hashicorp Vault bash oc adm policy remove-scc-from-user privileged \\ system:serviceaccount:hashicorp-vault:vault-csi-provider Delete the Hashicorp vault project bash oc delete project hashicorp-vault {% include_relative uninstall-kubernetes-secret-store-driver.md %}","title":"HashiCorp CSI"},{"location":"security/secrets-store-csi/hashicorp-vault/#installing-the-hashicorp-vault-secret-csi-driver","text":"The HashiCorp Vault Secret CSI Driver allows you to access secrets stored in HashiCorp Vault as Kubernetes Volumes.","title":"Installing the HashiCorp Vault Secret CSI Driver"},{"location":"security/secrets-store-csi/hashicorp-vault/#prerequisites","text":"An OpenShift Cluster (ROSA, ARO, OSD, and OCP 4.x all work) kubectl helm v3 {% include_relative install-kubernetes-secret-store-driver.md %}","title":"Prerequisites"},{"location":"security/secrets-store-csi/hashicorp-vault/#install-hashicorp-vault-with-csi-driver-enabled","text":"Add the HashiCorp Helm Repository bash helm repo add hashicorp https://helm.releases.hashicorp.com Update your Helm Repositories bash helm repo update Create a namespace for Vault bash oc new-project hashicorp-vault Create a SCC for the CSI driver bash oc adm policy add-scc-to-user privileged \\ system:serviceaccount:hashicorp-vault:vault-csi-provider Create a values file for Helm to use bash cat << EOF > values.yaml global: openshift: true csi: enabled: true daemonSet: providersDir: /var/run/secrets-store-csi-providers injector: enabled: false server: image: repository: \"registry.connect.redhat.com/hashicorp/vault\" tag: \"1.8.0-ubi\" dev: enabled: true EOF Install Hashicorp Vault with CSI enabled bash helm install -n hashicorp-vault vault \\ hashicorp/vault --values values.yaml Patch the CSI daemonset Currently the CSI has a bug in its manifest which we need to patch bash kubectl patch daemonset vault-csi-provider --type='json' \\ -p='[{\"op\": \"add\", \"path\": \"/spec/template/spec/containers/0/securityContext\", \"value\": {\"privileged\": true} }]'","title":"Install HashiCorp Vault with CSI driver enabled"},{"location":"security/secrets-store-csi/hashicorp-vault/#configure-hashicorp-vault","text":"Get a bash prompt inside the Vault pod bash oc exec -it vault-0 -- bash Create a Secret in Vault bash vault kv put secret/db-pass password=\"hunter2\" Configure Vault to use Kubernetes Auth bash vault auth enable kubernetes Check your Cluster's token issuer bash oc get authentication.config cluster \\ -o json | jq -r .spec.serviceAccountIssuer Configure Kubernetes auth method If the issuer here does not match the above, update it. bash vault write auth/kubernetes/config \\ issuer=\"https://kubernetes.default.svc.cluster.local\" \\ token_reviewer_jwt=\"$(cat /var/run/secrets/kubernetes.io/serviceaccount/token)\" \\ kubernetes_host=\"https://$KUBERNETES_PORT_443_TCP_ADDR:443\" \\ kubernetes_ca_cert=@/var/run/secrets/kubernetes.io/serviceaccount/ca.crt Create a policy for our app bash vault policy write internal-app - <<EOF path \"secret/data/db-pass\" { capabilities = [\"read\"] } EOF Create an auth role to access it bash vault write auth/kubernetes/role/database \\ bound_service_account_names=webapp-sa \\ bound_service_account_namespaces=default \\ policies=internal-app \\ ttl=20m exit from the vault-0 pod bash exit","title":"Configure Hashicorp Vault"},{"location":"security/secrets-store-csi/hashicorp-vault/#deploy-a-sample-application","text":"Create a SecretProviderClass in the default namespace bash cat <<EOF | kubectl apply -f - apiVersion: secrets-store.csi.x-k8s.io/v1alpha1 kind: SecretProviderClass metadata: name: vault-database namespace: default spec: provider: vault parameters: vaultAddress: \"http://vault.hashicorp-vault:8200\" roleName: \"database\" objects: | - objectName: \"db-password\" secretPath: \"secret/data/db-pass\" secretKey: \"password\" EOF Create a service account webapp-sa bash kubectl create serviceaccount -n default webapp-sa Create a Pod to use the secret bash cat << EOF | kubectl apply -f - kind: Pod apiVersion: v1 metadata: name: webapp namespace: default spec: serviceAccountName: webapp-sa containers: - image: jweissig/app:0.0.1 name: webapp volumeMounts: - name: secrets-store-inline mountPath: \"/mnt/secrets-store\" readOnly: true volumes: - name: secrets-store-inline csi: driver: secrets-store.csi.k8s.io readOnly: true volumeAttributes: secretProviderClass: \"vault-database\" EOF Check the Pod has the secret bash kubectl -n default exec webapp \\ -- cat /mnt/secrets-store/db-password The output should match bash hunter2","title":"Deploy a sample application"},{"location":"security/secrets-store-csi/hashicorp-vault/#uninstall-hashicorp-vault-with-csi-driver-enabled","text":"Delete the pod and bash kubectl delete -n default pod webapp kubectl delete -n default secretproviderclass vault-database kubectl delete -n default serviceaccount webapp-sa Delete the Hashicorp Vault Helm bash helm delete -n hashicorp-vault vault Delete the SCC for Hashicorp Vault bash oc adm policy remove-scc-from-user privileged \\ system:serviceaccount:hashicorp-vault:vault-csi-provider Delete the Hashicorp vault project bash oc delete project hashicorp-vault {% include_relative uninstall-kubernetes-secret-store-driver.md %}","title":"Uninstall HashiCorp Vault with CSI driver enabled"},{"location":"security/secrets-store-csi/install-kubernetes-secret-store-driver/","text":"Installing the Kubernetes Secret Store CSI Create an OpenShift Project to deploy the CSI into bash oc new-project k8s-secrets-store-csi Set SecurityContextConstraints to allow the CSI driver to run (otherwise the DaemonSet will not be able to create Pods) bash oc adm policy add-scc-to-user privileged \\ system:serviceaccount:k8s-secrets-store-csi:secrets-store-csi-driver Add the Secrets Store CSI Driver to your Helm Repositories bash helm repo add secrets-store-csi-driver \\ https://kubernetes-sigs.github.io/secrets-store-csi-driver/charts Update your Helm Repositories bash helm repo update Install the secrets store csi driver bash helm install -n k8s-secrets-store-csi csi-secrets-store \\ secrets-store-csi-driver/secrets-store-csi-driver \\ --version v1.0.1 \\ --set \"linux.providersDir=/var/run/secrets-store-csi-providers\" Check that the Daemonsets is running bash kubectl --namespace=k8s-secrets-store-csi get pods -l \"app=secrets-store-csi-driver\" You should see the following bash NAME READY STATUS RESTARTS AGE csi-secrets-store-secrets-store-csi-driver-cl7dv 3/3 Running 0 57s csi-secrets-store-secrets-store-csi-driver-gbz27 3/3 Running 0 57s","title":"Install kubernetes secret store driver"},{"location":"security/secrets-store-csi/install-kubernetes-secret-store-driver/#installing-the-kubernetes-secret-store-csi","text":"Create an OpenShift Project to deploy the CSI into bash oc new-project k8s-secrets-store-csi Set SecurityContextConstraints to allow the CSI driver to run (otherwise the DaemonSet will not be able to create Pods) bash oc adm policy add-scc-to-user privileged \\ system:serviceaccount:k8s-secrets-store-csi:secrets-store-csi-driver Add the Secrets Store CSI Driver to your Helm Repositories bash helm repo add secrets-store-csi-driver \\ https://kubernetes-sigs.github.io/secrets-store-csi-driver/charts Update your Helm Repositories bash helm repo update Install the secrets store csi driver bash helm install -n k8s-secrets-store-csi csi-secrets-store \\ secrets-store-csi-driver/secrets-store-csi-driver \\ --version v1.0.1 \\ --set \"linux.providersDir=/var/run/secrets-store-csi-providers\" Check that the Daemonsets is running bash kubectl --namespace=k8s-secrets-store-csi get pods -l \"app=secrets-store-csi-driver\" You should see the following bash NAME READY STATUS RESTARTS AGE csi-secrets-store-secrets-store-csi-driver-cl7dv 3/3 Running 0 57s csi-secrets-store-secrets-store-csi-driver-gbz27 3/3 Running 0 57s","title":"Installing the Kubernetes Secret Store CSI"},{"location":"security/secrets-store-csi/uninstall-kubernetes-secret-store-driver/","text":"Uninstalling the Kubernetes Secret Store CSI Delete the secrets store csi driver bash helm delete -n k8s-secrets-store-csi csi-secrets-store Delete the SecurityContextConstraints bash oc adm policy remove-scc-from-user privileged \\ system:serviceaccount:k8s-secrets-store-csi:secrets-store-csi-driver","title":"Uninstall kubernetes secret store driver"},{"location":"security/secrets-store-csi/uninstall-kubernetes-secret-store-driver/#uninstalling-the-kubernetes-secret-store-csi","text":"Delete the secrets store csi driver bash helm delete -n k8s-secrets-store-csi csi-secrets-store Delete the SecurityContextConstraints bash oc adm policy remove-scc-from-user privileged \\ system:serviceaccount:k8s-secrets-store-csi:secrets-store-csi-driver","title":"Uninstalling the Kubernetes Secret Store CSI"}]}